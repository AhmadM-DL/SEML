{"icse/icse": {"2017": [{"id": "conf/icse/0004CC17", "title": "Semantically enhanced software traceability using deep learning techniques.", "authors": ["Jin Guo", "Jinghui Cheng", "Jane Cleland-Huang"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.9", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.9", "http://dl.acm.org/citation.cfm?id=3097370"], "tag": ["Research track:\nTraceability"], "abstract": "In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing."}, {"id": "conf/icse/GopalakrishnanS17", "title": "Can latent topics in source code predict missing architectural tactics?", "authors": ["Raghuram Gopalakrishnan", "Palak Sharma", "Mehdi Mirakhorli", "Matthias Galster"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.10", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.10", "http://dl.acm.org/citation.cfm?id=3097371"], "tag": ["Research track:\nTraceability"], "abstract": "Architectural tactics such as heartbeat, resource pooling, and scheduling provide solutions to satisfy reliability, security, performance, and other critical characteristics of a software system. Current design practices advocate rigorous up-front analysis of the system's quality concerns to identify tactics and where in the code they should be used. In this paper, we explore a bottom-up approach to recommend architectural tactics based on latent topics discovered in the source code of projects. We present a recommender system developed by building predictor models which capture relationships between topical concepts in source code and the use of specific architectural tactics in that code. Based on an extensive analysis of over 116,000 open source systems, we identify significant correlations between latent topics in source code and the usage of architectural tactics. We use this information to construct a predictor for generating tactic recommendations. Our approach is validated through a series of experiments which demonstrate the ability to generate package-level tactic recommendations. We provide further validation via two large-scale studies of Apache Hive and Hadoop to illustrate that our recommender system predicts tactics that are actually implemented by developers in later releases."}, {"id": "conf/icse/ZhouGCHPG17", "title": "Analyzing APIs documentation and code to detect directive defects.", "authors": ["Yu Zhou", "Ruihang Gu", "Taolue Chen", "Zhiqiu Huang", "Sebastiano Panichella", "Harald C. Gall"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.11", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.11", "http://dl.acm.org/citation.cfm?id=3097373"], "tag": ["Research track:\nDocumentation"], "abstract": "Application Programming Interface (API) documents represent one of the most important references for API users. However, it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself. Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs. In this paper, we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing. Particularly, we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations. A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results. We evaluate our approach on parts of well documented JDK 1.8 APIs. Experiment results show that, out of around 2000 API usage constraints, our approach can detect 1158 defective document directives, with a precision rate of 81.6%, and a recall rate of 82.0%, which demonstrates its practical feasibility."}, {"id": "conf/icse/JiangZRZ17", "title": "An unsupervised approach for discovering relevant tutorial fragments for APIs.", "authors": ["He Jiang", "Jingxuan Zhang", "Zhilei Ren", "Tao Zhang"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.12", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.12", "http://dl.acm.org/citation.cfm?id=3097374"], "tag": ["Research track:\nDocumentation"], "abstract": "Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely Fragment Recommender for APIs with PageRank and Topic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated."}, {"id": "conf/icse/RodegheroJAM17", "title": "Detecting user story information in developer-client conversations to generate extractive summaries.", "authors": ["Paige Rodeghero", "Siyuan Jiang", "Ameer Armaly", "Collin McMillan"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.13", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.13", "http://dl.acm.org/citation.cfm?id=3097375"], "tag": ["Research track:\nDocumentation"], "abstract": "User stories are descriptions of functionality that a software user needs. They play an important role in determining which software requirements and bug fixes should be handled and in what order. Developers elicit user stories through meetings with customers. But user story elicitation is complex, and involves many passes to accommodate shifting and unclear customer needs. The result is that developers must take detailed notes during meetings or risk missing important information. Ideally, developers would be freed of the need to take notes themselves, and instead speak naturally with their customers. This paper is a step towards that ideal. We present a technique for automatically extracting information relevant to user stories from recorded conversations between customers and developers. We perform a qualitative study to demonstrate that user story information exists in these conversations in a sufficient quantity to extract automatically. From this, we found that roughly 10.2% of these conversations contained user story information. Then, we test our technique in a quantitative study to determine the degree to which our technique can extract user story information. In our experiment, our process obtained about 70.8% precision and 18.3% recall on the information."}, {"id": "conf/icse/TsantalisMR17", "title": "Clone refactoring with lambda expressions.", "authors": ["Nikolaos Tsantalis", "Davood Mazinanian", "Shahriar Rostami"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.14", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.14", "http://dl.acm.org/citation.cfm?id=3097377"], "tag": ["Research track:\nRefactoring"], "abstract": "Lambda expressions have been introduced in Java 8 to support functional programming and enable behavior parameterization by passing functions as parameters to methods. The majority of software clones (duplicated code) are known to have behavioral differences (i.e., Type-2 and Type-3 clones). However, to the best of our knowledge, there is no previous work to investigate the utility of Lambda expressions for parameterizing such behavioral differences in clones. In this paper, we propose a technique that examines the applicability of Lambda expressions for the refactoring of clones with behavioral differences. Moreover, we empirically investigate the applicability and characteristics of the Lambda expressions introduced to refactor a large dataset of clones. Our findings show that Lambda expressions enable the refactoring of a significant portion of clones that could not be refactored by any other means."}, {"id": "conf/icse/ChenJ17", "title": "Characterizing and detecting anti-patterns in the logging code.", "authors": ["Boyuan Chen", "Zhen Ming (Jack) Jiang"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.15", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.15", "http://dl.acm.org/citation.cfm?id=3097378"], "tag": ["Research track:\nRefactoring"], "abstract": "Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code). In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers."}, {"id": "conf/icse/KhatchadourianM17", "title": "Automated refactoring of legacy Java software to default methods.", "authors": ["Raffi Khatchadourian", "Hidehiko Masuhara"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.16", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.16", "http://dl.acm.org/citation.cfm?id=3097379"], "tag": ["Research track:\nRefactoring"], "abstract": "Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software."}, {"id": "conf/icse/PonzanelliSBMOP17", "title": "Supporting software developers with a holistic recommender system.", "authors": ["Luca Ponzanelli", "Simone Scalabrino", "Gabriele Bavota", "Andrea Mocci", "Rocco Oliveto", "Massimiliano Di Penta", "Michele Lanza"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.17", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.17", "http://dl.acm.org/citation.cfm?id=3097381"], "tag": ["Research track:\nRecommendation systems"], "abstract": "The promise of recommender systems is to provide intelligent support to developers during their programming tasks. Such support ranges from suggesting program entities to taking into account pertinent Q&A pages. However, current recommender systems limit the context analysis to change history and developers' activities in the IDE, without considering what a developer has already consulted or perused, e.g., by performing searches from the Web browser. Given the faceted nature of many programming tasks, and the incompleteness of the information provided by a single artifact, several heterogeneous resources are required to obtain the broader picture needed by a developer to accomplish a task. We present Libra, a holistic recommender system. It supports the process of searching and navigating the information needed by constructing a holistic meta-information model of the resources perused by a developer, analyzing their semantic relationships, and augmenting the web browser with a dedicated interactive navigation chart. The quantitative and qualitative evaluation of Libra provides evidence that a holistic analysis of a developer's information context can indeed offer comprehensive and contextualized support to information navigation and retrieval during software development."}, {"id": "conf/icse/PalombaSCPGFL17", "title": "Recommending and localizing change requests for mobile apps based on user reviews.", "authors": ["Fabio Palomba", "Pasquale Salza", "Adelina Ciurumelea", "Sebastiano Panichella", "Harald C. Gall", "Filomena Ferrucci", "Andrea De Lucia"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.18", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.18", "http://dl.acm.org/citation.cfm?id=3097382"], "tag": ["Research track:\nRecommendation systems"], "abstract": "Researchers have proposed several approaches to extract information from user reviews useful for maintaining and evolving mobile apps. However, most of them just perform automatic classification of user reviews according to specific keywords (e.g., bugs, features). Moreover, they do not provide any support for linking user feedback to the source code components to be changed, thus requiring a manual, time-consuming, and error-prone task. In this paper, we introduce CHANGEADVISOR, a novel approach that analyzes the structure, semantics, and sentiments of sentences contained in user reviews to extract useful (user) feedback from maintenance perspectives and recommend to developers changes to software artifacts. It relies on natural language processing and clustering algorithms to group user reviews around similar user needs and suggestions for change. Then, it involves textual based heuristics to determine the code artifacts that need to be maintained according to the recommended software changes. The quantitative and qualitative studies carried out on 44,683 user reviews of 10 open source mobile apps and their original developers showed a high accuracy of CHANGEADVISOR in (i) clustering similar user change requests and (ii) identifying the code components impacted by the suggested changes. Moreover, the obtained results show that ChangeAdvisor is more accurate than a baseline approach for linking user feedback clusters to the source code in terms of both precision (+47%) and recall (+38%)."}, {"id": "conf/icse/VendomeVBPGP17", "title": "Machine learning-based detection of open source license exceptions.", "authors": ["Christopher Vendome", "Mario Linares V\u00e1squez", "Gabriele Bavota", "Massimiliano Di Penta", "Daniel M. Germ\u00e1n", "Denys Poshyvanyk"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.19", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.19", "http://dl.acm.org/citation.cfm?id=3097383"], "tag": ["Research track:\nRecommendation systems"], "abstract": "From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications."}, {"id": "conf/icse/SedanoRP17", "title": "Software development waste.", "authors": ["Todd Sedano", "Paul Ralph", "C\u00e9cile P\u00e9raire"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.20", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.20", "http://dl.acm.org/citation.cfm?id=3097385"], "tag": ["Research track:\nSoftware process"], "abstract": "Context: Since software development is a complex socio-technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user. Objective: The purpose of this paper is to identify and describe different types of waste in software development. Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation. Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development. Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types. Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication."}, {"id": "conf/icse/HodaN17", "title": "Becoming agile: a grounded theory of agile transitions in practice.", "authors": ["Rashina Hoda", "James Noble"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.21", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.21", "http://dl.acm.org/citation.cfm?id=3097386"], "tag": ["Research track:\nSoftware process"], "abstract": "Agile adoption is typically understood as a one-off organizational process involving a staged selection of agile development practices. This view of agility fails to explain the differences in the pace and effectiveness of individual teams transitioning to agile development. Based on a Grounded Theory study of 31 agile practitioners drawn from 18 teams across five countries, we present a grounded theory of becoming agile as a network of on-going transitions across five dimensions: software development practices, team practices, management approach, reflective practices, and culture. The unique position of a software team through this network, and their pace of progress along the five dimensions, explains why individual agile teams present distinct manifestations of agility and unique transition experiences. The theory expands the current understanding of agility as a holistic and complex network of on-going multidimensional transitions, and will help software teams, their managers, and organizations better navigate their individual agile journeys."}, {"id": "conf/icse/FilippovaTH17", "title": "From diversity by numbers to diversity as process: supporting inclusiveness in software development teams with brainstorming.", "authors": ["Anna Filippova", "Erik H. Trainer", "James D. Herbsleb"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.22", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.22", "http://dl.acm.org/citation.cfm?id=3097387"], "tag": ["Research track:\nSoftware process"], "abstract": "Negative experiences in diverse software development teams have the potential to turn off minority participants from future team-based software development activity. We examine the use of brainstorming as one concrete team processes that may be used to improve the satisfaction of minority developers when working in a group. Situating our study in time-intensive hackathon-like environments where engagement of all team members is particularly crucial, we use a combination of survey and interview data to test our propositions. We find that brainstorming strategies are particularly effective for team members who identify as minorities, and support satisfaction with both the process and outcomes of teamwork through different mechanisms."}, {"id": "conf/icse/JoblinAHM17", "title": "Classifying developers into core and peripheral: an empirical study on count and network metrics.", "authors": ["Mitchell Joblin", "Sven Apel", "Claus Hunsen", "Wolfgang Mauerer"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.23", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.23", "http://dl.acm.org/citation.cfm?id=3097389"], "tag": ["Research track:\nStudies of software developers"], "abstract": "Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers' positions and stability within the developer network. In a study of 10 substantial open-source projects, we found that the primary difference between the count-based and our proposed network-based core-peripheral operationalizations is that the network-based ones agree more with developer perception than count-based ones. Furthermore, we demonstrate that a relational perspective can reveal further meaningful insights, such as that core developers exhibit high positional stability, upper positions in the hierarchy, and high levels of coordination with other core developers, which confirms assumptions of previous work."}, {"id": "conf/icse/FloydSW17", "title": "Decoding the representation of code in the brain: an fMRI study of code review and expertise.", "authors": ["Benjamin Floyd", "Tyler Santander", "Westley Weimer"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.24", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.24", "http://dl.acm.org/citation.cfm?id=3097390"], "tag": ["Research track:\nStudies of software developers"], "abstract": "Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79%, p <; 0.001). Further, we find that the same set of brain regions distinguish between code and prose (near-perfect correlation, r = 0.99, p <; 0.001). Finally, we find that task distinctions are modulated by expertise, such that greater skill predicts a less differentiated neural representation (r = -0.44, p = 0.016) indicating that more skilled participants treat code and prose more similarly at a neural activation level."}, {"id": "conf/icse/LeeCB17", "title": "Understanding the impressions, motivations, and barriers of one time code contributors to FLOSS projects: a survey.", "authors": ["Amanda Lee", "Jeffrey C. Carver", "Amiangshu Bosu"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.25", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.25", "http://dl.acm.org/citation.cfm?id=3097391"], "tag": ["Research track:\nStudies of software developers"], "abstract": "Successful Free/Libre Open Source Software (FLOSS) projects must attract and retain high-quality talent. Researchers have invested considerable effort in the study of core and peripheral FLOSS developers. To this point, one critical subset of developers that have not been studied are One-Time code Contributors (OTC) - those that have had exactly one patch accepted. To understand why OTCs have not contributed another patch and provide guidance to FLOSS projects on retaining OTCs, this study seeks to understand the impressions, motivations, and barriers experienced by OTCs. We conducted an online survey of OTCs from 23 popular FLOSS projects. Based on the 184 responses received, we observed that OTCs generally have positive impressions of their FLOSS project and are driven by a variety of motivations. Most OTCs primarily made contributions to fix bugs that impeded their work and did not plan on becoming long term contributors. Furthermore, OTCs encounter a number of barriers that prevent them from continuing to contribute to the project. Based on our findings, there are some concrete actions FLOSS projects can take to increase the chances of converting OTCs into long-term contributors."}, {"id": "conf/icse/ThomeSBB17", "title": "Search-driven string constraint solving for vulnerability detection.", "authors": ["Julian Thom\u00e9", "Lwin Khin Shar", "Domenico Bianculli", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.26", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.26", "http://dl.acm.org/citation.cfm?id=3097393"], "tag": ["Research track:\nSearch-based software engineering"], "abstract": "Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-of-the-art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one, this leads to limited effectiveness in finding vulnerabilities. In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support. We have implemented the proposed search-driven constraint solving technique in the ACO-Solver tool, which we have evaluated in the context of injection and XSS vulnerability detection for Java Web applications. We have assessed the benefits and costs of combining the proposed technique with two state-of-the-art constraint solvers (Z3-str2 and CVC4). The experimental results, based on a benchmark with 104 constraints derived from nine realistic Web applications, show that our approach, when combined in a state-of-the-art solver, significantly improves the number of detected vulnerabilities (from 4.7% to 71.9% for Z3-str2, from 85.9% to 100.0% for CVC4), and solves several cases on which the solver fails when used stand-alone (46 more solved cases for Z3-str2, and 11 more for CVC4), while still keeping the execution time affordable in practice."}, {"id": "conf/icse/SoltaniPD17", "title": "A guided genetic algorithm for automated crash reproduction.", "authors": ["Mozhan Soltani", "Annibale Panichella", "Arie van Deursen"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.27", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.27", "http://dl.acm.org/citation.cfm?id=3097394"], "tag": ["Research track:\nSearch-based software engineering"], "abstract": "To reduce the effort developers have to make for crash debugging, researchers have proposed several solutions for automatic failure reproduction. Recent advances proposed the use of symbolic execution, mutation analysis, and directed model checking as underling techniques for post-failure analysis of crash stack traces. However, existing approaches still cannot reproduce many real-world crashes due to such limitations as environment dependencies, path explosion, and time complexity. To address these challenges, we present EvoCrash, a post-failure approach which uses a novel Guided Genetic Algorithm (GGA) to cope with the large search space characterizing real-world software programs. Our empirical study on three open-source systems shows that EvoCrash can replicate 41 (82%) of real-world crashes, 34 (89%) of which are useful reproductions for debugging purposes, outperforming the state-of-the-art in crash replication."}, {"id": "conf/icse/LiuSSJGS17", "title": "Stochastic optimization of program obfuscation.", "authors": ["Han Liu", "Chengnian Sun", "Zhendong Su", "Yu Jiang", "Ming Gu", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.28", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.28", "http://dl.acm.org/citation.cfm?id=3097395"], "tag": ["Research track:\nSearch-based software engineering"], "abstract": "Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = \u3008t<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sub>, t<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">2</sub>, ..., t<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">n</sub>\u3009 (\u2200i \u2208 [1, n]. t<sub xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">i</sub> \u2208 T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure* for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure* outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure* is practical and can reduce the human attack success rate by 30%."}, {"id": "conf/icse/HawkinsD17", "title": "ZenIDS: introspective intrusion detection for PHP applications.", "authors": ["Byron Hawkins", "Brian Demsky"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.29", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.29", "http://dl.acm.org/citation.cfm?id=3097397"], "tag": ["Research track:\nWeb applications"], "abstract": "Since its first appearance more than 20 years ago, PHP has steadily increased in popularity, and has become the foundation of the Internet's most popular content management systems (CMS). Of the world's 1 million most visited websites, nearly half use a CMS, and WordPress alone claims 25% market share of all websites. While their easy-to-use templates and components have greatly simplified the work of developing high quality websites, it comes at the cost of software vulnerabilities that are inevitable in such large and rapidly evolving frameworks. Intrusion Detection Systems (IDS) are often used to protect Internet-facing applications, but conventional techniques struggle to keep up with the fast pace of development in today's web applications. Rapid changes to application interfaces increase the workload of maintaining an IDS whitelist, yet the broad attack surface of a web application makes for a similarly verbose blacklist. We developed ZenIDS to dynamically learn the trusted execution paths of an application during a short online training period and report execution anomalies as potential intrusions. We implement ZenIDS as a PHP extension supported by 8 hooks instrumented in the PHP interpreter. Our experiments demonstrate its effectiveness monitoring live web traffic for one year to 3 large PHP applications, detecting malicious requests with a false positive rate of less than .01% after training on fewer than 4,000 requests. ZenIDS excludes the vast majority of deployed PHP code from the whitelist because it is never used for valid requests-yet could potentially be exploited by a remote adversary. We observe 5% performance overhead (or less) for our applications vs. an optimized vanilla LAMP stack."}, {"id": "conf/icse/WitternYZDL17", "title": "Statically checking web API requests in JavaScript.", "authors": ["Erik Wittern", "Annie T. T. Ying", "Yunhui Zheng", "Julian Dolby", "Jim Alain Laredo"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.30", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.30", "http://dl.acm.org/citation.cfm?id=3097398"], "tag": ["Research track:\nWeb applications"], "abstract": "Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests."}, {"id": "conf/icse/SayaghKA17", "title": "On cross-stack configuration errors.", "authors": ["Mohammed Sayagh", "Noureddine Kerzazi", "Bram Adams"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.31", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.31", "http://dl.acm.org/citation.cfm?id=3097399"], "tag": ["Research track:\nWeb applications"], "abstract": "Today's web applications are deployed on powerful software stacks such as MEAN (JavaScript) or LAMP (PHP), which consist of multiple layers such as an operating system, web server, database, execution engine and application framework, each of which provide resources to the layer just above it. These powerful software stacks unfortunately are plagued by so-called cross-stack configuration errors (CsCEs), where a higher layer in the stack suddenly starts to behave incorrectly or even crash due to incorrect configuration choices in lower layers. Due to differences in programming languages and lack of explicit links between configuration options of different layers, sysadmins and developers have a hard time identifying the cause of a CsCE, which is why this paper (1) performs a qualitative analysis of 1,082 configuration errors to understand the impact, effort and complexity of dealing with CsCEs, then (2) proposes a modular approach that plugs existing source code analysis (slicing) techniques, in order to recommend the culprit configuration option. Empirical evaluation of this approach on 36 real CsCEs of the top 3 LAMP stack layers shows that our approach reports the misconfigured option with an average rank of 2.18 for 32 of the CsCEs, and takes only few minutes, making it practically useful."}, {"id": "conf/icse/ChoudharyLP17", "title": "Efficient detection of thread safety violations via coverage-guided generation of concurrent tests.", "authors": ["Ankit Choudhary", "Shan Lu", "Michael Pradel"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.32", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.32", "http://dl.acm.org/citation.cfm?id=3097401"], "tag": ["Research track:\nConcurrency"], "abstract": "As writing concurrent programs is challenging, developers often rely on thread-safe classes, which encapsulate most synchronization issues. Testing such classes is crucial to ensure the correctness of concurrent programs. An effective approach to uncover otherwise missed concurrency bugs is to automatically generate concurrent tests. Existing approaches either create tests randomly, which is inefficient, build on a computationally expensive analysis of potential concurrency bugs exposed by sequential tests, or focus on exposing a particular kind of concurrency bugs, such as atomicity violations. This paper presents CovCon, a coverage-guided approach to generate concurrent tests. The key idea is to measure how often pairs of methods have already been executed concurrently and to focus the test generation on infrequently or not at all covered pairs of methods. The approach is independent of any particular bug pattern, allowing it to find arbitrary concurrency bugs, and is computationally inexpensive, allowing it to generate many tests in short time. We apply CovCon to 18 thread-safe Java classes, and it detects concurrency bugs in 17 of them. Compared to five state of the art approaches, CovCon detects more bugs than any other approach while requiring less time. Specifically, our approach finds bugs faster in 38 of 47 cases, with speedups of at least 4x for 22 of 47 cases."}, {"id": "conf/icse/ZhangW17", "title": "RClassify: classifying race conditions in web applications via deterministic replay.", "authors": ["Lu Zhang", "Chao Wang"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.33", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.33", "http://dl.acm.org/citation.cfm?id=3097402"], "tag": ["Research track:\nConcurrency"], "abstract": "Race conditions are common in web applications but are difficult to diagnose and repair. Although there exist tools for detecting races in web applications, they all report a large number of false positives. That is, the races they report are either bogus, meaning they can never occur in practice, or benign, meaning they do not lead to erroneous behaviors. Since manually diagnosing them is tedious and error prone, reporting these race warnings to developers would be counter-productive. We propose a platform-agnostic, deterministic replay-based method for identifying not only the real but also the truly harmful race conditions. It relies on executing each pair of racing events in two different orders and assessing their impact on the program state: we say a race is harmful only if (1) both of the two executions arefeasible and (2) they lead to different program states. We have evaluated our evidence-based classification method on a large set of real websites from Fortune-500 companies and demonstrated that it significantly outperforms all state-of-the-art techniques."}, {"id": "conf/icse/AdamsenMKSTS17", "title": "Repairing event race errors by controlling nondeterminism.", "authors": ["Christoffer Quist Adamsen", "Anders M\u00f8ller", "Rezwana Karim", "Manu Sridharan", "Frank Tip", "Koushik Sen"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.34", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.34", "http://dl.acm.org/citation.cfm?id=3097403"], "tag": ["Research track:\nConcurrency"], "abstract": "Modern web applications are written in an event-driven style, in which event handlers execute asynchronously in response to user or system events. The nondeterminism arising from this programming style can lead to pernicious errors. Recent work focuses on detecting event races and classifying them as harmful or harmless. However, since modifying the source code to prevent harmful races can be a difficult and error-prone task, it may be preferable to steer away from the bad executions. In this paper, we present a technique for automated repair of event race errors in JavaScript web applications. Our approach relies on an event controller that restricts event handler scheduling in the browser according to a specified repair policy, by intercepting and carefully postponing or discarding selected events. We have implemented the technique in a tool called EventRaceCommander, which relies entirely on source code instrumentation, and evaluated it by repairing more than 100 event race errors that occur in the web applications from the largest 20 of the Fortune 500 companies. Our results show that application-independent repair policies usually suffice to repair event race errors without excessive negative impact on performance or user experience, though application-specific repair policies that target specific event races are sometimes desirable."}, {"id": "conf/icse/RasthoferATP17", "title": "Making malory behave maliciously: targeted fuzzing of android execution environments.", "authors": ["Siegfried Rasthofer", "Steven Arzt", "Stefan Triller", "Michael Pradel"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.35", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.35", "http://dl.acm.org/citation.cfm?id=3097405"], "tag": ["Research track:\nMobile application security"], "abstract": "Android applications, or apps, provide useful features to end-users, but many apps also contain malicious behavior. Modern malware makes understanding such behavior challenging by behaving maliciously only under particular conditions. For example, a malware app may check whether it runs on a real device and not an emulator, in a particular country, and alongside a specific target app, such as a vulnerable banking app. To observe the malicious behavior, a security analyst must find out and emulate all these app-specific constraints. This paper presents FuzzDroid, a framework for automatically generating an Android execution environment where an app exposes its malicious behavior. The key idea is to combine an extensible set of static and dynamic analyses through a search-based algorithm that steers the app toward a configurable target location. On recent malware, the approach reaches the target location in 75% of the apps. In total, we reach 240 code locations within an average time of only one minute. To reach these code locations, FuzzDroid generates 106 different environments, too many for a human analyst to create manually."}, {"id": "conf/icse/LeeBSSZM17", "title": "A SEALANT for inter-app security holes in android.", "authors": ["Youn Kyu Lee", "Jae Young Bang", "Gholamreza Safi", "Arman Shahbazian", "Yixue Zhao", "Nenad Medvidovic"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.36", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.36", "http://dl.acm.org/citation.cfm?id=3097406"], "tag": ["Research track:\nMobile application security"], "abstract": "Android's communication model has a major security weakness: malicious apps can manipulate other apps into performing unintended operations and can steal end-user data, while appearing ordinary and harmless. This paper presents SEALANT, a technique that combines static analysis of app code, which infers vulnerable communication channels, with runtime monitoring of inter-app communication through those channels, which helps to prevent attacks. SEALANT's extensive evaluation demonstrates that (1) it detects and blocks inter-app attacks with high accuracy in a corpus of over 1,100 real-world apps, (2) it suffers from fewer false alarms than existing techniques in several representative scenarios, (3) its performance overhead is negligible, and (4) end-users do not find it challenging to adopt."}, {"id": "conf/icse/TsutanoBSRD17", "title": "An efficient, robust, and scalable approach for analyzing interacting android apps.", "authors": ["Yutaka Tsutano", "Shakthi Bachala", "Witawas Srisa-an", "Gregg Rothermel", "Jackson Dinh"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.37", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.37", "http://dl.acm.org/citation.cfm?id=3097407"], "tag": ["Research track:\nMobile application security"], "abstract": "When multiple apps on an Android platform interact, faults and security vulnerabilities can occur. Software engineers need to be able to analyze interacting apps to detect such problems. Current approaches for performing such analyses, however, do not scale to the numbers of apps that may need to be considered, and thus, are impractical for application to real-world scenarios. In this paper, we introduce JITANA, a program analysis framework designed to analyze multiple Android apps simultaneously. By using a classloader-based approach instead of a compiler-based approach such as SOOT, JITANA is able to simultaneously analyze large numbers of interacting apps, perform on-demand analysis of large libraries, and effectively analyze dynamically generated code. Empirical studies of JITANA show that it is substantially more efficient than a state-of-the-art approach, and that it can effectively and efficiently analyze complex apps including Facebook, Pokemon Go, and Pandora that the state-of-the-art approach cannot handle."}, {"id": "conf/icse/LiWWWWLXH17", "title": "LibD: scalable and precise third-party library detection in android markets.", "authors": ["Menghao Li", "Wei Wang", "Pei Wang", "Shuai Wang", "Dinghao Wu", "Jian Liu", "Rui Xue", "Wei Huo"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.38", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.38", "http://dl.acm.org/citation.cfm?id=3097409"], "tag": ["Research track:\nMobile application development"], "abstract": "With the thriving of the mobile app markets, third-party libraries are pervasively integrated in the Android applications. Third-party libraries provide functionality such as advertisements, location services, and social networking services, making multi-functional app development much more productive. However, the spread of vulnerable or harmful third-party libraries may also hurt the entire mobile ecosystem, leading to various security problems. The Android platform suffers severely from such problems due to the way its ecosystem is constructed and maintained. Therefore, third-party Android library identification has emerged as an important problem which is the basis of many security applications such as repackaging detection and malware analysis. According to our investigation, existing work on Android library detection still requires improvement in many aspects, including accuracy and obfuscation resilience. In response to these limitations, we propose a novel approach to identifying third-party Android libraries. Our method utilizes the internal code dependencies of an Android app to detect and classify library candidates. Different from most previous methods which classify detected library candidates based on similarity comparison, our method is based on feature hashing and can better handle code whose package and method names are obfuscated. Based on this approach, we have developed a prototypical tool called LibD and evaluated it with an update-to-date and large-scale dataset. Our experimental results on 1,427,395 apps show that compared to existing tools, LibD can better handle multi-package third-party libraries in the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability."}, {"id": "conf/icse/0006R17", "title": "Analysis and testing of notifications in Android wear applications.", "authors": ["Hailong Zhang", "Atanas Rountev"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.39", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.39", "http://dl.acm.org/citation.cfm?id=3097410"], "tag": ["Research track:\nMobile application development"], "abstract": "Android Wear (AW) is Google's platform for developing applications for wearable devices. Our goal is to make a first step toward a foundation for analysis and testing of AW apps. We focus on a core feature of such apps: notifications issued by a handheld device (e.g., a smartphone) and displayed on a wearable device (e.g., a smartwatch). We first define a formal semantics of AW notifications in order to capture the core features and behavior of the notification mechanism. Next, we describe a constraint-based static analysis to build a model of this run-time behavior. We then use this model to develop a novel testing tool for AW apps. The tool contains a testing framework together with components to support AW-specific coverage criteria and to automate the generation of GUI events on the wearable. These contributions advance the state of the art in the increasingly important area of software for wearable devices."}, {"id": "conf/icse/XueLYWW17", "title": "Adaptive unpacking of Android apps.", "authors": ["Lei Xue", "Xiapu Luo", "Le Yu", "Shuai Wang", "Dinghao Wu"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.40", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.40", "http://dl.acm.org/citation.cfm?id=3097411"], "tag": ["Research track:\nMobile application development"], "abstract": "More and more app developers use the packing services (or packers) to prevent attackers from reverse engineering and modifying the executable (or Dex files) of their apps. At the same time, malware authors also use the packers to hide the malicious component and evade the signature-based detection. Although there are a few recent studies on unpacking Android apps, it has been shown that the evolving packers can easily circumvent them because they are not adaptive to the changes of packers. In this paper, we propose a novel adaptive approach and develop a new system, named PackerGrind, to unpack Android apps. We also evaluate PackerGrind with real packed apps, and the results show that PackerGrind can successfully reveal the packers' protection mechanisms and recover the Dex files with low overhead, showing that our approach can effectively handle the evolution of packers."}, {"id": "conf/icse/SongL17", "title": "Performance diagnosis for inefficient loops.", "authors": ["Linhai Song", "Shan Lu"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.41", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.41", "http://dl.acm.org/citation.cfm?id=3097413"], "tag": ["Research track:\nDebugging"], "abstract": "Writing efficient software is difficult. Design and implementation defects can cause severe performance degradation. Unfortunately, existing performance diagnosis techniques like profilers are still preliminary. They can locate code regions that consume resources, but not the ones that waste resources. In this paper, we first design a root-cause and fix-strategy taxonomy for inefficient loops, one of the most common performance problems in the field. We then design a static-dynamic hybrid analysis tool, LDoctor, to provide accurate performance diagnosis for loops. We further use sampling techniques to lower the run-time overhead without degrading the accuracy or latency of LDoctor diagnosis. Evaluation using real-world performance problems shows that LDoctor can provide better coverage and accuracy than existing techniques, with low overhead."}, {"id": "conf/icse/MaCZZX17", "title": "How do developers fix cross-project correlated bugs?: a case study on the GitHub scientific python ecosystem.", "authors": ["Wanwangying Ma", "Lin Chen", "Xiangyu Zhang", "Yuming Zhou", "Baowen Xu"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.42", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.42", "http://dl.acm.org/citation.cfm?id=3097414"], "tag": ["Research track:\nDebugging"], "abstract": "GitHub, a popular social-software-development platform, has fostered a variety of software ecosystems where projects depend on one another and practitioners interact with each other. Projects within an ecosystem often have complex inter-dependencies that impose new challenges in bug reporting and fixing. In this paper, we conduct an empirical study on cross-project correlated bugs, i.e., causally related bugs reported to different projects, focusing on two aspects: 1) how developers track the root causes across projects, and 2) how the downstream developers coordinate to deal with upstream bugs. Through manual inspection of bug reports collected from the scientific Python ecosystem and an online survey with developers, this study reveals the common practices of developers and the various factors in fixing cross-project bugs. These findings provide implications for future software bug analysis in the scope of ecosystem, as well as shed light on the requirements of issue trackers for such bugs."}, {"id": "conf/icse/LinSXLD17", "title": "Feedback-based debugging.", "authors": ["Yun Lin", "Jun Sun", "Yinxing Xue", "Yang Liu", "Jin Song Dong"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.43", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.43", "http://dl.acm.org/citation.cfm?id=3097415"], "tag": ["Research track:\nDebugging"], "abstract": "Software debugging has long been regarded as a time and effort consuming task. In the process of debugging, developers usually need to manually inspect many program steps to see whether they deviate from their intended behaviors. Given that intended behaviors usually exist nowhere but in human mind, the automation of debugging turns out to be extremely hard, if not impossible. In this work, we propose a feedback-based debugging approach, which (1) builds on light-weight human feedbacks on a buggy program and (2) regards the feedbacks as partial program specification to infer suspicious steps of the buggy execution. Given a buggy program, we record its execution trace and allow developers to provide light-weight feedback on trace steps. Based on the feedbacks, we recommend suspicious steps on the trace. Moreover, our approach can further learn and approximate bug-free paths, which helps reduce required feedbacks to expedite the debugging process. We conduct an experiment to evaluate our approach with simulated feedbacks on 3409 mutated bugs across 3 open source projects. The results show that our feedback-based approach can detect 92.8% of the bugs and 65% of the detected bugs require less than 20 feedbacks. In addition, we implement our proof-of-concept tool, Microbat, and conduct a user study involving 16 participants on 3 debugging tasks. The results show that, compared to the participants using the baseline tool, Whyline, the ones using Microbat can spend on average 55.8% less time to locate the bugs."}, {"id": "conf/icse/RolimSDPGGSH17", "title": "Learning syntactic program transformations from examples.", "authors": ["Reudismam Rolim", "Gustavo Soares", "Loris D'Antoni", "Oleksandr Polozov", "Sumit Gulwani", "Rohit Gheyi", "Ryo Suzuki", "Bj\u00f6rn Hartmann"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.44", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.44", "http://dl.acm.org/citation.cfm?id=3097417"], "tag": ["Research track:\nProgram synthesis and repair"], "abstract": "Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average."}, {"id": "conf/icse/XiongWYZH0017", "title": "Precise condition synthesis for program repair.", "authors": ["Yingfei Xiong", "Jie Wang", "Runfa Yan", "Jiachen Zhang", "Shi Han", "Gang Huang", "Lu Zhang"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.45", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.45", "http://dl.acm.org/citation.cfm?id=3097418"], "tag": ["Research track:\nProgram synthesis and repair"], "abstract": "Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an \"if\" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects. Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%."}, {"id": "conf/icse/AquinoDP17", "title": "Heuristically matching solution spaces of arithmetic formulas to efficiently reuse solutions.", "authors": ["Andrea Aquino", "Giovanni Denaro", "Mauro Pezz\u00e8"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.46", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.46", "http://dl.acm.org/citation.cfm?id=3097419"], "tag": ["Research track:\nProgram synthesis and repair"], "abstract": "Many symbolic program analysis techniques rely on SMT solvers to verify properties of programs. Despite the remarkable progress made in the development of such tools, SMT solvers still represent a main bottleneck to the scalability of these techniques. Recent approaches tackle this bottleneck by reusing solutions of formulas that recur during program analysis, thus reducing the number of queries to SMT solvers. Current approaches only reuse solutions across formulas that are equivalent to, contained in or implied by other formulas, as identified through a set of predefined rules, and cannot reuse solutions across formulas that differ in their structure, even if they share some potentially reusable solutions. In this paper, we propose a novel approach that can reuse solutions across formulas that share at least one solution, regardless of their structural resemblance. Our approach exploits a novel heuristic to efficiently identify solutions computed for previously solved formulas and most likely shared by new formulas. The results of an empirical evaluation of our approach on two different logics show that our approach can identify on average more reuse opportunities and is markedly faster than competing approaches."}, {"id": "conf/icse/NguyenNPN17", "title": "Exploring API embedding for API usages and applications.", "authors": ["Trong Duc Nguyen", "Anh Tuan Nguyen", "Hung Dang Phan", "Tien N. Nguyen"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.47", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.47", "http://dl.acm.org/citation.cfm?id=3097421"], "tag": ["Research track:\nMining software repositories"], "abstract": "Word2Vec is a class of neural network models that as being trainedfrom a large corpus of texts, they can produce for each unique word acorresponding vector in a continuous space in which linguisticcontexts of words can be observed. In this work, we study thecharacteristics of Word2Vec vectors, called API2VEC or API embeddings, for the API elements within the API sequences in source code. Ourempirical study shows that the close proximity of the API2VEC vectorsfor API elements reflects the similar usage contexts containing thesurrounding APIs of those API elements. Moreover, API2VEC can captureseveral similar semantic relations between API elements in API usagesvia vector offsets. We demonstrate the usefulness of API2VEC vectorsfor API elements in three applications. First, we build a tool thatmines the pairs of API elements that share the same usage relationsamong them. The other applications are in the code migrationdomain. We develop API2API, a tool to automatically learn the APImappings between Java and C# using a characteristic of the API2VECvectors for API elements in the two languages: semantic relationsamong API elements in their usages are observed in the two vectorspaces for the two languages as similar geometric arrangements amongtheir API2VEC vectors. Our empirical evaluation shows that API2APIrelatively improves 22.6% and 40.1% top-1 and top-5 accuracy over astate-of-the-art mining approach for API mappings. Finally, as anotherapplication in code migration, we are able to migrate equivalent APIusages from Java to C# with up to 90.6% recall and 87.2% precision."}, {"id": "conf/icse/ChenXW17", "title": "Unsupervised software-specific morphological forms inference from informal discussions.", "authors": ["Chunyang Chen", "Zhenchang Xing", "Ximing Wang"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.48", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.48", "http://dl.acm.org/citation.cfm?id=3097422"], "tag": ["Research track:\nMining software repositories"], "abstract": "Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject."}, {"id": "conf/icse/XuCCLS17", "title": "SPAIN: security patch analysis for binaries towards understanding the pain and pills.", "authors": ["Zhengzi Xu", "Bihuan Chen", "Mahinthan Chandramohan", "Yang Liu", "Fu Song"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.49", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.49", "http://dl.acm.org/citation.cfm?id=3097424"], "tag": ["Research track:\nProgram analysis I"], "abstract": "Software vulnerability is one of the major threats to software security. Once discovered, vulnerabilities are often fixed by applying security patches. In that sense, security patches carry valuable information about vulnerabilities, which could be used to discover, understand and fix (similar) vulnerabilities. However, most existing patch analysis approaches work at the source code level, while binary-level patch analysis often heavily relies on a lot of human efforts and expertise. Even worse, some vulnerabilities may be secretly patched without applying CVE numbers, or only the patched binary programs are available while the patches are not publicly released. These practices greatly hinder patch analysis and vulnerability analysis. In this paper, we propose a scalable binary-level patch analysis framework, named SPAIN, which can automatically identify security patches and summarize patch patterns and their corresponding vulnerability patterns. Specifically, given the original and patched versions of a binary program, we locate the patched functions and identify the changed traces (i.e., a sequence of basic blocks) that may contain security or non-security patches. Then we identify security patches through a semantic analysis of these traces and summarize the patterns through a taint analysis on the patched functions. The summarized patterns can be used to search similar patches or vulnerabilities in binary programs. Our experimental results on several real-world projects have shown that: i) SPAIN identified security patches with high accuracy and high scalability, ii) SPAIN summarized 5 patch patterns and their corresponding vulnerability patterns for 5 vulnerability types, and iii) SPAIN discovered security patches that were not documented, and discovered 3 zero-day vulnerabilities."}, {"id": "conf/icse/PadhyeS17", "title": "Travioli: a dynamic analysis for detecting data-structure traversals.", "authors": ["Rohan Padhye", "Koushik Sen"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.50", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.50", "http://dl.acm.org/citation.cfm?id=3097425"], "tag": ["Research track:\nProgram analysis I"], "abstract": "Traversal is one of the most fundamental operations on data structures, in which an algorithm systematically visits some or all of the data items of a data structure. We propose a dynamic analysis technique, called Travioli, for detecting data-structure traversals. We introduce the concept of acyclic execution contexts, which enables precise detection of traversals of arrays and linked data structures such as lists and trees in the presence of both loops and recursion. We describe how the information reported by Travioli can be used for visualizing data-structure traversals, manually generating performance regression tests, and for discovering performance bugs caused by redundant traversals. We evaluate Travioli on five real-world JavaScript programs. In our experiments, Travioli produced fewer than 4% false positives. We were able to construct performance tests for 93.75% of the reported true traversals. Travioli also found two asymptotic performance bugs in widely used JavaScript frameworks D3 and express."}, {"id": "conf/icse/SuCFR17", "title": "ProEva: runtime proactive performance evaluation based on continuous-time markov chains.", "authors": ["Guoxin Su", "Taolue Chen", "Yuan Feng", "David S. Rosenblum"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.51", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.51", "http://dl.acm.org/citation.cfm?id=3097426"], "tag": ["Research track:\nProgram analysis I"], "abstract": "Software systems, especially service-based software systems, need to guarantee runtime performance. If their performance is degraded, some reconfiguration countermeasures should be taken. However, there is usually some latency before the countermeasures take effect. It is thus important not only to monitor the current system status passively but also to predict its future performance proactively. Continuous-time Markov chains (CTMCs) are suitable models to analyze time-bounded performance metrics (e.g., how likely a performance degradation may occur within some future period). One challenge to harness CTMCs is the measurement of model parameters (i.e., transition rates) in CTMCs at runtime. As these parameters may be updated by the system or environment frequently, it is difficult for the model builder to provide precise parameter values. In this paper, we present a framework called ProEva, which extends the conventional technique of time-bounded CTMC model checking by admitting imprecise, interval-valued estimates for transition rates. The core method of ProEva computes asymptotic expressions and bounds for the imprecise model checking output. We also present an evaluation of accuracy and computational overhead for ProEva."}, {"id": "conf/icse/CoblenzNAMS17", "title": "Glacier: transitive class immutability for Java.", "authors": ["Michael J. Coblenz", "Whitney Nelson", "Jonathan Aldrich", "Brad A. Myers", "Joshua Sunshine"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.52", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.52", "http://dl.acm.org/citation.cfm?id=3097428"], "tag": ["Research track:\nProgram analysis II"], "abstract": "Though immutability has been long-proposed as a way to prevent bugs in software, little is known about how to make immutability support in programming languages effective for software engineers. We designed a new formalism that extends Java to support transitive class immutability, the form of immutability for which there is the strongest empirical support, and implemented that formalism in a tool called Glacier. We applied Glacier successfully to two real-world systems. We also compared Glacier to Java's final in a user study of twenty participants. We found that even after being given instructions on how to express immutability with final, participants who used final were unable to express immutability correctly, whereas almost all participants who used Glacier succeeded. We also asked participants to make specific changes to immutable classes and found that participants who used final all incorrectly mutated immutable state, whereas almost all of the participants who used Glacier succeeded. Glacier represents a promising approach to enforcing immutability in Java and provides a model for enforcement in other languages."}, {"id": "conf/icse/LandmanSV17", "title": "Challenges for static analysis of Java reflection: literature review and empirical study.", "authors": ["Davy Landman", "Alexander Serebrenik", "Jurgen J. Vinju"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.53", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.53", "http://dl.acm.org/citation.cfm?id=3097429"], "tag": ["Research track:\nProgram analysis II"], "abstract": "The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code."}, {"id": "conf/icse/HeoOY17", "title": "Machine-learning-guided selectively unsound static analysis.", "authors": ["Kihong Heo", "Hakjoo Oh", "Kwangkeun Yi"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.54", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.54", "http://dl.acm.org/citation.cfm?id=3097430"], "tag": ["Research track:\nProgram analysis II"], "abstract": "We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use an anomaly-detection technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision."}, {"id": "conf/icse/KafaliJPWS17", "title": "How good is a security policy against real breaches?: a HIPAA case study.", "authors": ["\u00d6zg\u00fcr Kafali", "Jasmine Jones", "Megan Petruso", "Laurie Williams", "Munindar P. Singh"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.55", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.55", "http://dl.acm.org/citation.cfm?id=3097432"], "tag": ["Research track:\nSecurity, safety, and privacy"], "abstract": "Policy design is an important part of software development. As security breaches increase in variety, designing a security policy that addresses all potential breaches becomes a nontrivial task. A complete security policy would specify rules to prevent breaches. Systematically determining which, if any, policy clause has been violated by a reported breach is a means for identifying gaps in a policy. Our research goal is to help analysts measure the gaps between security policies and reported breaches by developing a systematic process based on semantic reasoning. We propose SEMAVER, a framework for determining coverage of breaches by policies via comparison of individual policy clauses and breach descriptions. We represent a security policy as a set of norms. Norms (commitments, authorizations, and prohibitions) describe expected behaviors of users, and formalize who is accountable to whom and for what. A breach corresponds to a norm violation. We develop a semantic similarity metric for pairwise comparison between the norm that represents a policy clause and the norm that has been violated by a reported breach. We use the US Health Insurance Portability and Accountability Act (HIPAA) as a case study. Our investigation of a subset of the breaches reported by the US Department of Health and Human Services (HHS) reveals the gaps between HIPAA and reported breaches, leading to a coverage of 65%. Additionally, our classification of the 1,577 HHS breaches shows that 44% of the breaches are accidental misuses and 56% are malicious misuses. We find that HIPAA's gaps regarding accidental misuses are significantly larger than its gaps regarding malicious misuses."}, {"id": "conf/icse/BertolinoMPR17", "title": "Adaptive coverage and operational profile-based testing for reliability improvement.", "authors": ["Antonia Bertolino", "Breno Miranda", "Roberto Pietrantuono", "Stefano Russo"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.56", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.56", "http://dl.acm.org/citation.cfm?id=3097433"], "tag": ["Research track:\nSecurity, safety, and privacy"], "abstract": "We introduce covrel, an adaptive software testing approach based on the combined use of operational profile and coverage spectrum, with the ultimate goal of improving the delivered reliability of the program under test. Operational profile-based testing is a black-box technique that selects test cases having the largest impact on failure probability in operation, as such, it is considered well suited when reliability is a major concern. Program spectrum is a characterization of a program's behavior in terms of the code entities (e.g., branches, statements, functions) that are covered as the program executes. The driving idea of covrel is to complement operational profile information with white-box coverage measures based on count spectra, so as to dynamically select the most effective test cases for reliability improvement. In particular, we bias operational profile-based test selection towards those entities covered less frequently. We assess the approach by experiments with 18 versions from 4 subjects commonly used in software testing research, comparing results with traditional operational and coverage testing. Results show that exploiting operational and coverage data in a combined adaptive way actually pays in terms of reliability improvement, with covrel overcoming conventional operational testing in more than 80% of the cases."}, {"id": "conf/icse/BusariL17", "title": "RADAR: a lightweight tool for requirements and architecture decision analysis.", "authors": ["Saheed A. Busari", "Emmanuel Letier"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.57", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.57", "http://dl.acm.org/citation.cfm?id=3097435"], "tag": ["Research track:\nDevelopment tools and frameworks"], "abstract": "Uncertainty and conflicting stakeholders' objectives make many requirements and architecture decisions particularly hard. Quantitative probabilistic models allow software architects to analyse such decisions using stochastic simulation and multi-objective optimisation, but the difficulty of elaborating the models is an obstacle to the wider adoption of such techniques. To reduce this obstacle, this paper presents a novel modelling language and analysis tool, called RADAR, intended to facilitate requirements and architecture decision analysis. The language has relations to quantitative AND/OR goal models used in requirements engineering and to feature models used in software product lines. However, it simplifies such models to a minimum set of language constructs essential for decision analysis. The paper presents RADAR's modelling language, automated support for decision analysis, and evaluates its application to four real-world examples."}, {"id": "conf/icse/BehringerPB17", "title": "PEoPL: projectional editing of product lines.", "authors": ["Benjamin Behringer", "Jochen Palz", "Thorsten Berger"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.58", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.58", "http://dl.acm.org/citation.cfm?id=3097436"], "tag": ["Research track:\nDevelopment tools and frameworks"], "abstract": "The features of a software product line - a portfolio of system variants - can be realized using various implementation techniques (a. k. a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts. We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (<;45ms on average for our largest subject Berkeley DB)."}, {"id": "conf/icse/BarikSLHFMP17", "title": "Do developers read compiler error messages?", "authors": ["Titus Barik", "Justin Smith", "Kevin Lubick", "Elisabeth Holmes", "Jing Feng", "Emerson R. Murphy-Hill", "Chris Parnin"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.59", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.59", "http://dl.acm.org/citation.cfm?id=3097437"], "tag": ["Research track:\nDevelopment tools and frameworks"], "abstract": "In integrated development environments, developers receive compiler error messages through a variety of textual and visual mechanisms, such as popups and wavy red underlines. Although error messages are the primary means of communicating defects to developers, researchers have a limited understanding on how developers actually use these messages to resolve defects. To understand how developers use error messages, we conducted an eye tracking study with 56 participants from undergraduate and graduate software engineering courses at our university. The participants attempted to resolve common, yet problematic defects in a Java code base within the Eclipse development environment. We found that: 1) participants read error messages and the difficulty of reading these messages is comparable to the difficulty of reading source code, 2) difficulty reading error messages significantly predicts participants' task performance, and 3) participants allocate a substantial portion of their total task to reading error messages (13%-25%). The results of our study offer empirical justification for the need to improve compiler error messages for developers."}, {"id": "conf/icse/ChristakisEG017", "title": "A general framework for dynamic stub injection.", "authors": ["Maria Christakis", "Patrick Emmisberger", "Patrice Godefroid", "Peter M\u00fcller"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.60", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.60", "http://dl.acm.org/citation.cfm?id=3097438"], "tag": ["Research track:\nDevelopment tools and frameworks"], "abstract": "Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications."}, {"id": "conf/icse/ChekamPTH17", "title": "An empirical study on mutation, statement and branch coverage fault revelation that avoids the unreliable clean program assumption.", "authors": ["Thierry Titcheu Chekam", "Mike Papadakis", "Yves Le Traon", "Mark Harman"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.61", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.61", "http://dl.acm.org/citation.cfm?id=3097440"], "tag": ["Research track:\nTesting and debugging"], "abstract": "Many studies suggest using coverage concepts, such as branch coverage, as the starting point of testing, while others as the most prominent test quality indicator. Yet the relationship between coverage and fault-revelation remains unknown, yielding uncertainty and controversy. Most previous studies rely on the Clean Program Assumption, that a test suite will obtain similar coverage for both faulty and fixed ('clean') program versions. This assumption may appear intuitive, especially for bugs that denote small semantic deviations. However, we present evidence that the Clean Program Assumption does not always hold, thereby raising a critical threat to the validity of previous results. We then conducted a study using a robust experimental methodology that avoids this threat to validity, from which our primary finding is that strong mutation testing has the highest fault revelation of four widely-used criteria. Our findings also revealed that fault revelation starts to increase significantly only once relatively high levels of coverage are attained."}, {"id": "conf/icse/PearsonCJFAEPK17", "title": "Evaluating and improving fault localization.", "authors": ["Spencer Pearson", "Jos\u00e9 Campos", "Ren\u00e9 Just", "Gordon Fraser", "Rui Abreu", "Michael D. Ernst", "Deric Pang", "Benjamin Keller"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.62", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.62", "http://dl.acm.org/citation.cfm?id=3097441"], "tag": ["Research track:\nTesting and debugging"], "abstract": "Most fault localization techniques take as input a faulty program, and produce as output a ranked list of suspicious code locations at which the program may be defective. When researchers propose a new fault localization technique, they typically evaluate it on programs with known faults. The technique is scored based on where in its output list the defective code appears. This enables the comparison of multiple fault localization techniques to determine which one is better. Previous research has evaluated fault localization techniques using artificial faults, generated either by mutation tools or manually. In other words, previous research has determined which fault localization techniques are best at finding artificial faults. However, it is not known which fault localization techniques are best at finding real faults. It is not obvious that the answer is the same, given previous work showing that artificial faults have both similarities to and differences from real faults. We performed a replication study to evaluate 10 claims in the literature that compared fault localization techniques (from the spectrum-based and mutation-based families). We used 2995 artificial faults in 6 real-world programs. Our results support 7 of the previous claims as statistically significant, but only 3 as having non-negligible effect sizes. Then, we evaluated the same 10 claims, using 310 real faults from the 6 programs. Every previous result was refuted or was statistically and practically insignificant. Our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults. In light of these results, we identified a design space that includes many previously-studied fault localization techniques as well as hundreds of new techniques. We experimentally determined which factors in the design space are most important, using an overall set of 395 real faults. Then, we extended this design space with new techniques. Several of our novel techniques outperform all existing techniques, notably in terms of ranking defective code in the top-5 or top-10 reports."}, {"id": "conf/icse/Tzoref-BrillM17", "title": "Syntactic and semantic differencing for combinatorial models of test designs.", "authors": ["Rachel Tzoref-Brill", "Shahar Maoz"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.63", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.63", "http://dl.acm.org/citation.cfm?id=3097443"], "tag": ["Research track:\nTesting I"], "abstract": "Combinatorial test design (CTD) is an effective test design technique, considered to be a testing best practice. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. As the system under test evolves, e.g., due to iterative development processes and bug fixing, so does the test space, and thus, in the context of CTD, evolution translates into frequent manual model definition updates. Manually reasoning about the differences between versions of real-world models following such updates is infeasible due to their complexity and size. Moreover, representing the differences is challenging. In this work, we propose a first syntactic and semantic differencing technique for combinatorial models of test designs. We define a concise and canonical representation for differences between two models, and suggest a scalable algorithm for automatically computing and presenting it. We use our differencing technique to analyze the evolution of 42 real-world industrial models, demonstrating its applicability and scalability. Further, a user study with 16 CTD practitioners shows that comprehension of differences between real-world combinatorial model versions is challenging and that our differencing tool significantly improves the performance of less experienced practitioners. The analysis and user study provide evidence for the potential usefulness of our differencing approach. Our work advances the state-of-the-art in CTD with better capabilities for change comprehension and management."}, {"id": "conf/icse/SoutodG17", "title": "Balancing soundness and efficiency for practical testing of configurable systems.", "authors": ["Sabrina Souto", "Marcelo d'Amorim", "Rohit Gheyi"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.64", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.64", "http://dl.acm.org/citation.cfm?id=3097444"], "tag": ["Research track:\nTesting I"], "abstract": "Testing configurable systems is important and challenging due to the enormous space of configurations where errors can hide. Existing approaches to test these systems are often costly or unreliable. This paper proposes S-SPLat, a technique that combines heuristic sampling with symbolic search to obtain both breadth and depth in the exploration of the configuration space. S-SPLat builds on SPLat, our previously developed technique, that explores all reachable configurations from tests. In contrast to its predecessor, S-SPLat sacrifices soundness in favor of efficiency. We evaluated our technique on eight software product lines of various sizes and on a large configurable system - GCC. Considering the results for GCC, S-SPLat was able to reproduce all five bugs that we previously found in a previous study with SPLat but much faster and it was able to find two new bugs in a recent release of GCC. Results suggest that it is preferable to use a combination of simple heuristics to drive the symbolic search as opposed to a single heuristic. S-SPLat and our experimental infrastructure are publicly available."}, {"id": "conf/icse/0010ZPZMZ17", "title": "Automatic text input generation for mobile testing.", "authors": ["Peng Liu", "Xiangyu Zhang", "Marco Pistoia", "Yunhui Zheng", "Manoel Marques", "Lingfei Zeng"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.65", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.65", "http://dl.acm.org/citation.cfm?id=3097445"], "tag": ["Research track:\nTesting I"], "abstract": "Many designs have been proposed to improve the automated mobile testing. Despite these improvements, providing appropriate text inputs remains a prominent obstacle, which hinders the large-scale adoption of automated testing approaches. The key challenge is how to automatically produce the most relevant text in a use case context. For example, a valid website address should be entered in the address bar of a mobile browser app to continue the testing of the app, a singer's name should be entered in the search bar of a music recommendation app. Without the proper text inputs, the testing would get stuck. We propose a novel deep learning based approach to address the challenge, which reduces the problem to a minimization problem. Another challenge is how to make the approach generally applicable to both the trained apps and the untrained apps. We leverage the Word2Vec model to address the challenge. We have built our approaches as a tool and evaluated it with 50 iOS mobile apps including Firefox and Wikipedia. The results show that our approach significantly outperforms existing automatic text input generation methods."}, {"id": "conf/icse/PerezAD17", "title": "A test-suite diagnosability metric for spectrum-based fault localization approaches.", "authors": ["Alexandre Perez", "Rui Abreu", "Arie van Deursen"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.66", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.66", "http://dl.acm.org/citation.cfm?id=3097446"], "tag": ["Research track:\nTesting I"], "abstract": "Current metrics for assessing the adequacy of a test-suite plainly focus on the number of components (be it lines, branches, paths) covered by the suite, but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum-based fault localization techniques can perform accurate fault isolation. We propose a metric, called DDU, aimed at complementing adequacy measurements by quantifying a test-suite's diagnosability, i.e., the effectiveness of applying spectrum-based fault localization to pinpoint faults in the code in the event of test failures. Our aim is to increase the value generated by creating thorough test-suites, so they are not only regarded as error detection mechanisms but also as effective diagnostic aids that help widely-used fault-localization techniques to accurately pinpoint the location of bugs in the system. Our experiments show that optimizing a test suite with respect to DDU yields a 34% gain in spectrum-based fault localization report accuracy when compared to the standard branch-coverage metric."}, {"id": "conf/icse/ZhangK17", "title": "Automated transplantation and differential testing for clones.", "authors": ["Tianyi Zhang", "Miryung Kim"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.67", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.67", "http://dl.acm.org/citation.cfm?id=3097448"], "tag": ["Research track:\nTesting II"], "abstract": "Code clones are common in software. When applying similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacerbated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, Grafter transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, Grafter supports fine-grained differential testing at both the test outcome level and the intermediate program state level. In our evaluation on three open source projects, Grafter successfully reuses tests in 94% of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of G RAFTER, we systematically inject faults using a mutation testing tool, Major, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, Grafter detects 31% more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that Grafter should effectively complement static cloning bug finders."}, {"id": "conf/icse/RojasWCF17", "title": "Code defenders: crowdsourcing effective tests and subtle mutants with a mutation testing game.", "authors": ["Jos\u00e9 Miguel Rojas", "Thomas D. White", "Benjamin S. Clegg", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.68", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.68", "http://dl.acm.org/citation.cfm?id=3097449"], "tag": ["Research track:\nTesting II"], "abstract": "Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program, automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools."}, {"id": "conf/icse/ShiTLBC17", "title": "Optimizing test placement for module-level regression testing.", "authors": ["August Shi", "Suresh Thummalapenta", "Shuvendu K. Lahiri", "Nikolaj Bj\u00f8rner", "Jacek Czerwonka"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.69", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.69", "http://dl.acm.org/citation.cfm?id=3097450"], "tag": ["Research track:\nTesting II"], "abstract": "Modern build systems help increase developer productivity by performing incremental building and testing. These build systems view a software project as a group of interdependent modules and perform regression test selection at the module level. However, many large software projects have imprecise dependency graphs that lead to wasteful test executions. If a test belongs to a module that has more dependencies than the actual dependencies of the test, then it is executed unnecessarily whenever a code change impacts those additional dependencies. In this paper, we formulate the problem of wasteful test executions due to suboptimal placement of tests in modules. We propose a greedy algorithm to reduce the number of test executions by suggesting test movements while considering historical build information and actual dependencies of tests. We have implemented our technique, called TestOptimizer, on top of CloudBuild, the build system developed within Microsoft over the last few years. We have evaluated the technique on five large proprietary projects. Our results show that the suggested test movements can lead to a reduction of 21.66 million test executions (17.09%) across all our subject projects. We received encouraging feedback from the developers of these projects; they accepted and intend to implement \u224880% of our reported suggestions."}, {"id": "conf/icse/ChenBHXZX17", "title": "Learning to prioritize test programs for compiler testing.", "authors": ["Junjie Chen", "Yanwei Bai", "Dan Hao", "Yingfei Xiong", "Hongyu Zhang", "Bing Xie"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.70", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.70", "http://dl.acm.org/citation.cfm?id=3097451"], "tag": ["Research track:\nTesting II"], "abstract": "Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases."}, {"id": "conf/icse/JiangLYX17", "title": "What causes my test alarm?: automatic cause analysis for test alarms in system and integration testing.", "authors": ["He Jiang", "Xiaochen Li", "Zijiang Yang", "Jifeng Xuan"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.71", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.71", "http://dl.acm.org/citation.cfm?id=3097453"], "tag": ["Research track:\nDefect prediction"], "abstract": "Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions."}, {"id": "conf/icse/BocicB17", "title": "Symbolic model extraction for web application verification.", "authors": ["Ivan Bocic", "Tevfik Bultan"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.72", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.72", "http://dl.acm.org/citation.cfm?id=3097455"], "tag": ["Research track:\nFormal methods"], "abstract": "Modern web applications use complex data models and access control rules which lead to data integrity and access control errors. One approach to find such errors is to use formal verification techniques. However, as a first step, most formal verification techniques require extraction of a formal model which is a difficult problem in itself due to dynamic features of modern languages, and it is typically done either manually, or using ad hoc techniques. In this paper, we present a technique called symbolic model extraction for extracting formal data models from web applications. The key ideas of symbolic model extraction are 1) to use the source language interpreter for model extraction, which enables us to handle dynamic features of the language, 2) to use code instrumentation so that execution of each instrumented piece of code returns the formal model that corresponds to that piece of code, 3) to instrument the code dynamically so that the models of methods that are created at runtime can also be extracted, and 4) to execute both sides of branches during instrumented execution so that all program behaviors can be covered in a single instrumented execution. We implemented the symbolic model extraction technique for the Rails framework and used it to extract data and access control models from web applications. Our experiments demonstrate that symbolic model extraction is scalable and extracts formal models that are precise enough to find bugs in real-world applications without reporting too many false positives."}, {"id": "conf/icse/FaitelsonT17", "title": "UML diagram refinement (focusing on class- and use case diagrams).", "authors": ["David Faitelson", "Shmuel S. Tyszberowicz"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.73", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.73", "http://dl.acm.org/citation.cfm?id=3097456"], "tag": ["Research track:\nFormal methods"], "abstract": "Large and complicated UML models are not useful, because they are difficult to understand. This problem can be solved by using several diagrams of the same system at different levels of abstraction. Unfortunately, UML does not define an explicit set of rules for ensuring that diagrams at different levels of abstraction are consistent. We define such a set of rules, that we call diagram refinement. Diagram refinement is intuitive, and applicable to several kinds of UML diagrams (mostly to structural diagrams but also to use case diagrams), yet it rests on a solid mathematical basis-the theory of graph homomorphisms. We illustrate its usefulness with a series of examples."}, {"id": "conf/icse/ServantJ17", "title": "Fuzzy fine-grained code-history analysis.", "authors": ["Francisco Servant", "James A. Jones"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.74", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.74", "http://dl.acm.org/citation.cfm?id=3097458"], "tag": ["Research track:\nSoftware evolution"], "abstract": "Existing software-history techniques represent source-code evolution as an absolute and unambiguous mapping of lines of code in prior revisions to lines of code in subsequent revisions. However, the true evolutionary lineage of a line of code is often complex, subjective, and ambiguous. As such, existing techniques are predisposed to, both, overestimate and underestimate true evolution lineage. In this paper, we seek to address these issues by providing a more expressive model of code evolution, the fuzzy history graph, by representing code lineage as a continuous (i.e., fuzzy) metric rather than a discrete (i.e., absolute) one. Using this more descriptive model, we additionally provide a novel multi-revision code-history analysis - fuzzy history slicing. In our experiments over three real-world software systems, we found that the fuzzy history graph provides a tunable balance of precision and recall, and an overall improved accuracy over existing code-evolution models. Furthermore, we found that the use of such a fuzzy model of history provided improved accuracy for code-history analysis tasks."}, {"id": "conf/icse/GaoBB17", "title": "To type or not to type: quantifying detectable bugs in JavaScript.", "authors": ["Zheng Gao", "Christian Bird", "Earl T. Barr"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.75", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.75", "http://dl.acm.org/citation.cfm?id=3097459"], "tag": ["Research track:\nSoftware evolution"], "abstract": "JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!."}, {"id": "conf/icse/FabijanDOB17", "title": "The evolution of continuous experimentation in software product development: from data to a data-driven organization at scale.", "authors": ["Aleksander Fabijan", "Pavel A. Dmitriev", "Helena Holmstr\u00f6m Olsson", "Jan Bosch"], "DOIs": ["https://doi.org/10.1109/ICSE.2017.76", "http://doi.ieeecomputersociety.org/10.1109/ICSE.2017.76", "http://dl.acm.org/citation.cfm?id=3097460"], "tag": ["Research track:\nSoftware evolution"], "abstract": "Software development companies are increasingly aiming to become data-driven by trying to continuously experiment with the products used by their customers. Although familiar with the competitive edge that the A/B testing technology delivers, they seldom succeed in evolving and adopting the methodology. In this paper, and based on an exhaustive and collaborative case study research in a large software-intense company with highly developed experimentation culture, we present the evolution process of moving from ad-hoc customer data analysis towards continuous controlled experimentation at scale. Our main contribution is the \"Experimentation Evolution Model\" in which we detail three phases of evolution: technical, organizational and business evolution. With our contribution, we aim to provide guidance to practitioners on how to develop and scale continuous experimentation in software organizations with the purpose of becoming data-driven at scale."}], "2018": [{"id": "conf/icse/WenCWHC18", "title": "Context-aware patch generation for better automated program repair.", "authors": ["Ming Wen", "Junjie Chen", "Rongxin Wu", "Dan Hao", "Shing-Chi Cheung"], "DOIs": ["https://doi.org/10.1145/3180155.3180233", "http://ieeexplore.ieee.org/document/8453055"], "tag": ["Software repair I"], "abstract": "ABSTRACTThe effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (i.e., correct patches are either generated after incorrect plausible ones or not generated within the time budget).To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood.In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones."}, {"id": "conf/icse/HuaZWK18", "title": "Towards practical program repair with on-demand candidate generation.", "authors": ["Jinru Hua", "Mengshi Zhang", "Kaiyuan Wang", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1145/3180155.3180245", "http://ieeexplore.ieee.org/document/8453056"], "tag": ["Software repair I"], "abstract": "ABSTRACTEffective program repair techniques, which modify faulty programs to fix them with respect to given test suites, can substantially reduce the cost of manual debugging. A common repair approach is to iteratively first generate candidate programs with possible bug fixes and then validate them against the given tests until a candidate that passes all the tests is found. While this approach is conceptually simple, due to the potentially high number of candidates that need to first be generated and then be compiled and tested, existing repair techniques that embody this approach have relatively low effectiveness, especially for faults at a fine granularity.To tackle this limitation, we introduce a novel repair technique, SketchFix, which generates candidate fixes on demand (as needed) during the test execution. Instead of iteratively re-compiling and re-executing each actual candidate program, SketchFix translates faulty programs to sketches, i.e., partial programs with \"holes\", and compiles each sketch once which may represent thousands of concrete candidates. With the insight that the space of candidates can be reduced substantially by utilizing the runtime behaviors of the tests, SketchFix lazily initializes the candidates of the sketches while validating them against the test execution.We experimentally evaluate SketchFix on the Defects4J benchmark and the experimental results show that SketchFix works particularly well in repairing bugs with expression manipulation at the AST node-level granularity compared to other program repair techniques. Specifically, SketchFix correctly fixes 19 out of 357 defects in 23 minutes on average using the default setting. In addition, SketchFix finds the first repair with 1.6% of re-compilations (#compiled sketches/#candidates) and 3.0% of re-executions out of all repair candidates."}, {"id": "conf/icse/YiTMBR18", "title": "A correlation study between automated program repair and test-suite metrics.", "authors": ["Jooyong Yi", "Shin Hwei Tan", "Sergey Mechtaev", "Marcel B\u00f6hme", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1145/3180155.3182517", "http://ieeexplore.ieee.org/document/8453057"], "tag": ["Software repair I"], "abstract": "ABSTRACTAutomated program repair has attracted attention due to its potential to reduce debugging cost. Prior works show the feasibility of automated repair, and the research focus is gradually shifting towards the quality of generated patches. One promising direction is to control the quality of generated patches by controlling the quality of test-suites used. In this paper, 1we investigate the question: \"Can traditional test-suite metrics used in software testing be used for automated program repair?\". We empirically investigate the effectiveness of test-suite metrics (statement / branch coverage and mutation score) in controlling the reliability of repairs (the likelihood that repairs cause regressions). We conduct the largest-scale experiments to date with real-world software, and perform the first correlation study between test-suite metrics and the reliability of generated repairs. Our results show that by increasing test-suite metrics, the reliability of repairs tend to increase. Particularly, such trend is most strongly observed in statement coverage. This implies that traditional test-suite metrics used in software testing can also be used to improve the reliability of repairs in program repair."}, {"id": "conf/icse/MotwaniSJB18", "title": "Do automated program repair techniques repair hard and important bugs?", "authors": ["Manish Motwani", "Sandhya Sankaranarayanan", "Ren\u00e9 Just", "Yuriy Brun"], "DOIs": ["https://doi.org/10.1145/3180155.3182533", "http://ieeexplore.ieee.org/document/8453058"], "tag": ["Software repair I"], "abstract": "ABSTRACTAutomated program repair techniques use a buggy program and a partial specification (typically a test suite) to produce a program variant that satisfies the specification. While prior work has studied patch quality [10, 11] and maintainability [2], it has not examined whether automated repair techniques are capable of repairing defects that developers consider important or that are hard for developers to repair manually. This paper tackles those questions."}, {"id": "conf/icse/WangBWWCWW18", "title": "Software protection on the go: a large-scale empirical study on mobile app obfuscation.", "authors": ["Pei Wang", "Qinkun Bao", "Li Wang", "Shuai Wang", "Zhaofeng Chen", "Tao Wei", "Dinghao Wu"], "DOIs": ["https://doi.org/10.1145/3180155.3180169", "http://ieeexplore.ieee.org/document/8453059"], "tag": ["Apps and app stores I"], "abstract": "ABSTRACTThe prosperity of smartphone markets has raised new concerns about software security on mobile platforms, leading to a growing demand for effective software obfuscation techniques. Due to various differences between the mobile and desktop ecosystems, obfuscation faces both technical and non-technical challenges when applied to mobile software. Although there have been quite a few software security solution providers launching their mobile app obfuscation services, it is yet unclear how real-world mobile developers perform obfuscation as part of their software engineering practices.Our research takes a first step to systematically studying the deployment of software obfuscation techniques in mobile software development. With the help of an automated but coarse-grained method, we computed the likelihood of an app being obfuscated for over a million app samples crawled from Apple App Store. We then inspected the top 6600 instances and managed to identify 601 obfuscated versions of 539 iOS apps. By analyzing this sample set with extensive manual effort, we made various observations that reveal the status quo of mobile obfuscation in the real world, providing insights into understanding and improving the situation of software protection on mobile platforms."}, {"id": "conf/icse/WangQHSB018", "title": "GUILeak: tracing privacy policy claims on user input data for Android applications.", "authors": ["Xiaoyin Wang", "Xue Qin", "Mitra Bokaei Hosseini", "Rocky Slavin", "Travis D. Breaux", "Jianwei Niu"], "DOIs": ["https://doi.org/10.1145/3180155.3180196", "http://ieeexplore.ieee.org/document/8453060"], "tag": ["Apps and app stores I"], "abstract": "ABSTRACTThe Android mobile platform supports billions of devices across more than 190 countries around the world. This popularity coupled with user data collection by Android apps has made privacy protection a well-known challenge in the Android ecosystem. In practice, app producers provide privacy policies disclosing what information is collected and processed by the app. However, it is difficult to trace such claims to the corresponding app code to verify whether the implementation is consistent with the policy. Existing approaches for privacy policy alignment focus on information directly accessed through the Android platform (e.g., location and device ID), but are unable to handle user input, a major source of private information. In this paper, we propose a novel approach that automatically detects privacy leaks of user-entered data for a given Android app and determines whether such leakage may violate the app's privacy policy claims. For evaluation, we applied our approach to 120 popular apps from three privacy-relevant app categories: finance, health, and dating. The results show that our approach was able to detect 21 strong violations and 18 weak violations from the studied apps."}, {"id": "conf/icse/GaoZLK18", "title": "Online app review analysis for identifying emerging issues.", "authors": ["Cuiyun Gao", "Jichuan Zeng", "Michael R. Lyu", "Irwin King"], "DOIs": ["https://doi.org/10.1145/3180155.3180218", "http://ieeexplore.ieee.org/document/8453061"], "tag": ["Apps and app stores I"], "abstract": "ABSTRACTDetecting emerging issues (e.g., new bugs) timely and precisely is crucial for developers to update their apps. App reviews provide an opportunity to proactively collect user complaints and promptly improve apps' user experience, in terms of bug fixing and feature refinement. However, the tremendous quantities of reviews and noise words (e.g., misspelled words) increase the difficulties in accurately identifying newly-appearing app issues. In this paper, we propose a novel and automated framework IDEA, which aims to IDentify Emerging App issues effectively based on online review analysis. We evaluate IDEA on six popular apps from Google Play and Apple's App Store, employing the official app changelogs as our ground truth. Experiment results demonstrate the effectiveness of IDEA in identifying emerging app issues. Feedback from engineers and product managers shows that 88.9% of them think that the identified issues can facilitate app development in practice. Moreover, we have successfully applied IDEA to several products of Tencent, which serve hundreds of millions of users."}, {"id": "conf/icse/MoralesSKCA18", "title": "EARMO: an energy-aware refactoring approach for mobile apps.", "authors": ["Rodrigo Morales", "Rub\u00e9n Saborido", "Foutse Khomh", "Francisco Chicano", "Giuliano Antoniol"], "DOIs": ["https://doi.org/10.1145/3180155.3182524", "http://ieeexplore.ieee.org/document/8453062"], "tag": ["Apps and app stores I"], "abstract": "ABSTRACTWith millions of smartphones sold every year, the development of mobile apps has grown substantially. The battery power limitation of mobile devices has push developers and researchers to search for methods to improve the energy efficiency of mobile apps. We propose a multiobjective refactoring approach to automatically improve the architecture of mobile apps, while controlling for energy efficiency. In this extended abstract we briefly summarize our work."}, {"id": "conf/icse/BhatiaKS18", "title": "Neuro-symbolic program corrector for introductory programming assignments.", "authors": ["Sahil Bhatia", "Pushmeet Kohli", "Rishabh Singh"], "DOIs": ["https://doi.org/10.1145/3180155.3180219", "http://ieeexplore.ieee.org/document/8453063"], "tag": ["Software evolution and maintenance I"], "abstract": "ABSTRACTAutomatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming assignment, and are then queried to fix syntax errors in an incorrect programming submission by replacing or inserting the predicted tokens at the error location. We evaluate our technique on a dataset comprising of over 14,500 student submissions with syntax errors. Our method is able to repair syntax errors in 60% (8689) of submissions, and finds functionally correct repairs for 23.8% (3455) submissions."}, {"id": "conf/icse/RenJXY18", "title": "Automated localization for unreproducible builds.", "authors": ["Zhilei Ren", "He Jiang", "Jifeng Xuan", "Zijiang Yang"], "DOIs": ["https://doi.org/10.1145/3180155.3180224", "http://ieeexplore.ieee.org/document/8453064"], "tag": ["Software evolution and maintenance I"], "abstract": "ABSTRACTReproducibility is the ability of recreating identical binaries under pre-defined build environments. Due to the need of quality assurance and the benefit of better detecting attacks against build environments, the practice of reproducible builds has gained popularity in many open-source software repositories such as Debian and Bitcoin. However, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries.In this paper we propose an automated framework called RepLoc to localize the problematic files for unreproducible builds. RepLoc features a query augmentation component that utilizes the information extracted from the build logs, and a heuristic rule-based filtering component that narrows the search scope. By integrating the two components with a weighted file ranking module, RepLoc is able to automatically produce a ranked list of files that are helpful in locating the problematic files for the unreproducible builds. We have implemented a prototype and conducted extensive experiments over 671 real-world unreproducible Debian packages in four different categories. By considering the topmost ranked file only, RepLoc achieves an accuracy rate of 47.09%. If we expand our examination to the top ten ranked files in the list produced by RepLoc, the accuracy rate becomes 79.28%. Considering that there are hundreds of source code, scripts, Makefiles, etc., in a package, RepLoc significantly reduces the scope of localizing problematic files. Moreover, with the help of RepLoc, we successfully identified and fixed six new unreproducible packages from Debian and Guix."}, {"id": "conf/icse/LiZdO18", "title": "Enlightened debugging.", "authors": ["Xiangyu Li", "Shaowei Zhu", "Marcelo d'Amorim", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1145/3180155.3180242", "http://ieeexplore.ieee.org/document/8453065"], "tag": ["Software evolution and maintenance I"], "abstract": "ABSTRACTNumerous automated techniques have been proposed to reduce the cost of software debugging, a notoriously time-consuming and human-intensive activity. Among these techniques, Statistical Fault Localization (SFL) is particularly popular. One issue with SFL is that it is based on strong, often unrealistic assumptions on how developers behave when debugging. To address this problem, we propose Enlighten, an interactive, feedback-driven fault localization technique. Given a failing test, Enlighten (1) leverages SFL and dynamic dependence analysis to identify suspicious method invocations and corresponding data values, (2) presents the developer with a query about the most suspicious invocation expressed in terms of inputs and outputs, (3) encodes the developer feedback on the correctness of individual data values as extra program specifications, and (4) repeats these steps until the fault is found. We evaluated Enlighten in two ways. First, we applied Enlighten to 1,807 real and seeded faults in 3 open source programs using an automated oracle as a simulated user; for over 96% of these faults, Enlighten required less than 10 interactions with the simulated user to localize the fault, and a sensitivity analysis showed that the results were robust to erroneous responses. Second, we performed an actual user study on 4 faults with 24 participants and found that participants who used Enlighten performed significantly better than those not using our tool, in terms of both number of faults localized and time needed to localize the faults."}, {"id": "conf/icse/ScavuzzoNA18", "title": "Experiences and challenges in building a data intensive system for data migration.", "authors": ["Marco Scavuzzo", "Elisabetta Di Nitto", "Danilo Ardagna"], "DOIs": ["https://doi.org/10.1145/3180155.3182534", "http://ieeexplore.ieee.org/document/8453066"], "tag": ["Software evolution and maintenance I"], "abstract": "ABSTRACTRecent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees."}, {"id": "conf/icse/0008ZBPLO18", "title": "Sentiment analysis for software engineering: how far can we go?", "authors": ["Bin Lin", "Fiorella Zampetti", "Gabriele Bavota", "Massimiliano Di Penta", "Michele Lanza", "Rocco Oliveto"], "DOIs": ["https://doi.org/10.1145/3180155.3180195", "http://ieeexplore.ieee.org/document/8453067"], "tag": ["Human and social aspects of computing I"], "abstract": "ABSTRACTSentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context.We describe our experience in building a software library recommender exploiting developers' opinions mined from Stack Overflow. To reach our goal, we retrained---on a set of 40k manually labeled sentences/words extracted from Stack Overflow---a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of commonly used tools to identify the sentiment of SE related texts. Meanwhile, we also studied the impact of different datasets on tool performance. Our results should warn the research community about the strong limitations of current sentiment analysis tools."}, {"id": "conf/icse/ZhouSLXWK18", "title": "Identifying features in forks.", "authors": ["Shurui Zhou", "Stefan Stanciulescu", "Olaf Le\u00dfenich", "Yingfei Xiong", "Andrzej Wasowski", "Christian K\u00e4stner"], "DOIs": ["https://doi.org/10.1145/3180155.3180205", "http://ieeexplore.ieee.org/document/8453068"], "tag": ["Human and social aspects of computing I"], "abstract": "ABSTRACTFork-based development has been widely used both in open source communities and in industry, because it gives developers flexibility to modify their own fork without affecting others. Unfortunately, this mechanism has downsides: When the number of forks becomes large, it is difficult for developers to get or maintain an overview of activities in the forks. Current tools provide little help. We introduce Infox, an approach to automatically identify non-merged features in forks and to generate an overview of active forks in a project. The approach clusters cohesive code fragments using code and network-analysis techniques and uses information-retrieval techniques to label clusters with keywords. The clustering is effective, with 90 % accuracy on a set of known features. In addition, a human-subject evaluation shows that Infox can provide actionable insight for developers of forks."}, {"id": "conf/icse/RehmanMNUT18", "title": "Roles and impacts of hands-on software architects in five industrial case studies.", "authors": ["Inayat Rehman", "Mehdi Mirakhorli", "Meiyappan Nagappan", "Azat Aralbay Uulu", "Matthew Thornton"], "DOIs": ["https://doi.org/10.1145/3180155.3180234", "http://ieeexplore.ieee.org/document/8453069"], "tag": ["Human and social aspects of computing I"], "abstract": "ABSTRACTWhether software architects should also code is an enduring question. In order to satisfy performance, security, reliability and other quality concerns, architects need to compare and carefully choose a combination of architectural patterns, styles or tactics. Then later in the development cycle, these architectural choices must be implemented completely and correctly so there will not be any drift from envisioned design. In this paper, we use data analytics-based techniques to study five large-scale software systems, examining the impact and the role of software architects who write code on software quality. Our quantitative study is augmented with a follow up interview of architects. This paper provides empirical evidence for supporting the pragmatic opinions that architects should write code. Our analysis shows that implementing architectural tactics is more complex than delivering functionality, tactics are more error prone than software functionalities, and the architects tend to introduce fewer bugs into the implementation of architectural tactics compared to the developers."}, {"id": "conf/icse/CalefatoLMN18", "title": "Sentiment polarity detection for software development.", "authors": ["Fabio Calefato", "Filippo Lanubile", "Federico Maiorano", "Nicole Novielli"], "DOIs": ["https://doi.org/10.1145/3180155.3182519", "http://ieeexplore.ieee.org/document/8453070"], "tag": ["Human and social aspects of computing I"], "abstract": "ABSTRACTThe role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within software repositories and information sources. With a few notable exceptions [1][5], empirical software engineering studies have exploited off-the-shelf sentiment analysis tools. However, such tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports [2][4]. In particular, Jongeling et al. [2] show how the choice of the sentiment analysis tool may impact the conclusion validity of empirical studies because not only these tools do not agree with human annotation of developers' communication channels, but they also disagree among themselves.Our goal is to move beyond the limitations of off-the-shelf sentiment analysis tools when applied in the software engineering domain. Accordingly, we present Senti4SD, a sentiment polarity classifier for software developers' communication channels. Senti4SD exploits a suite of lexicon-based, keyword-based, and semantic features for appropriately dealing with the domain-dependent use of a lexicon. We built a Distributional Semantic Model (DSM) to derive the semantic features exploited by Senti4SD. Specifically, we ran word2vec [3] on a collection of over 20 million documents from Stack Overflow, thus obtaining word vectors that are representative of developers' communication style. The classifier is trained and validated using a gold standard of 4,423 Stack Overflow posts, including questions, answers, and comments, which were manually annotated for sentiment polarity.We release the full lab package2, which includes both the gold standard and the emotion annotation guidelines, to ease the execution of replications as well as new studies on emotion awareness in software engineering. To inform future research on word embedding for text categorization and information retrieval in software engineering, the replication kit also includes the DSM.Results. The contribution of the lexicon-based, keyword-based, and semantic features is assessed by our empirical evaluation leveraging different feature settings. With respect to SentiStrength [6], a mainstream off-the-shelf tool that we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. Furthermore, we provide empirical evidence of better performance also in presence of a minimal set of training documents."}, {"id": "conf/icse/MechtaevNNGR18", "title": "Semantic program repair using a reference implementation.", "authors": ["Sergey Mechtaev", "Manh-Dung Nguyen", "Yannic Noller", "Lars Grunske", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1145/3180155.3180247", "http://ieeexplore.ieee.org/document/8453071"], "tag": ["Software repair II"], "abstract": "ABSTRACTAutomated program repair has been studied via the use of techniques involving search, semantic analysis and artificial intelligence. Most of these techniques rely on tests as the correctness criteria, which causes the test overfitting problem. Although various approaches such as learning from code corpus have been proposed to address this problem, they are unable to guarantee that the generated patches generalize beyond the given tests. This work studies automated repair of errors using a reference implementation. The reference implementation is symbolically analyzed to automatically infer a specification of the intended behavior. This specification is then used to synthesize a patch that enforces conditional equivalence of the patched and the reference programs. The use of the reference implementation as an implicit correctness criterion alleviates overfitting in test-based repair. Besides, since we generate patches by semantic analysis, the reference program may have a substantially different implementation from the patched program, which distinguishes our approach from existing techniques for regression repair like Relifix. Our experiments in repairing the embedded Linux Busybox with GNU Coreutils as reference (and vice-versa) revealed that the proposed approach scales to real-world programs and enables the generation of more correct patches."}, {"id": "conf/icse/MahajanAMH18", "title": "Automated repair of mobile friendly problems in web pages.", "authors": ["Sonal Mahajan", "Negarsadat Abolhassani", "Phil McMinn", "William G. J. Halfond"], "DOIs": ["https://doi.org/10.1145/3180155.3180262", "http://ieeexplore.ieee.org/document/8453072"], "tag": ["Software repair II"], "abstract": "ABSTRACTMobile devices have become a primary means of accessing the Internet. Unfortunately, many websites are not designed to be mobile friendly. This results in problems such as unreadable text, cluttered navigation, and content overflowing a device's viewport; all of which can lead to a frustrating and poor user experience. Existing techniques are limited in helping developers repair these mobile friendly problems. To address this limitation of prior work, we designed a novel automated approach for repairing mobile friendly problems in web pages. Our empirical evaluation showed that our approach was able to successfully resolve mobile friendly problems in 95% of the evaluation subjects. In a user study, participants preferred our repaired versions of the subjects and also considered the repaired pages to be more readable than the originals."}, {"id": "conf/icse/TonderG18", "title": "Static automated program repair for heap properties.", "authors": ["Rijnard van Tonder", "Claire Le Goues"], "DOIs": ["https://doi.org/10.1145/3180155.3180250", "http://ieeexplore.ieee.org/document/8453073"], "tag": ["Software repair II"], "abstract": "ABSTRACTStatic analysis tools have demonstrated effectiveness at finding bugs in real world code. Such tools are increasingly widely adopted to improve software quality in practice. Automated Program Repair (APR) has the potential to further cut down on the cost of improving software quality. However, there is a disconnect between these effective bug-finding tools and APR. Recent advances in APR rely on test cases, making them inapplicable to newly discovered bugs or bugs difficult to test for deterministically (like memory leaks). Additionally, the quality of patches generated to satisfy a test suite is a key challenge. We address these challenges by adapting advances in practical static analysis and verification techniques to enable a new technique that finds and then accurately fixes real bugs without test cases. We present a new automated program repair technique using Separation Logic. At a high-level, our technique reasons over semantic effects of existing program fragments to fix faults related to general pointer safety properties: resource leaks, memory leaks, and null dereferences. The procedure automatically translates identified fragments into source-level patches, and verifies patch correctness with respect to reported faults. In this work we conduct the largest study of automatically fixing undiscovered bugs in real-world code to date. We demonstrate our approach by correctly fixing 55 bugs, including 11 previously undiscovered bugs, in 11 real-world projects."}, {"id": "conf/icse/LeTLG18", "title": "Overfitting in semantics-based automated program repair.", "authors": ["Xuan-Bach D. Le", "Ferdian Thung", "David Lo", "Claire Le Goues"], "DOIs": ["https://doi.org/10.1145/3180155.3182536", "http://ieeexplore.ieee.org/document/8453074"], "tag": ["Software repair II"], "abstract": "ABSTRACTExisting APR techniques can be generally divided into two families: semantics- vs. heuristics-based. Semantics-based APR uses symbolic execution and test suites to extract semantic constraints, and uses program synthesis to synthesize repairs that satisfy the extracted constraints. Heuristic-based APR generates large populations of repair candidates via source manipulation, and searches for the best among them. Both families largely rely on a primary assumption that a program is correctly patched if the generated patch leads the program to pass all provided test cases. Patch correctness is thus an especially pressing concern. A repair technique may generate overfitting patches, which lead a program to pass all existing test cases, but fails to generalize beyond them. In this work, we revisit the overfitting problem with a focus on semantics-based APR techniques, complementing previous studies of the overfitting problem in heuristics-based APR. We perform our study using IntroClass and Codeflaws benchmarks, two datasets well-suited for assessing repair quality, to systematically characterize and understand the nature of overfitting in semantics-based APR. We find that similar to heuristics-based APR, overfitting also occurs in semantics-based APR in various different ways."}, {"id": "conf/icse/HassanTBH18", "title": "Studying the dialogue between users and developers of free apps in the google play store.", "authors": ["Safwat Hassan", "Chakkrit Tantithamthavorn", "Cor-Paul Bezemer", "Ahmed E. Hassan"], "DOIs": ["https://doi.org/10.1145/3180155.3182523", "http://ieeexplore.ieee.org/document/8453075"], "tag": ["Apps and app stores II"], "abstract": "ABSTRACTThe popularity of mobile apps continues to grow over the past few years. Mobile app stores, such as the Google Play Store and Apple's App Store provide a unique user feedback mechanism to app developers through app reviews. In the Google Play Store (and most recently in the Apple App Store), developers are able to respond to such user feedback.Over the past years, mobile app reviews have been studied excessively by researchers. However, much of prior work (including our own prior work) incorrectly assumes that reviews are static in nature and that users never update their reviews. In a recent study, we started analyzing the dynamic nature of the review-response mechanism. Our previous study showed that responding to a review often has a positive effect on the rating that is given by the user to an app.In this paper [1], we revisit our prior finding in more depth by studying 4.5 million reviews with 126,686 responses of 2,328 top free-to-download apps in the Google Play Store. One of the major findings of our paper is that the assumption that reviews are static is incorrect. In particular, we find that developers and users in some cases use this response mechanism as a rudimentary user support tool, where dialogues emerge between users and developers through updated reviews and responses. Even though the messages are often simple, we find instances of as many as ten user-developer back-and-forth messages that occur via the response mechanism.Using a mixed-effect model, we identify that the likelihood of a developer responding to a review increases as the review rating gets lower or as the review content gets longer. In addition, we identify four patterns of developers: 1) developers who primarily respond to only negative reviews, 2) developers who primarily respond to negative reviews or to reviews based on their content, 3) developers who primarily respond to reviews which are posted shortly after the latest release of their app, and 4) developers who primarily respond to reviews which are posted long after the latest release of their app.We perform a qualitative analysis of developer responses to understand what drives developers to respond to a review. We manually analyzed a statistically representative random sample of 347 reviews with responses of the top ten apps with the highest number of developer responses. We identify seven drivers that make a developer respond to a review, of which the most important ones are to thank the users for using the app and to ask the user for more details about the reported issue.Our findings show that it can be worthwhile for app owners to respond to reviews, as responding may lead to an increase in the given rating. In addition, our findings show that studying the dialogue between users and developers provides valuable insights that can lead to improvements in the app store and the user support process.The main contributions of this paper are as follows: (1) Our paper is the first work to demonstrate the dynamic nature of reviews. (2) Furthermore, we are the first to demonstrate a peculiar use of the app-review platforms as a user support medium. (3) In addition, our work is the first work to deeply explore developer responses in a systematic manner. (4) Finally, our classification of developer-responses highlights the value of providing canned or even automated responses in next generation app-review platforms."}, {"id": "conf/icse/MoranLBJP18", "title": "Automated reporting of GUI design violations for mobile apps.", "authors": ["Kevin Moran", "Boyang Li", "Carlos Bernal-C\u00e1rdenas", "Dan Jelf", "Denys Poshyvanyk"], "DOIs": ["https://doi.org/10.1145/3180155.3180246", "http://ieeexplore.ieee.org/document/8453076"], "tag": ["Apps and app stores II"], "abstract": "ABSTRACTThe inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications.This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called Gvt and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that Gvt solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers & developers at Huawei to improve the quality of their mobile apps."}, {"id": "conf/icse/ZhaoLLM18", "title": "Leveraging program analysis to reduce user-perceived latency in mobile applications.", "authors": ["Yixue Zhao", "Marcelo Schmitt Laser", "Yingjun Lyu", "Nenad Medvidovic"], "DOIs": ["https://doi.org/10.1145/3180155.3180249", "http://ieeexplore.ieee.org/document/8453077"], "tag": ["Apps and app stores II"], "abstract": "ABSTRACTReducing network latency in mobile applications is an effective way of improving the mobile user experience and has tangible economic benefits. This paper presents PALOMA, a novel client-centric technique for reducing the network latency by prefetching HTTP requests in Android apps. Our work leverages string analysis and callback control-flow analysis to automatically instrument apps using PALOMA's rigorous formulation of scenarios that address \"what\" and \"when\" to prefetch. PALOMA has been shown to incur significant runtime savings (several hundred milliseconds per prefetchable HTTP request), both when applied on a reusable evaluation benchmark we have developed and on real applications."}, {"id": "conf/icse/TanDGR18", "title": "Repairing crashes in Android apps.", "authors": ["Shin Hwei Tan", "Zhen Dong", "Xiang Gao", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1145/3180155.3180243", "http://ieeexplore.ieee.org/document/8453078"], "tag": ["Apps and app stores II"], "abstract": "ABSTRACTAndroid apps are omnipresent, and frequently suffer from crashes --- leading to poor user experience and economic loss. Past work focused on automated test generation to detect crashes in Android apps. However, automated repair of crashes has not been studied. In this paper, we propose the first approach to automatically repair Android apps, specifically we propose a technique for fixing crashes in Android apps. Unlike most test-based repair approaches, we do not need a test-suite; instead a single failing test is meticulously analyzed for crash locations and reasons behind these crashes. Our approach hinges on a careful empirical study which seeks to establish common root-causes for crashes in Android apps, and then distills the remedy of these root-causes in the form of eight generic transformation operators. These operators are applied using a search-based repair framework embodied in our repair tool Droix. We also prepare a benchmark DroixBench capturing reproducible crashes in Android apps. Our evaluation of Droix on DroixBench reveals that the automatically produced patches are often syntactically identical to the human patch, and on some rare occasion even better than the human patch (in terms of avoiding regressions). These results confirm our intuition that our proposed transformations form a sufficient set of operators to patch crashes in Android."}, {"id": "conf/icse/Zhang18", "title": "Hybrid regression test selection.", "authors": ["Lingming Zhang"], "DOIs": ["https://doi.org/10.1145/3180155.3180198", "http://ieeexplore.ieee.org/document/8453079"], "tag": ["Regression testing"], "abstract": "ABSTRACTRegression testing is crucial but can be extremely costly. Regression Test Selection (RTS) aims to reduce regression testing cost by only selecting and running the tests that may be affected by code changes. To date, various RTS techniques analyzing at different granularities (e.g., at the basic-block, method, and file levels) have been proposed. RTS techniques working on finer granularities may be more precise in selecting tests, while techniques working on coarser granularities may have lower overhead. According to a recent study, RTS at the file level (FRTS) can have less overall testing time compared with a finer grained technique at the method level, and represents state-of-the-art RTS. In this paper, we present the first hybrid RTS approach, HyRTS, that analyzes at multiple granularities to combine the strengths of traditional RTS techniques at different granularities. We implemented the basic HyRTS technique by combining the method and file granularity RTS. The experimental results on 2707 revisions of 32 projects, totalling over 124 Million LoC, demonstrate that HyRTS outperforms state-of-the-art FRTS significantly in terms of selected test ratio and the offline testing time. We also studied the impacts of each type of method-level changes, and further designed two new HyRTS variants based on the study results. Our additional experiments show that transforming instance method additions/deletions into file-level changes produces an even more effective HyRTS variant that can significantly outperform FRTS in both offline and online testing time."}, {"id": "conf/icse/VahabzadehS018", "title": "Fine-grained test minimization.", "authors": ["Arash Vahabzadeh", "Andrea Stocco", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1145/3180155.3180203", "http://ieeexplore.ieee.org/document/8453080"], "tag": ["Regression testing"], "abstract": "ABSTRACTAs a software system evolves, its test suite can accumulate redundancies over time. Test minimization aims at removing redundant test cases. However, current techniques remove whole test cases from the test suite using test adequacy criteria, such as code coverage. This has two limitations, namely (1) by removing a whole test case the corresponding test assertions are also lost, which can inhibit test suite effectiveness, (2) the issue of partly redundant test cases, i.e., tests with redundant test statements, is ignored. We propose a novel approach for fine-grained test case minimization. Our analysis is based on the inference of a test suite model that enables automated test reorganization within test cases. It enables removing redundancies at the test statement level, while preserving the coverage and test assertions of the test suite. We evaluated our approach, implemented in a tool called Testler, on the test suites of 15 open source projects. Our analysis shows that over 4,639 (24%) of the tests in these test suites are partly redundant, with over 11,819 redundant test statements in total. Our results show that Testler removes 43% of the redundant test statements, reducing the number of partly redundant tests by 52%. As a result, test suite execution time is reduced by up to 37% (20% on average), while maintaining the original statement coverage, branch coverage, test assertions, and fault detection capability."}, {"id": "conf/icse/MirandaCVB18", "title": "FAST approaches to scalable similarity-based test case prioritization.", "authors": ["Breno Miranda", "Emilio Cruciani", "Roberto Verdecchia", "Antonia Bertolino"], "DOIs": ["https://doi.org/10.1145/3180155.3180210", "http://ieeexplore.ieee.org/document/8453081"], "tag": ["Regression testing"], "abstract": "ABSTRACTMany test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes."}, {"id": "conf/icse/WangZCKBG18", "title": "Towards refactoring-aware regression test selection.", "authors": ["Kaiyuan Wang", "Chenguang Zhu", "Ahmet \u00c7elik", "Jongwook Kim", "Don S. Batory", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1145/3180155.3180254", "http://ieeexplore.ieee.org/document/8453082"], "tag": ["Regression testing"], "abstract": "ABSTRACTRegression testing checks that recent project changes do not break previously working functionality. Although important, regression testing is costly when changes are frequent. Regression test selection (RTS) optimizes regression testing by running only tests whose results might be affected by a change. Traditionally, RTS collects dependencies (e.g., on files) for each test and skips the tests, at a new project revision, whose dependencies did not change. Existing RTS techniques do not differentiate behavior-preserving transformations (i.e., refactorings) from other code changes. As a result, tests are run more frequently than necessary.We present the first step towards a refactoring-aware RTS technique, dubbed Reks, which skips tests affected only by behavior-preserving changes. Reks defines rules to update the test dependencies without running the tests. To ensure that Reks does not hide any bug introduced by the refactoring engines, we integrate Reks only in the pre-submit testing phase, which happens on the developers' machines. We evaluate Reks by measuring the savings in the testing effort. Specifically, we reproduce 100 refactoring tasks performed by developers of 37 projects on GitHub. Our results show that Reks would not run, on average, 33% of available tests (that would be run by a refactoring-unaware RTS technique). Additionally, we systematically run 27 refactoring types on ten projects. The results, based on 74,160 refactoring tasks, show that Reks would not run, on average, 16% of tests (max: 97% and SD: 24%). Finally, our results show that the Reks update rules are efficient."}, {"id": "conf/icse/StevensonW18", "title": "Inheritance usage patterns in open-source systems.", "authors": ["Jamie Stevenson", "Murray I. Wood"], "DOIs": ["https://doi.org/10.1145/3180155.3180168", "http://ieeexplore.ieee.org/document/8453083"], "tag": ["Open-source systems"], "abstract": "ABSTRACTThis research investigates how object-oriented inheritance is actually used in practice. The aim is to close the gap between inheritance guidance and inheritance practice. It is based on detailed analyses of 2440 inheritance hierarchies drawn from 14 open-source systems. The original contributions made by this paper concern pragmatic assessment of inheritance hierarchy design quality. The findings show that inheritance is very widely used but that most of the usage patterns that occur in practice are simple in structure. They are so simple that they may not require much inheritance-specific design consideration. On the other hand, the majority of classes defined using inheritance actually appear within a relatively small number of large, complex hierarchies. While some of these large hierarchies appear to have a consistent structure, often based on a problem domain model or a design pattern, others do not. Another contribution is that the quality of hierarchies, especially the large problematic ones, may be assessed in practice based on size, shape, and the definition and invocation of novel methods - all properties that can be detected automatically."}, {"id": "conf/icse/Steinmacher0WG18", "title": "Almost there: a study on quasi-contributors in open source software projects.", "authors": ["Igor Steinmacher", "Gustavo Pinto", "Igor Scaliante Wiese", "Marco Aur\u00e9lio Gerosa"], "DOIs": ["https://doi.org/10.1145/3180155.3180208", "http://ieeexplore.ieee.org/document/8453084"], "tag": ["Open-source systems"], "abstract": "ABSTRACTRecent studies suggest that well-known OSS projects struggle to find the needed workforce to continue evolving---in part because external developers fail to overcome their first contribution barriers. In this paper, we investigate how and why quasi-contributors (external developers who did not succeed in getting their contributions accepted to an OSS project) fail. To achieve our goal, we collected data from 21 popular, non-trivial GitHub projects, identified quasi-contributors, and analyzed their pull-requests. In addition, we conducted surveys with quasi-contributors, and projects' integrators, to understand their perceptions about nonacceptance. We found 10,099 quasi-contributors --- about 70% of the total actual contributors --- that submitted 12,367 nonaccepted pull-requests. In five projects, we found more quasi-contributors than actual contributors. About one-third of the developers who took our survey disagreed with the nonacceptance, and around 30% declared the nonacceptance demotivated or prevented them from placing another pull-request. The main reasons for pull-request nonacceptance from the quasi-contributors' perspective were \"superseded/duplicated pull-request\" and \"mismatch between developer's and team's vision/opinion.\" A manual analysis of a representative sample of 263 pull-requests corroborated with this finding. We also found reasons related to the relationship with the community and lack of experience or commitment from the quasi-contributors. This empirical study is particularly relevant to those interested in fostering developers' participation and retention in OSS communities."}, {"id": "conf/icse/BagherzadehKBHD18", "title": "Analyzing a decade of Linux system calls.", "authors": ["Mojtaba Bagherzadeh", "Nafiseh Kahani", "Cor-Paul Bezemer", "Ahmed E. Hassan", "Juergen Dingel", "James R. Cordy"], "DOIs": ["https://doi.org/10.1145/3180155.3182518", "http://ieeexplore.ieee.org/document/8453085"], "tag": ["Open-source systems"], "abstract": "ABSTRACTThe Linux kernel provides its services to the application layer using so-called system calls. All system calls combined form the Application Programming Interface (API) of the kernel. Hence, system calls provide us with a window into the development process and design decisions that are made for the Linux kernel. Our paper [1] presents the result of an empirical study of the changes (8,770) that were made to the system calls during the last decade (i.e., from April 2005 to December 2014). The main contributions and most important findings of our study are:(1) An overview of the Linux system calls. As of December 2014, 396 system calls existed in the Linux kernel. They can be categorized into 10 groups (process management, signal processing, and so on). 76 of the system calls were added over the last decade (new system calls). A new system call is usually not activated for all architectures at the same time. 40 out of 76 (53%) new system calls and 102 of the 393 (26%) existing system calls were sibling calls. A sibling call is a system call that is similar in functionality, and often in name, to another system call.(2) A study of the evolution of the Linux system calls over the last decade in terms of the size and type of changes that were made to the system calls. With an average growth of 25 LOC per day, the Linux system calls are relatively stable. The commits that are made to system calls are slightly more scattered than kernel commits. There exists a small group of very active system call developers. 8,288 of the 8,770 studied commits (95%) were made to maintain, improve and fix bugs in system calls. 36% of the system call-related commits were bug fixes. 4,498 (50%) of the commits were made to only 25 (6%) of the 393 system calls. 35% of the system call-related commits were made to conduct code restructuring, and 36% of the system call-related commits were made to fix bugs.(3) A study of the type of bug fixes that were made to the system calls over the last decade. Developers make mistakes in the seemingly trivial activation process of a system call. The steps that are required to activate a system call, such as assigning the unique number and updating the system call table, are performed manually. 58% of the bug fix commits were made to fix semantic bugs. The proportion of bug fixes that fixed memory-related bugs remained constant throughout the last decade.(4) An analysis of the results and a discussion of their implications.Generalizability of results. We compared Linux system calls with FreeBSD system calls, to validate that our results are generalizable to other UNIX-based operating systems. Our findings for the FreeBSD operating system confirm that other UNIX-based operating systems use a system call mechanism that is similar to that of Linux. Therefore, we can safely assume that our findings are of value to other UNIX-based operating systems.Suggestion for automation. First, we suggest the automation of simple, reoccurring tasks in Linux, such as adding and removing system calls. Our study on FreeBSD shows that such tasks can successfully be automated. Second, we suggest that historical information about the evolution of a kernel API should be used to guide the testing process. Finally, we suggest that existing automated testing tools are extended to support testing system calls.Maintenance Effort. Compared to regular software systems, kernel APIs require an additional type of maintenance that involves adding and removing system calls. Also, approximately 11% of the maintenance effort of a kernel API is assigned to the infrastructure for providing the API.Overall, the results of our study can be beneficial to practitioners, researchers, and more specifically kernel developers, by providing insights related to the challenges and problems that come with long term maintenance of a kernel API, such as the long-lived Linux kernel API. We have published our classification of 8,870 system call-related changes [1] so that it can be used to conduct further studies.The full paper is accepted for publication in the Empirical Software Engineering journal, and can be found at: https://link.springer.com/article/10.1007/s10664-017-9551-z."}, {"id": "conf/icse/VendomeGPBVP18", "title": "To distribute or not to distribute?: why licensing bugs matter.", "authors": ["Christopher Vendome", "Daniel M. Germ\u00e1n", "Massimiliano Di Penta", "Gabriele Bavota", "Mario Linares V\u00e1squez", "Denys Poshyvanyk"], "DOIs": ["https://doi.org/10.1145/3180155.3180221", "http://ieeexplore.ieee.org/document/8453086"], "tag": ["Open-source systems"], "abstract": "ABSTRACTSoftware licenses dictate how source code or binaries can be modified, reused, and redistributed. In the case of open source projects, software licenses generally fit into two main categories, permissive and restrictive, depending on the degree to which they allow redistribution or modification under licenses different from the original one(s). Developers and organizations can also modify existing licenses, creating custom licenses with specific permissive/restrictive terms. Having such a variety of software licenses can create confusion among software developers, and can easily result in the introduction of licensing bugs, not necessarily limited to well-known license incompatibilities. In this work, we report a study aimed at characterizing licensing bugs by (i) building a catalog categorizing the types of licensing bugs developers and other stakeholders face, and (ii) understanding the implications licensing bugs have on the software projects they affect. The presented study is the result of the manual analysis of 1,200 discussions related to licensing bugs carried out in issue trackers and in five legal mailing lists of open source communities. Our findings uncover new types of licensing bugs not addressed in prior literature, and a detailed assessment of their implications."}, {"id": "conf/icse/MarianiPZ18", "title": "Augusto: exploiting popular functionalities for the generation of semantic GUI tests with Oracles.", "authors": ["Leonardo Mariani", "Mauro Pezz\u00e8", "Daniele Zuddas"], "DOIs": ["https://doi.org/10.1145/3180155.3180162", "http://ieeexplore.ieee.org/document/8453087"], "tag": ["Test generation"], "abstract": "ABSTRACTTesting software applications by interacting with their graphical user interface (GUI) is an expensive and complex process. Current automatic test case generation techniques implement explorative approaches that, although producing useful test cases, have a limited capability of covering semantically relevant interactions, thus frequently missing important testing scenarios. These techniques typically interact with the available widgets following the structure of the GUI, without any guess about the functions that are executed.In this paper we propose Augusto, a test case generation technique that exploits a built-in knowledge of the semantics associated with popular and well-known functionalities, such as CRUD operations, to automatically generate effective test cases with automated functional oracles. Empirical results indicate that Augusto can reveal faults that cannot be revealed with state of the art techniques."}, {"id": "conf/icse/Wang0CZWL18", "title": "Towards optimal concolic testing.", "authors": ["Xinyu Wang", "Jun Sun", "Zhenbang Chen", "Peixin Zhang", "Jingyi Wang", "Yun Lin"], "DOIs": ["https://doi.org/10.1145/3180155.3180177", "http://ieeexplore.ieee.org/document/8453088"], "tag": ["Test generation"], "abstract": "ABSTRACTConcolic testing integrates concrete execution (e.g., random testing) and symbolic execution for test case generation. It is shown to be more cost-effective than random testing or symbolic execution sometimes. A concolic testing strategy is a function which decides when to apply random testing or symbolic execution, and if it is the latter case, which program path to symbolically execute. Many heuristics-based strategies have been proposed. It is still an open problem what is the optimal concolic testing strategy. In this work, we make two contributions towards solving this problem. First, we show the optimal strategy can be defined based on the probability of program paths and the cost of constraint solving. The problem of identifying the optimal strategy is then reduced to a model checking problem of Markov Decision Processes with Costs. Secondly, in view of the complexity in identifying the optimal strategy, we design a greedy algorithm for approximating the optimal strategy. We conduct two sets of experiments. One is based on randomly generated models and the other is based on a set of C programs. The results show that existing heuristics have much room to improve and our greedy algorithm often outperforms existing heuristics."}, {"id": "conf/icse/TianPJR18", "title": "DeepTest: automated testing of deep-neural-network-driven autonomous cars.", "authors": ["Yuchi Tian", "Kexin Pei", "Suman Jana", "Baishakhi Ray"], "DOIs": ["https://doi.org/10.1145/3180155.3180220", "http://ieeexplore.ieee.org/document/8453089"], "tag": ["Test generation"], "abstract": "ABSTRACTRecent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge."}, {"id": "conf/icse/KimCK18", "title": "Precise concolic unit testing of C programs using extended units and symbolic alarm filtering.", "authors": ["Yunho Kim", "Yunja Choi", "Moonzoo Kim"], "DOIs": ["https://doi.org/10.1145/3180155.3180253", "http://ieeexplore.ieee.org/document/8453090"], "tag": ["Test generation"], "abstract": "ABSTRACTAutomated unit testing reduces manual effort to write unit test drivers/stubs and generate unit test inputs. However, automatically generated unit test drivers/stubs raise false alarms because they often over-approximate real contexts of a target function f and allow infeasible executions of f. To solve this problem, we have developed a concolic unit testing technique CONBRIO. To provide realistic context to f, it constructs an extended unit of f that consists of f and closelyrelevant functions to f. Also, CONBRIO filters out a false alarm by checking feasibility of a corresponding symbolic execution path with regard to f's symbolic calling contexts obtained by combining symbolic execution paths of f's closely related predecessor functions.In the experiments on the crash bugs of 15 real-world C programs, CONBRIO shows both high bug detection ability (i.e. 91.0% of the target bugs detected) and high precision (i.e. a true to false alarm ratio is 1:4.5). Also, CONBRIO detects 14 new bugs in 9 target C programs studied in papers on crash bug detection techniques."}, {"id": "conf/icse/YanSCX18", "title": "Spatio-temporal context reduction: a pointer-analysis-based static approach for detecting use-after-free vulnerabilities.", "authors": ["Hua Yan", "Yulei Sui", "Shiping Chen", "Jingling Xue"], "DOIs": ["https://doi.org/10.1145/3180155.3180178", "http://ieeexplore.ieee.org/document/8453091"], "tag": ["Program reduction techniques"], "abstract": "ABSTRACTZero-day Use-After-Free (UAF) vulnerabilities are increasingly popular and highly dangerous, but few mitigations exist. We introduce a new pointer-analysis-based static analysis, CRed, for finding UAF bugs in multi-MLOC C source code efficiently and effectively. CRed achieves this by making three advances: (i) a spatio-temporal context reduction technique for scaling down soundly and precisely the exponential number of contexts that would otherwise be considered at a pair of free and use sites, (ii) a multi-stage analysis for filtering out false alarms efficiently, and (iii) a path-sensitive demand-driven approach for finding the points-to information required.We have implemented CRed in LLVM-3.8.0 and compared it with four different state-of-the-art static tools: CBMC (model checking), Clang (abstract interpretation), Coccinelle (pattern matching), and Supa (pointer analysis) using all the C test cases in Juliet Test Suite (JTS) and 10 open-source C applications. For the ground-truth validated with JTS, CRed detects all the 138 known UAF bugs as CBMC and Supa do while Clang and Coccinelle miss some bugs, with no false alarms from any tool. For practicality validated with the 10 applications (totaling 3+ MLOC), CRed reports 132 warnings including 85 bugs in 7.6 hours while the existing tools are either unscalable by terminating within 3 days only for one application (CBMC) or impractical by finding virtually no bugs (Clang and Coccinelle) or issuing an excessive number of false alarms (Supa)."}, {"id": "conf/icse/LuCJM18", "title": "Program splicing.", "authors": ["Yanxin Lu", "Swarat Chaudhuri", "Chris Jermaine", "David Melski"], "DOIs": ["https://doi.org/10.1145/3180155.3180190", "http://ieeexplore.ieee.org/document/8453092"], "tag": ["Program reduction techniques"], "abstract": "ABSTRACTWe introduce program splicing, a programming methodology that aims to automate the workflow of copying, pasting, and modifying code available online. Here, the programmer starts by writing a \"draft\" that mixes unfinished code, natural language comments, and correctness requirements. A program synthesizer that interacts with a large, searchable database of program snippets is used to automatically complete the draft into a program that meets the requirements. The synthesis process happens in two stages. First, the synthesizer identifies a small number of programs in the database that are relevant to the synthesis task. Next it uses an enumerative search to systematically fill the draft with expressions and statements from these relevant programs. The resulting program is returned to the programmer, who can modify it and possibly invoke additional rounds of synthesis.We present an implementation of program splicing, called Splicer, for the Java programming language. Splicer uses a corpus of over 3.5 million procedures from an open-source software repository. Our evaluation uses the system in a suite of everyday programming tasks, and includes a comparison with a state-of-the-art competing approach as well as a user study. The results point to the broad scope and scalability of program splicing and indicate that the approach can significantly boost programmer productivity."}, {"id": "conf/icse/TrabishMRC18", "title": "Chopped symbolic execution.", "authors": ["David Trabish", "Andrea Mattavelli", "Noam Rinetzky", "Cristian Cadar"], "DOIs": ["https://doi.org/10.1145/3180155.3180251", "http://ieeexplore.ieee.org/document/8453093"], "tag": ["Program reduction techniques"], "abstract": "ABSTRACTSymbolic execution is a powerful program analysis technique that systematically explores multiple program paths. However, despite important technical advances, symbolic execution often struggles to reach deep parts of the code due to the well-known path explosion problem and constraint solving limitations.In this paper, we propose chopped symbolic execution, a novel form of symbolic execution that allows users to specify uninteresting parts of the code to exclude during the analysis, thus only targeting the exploration to paths of importance. However, the excluded parts are not summarily ignored, as this may lead to both false positives and false negatives. Instead, they are executed lazily, when their effect may be observable by code under analysis. Chopped symbolic execution leverages various on-demand static analyses at runtime to automatically exclude code fragments while resolving their side effects, thus avoiding expensive manual annotations and imprecision.Our preliminary results show that the approach can effectively improve the effectiveness of symbolic execution in several different scenarios, including failure reproduction and test suite augmentation."}, {"id": "conf/icse/SunLZGS18", "title": "Perses: syntax-guided program reduction.", "authors": ["Chengnian Sun", "Yuanbo Li", "Qirun Zhang", "Tianxiao Gu", "Zhendong Su"], "DOIs": ["https://doi.org/10.1145/3180155.3180236", "http://ieeexplore.ieee.org/document/8453094"], "tag": ["Program reduction techniques"], "abstract": "ABSTRACTGiven a program P that exhibits a certain property \u03a8 (e.g., a C program that crashes GCC when it is being compiled), the goal of program reduction is to minimize P to a smaller variant P\u2032 that still exhibits the same property, i.e., \u03a8(P\u2032). Program reduction is important and widely demanded for testing and debugging. For example, all compiler/interpreter development projects need effective program reduction to minimize failure-inducing test programs to ease debugging. However, state-of-the-art program reduction techniques --- notably Delta Debugging (DD), Hierarchical Delta Debugging (HDD), and C-Reduce --- do not perform well in terms of speed (reduction time) and quality (size of reduced programs), or are highly customized for certain languages and thus lack generality.This paper presents Perses, a novel framework for effective, efficient, and general program reduction. The key insight is to exploit, in a general manner, the formal syntax of the programs under reduction and ensure that each reduction step considers only smaller, syntactically valid variants to avoid futile efforts on syntactically invalid variants. Our framework supports not only deletion (as for DD and HDD), but also general, effective program transformations.We have designed and implemented Perses, and evaluated it for two language settings: C and Java. Our evaluation results on 20 C programs triggering bugs in GCC and Clang demonstrate Perses's strong practicality compared to the state-of-the-art: (1) smaller size --- Perses's results are respectively 2% and 45% in size of those from DD and HDD; and (2) shorter reduction time --- Perses takes 23% and 47% time taken by DD and HDD respectively. Even when compared to the highly customized and optimized C-Reduce for C/C++, Perses takes only 38-60% reduction time."}, {"id": "conf/icse/MengNYZA18", "title": "Secure coding practices in Java: challenges and vulnerabilities.", "authors": ["Na Meng", "Stefan Nagy", "Danfeng (Daphne) Yao", "Wenjie Zhuang", "Gustavo Arango Argoty"], "DOIs": ["https://doi.org/10.1145/3180155.3180201", "http://ieeexplore.ieee.org/document/8453095"], "tag": ["Security, privacy and trust I"], "abstract": "ABSTRACTThe Java platform and its third-party libraries provide useful features to facilitate secure coding. However, misusing them can cost developers time and effort, as well as introduce security vulnerabilities in software. We conducted an empirical study on StackOverflow posts, aiming to understand developers' concerns on Java secure coding, their programming obstacles, and insecure coding practices.We observed a wide adoption of the authentication and authorization features provided by Spring Security---a third-party framework designed to secure enterprise applications. We found that programming challenges are usually related to APIs or libraries, including the complicated cross-language data handling of cryptography APIs, and the complex Java-based or XML-based approaches to configure Spring Security. In addition, we reported multiple security vulnerabilities in the suggested code of accepted answers on the StackOverfow forum. The vulnerabilities included disabling the default protection against Cross-Site Request Forgery (CSRF) attacks, breaking SSL/TLS security through bypassing certificate validation, and using insecure cryptographic hash functions. Our findings reveal the insufficiency of secure coding assistance and documentation, as well as the huge gap between security theory and coding practices."}, {"id": "conf/icse/0013P018", "title": "EnMobile: entity-based characterization and analysis of mobile malware.", "authors": ["Wei Yang", "Mukul R. Prasad", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3180155.3180223", "http://ieeexplore.ieee.org/document/8453096"], "tag": ["Security, privacy and trust I"], "abstract": "ABSTRACTModern mobile malware tend to conduct their malicious exploits through sophisticated patterns of interactions that involve multiple entities, e.g., the mobile platform, human users, and network locations. Such malware often evade the detection by existing approaches due to their limited expressiveness and accuracy in characterizing and detecting these malware. To address these issues, in this paper, we recognize entities in the environment of an app, the app's interactions with such entities, and the provenance of these interactions, i.e., the intent and ownership of each interaction, as the key to comprehensively characterizing modern mobile apps, and mobile malware in particular. With this insight, we propose a novel approach named EnMobile including a new entity-based characterization of mobile-app behaviors, and corresponding static analyses, to accurately characterize an app's interactions with entities. We implement EnMobile and provide a practical application of EnMobile in a signature-based scheme for detecting mobile malware. We evaluate EnMobile on a set of 6614 apps consisting of malware from Genome and Drebin along with benign apps from Google Play. Our results show that EnMobile detects malware with substantially higher precision and recall than four state-of-the-art approaches, namely Apposcopy, Drebin, MUDFLOW, and AppContext."}, {"id": "conf/icse/LabunetsMPMO18", "title": "Model comprehension for security risk assessment: an empirical comparison of tabular vs. graphical representations.", "authors": ["Katsiaryna Labunets", "Fabio Massacci", "Federica Paci", "Sabrina Marczak", "Fl\u00e1vio Moreira de Oliveira"], "DOIs": ["https://doi.org/10.1145/3180155.3182511", "http://ieeexplore.ieee.org/document/8453097"], "tag": ["Security, privacy and trust I"], "abstract": "ABSTRACTContext: Tabular and graphical representations are used to communicate security risk assessments for IT systems. However, there is no consensus on which type of representation better supports the comprehension of risks (such as the relationships between threats, vulnerabilities and security controls). Vessey's cognitive fit theory predicts that graphs should be better because they capture spatial relationships. Method: We report the results of two studies performed in two countries with 69 and 83 participants respectively, in which we assessed the effectiveness of tabular and graphical representations concerning the extraction of correct information about security risks. Results: Participants who applied tabular risk models gave more precise and complete answers to the comprehension questions when requested to find simple and complex information about threats, vulnerabilities, or other elements of the risk models. Conclusions: Our findings can be explained by Vessey's cognitive fit theory as tabular models implicitly capture elementary linear spatial relationships. Interest for ICSE: It is almost taken for granted in Software Engineering that graphical-, diagram-based models are \"the\" way to go (e.g., the SE Body of Knowledge [3]). This paper provides some experimental-based doubts that this might not always be the case. It will provide an interesting debate that might ripple to traditional requirements and design notations outside security."}, {"id": "conf/icse/HadarHATBSB18", "title": "Privacy by designers: software developers' privacy mindset.", "authors": ["Irit Hadar", "Tomer Hasson", "Oshrat Ayalon", "Eran Toch", "Michael Birnhack", "Sofia Sherman", "Arod Balissa"], "DOIs": ["https://doi.org/10.1145/3180155.3182531", "http://ieeexplore.ieee.org/document/8453098"], "tag": ["Security, privacy and trust I"], "abstract": "ABSTRACTPrivacy by design (PbD) is a policy measure that calls for embedding privacy into the design of technologies at early stages of the development process and throughout its lifecycle. By introducing privacy considerations into the technological design, PbD delegates responsibility over privacy to those in charge of the design, namely software developers who design information technologies (hereafter called developers). Thus, for PbD to be a viable option, it is important to understand developers' perceptions, interpretation and practices as to informational privacy."}, {"id": "conf/icse/Mayr-DornE18", "title": "Does the propagation of artifact changes across tasks reflect work dependencies?", "authors": ["Christoph Mayr-Dorn", "Alexander Egyed"], "DOIs": ["https://doi.org/10.1145/3180155.3180185", "http://ieeexplore.ieee.org/document/8453099"], "tag": ["Empirical software engineering"], "abstract": "ABSTRACTDevelopers commonly define tasks to help coordinate software development efforts---whether they be feature implementation, refactoring, or bug fixes. Developers establish links between tasks to express implicit dependencies that needs explicit handling---dependencies that often require the developers responsible for a given task to assess how changes in a linked task affect their own work and vice versa (i.e., change propagation). While seemingly useful, it is unknown if change propagation indeed coincides with task links.No study has investigated to what extent change propagation actually occurs between task pairs and whether it is able to serve as a metric for characterizing the underlying task dependency. In this paper, we study the temporal relationship between developer reading and changing of source code in relationship to task links We identify seven situations that explain the varying correlation of change propagation with linked task pairs and find six motifs describing when change propagation occurs between non-linked task pairs. Our paper demonstrates that task links are indeed useful for recommending which artifacts to monitor for changes, which developers to involve in a task, or which tasks to inspect."}, {"id": "conf/icse/FanSCMLXPS18", "title": "Large-scale analysis of framework-specific exceptions in Android apps.", "authors": ["Lingling Fan", "Ting Su", "Sen Chen", "Guozhu Meng", "Yang Liu", "Lihua Xu", "Geguang Pu", "Zhendong Su"], "DOIs": ["https://doi.org/10.1145/3180155.3180222", "http://ieeexplore.ieee.org/document/8453100"], "tag": ["Empirical software engineering"], "abstract": "ABSTRACTMobile apps have become ubiquitous. For app developers, it is a key priority to ensure their apps' correctness and reliability. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to guide developers, or help improve testing and analysis tools. However, such studies do not exist --- this paper fills this gap. Over a four-month long effort, we have collected 16,245 unique exception traces from 2,486 open-source Android apps, and observed that framework-specific exceptions account for the majority of these crashes. We then extensively investigated the 8,243 framework-specific exceptions (which took six person-months): (1) identifying their characteristics (e.g., manifestation locations, common fault categories), (2) evaluating their manifestation via state-of-the-art bug detection techniques, and (3) reviewing their fixes. Besides the insights they provide, these findings motivate and enable follow-up research on mobile apps, such as bug detection, fault localization and patch generation. In addition, to demonstrate the utility of our findings, we have optimized Stoat, a dynamic testing tool, and implemented ExLocator, an exception localization tool, for Android apps. Stoat is able to quickly uncover three previously-unknown, confirmed/fixed crashes in Gmail and Google+; ExLocator is capable of precisely locating the root causes of identified exceptions in real-world apps. Our substantial dataset is made publicly available to share with and benefit the community."}, {"id": "conf/icse/MadeyskiK18", "title": "Effect sizes and their variance for AB/BA crossover design studies.", "authors": ["Lech Madeyski", "Barbara A. Kitchenham"], "DOIs": ["https://doi.org/10.1145/3180155.3182556", "http://ieeexplore.ieee.org/document/8453101"], "tag": ["Empirical software engineering"], "abstract": "ABSTRACTWe addressed the issues related to repeated measures experimental design such as an AB/BA crossover design that have been neither discussed nor addressed in the software engineering literature.Firstly, there are potentially two different standardized mean difference effect sizes that can be calculated, depending on whether the mean difference is standardized by the pooled within groups variance or the within-participants variance. Hence, we provided equations for non-standardized and standardized effect sizes and explained the need for two different types of standardized effect size, one for the repeated measures and one that would be equivalent to an independent groups design.Secondly, as for any estimated parameters and also for the purposes of undertaking meta-analysis, it is necessary to calculate the variance of the standardized mean difference effect sizes (which is not the same as the variance of the study). Hence, we provided formulas for the small sample size effect size variance and the medium sample size approximation to the effect size variance, for both types of standardized effect size.We also presented the model underlying the AB/BA crossover design and provided two examples (an empirical analysis of the real data set by Scanniello, as well as simulated data) to demonstrate how to construct the two standardized mean difference effect sizes and their variances, both from standard descriptive statistics and from the outputs provided by the linear mixed model package lme4 in R.A conclusion is that crossover designs should be considered (instead of between groups design) only if:\u2022 previous research has suggested that \u03c1 is greater than zero and preferably greater than 0.25;\u2022 there is either strong theoretical argument, or empirical evidence from a well-powered study, that the period by technique interaction is negligible.Summarizing, our journal first paper [3]:(1) Presents the formulas needed to calculate both non-standardized and standardized mean difference effect sizes for AB/BA crossover designs (see Section 4 and 5 of our paper [3]).(2) Presents the formulas needed to estimate the variances of the non-standardized and standardized effect sizes which in the later cases need to be appropriate for the small to medium sample sizes commonly used in software engineering crossover designs (see Section 5 of our paper [3]).(3) Explains how to calculate the effect sizes and their variances both from the descriptive statistics that should be reported and from the raw data (see Section 6 of our paper [3]).It is worth mentioning that we based our formulas on our own corrections to the formulas presented earlier by Curtin et al. [1]. Our corrections for the variances of standardized weighted mean difference of an AB/BA cross-over trial were accepted by the author of the original formulas (Curtin), submitted jointly as a letter to Editor of Statistics in Medicine to assure the widespread (also beyond the software engineering domain) adoption of the corrected formulas, and accepted [2]. We proposed an alternative formulation of the standardized effect size for individual difference effects that is comparable with the standardized effect size commonly used for pretest/posttest studies. We also corrected the small sample size and moderate sample size variances reported by Curtin et al. for both the individual difference effect size and the standardized effect size comparable to independent groups trials, showing the derivation of the formulas from the variance of a t-variable. Using these results, researchers can now correctly calculate standardized effect size variances, allowing the calculation of confidence intervals for AB/BA cross-over trials, which in turn provides a direct link to null hypothesis testing and supports meta-analysis. Meta-analysts can now validly aggregate together results from independent groups, pretest/posttest and AB/BA cross-over trials. Last but not least, the presented contributions allow corrections of previously reported results."}, {"id": "conf/icse/HammadGM18", "title": "A large-scale empirical study on the effects of code obfuscations on Android apps and anti-malware products.", "authors": ["Mahmoud Hammad", "Joshua Garcia", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3180155.3180228", "http://ieeexplore.ieee.org/document/8453102"], "tag": ["Empirical software engineering"], "abstract": "ABSTRACTThe Android platform has been the dominant mobile platform in recent years resulting in millions of apps and security threats against those apps. Anti-malware products aim to protect smartphone users from these threats, especially from malicious apps. However, malware authors use code obfuscation on their apps to evade detection by anti-malware products. To assess the effects of code obfuscation on Android apps and anti-malware products, we have conducted a large-scale empirical study that evaluates the effectiveness of the top anti-malware products against various obfuscation tools and strategies. To that end, we have obfuscated 3,000 benign apps and 3,000 malicious apps and generated 73,362 obfuscated apps using 29 obfuscation strategies from 7 open-source, academic, and commercial obfuscation tools. The findings of our study indicate that (1) code obfuscation significantly impacts Android anti-malware products; (2) the majority of anti-malware products are severely impacted by even trivial obfuscations; (3) in general, combined obfuscation strategies do not successfully evade anti-malware products more than individual strategies; (4) the detection of anti-malware products depend not only on the applied obfuscation strategy but also on the leveraged obfuscation tool; (5) anti-malware products are slow to adopt signatures of malicious apps; and (6) code obfuscation often results in changes to an app's semantic behaviors."}, {"id": "conf/icse/AjienkaCC18", "title": "An empirical study on the interplay between semantic coupling and co-change of software classes.", "authors": ["Nemitari Ajienka", "Andrea Capiluppi", "Steve Counsell"], "DOIs": ["https://doi.org/10.1145/3180155.3190833", "http://ieeexplore.ieee.org/document/8453103"], "tag": ["Empirical software engineering"], "abstract": "ABSTRACTThe evolution of software systems is an inevitable process which has to be managed effectively to enhance software quality. Change impact analysis (CIA) is a technique that identifies impact sets, i.e., the set of classes that require correction as a result of a change made to a class or artefact. These sets can also be considered as ripple effects and typically non-local: changes propagate to different parts of a system.Two classes are considered logically coupled if they have co-changed in the past; past research has shown that the precision of CIA techniques increases if logical and semantic coupling (i.e., the extent to which the lexical content of two classes is related) are both considered. However, the relationship between semantic and logical coupling of software artefacts has not been extensively studied and no dependencies established between these two types of coupling. Are two often co-changed artefacts also strongly connected from a semantic point of view? Are two semantically similar artefacts bound to co-change in the future? Answering those questions would help increase the precision of CIA. It would also help software maintainers to focus on a smaller subset of artefacts more likely to co-evolve in the future.This study investigated the relationship between semantic and logical coupling. Using Chi-squared statistical tests, we identified similarities in semantic coupling using class corpora and class identifiers. We then computed Spearman's rank correlation between semantic and logical coupling metrics for class pairs to detect whether semantic and logical relationships co-varied in OO software. Finally, we investigated the overlap between semantic and logical relationships by identifying the proportion of classes linked through both coupling types. Our empirical study and results were based on seventy-nine open-source software projects. Results showed that: (a) measuring the semantic similarity of classes by using their identifiers is computationally efficient; (b) using identifier-based coupling can be used interchangeably with semantic similarity based on their corpora, albeit not always; (c) no correlation between the strengths of semantic and change coupling was found. Finally, (d) a directional relationship between the two was identified; 70% of semantic dependencies are linked through change coupling but not vice versa.Based on our findings, we conclude that identifying more efficient methods of semantic coupling computation as well as a directional relationship between semantic and change dependencies could help to improve CIA methods that integrate semantic coupling information. This may also help to reveal implicit dependencies not captured by static source code analysis."}, {"id": "conf/icse/BellLHEYM18", "title": "DeFlaker: automatically detecting flaky tests.", "authors": ["Jonathan Bell", "Owolabi Legunsen", "Michael Hilton", "Lamyaa Eloussi", "Tifany Yung", "Darko Marinov"], "DOIs": ["https://doi.org/10.1145/3180155.3180164", "http://ieeexplore.ieee.org/document/8453104"], "tag": ["Test improvement"], "abstract": "ABSTRACTDevelopers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle.We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1, 874 flaky tests from 4, 846 failures, with a low false alarm rate (1.5%). DeFlaker had a higher recall (95.5% vs. 23%) of confirmed flaky tests than Maven's default flaky test detector."}, {"id": "conf/icse/ChoiSNW18", "title": "DetReduce: minimizing Android GUI test suites for regression testing.", "authors": ["Wontae Choi", "Koushik Sen", "George C. Necula", "Wenyu Wang"], "DOIs": ["https://doi.org/10.1145/3180155.3180173", "http://ieeexplore.ieee.org/document/8453105"], "tag": ["Test improvement"], "abstract": "ABSTRACTIn recent years, several automated GUI testing techniques for Android apps have been proposed. These tools have been shown to be effective in achieving good test coverage and in finding bugs without human intervention. Being automated, these tools typically run for a long time (say, for several hours), either until they saturate test coverage or until a testing time budget expires. Thus, these automated tools are not good at generating concise regression test suites that could be used for testing in incremental development of the apps and in regression testing.We propose a heuristic technique that helps create a small regression test suite for an Android app from a large test suite generated by an automated Android GUI testing tool. The key insight behind our technique is that if we can identify and remove some common forms of redundancies introduced by existing automated GUI testing tools, then we can drastically lower the time required to minimize a GUI test suite. We have implemented our algorithm in a prototype tool called DetReduce. We applied DetReduce to several Android apps and found that DetReduce reduces a test-suite by an average factor of 16.9\u00d7 in size and 14.7\u00d7 in running time. We also found that for a test suite generated by running SwiftHand and a randomized test generation algorithm for 8 hours, DetReduce minimizes the test suite in an average of 14.6 hours."}, {"id": "conf/icse/MarcozziBKPPC18", "title": "Time to clean your test objectives.", "authors": ["Micha\u00ebl Marcozzi", "S\u00e9bastien Bardin", "Nikolai Kosmatov", "Mike Papadakis", "Virgile Prevosto", "Lo\u00efc Correnson"], "DOIs": ["https://doi.org/10.1145/3180155.3180191", "http://ieeexplore.ieee.org/document/8453106"], "tag": ["Test improvement"], "abstract": "ABSTRACTTesting is the primary approach for detecting software defects. A major challenge faced by testers lies in crafting efficient test suites, able to detect a maximum number of bugs with manageable effort. To do so, they rely on coverage criteria, which define some precise test objectives to be covered. However, many common criteria specify a significant number of objectives that occur to be infeasible or redundant in practice, like covering dead code or semantically equal mutants. Such objectives are well-known to be harmful to the design of test suites, impacting both the efficiency and precision of the tester's effort. This work introduces a sound and scalable technique to prune out a significant part of the infeasible and redundant objectives produced by a panel of white-box criteria. In a nutshell, we reduce this task to proving the validity of logical assertions in the code under test. The technique is implemented in a tool that relies on weakest-precondition calculus and SMT solving for proving the assertions. The tool is built on top of the Frama-C verification platform, which we carefully tune for our specific scalability needs. The experiments reveal that the pruning capabilities of the tool can reduce the number of targeted test objectives in a program by up to 27% and scale to real programs of 200K lines, making it possible to automate a painstaking part of their current testing process."}, {"id": "conf/icse/KwonKR18", "title": "Prioritizing browser environments for web application test execution.", "authors": ["Jung-Hyun Kwon", "In-Young Ko", "Gregg Rothermel"], "DOIs": ["https://doi.org/10.1145/3180155.3180244", "http://ieeexplore.ieee.org/document/8453107"], "tag": ["Test improvement"], "abstract": "ABSTRACTWhen testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial and popular open-source web applications. Our results show that our techniques outperform two baseline methods, namely, no ordering and random ordering, in terms of the cost-effectiveness. The improvement rates ranged from \u221212.24% to 39.05% for no ordering, and from \u22120.04% to 45.85% for random ordering."}, {"id": "conf/icse/LinBH18", "title": "An empirical study of early access games on the steam platform.", "authors": ["Dayi Lin", "Cor-Paul Bezemer", "Ahmed E. Hassan"], "DOIs": ["https://doi.org/10.1145/3180155.3182512", "http://ieeexplore.ieee.org/document/8453108"], "tag": ["Empirical studies of code"], "abstract": "ABSTRACT\"Early access\" is a release strategy for software that allows consumers to purchase an unfinished version of the software. In turn, consumers can influence the software development process by giving developers early feedback. This early access model has become increasingly popular through digital distribution platforms, such as Steam which is the most popular distribution platform for games. The plethora of options offered by Steam to communicate between developers and game players contribute to the popularity of the early access model.The early access model made a name for itself through several successful games, such as the DayZ game. The multiplayer survival-based game reached 400,000 sales during its first week as an early access game. However, the benefits of the early access model have been questioned as well. For instance, the Spacebase DF-9 game abandoned the early access stage unexpectedly, disappointing many players of the game. Shortly after abandoning the early access stage and terminating the development, twelve employees were laid off including the programmer and project lead.In this paper [1], we conduct an empirical study on 1,182 Early Access Games (EAGs) on the Steam platform to understand the characteristics, advantages and limitations of the early access model. We find that 15% of the games on Steam make use of the early access model, with the most popular EAG having as many as 29 million owners. 88% of the EAGs are classified by their developers as so-called \"indie\" games, indicating that most EAGs are developed by individual developers or small studios.We study the interaction between players and developers of EAGs and the Steam platform. We observe that on the one hand, developers update their games more frequently in the early access stage. On the other hand, the percentage of players that review a game during its early access stage is lower than the percentage of players that review the game after it leaves the early access stage. However, the average rating of the reviews is much higher during the early access stage, suggesting that players are more tolerant of imperfections in the early access stage. The positive review rate does not correlate with the length of the early access stage nor with the game update frequency during the early access stage.In addition, we discuss several learned lessons from the failure of an early access game. The main learned lesson from this failure is that communication between the game developer and the players of the EAG is crucial. Players enjoy getting involved in the development of an early access game and they get emotionally involved in the decision-making about the game.Based on our findings, we suggest game developers to use the early access model as a method for eliciting early feedback and more positive reviews to attract additional new players. In addition, our findings suggest that developers can determine their release schedule without worrying about the length of the early access stage and the game update frequency during the early access stage."}, {"id": "conf/icse/DanglotPBM18", "title": "Correctness attraction: a study of stability of software behavior under runtime perturbation.", "authors": ["Benjamin Danglot", "Philippe Preux", "Benoit Baudry", "Martin Monperrus"], "DOIs": ["https://doi.org/10.1145/3180155.3182548", "http://ieeexplore.ieee.org/document/8453109"], "tag": ["Empirical studies of code"], "abstract": "ABSTRACTCan the execution of software be perturbed without breaking the correctness of the output? In this paper, we devise a protocol to answer this question from a novel perspective. In an experimental study, we observe that many perturbations do not break the correctness in ten subject programs. We call this phenomenon \"correctness attraction\". The uniqueness of this protocol is that it considers a systematic exploration of the perturbation space as well as perfect oracles to determine the correctness of the output. To this extent, our findings on the stability of software under execution perturbations have a level of validity that has never been reported before in the scarce related work. A qualitative manual analysis enables us to set up the first taxonomy ever of the reasons behind correctness attraction."}, {"id": "conf/icse/PalombaBPFOL18", "title": "On the diffuseness and the impact on maintainability of code smells: a large scale empirical investigation.", "authors": ["Fabio Palomba", "Gabriele Bavota", "Massimiliano Di Penta", "Fausto Fasano", "Rocco Oliveto", "Andrea De Lucia"], "DOIs": ["https://doi.org/10.1145/3180155.3182532", "http://ieeexplore.ieee.org/document/8453110"], "tag": ["Empirical studies of code"], "abstract": "ABSTRACTCode smells were defined as symptoms of poor design choices applied by programmers during the development of a software project [2]. They might hinder the comprehensibility and maintainability of software systems [5]. Similarly to some previous work [3, 4, 6, 7] in this paper we investigate the relationship between the presence of code smells and the software change- and fault-proneness. Specifically, while previous work shows a significant correlation between smells and code change/fault-proneness, the empirical evidence provided so far is still limited because of:Limited size of previous studies: The study by Khomh et al. [4] was conducted on four open source systems, while the study by D'Ambros et al. [1] was performed on seven systems. Furthermore, the studies by Li and Shatnawi [6], Olbrich et al. [7], and Gatrell and Counsell [3] were conducted considering the change history of only one software project.Detected smells vs. manually validated smells: Previouswork studying the impact of code smells on change- and fault-proneness relied on data obtained from automatic smell detectors, whose imprecisions might have affected the results. Lack of analysis of the magnitude: Previouswork indicated that some smells can be more harmful than others, but the analysis did not take into account the magnitude of the observed phenomenon. For example, even if a specific smell type may be considered harmful when analyzing its impact on maintainability, this may not be relevant in case the number of occurrences of such a smell type in software projects is limited.Lack of analysis of the magnitude of the effect: Previouswork indicated that classes affected by code smells have more chances to exhibit defects (or to undergo changes) than other classes. However, no study has observed the magnitude of such changes and defects, i.e., no study addressed the question: How many defects would exhibit on average a class affected by a code smell as compared to another class affected by a different kind of smell, or not affected by any smell at all?Lack of within-artifact analysis: A class might be intrinsically change- and/or fault-prone, e.g., because it plays a core role in the system. Hence, the class may be intrinsically \"smelly\". Instead, there may be classes that become smelly during their lifetime because of maintenance activities. Or else, classes where the smell was removed, possibly because of refactoring activities. For such classes, it is of paramount importance to analyze the change- and fault-proneness of the class during its evolution, in order to better relate the cause (presence of smell) with the possible effect (change- or fault-proneness).Lack of a temporal relation analysis: While previouswork correlated the presence of code smells with high fault- and changeproneness, one may wonder whether the artifact was smelly when the fault was introduced, or whether the fault was introduced before the class became smelly.To cope with the aforementioned issues, this paper aims at corroborating previous empirical research on the impact of code smells by analyzing their diffuseness and effect on change- and faultproneness on a total of 395 releases of 30 open source systems, considering 13 different code smell types manually identified. Our results showed that classes affected by code smells tend to be significantly more change- and fault-prone than classes not affected by design problems, however their removal might be not always beneficial for improving source code maintainability."}, {"id": "conf/icse/TsantalisMEMD18", "title": "Accurate and efficient refactoring detection in commit history.", "authors": ["Nikolaos Tsantalis", "Matin Mansouri", "Laleh Mousavi Eshkevari", "Davood Mazinanian", "Danny Dig"], "DOIs": ["https://doi.org/10.1145/3180155.3180206", "http://ieeexplore.ieee.org/document/8453111"], "tag": ["Empirical studies of code"], "abstract": "ABSTRACTRefactoring detection algorithms have been crucial to a variety of applications: (i) empirical studies about the evolution of code, tests, and faults, (ii) tools for library API migration, (iii) improving the comprehension of changes and code reviews, etc. However, recent research has questioned the accuracy of the state-of-the-art refactoring detection tools, which poses threats to the reliability of their application. Moreover, previous refactoring detection tools are very sensitive to user-provided similarity thresholds, which further reduces their practical accuracy. In addition, their requirement to build the project versions/revisions under analysis makes them inapplicable in many real-world scenarios.To reinvigorate a previously fruitful line of research that has stifled, we designed, implemented, and evaluated RMiner, a technique that overcomes the above limitations. At the heart of RMiner is an AST-based statement matching algorithm that determines refactoring candidates without requiring user-defined thresholds. To empirically evaluate RMiner, we created the most comprehensive oracle to date that uses triangulation to create a dataset with considerably reduced bias, representing 3,188 refactorings from 185 open-source projects. Using this oracle, we found that RMiner has a precision of 98% and recall of 87%, which is a significant improvement over the previous state-of-the-art."}, {"id": "conf/icse/CalinescuWGIHK18", "title": "ENTRUST: engineering trustworthy self-adaptive software with dynamic assurance cases.", "authors": ["Radu Calinescu", "Danny Weyns", "Simos Gerasimou", "M. Usman Iftikhar", "Ibrahim Habli", "Tim Kelly"], "DOIs": ["https://doi.org/10.1145/3180155.3182540", "http://ieeexplore.ieee.org/document/8453112"], "tag": ["Security, privacy and trust II"], "abstract": "ABSTRACTSoftware systems are increasingly expected to cope with variable workloads, component failures and other uncertainties through self-adaptation. As such, self-adaptive software has been the subject of intense research over the past decade [3, 4, 9, 10]."}, {"id": "conf/icse/FreyRAPN18", "title": "The good, the bad and the ugly: a study of security decisions in a cyber-physical systems game.", "authors": ["Sylvain Frey", "Awais Rashid", "Pauline Anthonysamy", "Maria Pinto-Albuquerque", "Syed Asad Naqvi"], "DOIs": ["https://doi.org/10.1145/3180155.3182549", "http://ieeexplore.ieee.org/document/8453113"], "tag": ["Security, privacy and trust II"], "abstract": "ABSTRACTMotivation: The security of any system is a direct consequence of stakeholders' decisions regarding security requirements. Such decisions are taken with varying degrees of expertise, and little is currently understood about how various demographics - security experts, general computer scientists, managers - approach security decisions and the strategies that underpin those decisions. What are the typical decision patterns, the consequences of such patterns and their impact on the security of the system in question? Nor is there any substantial understanding of how the strategies and decision patterns of these different groups contrast. Is security expertise necessarily an advantage when making security decisions in a given context? Answers to these questions are key to understanding the \"how\" and \"why\" behind security decision processes.The Game: In this talk1, we present a tabletop game: Decisions and Disruptions (D-D)2 that tasks a group of players with managing the security of a small utility company while facing a variety of threats. The game is kept short - 2 hours - and simple enough to be played without prior training. A cyber-physical infrastructure, depicted through a Lego\u00ae board, makes the game easy to understand and accessible to players from varying backgrounds and security expertise, without being too trivial a setting for security experts.Key insights: We played D-D with 43 players divided into homogeneous groups: 4 groups of security experts, 4 groups of nontechnical managers and 4 groups of general computer scientists.\u2022 Strategies: Security experts had a strong interest in advanced technological solutions and tended to neglect intelligence gathering, to their own detriment. Managers, too, were technology-driven and focused on data protection while neglecting human factors more than other groups. Computer scientists tended to balance human factors and intelligence gathering with technical solutions, and achieved the best results of the three demographics.\u2022 Decision Processes: Technical experience significantly changes the way players think. Teams with little technical experience had shallow, intuition-driven discussions with few concrete arguments. Technical teams, and the most experienced in particular, had much richer debates, driven by concrete scenarios, anecdotes from experience, and procedural thinking. Security experts showed a high confidence in their decisions - despite some of them having bad consequences - while the other groups tended to doubt their own skills - even when they were playing good games.\u2022 Patterns: A number of characteristic plays were identified, some good (balance between priorities, open-mindedness, and adapting strategies based on inputs that challenge one's pre-conceptions), some bad (excessive focus on particular issues, confidence in charismatic leaders), some ugly (\"tunnel vision\" syndrome by over-confident players). These patterns are documented in the full paper - showing the virtue of the positive ones, discouraging the negative ones, and inviting the readers to do their own introspection.Conclusion: Beyond the analysis of the security decisions of the three demographics, there is a definite educational and awareness-raising aspect to D-D (as noted consistently by players in all our subject groups). Game boxes will be brought to the conference for demonstration purposes, and the audience will be invited to experiment with D-D themselves, make their own decisions, and reflect on their own perception of security."}, {"id": "conf/icse/GarciaHM18", "title": "Lightweight, obfuscation-resilient detection and family identification of Android malware.", "authors": ["Joshua Garcia", "Mahmoud Hammad", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3180155.3182551", "http://ieeexplore.ieee.org/document/8453114"], "tag": ["Security, privacy and trust II"], "abstract": "ABSTRACTThe number of malicious Android apps has been and continues to increase rapidly. These malware can damage or alter other files or settings, install additional applications, obfuscate their behaviors, propagate quickly, and so on. To identify and handle such malware, a security analyst can significantly benefit from identifying the family to which a malicious app belongs rather than only detecting if an app is malicious. To address these challenges, we present a novel machine learning-based Android malware detection and family-identification approach, RevealDroid, that operates without the need to perform complex program analyses or extract large sets of features. RevealDroid's selected features leverage categorized Android API usage, reflection-based features, and features from native binaries of apps. We assess RevealDroid for accuracy, efficiency, and obfuscation resilience using a large dataset consisting of more than 54,000 malicious and benign apps. Our experiments show that RevealDroid achieves an accuracy of 98% in detection of malware and an accuracy of 95% in determination of their families. We further demonstrate RevealDroid's superiority against state-of-the-art approaches. [URL of original paper: https://dl.acm.org/citation.cfm?id=3162625]"}, {"id": "conf/icse/MorrisonPXCW18", "title": "Are vulnerabilities discovered and resolved like other defects?", "authors": ["Patrick J. Morrison", "Rahul Pandita", "Xusheng Xiao", "Ram Chillarege", "Laurie Williams"], "DOIs": ["https://doi.org/10.1145/3180155.3182553", "http://ieeexplore.ieee.org/document/8453115"], "tag": ["Security, privacy and trust II"], "abstract": "ABSTRACTContext: Software defect data has long been used to drive software development process improvement. If security defects (i.e.,vulnerabilities) are discovered and resolved by different software development practices than non-security defects, the knowledge of that distinction could be applied to drive process improvement.Objective:The goal of this research is to support technical leaders in making security-specific software development process improvements by analyzing the differences between the discovery and resolution of defects versus that of vulnerabilities.Method: We extend Orthogonal Defect Classification (ODC) [1], a scheme for classifying software defects to support software development process improvement, to study process-related differences between vulnerabilities and defects, creating ODC + Vulnerabilities (ODC+V). We applied ODC+V to classify 583 vulnerabilities and 583 defects across 133 releases of three open-source projects (Firefox, phpMyAdmin, and Chrome).Results: Compared with defects, vulnerabilities are found later in the development cycle and are more likely to be resolved through changes to conditional logic. In Firefox, vulnerabilities are resolved 33% more quickly than defects. From a process improvement perspective, these results indicate opportunities may exist for more efficient vulnerability detection and resolution.Figures 1 and 2 present the percentage of defects and vulnerabilities found in each Activity for Firefox and phpMyAdmin, ordered from left to right as a timeline, first by pre-release, then by postrelease. In these projects, pre-release effort in vulnerability and defect detection correlates with pre-release vulnerability and defect resolution.Conclusion: We found ODC+V's property of associating vulnerability and defect discovery and resolution events with their software development process contexts helpful for gaining insight into three open source software projects. The addition of the Securitylmpact attribute, in particular, brought visibility into when threat types are discovered during the development process. We would expect use of ODC+V (and of base ODC) periodically over time to be helpful for steering software development projects toward their quality assurance goals.We give our full report in Morrison et al. [2] 1"}, {"id": "conf/icse/AnicheTSW0SG18", "title": "How modern news aggregators help development communities shape and share knowledge.", "authors": ["Maur\u00edcio Finavaro Aniche", "Christoph Treude", "Igor Steinmacher", "Igor Wiese", "Gustavo Pinto", "Margaret-Anne D. Storey", "Marco Aur\u00e9lio Gerosa"], "DOIs": ["https://doi.org/10.1145/3180155.3180180", "http://ieeexplore.ieee.org/document/8453116"], "tag": ["Communities and ecosystems"], "abstract": "ABSTRACTMany developers rely on modern news aggregator sites such as Reddit and Hacker News to stay up to date with the latest technological developments and trends. In order to understand what motivates developers to contribute, what kind of content is shared, and how knowledge is shaped by the community, we interviewed and surveyed developers that participate on the Reddit programming subreddit and we analyzed a sample of posts on both Reddit and Hacker News. We learned what kind of content is shared in these websites and developer motivations for posting, sharing, discussing, evaluating, and aggregating knowledge on these aggregators, while revealing challenges developers face in terms of how content and participant behavior is moderated. Our insights aim to improve the practices developers follow when using news aggregators, as well as guide tool makers on how to improve their tools. Our findings are also relevant to researchers that study developer communities of practice."}, {"id": "conf/icse/TrockmanZKV18", "title": "Adding sparkle to social coding: an empirical study of repository badges in the npm ecosystem.", "authors": ["Asher Trockman", "Shurui Zhou", "Christian K\u00e4stner", "Bogdan Vasilescu"], "DOIs": ["https://doi.org/10.1145/3180155.3180209", "http://ieeexplore.ieee.org/document/8453117"], "tag": ["Communities and ecosystems"], "abstract": "ABSTRACTIn fast-paced, reuse-heavy, and distributed software development, the transparency provided by social coding platforms like GitHub is essential to decision making. Developers infer the quality of projects using visible cues, known as signals, collected from personal profile and repository pages. We report on a large-scale, mixed-methods empirical study of npm packages that explores the emerging phenomenon of repository badges, with which maintainers signal underlying qualities about their projects to contributors and users. We investigate which qualities maintainers intend to signal and how well badges correlate with those qualities. After surveying developers, mining 294,941 repositories, and applying statistical modeling and time-series analyses, we find that non-trivial badges, which display the build status, test coverage, and up-to-dateness of dependencies, are mostly reliable signals, correlating with more tests, better pull requests, and fresher dependencies. Displaying such badges correlates with best practices, but the effects do not always persist."}, {"id": "conf/icse/GermanRPYII18", "title": "\"Was my contribution fairly reviewed?\": a framework to study the perception of fairness in modern code reviews.", "authors": ["Daniel M. Germ\u00e1n", "Gregorio Robles", "Germ\u00e1n Poo-Caama\u00f1o", "Xin Yang", "Hajimu Iida", "Katsuro Inoue"], "DOIs": ["https://doi.org/10.1145/3180155.3180217", "http://ieeexplore.ieee.org/document/8453118"], "tag": ["Communities and ecosystems"], "abstract": "ABSTRACTModern code reviews improve the quality of software products. Although modern code reviews rely heavily on human interactions, little is known regarding whether they are performed fairly. Fairness plays a role in any process where decisions that affect others are made. When a system is perceived to be unfair, it affects negatively the productivity and motivation of its participants. In this paper, using fairness theory we create a framework that describes how fairness affects modern code reviews. To demonstrate its applicability, and the importance of fairness in code reviews, we conducted an empirical study that asked developers of a large industrial open source ecosystem (OpenStack) what their perceptions are regarding fairness in their code reviewing process. Our study shows that, in general, the code review process in OpenStack is perceived as fair; however, a significant portion of respondents perceive it as unfair. We also show that the variability in the way they prioritize code reviews signals a lack of consistency and the existence of bias (potentially increasing the perception of unfairness). The contributions of this paper are: (1) we propose a framework---based on fairness theory---for studying and managing social behaviour in modern code reviews, (2) we provide support for the framework through the results of a case study on a large industrial-backed open source project, (3) we present evidence that fairness is an issue in the code review process of a large open source ecosystem, and, (4) we present a set of guidelines for practitioners to address unfairness in modern code reviews."}, {"id": "conf/icse/RuscioFMM18", "title": "Collaborative model-driven software engineering: a classification framework and a research map.", "authors": ["Davide Di Ruscio", "Mirco Franzago", "Henry Muccini", "Ivano Malavolta"], "DOIs": ["https://doi.org/10.1145/3180155.3182543", "http://ieeexplore.ieee.org/document/8453119"], "tag": ["Communities and ecosystems"], "abstract": "ABSTRACTThis proposal is about a study we recently published in the IEEE Transaction of Software Engineering journal [4].Context: Collaborative software engineering (CoSE) deals with methods, processes and tools for enhancing collaboration, communication, and co-ordination (3C) among team members [5]. CoSE can be employed to conceive different kinds of artifacts during the development and evolution of software systems. For instance, when focusing on software design, multiple stakeholders with different expertise and responsibility collaborate on the system design. Model-Driven Software Engineering (MDSE) provides suitable techniques and tools for specifying, manipulating, and analyzing modeling artifacts including metamodels, models, and transformations [1]. Collaborative MDSE consists of methods or techniques in which multiple stakeholders manage, collaborate, and are aware of each others' work on a set of shared models. A collaborative MDSE approach is composed of three main complementary dimensions: (i) a model management infrastructure for managing the life cycle of the models, (ii) a set of collaboration means for allowing involved stakeholders to work on the modelling artifacts collaboratively, and (iii) a set of communication means for allowing involved stakeholders to exchange, share, and communicate information within the team. Collaborative MDSE is attracting several research efforts from different research areas (e.g., model-driven engineering, global software engineering, etc.), resulting in a variegated scientific body of knowledge on the topic.Objective: In this study we aim at identifying, classifying, and understanding existing collaborative MDSE approaches. More specifically, our goal is to assess (i) the key characteristics of collaborative MDSE approaches (e.g., model editing environments, model versioning mechanisms, model repositories, support for communication and decision making), (ii) their faced challenges and limitations, and (iii) the interest of researchers in collaborative MDSE approaches over time and their focus on the three dimensions of collaborative MDSE.Method: In order to achieve this, we designed and conducted a systematic mapping study [6] on collaborative MDSE. Starting from over 3,000 potentially relevant studies, we applied a rigorous selection procedure [3] resulting in 106 selected papers, further clustered into 48 primary studies, along a time span of nineteen years. A suitable classification framework has been empirically defined and rigorously applied for extracting key information from each selected study. We collated, summarized, and analyzed extracted data by applying scientifically sound data synthesis techniques.Results: In addition to a number of specific insights, our analysis revealed the following key findings: (i) there is a growing scientific interest on collaborative MDSE in the last years; (ii) multi-view modeling, validation support, reuse, and branching are more rarely covered with respect to other aspects about collaborative MDSE; (iii) different primary studies focus differently on individual dimensions of collaborative MDSE (i.e., model management, collaboration, and communication); (iv) most approaches are language-specific, with a prominence of UML-based approaches; (v) few approaches support the interplay between synchronous and asynchronous collaboration.Conclusion: This study gives a solid foundation for a thorough identification and comparison of existing and future approaches for collaborative MDSE [2]. Those results can be used by both researchers and practitioners for identifying existing research/technical gaps to attack, better scoping their own contributions to the field, or better understanding or refining existing ones."}, {"id": "conf/icse/WuWCZ18", "title": "ChangeLocator: locate crash-inducing changes based on crash reports.", "authors": ["Rongxin Wu", "Ming Wen", "Shing-Chi Cheung", "Hongyu Zhang"], "DOIs": ["https://doi.org/10.1145/3180155.3182516", "http://ieeexplore.ieee.org/document/8453120"], "tag": ["Testing I"], "abstract": "ABSTRACTSoftware crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crash-inducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach."}, {"id": "conf/icse/PapadakisSYB18", "title": "Are mutation scores correlated with real fault detection?: a large scale empirical study on the relationship between mutants and real faults.", "authors": ["Mike Papadakis", "Donghwan Shin", "Shin Yoo", "Doo-Hwan Bae"], "DOIs": ["https://doi.org/10.1145/3180155.3180183", "http://ieeexplore.ieee.org/document/8453121"], "tag": ["Testing I"], "abstract": "ABSTRACTEmpirical validation of software testing studies is increasingly relying on mutants. This practice is motivated by the strong correlation between mutant scores and real fault detection that is reported in the literature. In contrast, our study shows that correlations are the results of the confounding effects of the test suite size. In particular, we investigate the relation between two independent variables, mutation score and test suite size, with one dependent variable the detection of (real) faults. We use two data sets, CoreBench and Defects4J, with large C and Java programs and real faults and provide evidence that all correlations between mutation scores and real fault detection are weak when controlling for test suite size. We also find that both independent variables significantly influence the dependent one, with significantly better fits, but overall with relative low prediction power. By measuring the fault detection capability of the top ranked, according to mutation score, test suites (opposed to randomly selected test suites of the same size), we find that achieving higher mutation scores improves significantly the fault detection. Taken together, our data suggest that mutants provide good guidance for improving the fault detection of test suites, but their correlation with fault detection are weak."}, {"id": "conf/icse/DutraLBS18", "title": "Efficient sampling of SAT solutions for testing.", "authors": ["Rafael Dutra", "Kevin Laeufer", "Jonathan Bachrach", "Koushik Sen"], "DOIs": ["https://doi.org/10.1145/3180155.3180248", "http://ieeexplore.ieee.org/document/8453122"], "tag": ["Testing I"], "abstract": "ABSTRACTIn software and hardware testing, generating multiple inputs which satisfy a given set of constraints is an important problem with applications in fuzz testing and stimulus generation. However, it is a challenge to perform the sampling efficiently, while generating a diverse set of inputs which satisfy the constraints. We developed a new algorithm QuickSampler which requires a small number of solver calls to produce millions of samples which satisfy the constraints with high probability. We evaluate QuickSampler on large real-world benchmarks and show that it can produce unique valid solutions orders of magnitude faster than other state-of-the-art sampling tools, with a distribution which is reasonably close to uniform in practice."}, {"id": "conf/icse/McIntoshK18", "title": "Are fix-inducing changes a moving target?: a longitudinal case study of just-in-time defect prediction.", "authors": ["Shane McIntosh", "Yasutaka Kamei"], "DOIs": ["https://doi.org/10.1145/3180155.3182514", "http://ieeexplore.ieee.org/document/8453123"], "tag": ["Testing I"], "abstract": "ABSTRACTChange-level defect prediction [5], a.k.a., Just-In-Time (JIT) defect prediction [1], is an alternative to module-level defect prediction that offers several advantages. First, since code changes are often smaller than modules (e.g., classes), JIT predictions are made at a finer granularity, which localizes the inspection process. Second, while modules have a group of authors, changes have only one, which makes triaging JIT predictions easier. Finally, unlike module level prediction, JIT models can scan changes as they are being produced, which means that problems can be investigated while design decisions are still fresh in the developers' minds.Despite the advantages of JIT defect prediction, like all prediction models, they assume that the properties of past events (fix-inducing changes) are similar to the properties of future ones. This assumption may not hold---the properties of fix-inducing changes in one time period may be different from those of another period. In our paper [4], we set out to address the following central question:Do the important properties of fix-inducing changes remain consistent as systems evolve?To address our central question, we train JIT models using six families of code change properties, which are primarily derived from prior studies [1-3, 5]. These properties measure: (a) the magnitude of the change (Size); (b) the dispersion of the changes across modules (Diffusion); (c) the defect proneness of prior changes to the modified modules (History); (d) the experience of the author (Auth. Exp.) and (e) code reviewer(s) (Rev. Exp.); and (f) the amount of participation in the review of the code change (Review).Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that the answer to our central question is no:\u2022 JIT models lose a large proportion of their discriminatory power (AUC) and calibration (Brier) scores one year after being trained.\u2022 The magnitude of the importance scores of code change properties fluctuate as systems evolve (e.g., Figure 1 shows fluctuations across six-month periods of OpenStack).\u2022 These fluctuations can lead to consistent overestimates (and underestimates) of the future impact of the studied families of code change properties.To mitigate the impact on model performance, researchers and practitioners should add recently accumulated data to the training set and retrain JIT models to contain fresh data from within the last three months. To better calibrate quality improvement plans (which are based on interpretation of the importance scores of code change properties), researchers and practitioners should put a greater emphasis on larger caches of data, which contain at least six months worth of data, to smooth the effect of spikes and troughs in the importance of properties of fix-inducing changes."}, {"id": "conf/icse/SawantADB18", "title": "Understanding developers' needs on deprecation as a language feature.", "authors": ["Anand Ashok Sawant", "Maur\u00edcio Finavaro Aniche", "Arie van Deursen", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1145/3180155.3180170", "http://ieeexplore.ieee.org/document/8453124"], "tag": ["Studying software engineers I"], "abstract": "ABSTRACTDeprecation is a language feature that allows API producers to mark a feature as obsolete. We aim to gain a deep understanding of the needs of API producers and consumers alike regarding deprecation. To that end, we investigate why API producers deprecate features, whether they remove deprecated features, how they expect consumers to react, and what prompts an API consumer to react to deprecation. To achieve this goal we conduct semi-structured interviews with 17 third-party Java API producers and survey 170 Java developers. We observe that the current deprecation mechanism in Java and the proposal to enhance it does not address all the needs of a developer. This leads us to propose and evaluate three further enhancements to the deprecation mechanism."}, {"id": "conf/icse/BellerSSZ18", "title": "On the dichotomy of debugging behavior among programmers.", "authors": ["Moritz Beller", "Niels Spruit", "Diomidis Spinellis", "Andy Zaidman"], "DOIs": ["https://doi.org/10.1145/3180155.3180175", "http://ieeexplore.ieee.org/document/8453125"], "tag": ["Studying software engineers I"], "abstract": "ABSTRACTDebugging is an inevitable activity in most software projects, often difficult and more time-consuming than expected, giving it the nickname the \"dirty little secret of computer science.\" Surprisingly, we have little knowledge on how software engineers debug software problems in the real world, whether they use dedicated debugging tools, and how knowledgeable they are about debugging. This study aims to shed light on these aspects by following a mixed-methods research approach. We conduct an online survey capturing how 176 developers reflect on debugging. We augment this subjective survey data with objective observations on how 458 developers use the debugger included in their integrated development environments (IDEs) by instrumenting the popular Eclipse and IntelliJ IDEs with the purpose-built plugin WatchDog 2.0. To clarify the insights and discrepancies observed in the previous steps, we followed up by conducting interviews with debugging experts and regular debugging users. Our results indicate that IDE-provided debuggers are not used as often as expected, as \"printf debugging\" remains a feasible choice for many programmers. Furthermore, both knowledge and use of advanced debugging features are low. These results call to strengthen hands-on debugging experience in computer science curricula and have already refined the implementation of modern IDE debuggers."}, {"id": "conf/icse/XiaBLXHL18", "title": "Measuring program comprehension: a large-scale field study with professionals.", "authors": ["Xin Xia", "Lingfeng Bao", "David Lo", "Zhenchang Xing", "Ahmed E. Hassan", "Shanping Li"], "DOIs": ["https://doi.org/10.1145/3180155.3182538", "http://ieeexplore.ieee.org/document/8453126"], "tag": ["Studying software engineers I"], "abstract": "ABSTRACTDuring software development and maintenance, developers spend a considerable amount of time on program comprehension. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions.In this paper [1], we perform a more realistic investigation of program comprehension activities. To do this, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We collect 3,148 working hour data from 78 professional developers in a field study. We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. Then we measure comprehension time by calculating the time that developers spend on program comprehension. We find that on average developers spend ~ 58% of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension."}, {"id": "conf/icse/Kim0DB18", "title": "Data scientists in software teams: state of the art and challenges.", "authors": ["Miryung Kim", "Thomas Zimmermann", "Robert DeLine", "Andrew Begel"], "DOIs": ["https://doi.org/10.1145/3180155.3182515", "http://ieeexplore.ieee.org/document/8453127"], "tag": ["Studying software engineers I"], "abstract": "ABSTRACTThe demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams. For example, Face-book, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams."}, {"id": "conf/icse/YuJ18", "title": "Dataflow tunneling: mining inter-request data dependencies for request-based applications.", "authors": ["Xiao Yu", "Guoliang Jin"], "DOIs": ["https://doi.org/10.1145/3180155.3180171", "http://ieeexplore.ieee.org/document/8453128"], "tag": ["Program analysis I"], "abstract": "ABSTRACTRequest-based applications, e.g., most server-side applications, expose services to users in a request-based paradigm, in which requests are served by request-handler methods. An important task for request-based applications is inter-request analysis, which analyzes request-handler methods that are related by inter-request data dependencies together. However, in the request-based paradigm, data dependencies between related request-handler methods are implicitly established by the underlying frameworks that execute these methods. As a result, existing analysis tools are usually limited to the scope of each single method without the knowledge of dependencies between different methods.In this paper, we design an approach called dataflow tunneling to capture inter-request data dependencies from concrete application executions and produce data-dependency specifications. Our approach answers two key questions: (1) what request-handler methods have data dependencies and (2) what these data dependencies are. Our evaluation using applications developed with two representative and popular frameworks shows that our approach is general and accurate. We also present a characteristic study and a use case of cache tuning based on the mined specifications. We envision that our approach can provide key information to enable future inter-request analysis techniques."}, {"id": "conf/icse/ZhangSX18", "title": "Launch-mode-aware context-sensitive activity transition analysis.", "authors": ["Yifei Zhang", "Yulei Sui", "Jingling Xue"], "DOIs": ["https://doi.org/10.1145/3180155.3180188", "http://ieeexplore.ieee.org/document/8453129"], "tag": ["Program analysis I"], "abstract": "ABSTRACTExisting static analyses model activity transitions in Android apps context-insensitively, making it impossible to distinguish different activity launch modes, reducing the pointer analysis precision for an activity's callbacks, and potentially resulting in infeasible activity transition paths. In this paper, we introduce Chime, a launch-mode-aware context-sensitive activity transition analysis that models different instances of an activity class according to its launch mode and the transitions between activities context-sensitively, by working together with an object-sensitive pointer analysis.Our evaluation shows that our context-sensitive activity transition analysis is more precise than its context-insensitive counterpart in capturing activity transitions, facilitating GUI testing, and improving the pointer analysis precision."}, {"id": "conf/icse/000118", "title": "UFO: predictive concurrency use-after-free detection.", "authors": ["Jeff Huang"], "DOIs": ["https://doi.org/10.1145/3180155.3180225", "http://ieeexplore.ieee.org/document/8453130"], "tag": ["Program analysis I"], "abstract": "ABSTRACTUse-After-Free (UAF) vulnerabilities are caused by the program operating on a dangling pointer and can be exploited to compromise critical software systems. While there have been many tools to mitigate UAF vulnerabilities, UAF remains one of the most common attack vectors. UAF is particularly difficult to detect in concurrent programs, in which a UAF may only occur with rare thread schedules. In this paper, we present a novel technique, UFO, that can precisely predict UAFs based on a single observed execution trace with a provably higher detection capability than existing techniques with no false positives. The key technical advancement of UFO is an extended maximal thread causality model that captures the largest possible set of feasible traces that can be inferred from a given multithreaded execution trace. By formulating UAF detection as a constraint solving problem atop this model, we can explore a much larger thread scheduling space than classical happens-before based techniques. We have evaluated UFO on several real-world large complex C/C++ programs including Chromium and FireFox. UFO scales to real-world systems with hundreds of millions of events in their execution and has detected a large number of real concurrency UAFs."}, {"id": "conf/icse/UpadhyayaR18", "title": "Collective program analysis.", "authors": ["Ganesha Upadhyaya", "Hridesh Rajan"], "DOIs": ["https://doi.org/10.1145/3180155.3180252", "http://ieeexplore.ieee.org/document/8453131"], "tag": ["Program analysis I"], "abstract": "ABSTRACTPopularity of data-driven software engineering has led to an increasing demand on the infrastructures to support efficient execution of tasks that require deeper source code analysis. While task optimization and parallelization are the adopted solutions, other research directions are less explored. We present collective program analysis (CPA), a technique for scaling large scale source code analyses, especially those that make use of control and data flow analysis, by leveraging analysis specific similarity. Analysis specific similarity is about, whether two or more programs can be considered similar for a given analysis. The key idea of collective program analysis is to cluster programs based on analysis specific similarity, such that running the analysis on one candidate in each cluster is sufficient to produce the result for others. For determining analysis specific similarity and clustering analysis-equivalent programs, we use a sparse representation and a canonical labeling scheme. Our evaluation shows that for a variety of source code analyses on a large dataset of programs, substantial reduction in the analysis time can be achieved; on average a 69% reduction when compared to a baseline and on average a 36% reduction when compared to a prior technique. We also found that a large amount of analysis-equivalent programs exists in large datasets."}, {"id": "conf/icse/PhanNTTNN18", "title": "Statistical learning of API fully qualified names in code snippets of online forums.", "authors": ["Hung Phan", "Hoan Anh Nguyen", "Ngoc M. Tran", "Linh H. Truong", "Anh Tuan Nguyen", "Tien N. Nguyen"], "DOIs": ["https://doi.org/10.1145/3180155.3180230", "http://ieeexplore.ieee.org/document/8453132"], "tag": ["Human and social aspects of computing II"], "abstract": "ABSTRACTSoftware developers often make use of the online forums such as StackOverflow (SO) to learn how to use software libraries and their APIs. However, the code snippets in such a forum often contain undeclared, ambiguous, or largely unqualified external references. Such declaration ambiguity and external reference ambiguity present challenges for developers in learning to correctly use the APIs. In this paper, we propose StatType, a statistical approach to resolve the fully qualified names (FQNs) for the API elements in such code snippets. Unlike existing approaches that are based on heuristics, StatType has two well-integrated factors. We first learn from a large training code corpus the FQNs that often co-occur. Then, to derive the FQN for an API name in a code snippet, we use that knowledge and also leverage the context consisting of neighboring API names. To realize those factors, we treat the problem as statistical machine translation from source code with partially qualified names to source code with FQNs of the APIs. Our empirical evaluation on real-world code and StackOverflow posts shows that StatType achieves very high accuracy with 97.6% precision and 96.7% recall, which is 16.5% relatively higher than the state-of-the-art approach."}, {"id": "conf/icse/HeadSMK18", "title": "When not to comment: questions and tradeoffs with API documentation for C++ projects.", "authors": ["Andrew Head", "Caitlin Sadowski", "Emerson R. Murphy-Hill", "Andrea Knight"], "DOIs": ["https://doi.org/10.1145/3180155.3180176", "http://ieeexplore.ieee.org/document/8453133"], "tag": ["Human and social aspects of computing II"], "abstract": "ABSTRACTWithout usable and accurate documentation of how to use an API, developers can find themselves deterred from reusing relevant code. In C++, one place developers can find documentation is in a header file. When information is missing, they may look at the corresponding implementation code. To understand what's missing from C++ API documentation and the factors influencing whether it will be fixed, we conducted a mixed-methods study involving two experience sampling surveys with hundreds of developers at the moment they visited implementation code, interviews with 18 of those developers, and interviews with 8 API maintainers. In many cases, updating documentation may provide only limited value for developers, while requiring effort maintainers don't want to invest. We identify a set of questions maintainers and tool developers should consider when improving API-level documentation."}, {"id": "conf/icse/HempelLLC18", "title": "Deuce: a lightweight user interface for structured editing.", "authors": ["Brian Hempel", "Justin Lubin", "Grace Lu", "Ravi Chugh"], "DOIs": ["https://doi.org/10.1145/3180155.3180165", "http://ieeexplore.ieee.org/document/8453134"], "tag": ["Human and social aspects of computing II"], "abstract": "ABSTRACTWe present a structure-aware code editor, called Deuce, that is equipped with direct manipulation capabilities for invoking automated program transformations. Compared to traditional refactoring environments, DEUCE employs a direct manipulation interface that is tightly integrated within a text-based editing workflow. In particular, Deuce draws (i) clickable widgets atop the source code that allow the user to structurally select the unstructured text for subexpressions and other relevant features, and (ii) a lightweight, interactive menu of potential transformations based on the current selections. We implement and evaluate our design with mostly standard transformations in the context of a small functional programming language. A controlled user study with 21 participants demonstrates that structural selection is preferred to a more traditional text-selection interface and may be faster overall once users gain experience with the tool. These results accord with Deuce's aim to provide human-friendly structural interactions on top of familiar text-based editing."}, {"id": "conf/icse/ChenSMXL18", "title": "From UI design image to GUI skeleton: a neural machine translator to bootstrap mobile GUI implementation.", "authors": ["Chunyang Chen", "Ting Su", "Guozhu Meng", "Zhenchang Xing", "Yang Liu"], "DOIs": ["https://doi.org/10.1145/3180155.3180240", "http://ieeexplore.ieee.org/document/8453135"], "tag": ["Human and social aspects of computing II"], "abstract": "ABSTRACTA GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach."}, {"id": "conf/icse/SpadiniASBB18", "title": "When testing meets code review: why and how developers review tests.", "authors": ["Davide Spadini", "Maur\u00edcio Finavaro Aniche", "Margaret-Anne D. Storey", "Magiel Bruntink", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1145/3180155.3180192", "http://ieeexplore.ieee.org/document/8453136"], "tag": ["Testing II"], "abstract": "ABSTRACTAutomated testing is considered an essential process for ensuring software quality. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. For production code, many open source and industrial software projects employ code review, a well-established software quality practice, but the question remains whether and how code review is also used for ensuring the quality of test code. The aim of this research is to answer this question and to increase our understanding of what developers think and do when it comes to reviewing test code. We conducted both quantitative and qualitative methods to analyze more than 300,000 code reviews, and interviewed 12 developers about how they review test files. This work resulted in an overview of current code reviewing practices, a set of identified obstacles limiting the review of test code, and a set of issues that developers would like to see improved in code review tools. The study reveals that reviewing test files is very different from reviewing production files, and that the navigation within the review itself is one of the main issues developers currently face. Based on our findings, we propose a series of recommendations and suggestions for the design of tools and future research."}, {"id": "conf/icse/LiangER18", "title": "Redefining prioritization: continuous prioritization for continuous integration.", "authors": ["Jingjing Liang", "Sebastian G. Elbaum", "Gregg Rothermel"], "DOIs": ["https://doi.org/10.1145/3180155.3180213", "http://ieeexplore.ieee.org/document/8453137"], "tag": ["Testing II"], "abstract": "ABSTRACTContinuous integration (CI) development environments allow software engineers to frequently integrate and test their code. While CI environments provide advantages, they also utilize non-trivial amounts of time and resources. To address this issue, researchers have adapted techniques for test case prioritization (TCP) to CI environments. To date, however, the techniques considered have operated on test suites, and have not achieved substantial improvements. Moreover, they can be inappropriate to apply when system build costs are high. In this work we explore an alternative: prioritization of commits. We use a lightweight approach based on test suite failure and execution history that is highly efficient; our approach \"continuously\" prioritizes commits that are waiting for execution in response to the arrival of each new commit and the completion of each previously scheduled commit. We have evaluated our approach on three non-trivial CI data sets. Our results show that our approach can be more effective than prior techniques."}, {"id": "conf/icse/BenninKPMM18", "title": "MAHAKIL: diversity based oversampling approach to alleviate the class imbalance issue in software defect prediction.", "authors": ["Kwabena Ebo Bennin", "Jacky Keung", "Passakorn Phannachitta", "Akito Monden", "Solomon Mensah"], "DOIs": ["https://doi.org/10.1145/3180155.3182520", "http://ieeexplore.ieee.org/document/8453138"], "tag": ["Testing II"], "abstract": "ABSTRACTThis study presents MAHAKIL, a novel and efficient synthetic over-sampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with five other sampling approaches using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant pf values than the other oversampling approaches, based on robust statistical tests."}, {"id": "conf/icse/HabayebMMB18", "title": "On the use of hidden Markov model to predict the time to fix bugs.", "authors": ["Mayy Habayeb", "Syed Shariyar Murtaza", "Andriy V. Miranskyy", "Ayse Basar Bener"], "DOIs": ["https://doi.org/10.1145/3180155.3182522", "http://ieeexplore.ieee.org/document/8453139"], "tag": ["Testing II"], "abstract": "ABSTRACTA significant amount of time is spent by software developers in investigating bug reports. It is useful to indicate when a bug report will be closed, since it would help software teams to prioritise their work. Several studies have been conducted to address this problem in the past decade. Most of these studies have used the frequency of occurrence of certain developer activities as input attributes in building their prediction models. However, these approaches tend to ignore the temporal nature of the occurrence of these activities. In this paper, a novel approach using Hidden Markov models (HMMs) and temporal sequences of developer activities is proposed. The approach is empirically demonstrated in a case study using eight years of bug reports collected from the Firefox project. We provide additional details below. In a software bug repository, recorded developer activities occur sequentially. For example, activity C (a certain person has been copied on the bug report) is followed by activity A (bug confirmed and assigned to a named developer), which in turn is followed by activity Z (bug reached status resolved). Additional piece of information is developers' level of expertise, such as novice (N), intermediate (M), or experienced (E), at the time of report creation. We combine these data together to produce a sequence of temporal activities associated with bug reports in the Firefox bug repository."}, {"id": "conf/icse/KalliamvakouB0B18", "title": "What makes a great manager of software engineers?", "authors": ["Eirini Kalliamvakou", "Christian Bird", "Thomas Zimmermann", "Andrew Begel", "Robert DeLine", "Daniel M. Germ\u00e1n"], "DOIs": ["https://doi.org/10.1145/3180155.3182525", "http://ieeexplore.ieee.org/document/8453140"], "tag": ["Studying software engineers II"], "abstract": "ABSTRACTHaving great managers is as critical to success as having a good team or organization. A great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skill), it has overlooked the software engineering manager. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups."}, {"id": "conf/icse/KopecBNKWC18", "title": "Older adults and hackathons: a qualitative study.", "authors": ["Wieslaw Kopec", "Bartlomiej Balcerzak", "Radoslaw Nielek", "Grzegorz Kowalik", "Adam Wierzbicki", "Fabio Casati"], "DOIs": ["https://doi.org/10.1145/3180155.3182547", "http://ieeexplore.ieee.org/document/8453141"], "tag": ["Studying software engineers II"], "abstract": "ABSTRACTGlobally observed trends in aging indicate that older adults constitute a growing share of the population and an increasing demographic in the modern technologies marketplace. Therefore, it has become important to address the issue of participation of older adults in the process of developing solutions suitable for their group. In this study, we approached this topic by organizing a hackathon involving teams of young programmers and older adult participants. In our paper we describe a case study of that hackathon, in which our objective was to motivate older adults to participate in software engineering processes. Based on our results from an array of qualitative methods, we propose a set of good practices that may lead to improved older adult participation in similar events and an improved process of developing apps that target older adults."}, {"id": "conf/icse/HannebauerHG18", "title": "Does syntax highlighting help programming novices?", "authors": ["Christoph Hannebauer", "Marc Hesenius", "Volker Gruhn"], "DOIs": ["https://doi.org/10.1145/3180155.3182554", "http://ieeexplore.ieee.org/document/8453142"], "tag": ["Studying software engineers II"], "abstract": "ABSTRACTProgram comprehension is an important skill for programmers - extending and debugging existing source code is part of the daily routine. Syntax highlighting is one of the most common tools used to support developers in understanding algorithms. However, most research on code highlighting is more than 20 years old, when programmers used a completely different tool chain. Newer results on the effect of syntax highlighting as used in modern Integrated Development Environments (IDEs) are inconclusive."}, {"id": "conf/icse/ClaesMKA18", "title": "Do programmers work at night or during the weekend?", "authors": ["Ma\u00eblick Claes", "Mika V. M\u00e4ntyl\u00e4", "Miikka Kuutila", "Bram Adams"], "DOIs": ["https://doi.org/10.1145/3180155.3180193", "http://ieeexplore.ieee.org/document/8453143"], "tag": ["Studying software engineers II"], "abstract": "ABSTRACTAbnormal working hours can reduce work health, general well-being, and productivity, independent from a profession. To inform future approaches for automatic stress and overload detection, this paper establishes empirically collected measures of the work patterns of software engineers. To this aim, we perform the first large-scale study of software engineers' working hours by investigating the time stamps of commit activities of 86 large open source software projects, both containing hired and volunteer developers. We find that two thirds of software engineers mainly follow typical office hours, empirically established to be from 10h to 18h, and do not usually work during nights and weekends. Large variations between projects and individuals exist. Surprisingly, we found no support that project maturation would decrease abnormal working hours. In the Firefox case study, we found that hired developers work more during office hours while seniority, either in terms of number of commits or job status, did not impact working hours. We conclude that the use of working hours or timestamps of work products for stress detection requires establishing baselines at the level of individuals."}, {"id": "conf/icse/Lambers0TBH18", "title": "Multi-granular conflict and dependency analysis in software engineering based on graph transformation.", "authors": ["Leen Lambers", "Daniel Str\u00fcber", "Gabriele Taentzer", "Kristopher Born", "Jevgenij Huebert"], "DOIs": ["https://doi.org/10.1145/3180155.3180258", "http://ieeexplore.ieee.org/document/8453144"], "tag": ["Program analysis II"], "abstract": "ABSTRACTConflict and dependency analysis (CDA) of graph transformation has been shown to be a versatile foundation for understanding interactions in many software engineering domains, including software analysis and design, model-driven engineering, and testing. In this paper, we propose a novel static CDA technique that is multi-granular in the sense that it can detect all conflicts and dependencies on multiple granularity levels. Specifically, we provide an efficient algorithm suite for computing binary, coarse-grained, and fine-grained conflicts and dependencies: Binary granularity indicates the presence or absence of conflicts and dependencies, coarse granularity focuses on root causes for conflicts and dependencies, and fine granularity shows each conflict and dependency in full detail. Doing so, we can address specific performance and usability requirements that we identified in a literature survey of CDA usage scenarios. In an experimental evaluation, our algorithm suite computes conflicts and dependencies rapidly. Finally, we present a user study, in which the participants found our coarse-grained results more understandable than the fine-grained ones reported in a state-of-the-art tool. Our overall contribution is twofold: (i) we significantly speed up the computation of fine-grained and binary CDA results and, (ii) complement them with coarse-grained ones, which offer usability benefits for numerous use cases."}, {"id": "conf/icse/ShanNS18", "title": "Self-hiding behavior in Android apps: detection and characterization.", "authors": ["Zhiyong Shan", "Iulian Neamtiu", "Raina Samuel"], "DOIs": ["https://doi.org/10.1145/3180155.3180214", "http://ieeexplore.ieee.org/document/8453145"], "tag": ["Program analysis II"], "abstract": "ABSTRACTApplications (apps) that conceal their activities are fundamentally deceptive; app marketplaces and end-users should treat such apps as suspicious. However, due to its nature and intent, activity concealing is not disclosed up-front, which puts users at risk. In this paper, we focus on characterization and detection of such techniques, e.g., hiding the app or removing traces, which we call \"self hiding behavior\" (SHB). SHB has not been studied per se - rather it has been reported on only as a byproduct of malware investigations. We address this gap via a study and suite of static analyses targeted at SH in Android apps. Specifically, we present (1) a detailed characterization of SHB, (2) a suite of static analyses to detect such behavior, and (3) a set of detectors that employ SHB to distinguish between benign and malicious apps. We show that SHB ranges from hiding the app's presence or activity to covering an app's traces, e.g., by blocking phone calls/text messages or removing calls and messages from logs. Using our static analysis tools on a large dataset of 9,452 Android apps (benign as well as malicious) we expose the frequency of 12 such SH behaviors. Our approach is effective: it has revealed that malicious apps employ 1.5 SHBs per app on average. Surprisingly, SH behavior is also employed by legitimate (\"benign\") apps, which can affect users negatively in multiple ways. When using our approach for separating malicious from benign apps, our approach has high precision and recall (combined F-measure = 87.19%). Our approach is also efficient, with analysis typically taking just 37 seconds per app. We believe that our findings and analysis tool are beneficial to both app marketplaces and end-users."}, {"id": "conf/icse/PalombaPZOL18", "title": "The scent of a smell: an extensive comparison between textual and structural smells.", "authors": ["Fabio Palomba", "Annibale Panichella", "Andy Zaidman", "Rocco Oliveto", "Andrea De Lucia"], "DOIs": ["https://doi.org/10.1145/3180155.3182530", "http://ieeexplore.ieee.org/document/8453146"], "tag": ["Program analysis II"], "abstract": "ABSTRACTCode smells, i.e., symptoms of poor design and implementation choices applied by programmers during the development of a software project [2], represent an important factor contributing to technical debt [3]. The research community spent a lot of effort studying the extent to which code smells tend to remain in a software project for long periods of time [9], as well as their negative impact on non-functional properties of source code [4, 7]. As a consequence, several tools and techniques have been proposed to help developers in detecting code smells and to suggest refactoring opportunities (e.g., [5, 6, 8]).So far, almost all detectors identify code smells using structural properties of source code. However, recent studies have indicated that code smells detected by existing tools are generally ignored (and thus not refactored) by the developers [1]. A possible reason is that developers do not perceive the code smells identified by the tool as actual design problems or, if they do, they are not able to practically work on such code smells. In other words, there is misalignment between what is considered smelly by the tool and what is actually refactorable by developers.In a previous paper [6], we introduced a tool named TACO that uses textual analysis to detect code smells. The results indicated that textual and structural techniques are complementary: while some code smell instances in a software system can be correctly identified by both TACO and the alternative structural approaches, other instances can be only detected by one of the two [6].In this paper, we investigate whether code smells detected using textual information are as difficult to identify and refactor as structural smells or if they follow a different pattern during software evolution. We firstly performed a repository mining study considering 301 releases and 183,514 commits from 20 open source projects (i) to verify whether textually and structurally detected code smells are treated differently, and (ii) to analyze their likelihood of being resolved with regards to different types of code changes, e.g., refactoring operations. Since our quantitative study cannot explain relation and causation between code smell types and maintenance activities, we perform a qualitative study with 19 industrial developers and 5 software quality experts in order to understand (i) how code smells identified using different sources of information are perceived, and (ii) whether textually or structurally detected code smells are easier to refactor. In both studies, we focused on five code smell types, i.e., Blob, Feature Envy, Long Method, Misplaced Class, and Promiscuous Package.The results of our studies indicate that textually detected code smells are perceived as harmful as the structural ones, even though they do not exceed any typical software metrics' value (e.g., lines of code in a method). Moreover, design problems in source code affected by textual-based code smells are easier to identify and refactor. As a consequence, developers' activities tend to decrease the intensity of textual code smells, positively impacting their likelihood of being resolved. Vice versa, structural code smells typically increase in intensity over time, indicating that maintenance operations are not aimed at removing or limiting them. Indeed, while developers perceive source code affected by structural-based code smells as harmful, they face more problems in correctly identifying the actual design problems affecting these code components and/or the right refactoring operation to apply to remove them."}, {"id": "conf/icse/PatraDP18", "title": "ConflictJS: finding and understanding conflicts between JavaScript libraries.", "authors": ["Jibesh Patra", "Pooja N. Dixit", "Michael Pradel"], "DOIs": ["https://doi.org/10.1145/3180155.3180184", "http://ieeexplore.ieee.org/document/8453147"], "tag": ["Program analysis II"], "abstract": "ABSTRACTIt is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects."}, {"id": "conf/icse/BankenMG18", "title": "Debugging data flows in reactive programs.", "authors": ["Herman Banken", "Erik Meijer", "Georgios Gousios"], "DOIs": ["https://doi.org/10.1145/3180155.3180156", "http://ieeexplore.ieee.org/document/8453148"], "tag": ["Software comprehension"], "abstract": "ABSTRACTReactive Programming is a style of programming that provides developers with a set of abstractions that facilitate event handling and stream processing. Traditional debug tools lack support for Reactive Programming, leading developers to fallback to the most rudimentary debug tool available: logging to the console.In this paper, we present the design and implementation of RxFiddle, a visualization and debugging tool targeted to Rx, the most popular form of Reactive Programming. RxFiddle visualizes the dependencies and structure of the data flow, as well as the data inside the flow. We evaluate RxFiddle with an experiment involving 111 developers. The results show that RxFiddle can help developers finish debugging tasks faster than with traditional debugging tools."}, {"id": "conf/icse/KrugerWFSL18", "title": "Do you remember this source code?", "authors": ["Jacob Kr\u00fcger", "Jens Wiemann", "Wolfram Fenske", "Gunter Saake", "Thomas Leich"], "DOIs": ["https://doi.org/10.1145/3180155.3180215", "http://ieeexplore.ieee.org/document/8453149"], "tag": ["Software comprehension"], "abstract": "ABSTRACTBeing familiar with the source code of a program comprises knowledge about its purpose, structure, and details. Consequently, familiarity is an important factor in many contexts of software development, especially for maintenance and program comprehension. As a result, familiarity is considered to some extent in many different approaches, for example, to model costs or to identify experts. Still, all approaches we are aware of require a manual assessment of familiarity and empirical analyses of forgetting in software development are missing. In this paper, we address this issue with an empirical study that we conducted with 60 open-source developers. We used a survey to receive information on the developers' familiarity and analyze the responses based on data we extract from their used version control systems. The results show that forgetting is an important factor when considering familiarity and program comprehension of developers. We find that a forgetting curve is partly applicable for software development, investigate three factors - the number of edits, ratio of owned code, and tracking behavior - that can impact familiarity with code, and derive a general memory strength for our participants. Our findings can be used to scope approaches that have to consider familiarity and they provide insights into forgetting in the context of software development."}, {"id": "conf/icse/Alimadadi0P18", "title": "Inferring hierarchical motifs from execution traces.", "authors": ["Saba Alimadadi", "Ali Mesbah", "Karthik Pattabiraman"], "DOIs": ["https://doi.org/10.1145/3180155.3180216", "http://ieeexplore.ieee.org/document/8453150"], "tag": ["Software comprehension"], "abstract": "ABSTRACTProgram comprehension is a necessary step for performing many software engineering tasks. Dynamic analysis is effective in producing execution traces that assist comprehension. Traces are rich sources of information regarding the behaviour of a program. However, it is challenging to gain insight from traces due to their overwhelming amount of data and complexity. We propose a generic technique for facilitating comprehension by inferring recurring execution motifs. Inspired by bioinformatics, motifs are patterns in traces that are flexible to small changes in execution, and are captured in a hierarchical model. The hierarchical nature of the model provides an overview of the behaviour at a high-level, while preserving the execution details and intermediate levels in a structured manner. We design a visualization that allows developers to observe and interact with the model. We implement our approach in an open-source tool, called Sabalan, and evaluate it through a user experiment. The results show that using Sabalan improves developers' accuracy in performing comprehension tasks by 54%."}, {"id": "conf/icse/ArmalyRM18", "title": "A comparison of program comprehension strategies by blind and sighted programmers.", "authors": ["Ameer Armaly", "Paige Rodeghero", "Collin McMillan"], "DOIs": ["https://doi.org/10.1145/3180155.3182544", "http://ieeexplore.ieee.org/document/8453151"], "tag": ["Software comprehension"], "abstract": "ABSTRACTProgrammers who are blind use a screen reader to speak source code one word at a time, as though the code were text. For example, \"float f = 5.23;\" can be read as \"float f equals five point two three semicolon\". This process of reading is in stark contrast to sighted programmers, who skim source code rapidly with their eyes. At present, it is not known whether the difference in these processes has effects on the program comprehension gained from reading code. These effects are important because they could reduce both the usefulness of accessibility tools and the generalizability of software engineering studies to persons with low vision. Furthermore, a lack of knowledge about blind programmers contributes to a bias against employing blind programmers. Employers are unfamiliar with the idea of a blind programmer and as a result may feel unsure about hiring one."}, {"id": "conf/icse/XiongLZ0018", "title": "Identifying patch correctness in test-based program repair.", "authors": ["Yingfei Xiong", "Xinyuan Liu", "Muhan Zeng", "Lu Zhang", "Gang Huang"], "DOIs": ["https://doi.org/10.1145/3180155.3180182", "http://ieeexplore.ieee.org/document/8453152"], "tag": ["Performance and maintenance"], "abstract": "ABSTRACTTest-based automatic program repair has attracted a lot of attention in recent years. However, the test suites in practice are often too weak to guarantee correctness and existing approaches often generate a large number of incorrect patches.To reduce the number of incorrect patches generated, we propose a novel approach that heuristically determines the correctness of the generated patches. The core idea is to exploit the behavior similarity of test case executions. The passing tests on original and patched programs are likely to behave similarly while the failing tests on original and patched programs are likely to behave differently. Also, if two tests exhibit similar runtime behavior, the two tests are likely to have the same test results. Based on these observations, we generate new test inputs to enhance the test suites and use their behavior similarity to determine patch correctness.Our approach is evaluated on a dataset consisting of 139 patches generated from existing program repair systems including jGen-Prog, Nopol, jKali, ACS and HDRepair. Our approach successfully prevented 56.3% of the incorrect patches to be generated, without blocking any correct patches."}, {"id": "conf/icse/YangSLYC18", "title": "How not to structure your database-backed web applications: a study of performance bugs in the wild.", "authors": ["Junwen Yang", "Pranav Subramaniam", "Shan Lu", "Cong Yan", "Alvin Cheung"], "DOIs": ["https://doi.org/10.1145/3180155.3180194", "http://ieeexplore.ieee.org/document/8453153"], "tag": ["Performance and maintenance"], "abstract": "ABSTRACTMany web applications use databases for persistent data storage, and using Object Relational Mapping (ORM) frameworks is a common way to develop such database-backed web applications. Unfortunately, developing efficient ORM applications is challenging, as the ORM framework hides the underlying database query generation and execution. This problem is becoming more severe as these applications need to process an increasingly large amount of persistent data. Recent research has targeted specific aspects of performance problems in ORM applications. However, there has not been any systematic study to identify common performance anti-patterns in real-world such applications, how they affect resulting application performance, and remedies for them.In this paper, we try to answer these questions through a comprehensive study of 12 representative real-world ORM applications. We generalize 9 ORM performance anti-patterns from more than 200 performance issues that we obtain by studying their bug-tracking systems and profiling their latest versions. To prove our point, we manually fix 64 performance issues in their latest versions and obtain a median speedup of 2\u00d7 (and up to 39\u00d7 max) with fewer than 5 lines of code change in most cases. Many of the issues we found have been confirmed by developers, and we have implemented ways to identify other code fragments with similar issues as well."}, {"id": "conf/icse/ChenCXWCLX18", "title": "Speedoo: prioritizing performance optimization opportunities.", "authors": ["Zhifei Chen", "Bihuan Chen", "Lu Xiao", "Xiao Wang", "Lin Chen", "Yang Liu", "Baowen Xu"], "DOIs": ["https://doi.org/10.1145/3180155.3180229", "http://ieeexplore.ieee.org/document/8453154"], "tag": ["Performance and maintenance"], "abstract": "ABSTRACTPerformance problems widely exist in modern software systems. Existing performance optimization techniques, including profiling-based and pattern-based techniques, usually fail to consider the architectural impacts among methods that easily slow down the overall system performance. This paper contributes a new approach, named Speedoo, to identify groups of methods that should be treated together and deserve high priorities for performance optimization. The uniqueness of Speedoo is to measure and rank the performance optimization opportunities of a method based on 1) the architectural impact and 2) the optimization potential. For each highly ranked method, we locate a respective Optimization Space based on 5 performance patterns generalized from empirical observations. The top ranked optimization spaces are suggested to developers as potential optimization opportunities. Our evaluation on three real-life projects has demonstrated that 18.52% to 42.86% of methods in the top ranked optimization spaces indeed undertook performance optimization in the projects. This outperforms one of the state-of-the-art profiling tools YourKit by 2 to 3 times. An important implication of this study is that developers should treat methods in an optimization space together as a group rather than as individuals in performance optimization. The proposed approach can provide guidelines and reduce developers' manual effort."}, {"id": "conf/icse/ArifSS18", "title": "Empirical study on the discrepancy between performance testing results from virtual and physical environments.", "authors": ["Muhammad Moiz Arif", "Weiyi Shang", "Emad Shihab"], "DOIs": ["https://doi.org/10.1145/3180155.3182527", "http://ieeexplore.ieee.org/document/8453155"], "tag": ["Performance and maintenance"], "abstract": "ABSTRACTLarge software systems often undergo performance tests to ensure their capability to handle expected loads. These performance tests often consume large amounts of computing resources and time since heavy loads need to be generated. Making it worse, the ever evolving field requires frequent updates to the performance testing environment. In practice, virtual machines (VMs) are widely exploited to provide flexible and less costly environments for performance tests. However, the use of VMs may introduce confounding overhead (e.g., a higher than expected memory utilization with unstable I/O traffic) to the testing environment and lead to unrealistic performance testing results. Yet, little research has studied the impact on test results of using VMs in performance testing activities.To evaluate the discrepancy between the performance testing results from virtual and physical environments, we perform a case study on two open source systems - namely Dell DVD Store (DS2) and CloudStore. We conduct the same performance tests in both virtual and physical environments and compare the performance testing results based on the three aspects that are typically examined for performance testing results: 1) single performance metric (e.g. CPU Time from virtual environment vs. CPU Time from physical environment), 2) the relationship among performance metrics (e.g. correlation between CPU and I/O) and 3) performance models that are built to predict system performance. Our results show that 1) A single metric from virtual and physical environments do not follow the same distribution, hence practitioners cannot simply use a scaling factor to compare the performance between environments, 2) correlations among performance metrics in virtual environments are different from those in physical environments 3) statistical models built based on the performance metrics from virtual environments are different from the models built from physical environments suggesting that practitioners cannot use the performance testing results across virtual and physical environments. In order to assist the practitioners leverage performance testing results in both environments, we investigate ways to reduce the discrepancy. We find that such discrepancy can be reduced by normalizing performance metrics based on deviance. Overall, we suggest that practitioners should not use the performance testing results from virtual environment with the simple assumption of straightforward performance overhead. Instead, practitioners should consider leveraging normalization techniques to reduce the discrepancy before examining performance testing results from virtual and physical environments."}, {"id": "conf/icse/GralhaDWG018", "title": "The evolution of requirements practices in software startups.", "authors": ["Catarina Gralha", "Daniela E. Damian", "Anthony I. Wasserman", "Miguel Goul\u00e3o", "Jo\u00e3o Ara\u00fajo"], "DOIs": ["https://doi.org/10.1145/3180155.3180158", "http://ieeexplore.ieee.org/document/8453156"], "tag": ["Requirements and recommender systems"], "abstract": "ABSTRACTWe use Grounded Theory to study the evolution of requirements practices of 16 software startups as they grow and introduce new products and services. These startups operate in a dynamic environment, with significant time and market pressure, and rarely have time for systematic requirements analysis. Our theory describes the evolution of practice along six dimensions that emerged as relevant to their requirements activities: requirements artefacts, knowledge management, requirements-related roles, planning, technical debt and product quality. Beyond the relationships among the dimensions, our theory also explains the turning points that drove the evolution along these dimensions. These changes are reactive, rather than planned, suggesting an overall pragmatic lightness, i.e., flexibility, in the startups' evolution towards engineering practices for requirements. Our theory organises knowledge about evolving requirements practice in maturing startups, and provides practical insights for startups' assessing their own evolution as they face challenges to their growth. Our research also suggests that a startup's evolution along the six dimensions is not fundamental to its success, but has significant effects on their product, their employees and the company."}, {"id": "conf/icse/0002RGCM18", "title": "Traceability in the wild: automatically augmenting incomplete trace links.", "authors": ["Michael Rath", "Jacob Rendall", "Jin L. C. Guo", "Jane Cleland-Huang", "Patrick M\u00e4der"], "DOIs": ["https://doi.org/10.1145/3180155.3180207", "http://ieeexplore.ieee.org/document/8453157"], "tag": ["Requirements and recommender systems"], "abstract": "ABSTRACTSoftware and systems traceability is widely accepted as an essential element for supporting many software development tasks. Today's version control systems provide inbuilt features that allow developers to tag each commit with one or more issue ID, thereby providing the building blocks from which project-wide traceability can be established between feature requests, bug fixes, commits, source code, and specific developers. However, our analysis of six open source projects showed that on average only 60% of the commits were linked to specific issues. Without these fundamental links the entire set of project-wide links will be incomplete, and therefore not trustworthy. In this paper we address the fundamental problem of missing links between commits and issues. Our approach leverages a combination of process and text-related features characterizing issues and code changes to train a classifier to identify missing issue tags in commit messages, thereby generating the missing links. We conducted a series of experiments to evaluate our approach against six open source projects and showed that it was able to effectively recommend links for tagging issues at an average of 96% recall and 33% precision. In a related task for augmenting a set of existing trace links, the classifier returned precision at levels greater than 89% in all projects and recall of 50%."}, {"id": "conf/icse/SadeghiJGBM18", "title": "A temporal permission analysis and enforcement framework for Android.", "authors": ["Alireza Sadeghi", "Reyhaneh Jabbarvand", "Negar Ghorbani", "Hamid Bagheri", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3180155.3180172", "http://ieeexplore.ieee.org/document/8453158"], "tag": ["Requirements and recommender systems"], "abstract": "ABSTRACTPermission-induced attacks, i.e., security breaches enabled by permission misuse, are among the most critical and frequent issues threatening the security of Android devices. By ignoring the temporal aspects of an attack during the analysis and enforcement, the state-of-the-art approaches aimed at protecting the users against such attacks are prone to have low-coverage in detection and high-disruption in prevention of permission-induced attacks. To address this shortcomings, we present Terminator, a temporal permission analysis and enforcement framework for Android. Leveraging temporal logic model checking,Terminator's analyzer identifies permission-induced threats with respect to dynamic permission states of the apps. At runtime, Terminator's enforcer selectively leases (i.e., temporarily grants) permissions to apps when the system is in a safe state, and revokes the permissions when the system moves to an unsafe state realizing the identified threats. The results of our experiments, conducted over thousands of apps, indicate that Terminator is able to provide an effective, yet non-disruptive defense against permission-induced attacks. We also show that our approach, which does not require modification to the Android framework or apps' implementation logic, is highly reliable and widely applicable."}, {"id": "conf/icse/BarbosaG18", "title": "Global-aware recommendations for repairing violations in exception handling.", "authors": ["Eiji Adachi Barbosa", "Alessandro Garcia"], "DOIs": ["https://doi.org/10.1145/3180155.3182539", "http://ieeexplore.ieee.org/document/8453159"], "tag": ["Requirements and recommender systems"], "abstract": "ABSTRACTThis paper presents an extended abstract incorporated as a journal-first paper into the ICSE'18 program."}, {"id": "conf/icse/ChenTDZ18", "title": "RFC-directed differential testing of certificate validation in SSL/TLS implementations.", "authors": ["Chu Chen", "Cong Tian", "Zhenhua Duan", "Liang Zhao"], "DOIs": ["https://doi.org/10.1145/3180155.3180226", "http://ieeexplore.ieee.org/document/8453160"], "tag": ["Testing III"], "abstract": "ABSTRACTCertificate validation in Secure Socket Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS is correctly implemented. With this motivation, we propose a novel differential testing approach which is directed by the standard Request For Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e. certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-targeted since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations."}, {"id": "conf/icse/YuCWS018", "title": "Symbolic verification of regular properties.", "authors": ["Hengbiao Yu", "Zhenbang Chen", "Ji Wang", "Zhendong Su", "Wei Dong"], "DOIs": ["https://doi.org/10.1145/3180155.3180227", "http://ieeexplore.ieee.org/document/8453161"], "tag": ["Testing III"], "abstract": "ABSTRACTVerifying the regular properties of programs has been a significant challenge. This paper tackles this challenge by presenting symbolic regular verification (SRV) that offers significant speedups over the state-of-the-art. SRV is based on dynamic symbolic execution (DSE) and enabled by novel techniques for mitigating path explosion: (1) a regular property-oriented path slicing algorithm, and (2) a synergistic combination of property-oriented path slicing and guiding. Slicing prunes redundant paths, while guiding boosts the search for counterexamples. We have implemented SRV for Java and evaluated it on 15 real-world open-source Java programs (totaling 259K lines of code). Our evaluation results demonstrate the effectiveness and efficiency of SRV. Compared with the state-of-the-art --- pure DSE, pure guiding, and pure path slicing --- SRV achieves average speedups of more than 8.4X, 8.6X, and 7X, respectively, making symbolic regular property verification significantly more practical."}, {"id": "conf/icse/SeguraPTC18", "title": "Metamorphic testing of RESTful web APIs.", "authors": ["Sergio Segura", "Jos\u00e9 Antonio Parejo", "Javier Troya", "Antonio Ruiz Cort\u00e9s"], "DOIs": ["https://doi.org/10.1145/3180155.3182528", "http://ieeexplore.ieee.org/document/8453162"], "tag": ["Testing III"], "abstract": "ABSTRACTWeb Application Programming Interfaces (APIs) specify how to access services and data over the network, typically using Web services. Web APIs are rapidly proliferating as a key element to foster reusability, integration, and innovation, enabling new consumption models such as mobile or smart TV apps. Companies such as Facebook, Twitter, Google, eBay or Netflix receive billions of API calls every day from thousands of different third-party applications and devices, which constitutes more than half of their total traffic.As Web APIs are progressively becoming the cornerstone of software integration, their validation is getting more critical. In this context, the fast detection of bugs is of utmost importance to increase the quality of internal products and third-party applications. However, testing Web APIs is challenging mainly due to the difficulty to assess whether the output of an API call is correct, i.e., the oracle problem. For instance, consider the Web API of the popular music streaming service Spotify. Suppose a search for albums with the query \"redhouse\" returning 21 total matches: Is this output correct? Do all the albums in the result set contain the keyword? Are there any albums containing the keyword not included in the result set? Answering these questions is difficult, even with small result sets, and often infeasible when the results are counted by thousands or millions.Metamorphic testing alleviates the oracle problem by providing an alternative when the expected output of a test execution is complex or unknown. Rather than checking the output of an individual program execution, metamorphic testing checks whether multiple executions of the program under test fulfil certain necessary properties called metamorphic relations. For instance, consider the following metamorphic relation in Spotify: two searches for albums with the same query should return the same number of total results regardless of the size of pagination. Suppose that a new Spotify search is performed using the exact same query as before and increasing the maximum number of results per page from 20 (default value) to 50: This search returns 27 total albums (6 more matches than in the previous search), which reveals a bug. This is an example of a real and reproducible fault detected using the approach presented in this paper and reported to Spotify. According to Spotify developers, it was a regression fault caused by a fix with undesired side effects.In this paper [1], we present a metamorphic testing approach for the automated detection of faults in RESTful Web APIs (henceforth also referred to as simply Web APIs). We introduce the concept of metamorphic relation output patterns. A Metamorphic Relation Output Pattern (MROP) defines an abstract output relation typically identified in Web APIs, regardless of their application domain. Each MROP is defined in terms of set operations among test outputs such as equality, union, subset, or intersection. MROPs provide a helpful guide for the identification of metamorphic relations, broadening the scope of our work beyond a particular Web API. Based on the notion of MROP, a methodology is proposed for the application of the approach to any Web API following the REST architectural pattern.The approach was evaluated in several steps. First, we used the proposed methodology to identify 33 metamorphic relations in four Web APIs developed by undergraduate students. All the relations are instances of the proposed MROPs. Then, we assessed the effectiveness of the identified relations at revealing 317 automatically seeded faults (i.e., mutants) in the APIs under test. As a result, 302 seeded faults were detected, achieving a mutation score of 95.3%. Second, we evaluated the approach using real Web APIs and faults. In particular, we identified 20 metamorphic relations in the Web API of Spotify and 40 metamorphic relations in the Web API of YouTube. Each metamorphic relation was implemented and automatically executed using both random and manual test data. In total, 469K metamorphic tests were generated. As a result, 21 metamorphic relations were violated, and 11 issues revealed and reported (3 issues in Spotify and 8 issues in YouTube). To date, 10 of the reported issues have been either confirmed by the API developers or reproduced by other users supporting the effectiveness of our approach."}, {"id": "conf/icse/RamasubbuK18", "title": "Integrating technical debt management and software quality management processes: a framework and field tests.", "authors": ["Narayan Ramasubbu", "Chris F. Kemerer"], "DOIs": ["https://doi.org/10.1145/3180155.3182529", "http://ieeexplore.ieee.org/document/8453163"], "tag": ["Testing III"], "abstract": "ABSTRACTTechnical debt, defined as the maintenance obligations arising from shortcuts taken during the design, development, and deployment of software systems, has been shown to significantly impact the reliability and long-term evolution of software systems [1], [2]. Although academic research has moved beyond using technical debt only as a metaphor, and has begun compiling strong empirical evidence on the economic implications of technical debt, industry practitioners continue to find managing technical debt a challenging balancing act [3]. Despite the increasing awareness of the importance of managing technical debt in software product development, systematic processes for implementing technical debt management in software production have not been readily available.To address this gap, we developed and field tested a normative process framework that systematically incorporates steps for managing technical debt in commercial software production. The framework integrates processes required for technical debt management with existing software quality management processes prescribed by the project management body of knowledge (PMBOK) [4], and organizes the different processes for technical debt management under three steps: (1) make technical debt visible, (2) perform cost-benefit analysis, and (3) control technical debt. To implement the processes, we introduce a new artifact, called the technical debt register, which stores, for each software asset, the outstanding principal and the associated interest estimated for the technical debt embedded in the asset. The technical debt register also stores the desired control target for each software asset's technical debt, which is populated and used during the cost-benefit analysis and control target calculations.There are three main benefits from this integrated approach. First, it enables the uncovering of hidden technical debt embedded in systems. Established quality assurance and control practices can be utilized to effectively associate software defects with specific design and deployment decisions made by programmers. Such associations make technical debt visible to the team and thereby facilitate the quantification of debt-related principal and interest. Second, it helps to bridge the gaps that exist between the technical and economic assessments of technical debt, and aid in formulating actionable policies related to technical debt management. Finally, integrating technical debt management processes with established quality frameworks aids the wider adoption of emerging prescriptions for managing technical debt.We partnered with three commercial software product development organizations to implement the framework in real-world software production settings. All three organizations, irrespective of their varying software process maturity levels, were able to adopt the proposed framework and integrate the prescribed technical debt management processes with their existing software quality management processes. Our longitudinal data and case-study interviews indicate that the organizations were able to accrue economic benefits from the adoption and use of the integrated framework. And, based on our field study observations, we also identified a set of best practices that support the implementation and use of our framework: facilitating engagement between business and engineering stakeholders, adoption of policies based on a probabilistic analysis framework, and limiting process overheads."}, {"id": "conf/icse/WangCH18", "title": "Understanding the factors for fast answers in technical Q&A websites: an empirical study of four stack exchange websites.", "authors": ["Shaowei Wang", "Tse-Hsun Chen", "Ahmed E. Hassan"], "DOIs": ["https://doi.org/10.1145/3180155.3182521", "http://ieeexplore.ieee.org/document/8453164"], "tag": ["Mining software repositories"], "abstract": "ABSTRACTTechnical questions and answers (Q&A) websites accumulate a significant amount of knowledge from users. Developers are especially active on these Q&A websites, since developers are constantly facing new development challenges that require help from other experts. Over the years, Q&A website designers have derived several incentive systems (e.g., gamification) to encourage users to answer questions that are posted by others. However, the current incentive systems primarily focus on the quantity and quality of the answers instead of encouraging the rapid answering of questions. Improving the speed of getting an answer can significantly improve the user experience and increase user engagement on such Q&A websites.In this paper [1], we study the factors for fast answers on such Q&A websites. Our goal is to explore how one may improve the current incentive systems to motivate fast answering of questions. We use a logistic regression model to analyze 46 factors along four dimensions (i.e., question, asker, answer, and answerer dimension) in order to understand the relationship between the studied factors and the needed time to get an accepted answer. The question dimension calculates various textual and readability features of a question, as well as the popularity and difficulty of the question's tags. The asker dimension calculates the reputation of an asker and his/her historical tendency to get answers. The answer dimension computes textual features from the text of the accepted answer. The answerer dimension computes the historical activity level of the answerer who answered the question. We conduct our study on the four most popular (i.e., with the most questions) Q&A Stack Exchange websites: Stack Overflow, Mathematics, Ask Ubuntu, and Superuser. We find that i) factors in the answerer dimension have the strongest effect on the needed time to get an accepted answer, after controlling for other factors; ii) the current incentive system does not recognize non-frequent answerers who often answer questions which frequent answerers are not able to answer well. Such questions that are answered by non-frequent answerers are as important as those that are answered by frequent answerers; iii) the current incentive system motivates frequent answerers well, but such frequent answerers tend to answer short questions.Our findings suggest that the designers of Q&A website should improve their incentive systems to motivate non-frequent answerers to be more active and to answer questions faster, in order to shorten the waiting time for an answer (especially for questions that require specific knowledge that frequent answerers might not possess). In addition, the question answering incentive system needs to factor in the value and difficulty of answering the questions (e.g., by providing more rewards to harder questions or questions that remain unanswered for a long period of time)."}, {"id": "conf/icse/ZhongM18", "title": "Towards reusing hints from past fixes: an exploratory study on thousands of real samples.", "authors": ["Hao Zhong", "Na Meng"], "DOIs": ["https://doi.org/10.1145/3180155.3182550", "http://ieeexplore.ieee.org/document/8453165"], "tag": ["Mining software repositories"], "abstract": "ABSTRACTResearchers have recently proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta dependency graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlapped with each other in terms of the content, code structure, and identifier names of fixes. Our results show that if an APR approach composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach composes new fixes by fully or partially reusing the code structure of past fixes, up to 41.3% and 29.7% new fixes can be created."}, {"id": "conf/icse/0001URRK18", "title": "Are code examples on an online Q&A forum reliable?: a study of API misuse on stack overflow.", "authors": ["Tianyi Zhang", "Ganesha Upadhyaya", "Anastasia Reinhardt", "Hridesh Rajan", "Miryung Kim"], "DOIs": ["https://doi.org/10.1145/3180155.3180260", "http://ieeexplore.ieee.org/document/8453166"], "tag": ["Mining software repositories"], "abstract": "ABSTRACTProgrammers often consult an online Q&A forum such as Stack Overflow to learn new APIs. This paper presents an empirical study on the prevalence and severity of API misuse on Stack Overflow. To reduce manual assessment effort, we design ExampleCheck, an API usage mining framework that extracts patterns from over 380K Java repositories on GitHub and subsequently reports potential API usage violations in Stack Overflow posts. We analyze 217,818 Stack Overflow posts using ExampleCheck and find that 31% may have potential API usage violations that could produce unexpected behavior such as program crashes and resource leaks. Such API misuse is caused by three main reasons---missing control constructs, missing or incorrect order of API calls, and incorrect guard conditions. Even the posts that are accepted as correct answers or upvoted by other programmers are not necessarily more reliable than other posts in terms of API misuse. This study result calls for a new approach to augment Stack Overflow with alternative API usage details that are not typically shown in curated examples."}, {"id": "conf/icse/BaoXXLH18", "title": "Inference of development activities from interaction with uninstrumented applications.", "authors": ["Lingfeng Bao", "Zhenchang Xing", "Xin Xia", "David Lo", "Ahmed E. Hassan"], "DOIs": ["https://doi.org/10.1145/3180155.3182537", "http://ieeexplore.ieee.org/document/8453167"], "tag": ["Mining software repositories"], "abstract": "ABSTRACTStudying developers' behavior is crucial for designing effective techniques and tools to support developers' daily work. However, there are two challenges in collecting and analyzing developers' behavior data. First, instrumenting many software tools commonly used in real work settings (e.g., IDEs, web browsers) is difficult and requires significant resources. Second, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis.In this paper [1], to address these two challenges, we first use our ActivitySpace framework to improve the generalizability of the data collection. Then, we propose a Condition Random Field (CRF) based approach to segment and label the developers' low-level actions into a set of basic, yet meaningful development activities. To evaluate our proposed approach, we deploy the ActivitySpace framework in an industry partner's company and collect the real working data from ten professional developers' one-week work. We conduct an experiment with the collected data and a small number of initial human-labeled training data using the CRF model and the other three baselines (i.e., a heuristic-rules based method, a SVM classifier, and a random weighted classifier). The proposed CRF model achieves better performance (i.e., 0.728 accuracy and 0.672 macro-averaged F1-score) than the other three baselines."}, {"id": "conf/icse/KrieterTSSS18", "title": "Propagating configuration decisions with modal implication graphs.", "authors": ["Sebastian Krieter", "Thomas Th\u00fcm", "Sandro Schulze", "Reimar Schr\u00f6ter", "Gunter Saake"], "DOIs": ["https://doi.org/10.1145/3180155.3180159", "http://ieeexplore.ieee.org/document/8453168"], "tag": ["Models and modeling I"], "abstract": "ABSTRACTHighly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems."}, {"id": "conf/icse/MadalaDA18", "title": "A combinatorial approach for exposing off-nominal behaviors.", "authors": ["Kaushik Madala", "Hyunsook Do", "Daniel Aceituna"], "DOIs": ["https://doi.org/10.1145/3180155.3180204", "http://ieeexplore.ieee.org/document/8453169"], "tag": ["Models and modeling I"], "abstract": "ABSTRACTOff-nominal behaviors (ONBs) have been a major concern in the areas of embedded systems and safety-critical systems. To address ONB problems, some researchers have proposed model-based approaches that can expose ONBs by analyzing natural language requirements documents. While these approaches produced promising results, they require a lot of human effort and time. In this paper, to reduce human effort and time, we propose a combinatorial-based approach, Combinatorial Causal Component Model (Combi-CCM), which uses structured requirements patterns and combinations generated using the IPOG algorithm. We conducted an empirical study using several requirements documents to evaluate our approach, and our results indicate that the proposed approach can reduce human effort and time while maintaining the same ONB exposure ability obtained by the control techniques."}, {"id": "conf/icse/SousaOOBGLKMFOL18", "title": "Identifying design problems in the source code: a grounded theory.", "authors": ["Leonardo da Silva Sousa", "Anderson Oliveira", "Willian Nalepa Oizumi", "Simone D. J. Barbosa", "Alessandro Garcia", "Jaejoon Lee", "Marcos Kalinowski", "Rafael Maiani de Mello", "Baldoino Fonseca", "Roberto Felicio Oliveira", "Carlos Lucena", "Rodrigo B. de Paes"], "DOIs": ["https://doi.org/10.1145/3180155.3180239", "http://ieeexplore.ieee.org/document/8453170"], "tag": ["Models and modeling I"], "abstract": "ABSTRACTThe prevalence of design problems may cause re-engineering or even discontinuation of the system. Due to missing, informal or outdated design documentation, developers often have to rely on the source code to identify design problems. Therefore, developers have to analyze different symptoms that manifest in several code elements, which may quickly turn into a complex task. Although researchers have been investigating techniques to help developers in identifying design problems, there is little knowledge on how developers actually proceed to identify design problems. In order to tackle this problem, we conducted a multi-trial industrial experiment with professionals from 5 software companies to build a grounded theory. The resulting theory offers explanations on how developers identify design problems in practice. For instance, it reveals the characteristics of symptoms that developers consider helpful. Moreover, developers often combine different types of symptoms to identify a single design problem. This knowledge serves as a basis to further understand the phenomena and advance towards more effective identification techniques."}, {"id": "conf/icse/DamevskiCSKP18", "title": "Predicting future developer behavior in the IDE using topic models.", "authors": ["Kostadin Damevski", "Hui Chen", "David C. Shepherd", "Nicholas A. Kraft", "Lori L. Pollock"], "DOIs": ["https://doi.org/10.1145/3180155.3182541", "http://ieeexplore.ieee.org/document/8453171"], "tag": ["Models and modeling I"], "abstract": "ABSTRACTInteraction data, gathered from developers' daily clicks and key presses in the IDE, has found use in both empirical studies and in recommendation systems for software engineering. We observe that this data has several characteristics, common across IDEs:\u2022 exponentially distributed - some events or commands dominate the trace (e.g., cursor movement commands), while most other commands occur relatively infrequently.\u2022 noisy - the traces include spurious commands (or clicks), or unrelated events, that may not be important to the behavior of interest.\u2022 comprise of overlapping events and commands - specific commands can be invoked by separate mechanisms, and similar events can be triggered by different sources."}, {"id": "conf/icse/GuZ018", "title": "Deep code search.", "authors": ["Xiaodong Gu", "Hongyu Zhang", "Sunghun Kim"], "DOIs": ["https://doi.org/10.1145/3180155.3180167", "http://ieeexplore.ieee.org/document/8453172"], "tag": ["Code search, synthesis, performance"], "abstract": "ABSTRACTTo implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code.In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled.As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques."}, {"id": "conf/icse/SirresBKLKKT18", "title": "Augmenting and structuring user queries to support efficient free-form code search.", "authors": ["Raphael Sirres", "Tegawend\u00e9 F. Bissyand\u00e9", "Dongsun Kim", "David Lo", "Jacques Klein", "Kisub Kim", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3180155.3182513", "http://ieeexplore.ieee.org/document/8453173"], "tag": ["Code search, synthesis, performance"], "abstract": "ABSTRACTMotivation: Code search is an important activity in software development since developers are regularly searching [6] for code examples dealing with diverse programming concepts, APIs, and specific platform peculiarities. To help developers search for source code, several Internet-scale code search engines, such as OpenHub [5] and Codota [1] have been proposed. Unfortunately, these Internet-scale code search engines have limited performance since they treat source code as natural language documents. To improve the performance of search engines, the construction of the search space index as well as the mapping process of querying must address the challenge that \"no single word can be chosen to describe a programming concept in the best way\" [2]. This is known in the literature as the vocabulary mismatch problem [3].Approach: We propose a novel approach to augmenting user queries in a free-form code search scenario. This approach aims at improving the quality of code examples returned by Internet-scale code search engines by building a Code voCaBulary (CoCaBu) [7]. The originality of CoCaBu is that it addresses the vocabulary mismatch problem, by expanding/enriching/re-targeting a user's free-form query, building on similar questions in Q&A sites so that a code search engine can find highly relevant code in source code repositories. Figure 1 provides an overview of our approach.The search process begins with a free-form query from a user,i.e., a sentence written in a natural language:(a) For a given query, CoCaBu first searches for relevant posts in Q&A forums. The role of the Search Proxy is then to forward developer free-form queries to web search engines that can collect and rank entries in Q&A with the most relevant documents for the query.(b) CoCaBu then generates an augmented query based on the information in the relevant posts. It mainly leverages code snippets in the previously identified posts. The Code Query Generator then creates another query which includes not only the initial user query terms but also program elements. To accelerate this step in the search process, CoCaBu builds upfront a snippet index for Q&A posts.(c) Once the augmented query is constructed, CoCaBu searches source files for code locations that match the query terms. For this step, we crawl a large number of repositories and build upfront a code index of program elements in the source code.Contributions:\u2022 CoCaBuapproach to the vocabulary mismatch problem: We propose a technique for finding relevant code with freeform query terms that describe programming tasks, with no a-priori knowledge on the API keywords to search for.\u2022 GitSearchfree-form search engine for GitHub: We instantiate the CoCaBu approach based on indices of Java files built from GitHub and Q&A posts from Stack Overflow to find the most relevant code examples for developer queries.\u2022 Empirical user evaluation: Comparison with popular code search engines further shows that GitSearch is more effective in returning acceptable code search results. In addition, Comparison against web search engines indicates that GitSearch is a competitive alternative. Finally, via a live study, we show that users on Q&A sites may find GitSearch's real code examples acceptable as answers to developer questions.Concluding remarks: As a follow-up work, we have also leveraged Stack Overflow data to build a practical, novel, and efficient code-to-code search engine [4]."}, {"id": "conf/icse/KimKBC0KT18", "title": "FaCoY: a code-to-code search engine.", "authors": ["Kisub Kim", "Dongsun Kim", "Tegawend\u00e9 F. Bissyand\u00e9", "Eunjong Choi", "Li Li", "Jacques Klein", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3180155.3180187", "http://ieeexplore.ieee.org/document/8453174"], "tag": ["Code search, synthesis, performance"], "abstract": "ABSTRACTCode search is an unavoidable activity in software development. Various approaches and techniques have been explored in the literature to support code search tasks. Most of these approaches focus on serving user queries provided as natural language free-form input. However, there exists a wide range of use-case scenarios where a code-to-code approach would be most beneficial. For example, research directions in code transplantation, code diversity, patch recommendation can leverage a code-to-code search engine to find essential ingredients for their techniques. In this paper, we propose FaCoY, a novel approach for statically finding code fragments which may be semantically similar to user input code. FaCoY implements a query alternation strategy: instead of directly matching code query tokens with code in the search space, FaCoY first attempts to identify other tokens which may also be relevant in implementing the functional behavior of the input code. With various experiments, we show that (1) FaCoY is more effective than online code-to-code search engines; (2) FaCoY can detect more semantic code clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; (3) FaCoY, while static, can detect code fragments which are indeed similar with respect to runtime execution behavior; and (4) FaCoY can be useful in code/patch recommendation."}, {"id": "conf/icse/LoncaricET18", "title": "Generalized data structure synthesis.", "authors": ["Calvin Loncaric", "Michael D. Ernst", "Emina Torlak"], "DOIs": ["https://doi.org/10.1145/3180155.3180211", "http://ieeexplore.ieee.org/document/8453175"], "tag": ["Code search, synthesis, performance"], "abstract": "ABSTRACTData structure synthesis is the task of generating data structure implementations from high-level specifications. Recent work in this area has shown potential to save programmer time and reduce the risk of defects. Existing techniques focus on data structures for manipulating subsets of a single collection, but real-world programs often track multiple related collections and aggregate properties such as sums, counts, minimums, and maximums.This paper shows how to synthesize data structures that track subsets and aggregations of multiple related collections. Our technique decomposes the synthesis task into alternating steps of query synthesis and incrementalization. The query synthesis step implements pure operations over the data structure state by leveraging existing enumerative synthesis techniques, specialized to the data structures domain. The incrementalization step implements imperative state modifications by re-framing them as fresh queries that determine what to change, coupled with a small amount of code to apply the change. As an added benefit of this approach over previous work, the synthesized data structure is optimized for not only the queries in the specification but also the required update operations. We have evaluated our approach in four large case studies, demonstrating that these extensions are broadly applicable."}, {"id": "conf/icse/SemerathNV18", "title": "A graph solver for the automated generation of consistent domain-specific models.", "authors": ["Oszk\u00e1r Semer\u00e1th", "Andr\u00e1s Szabolcs Nagy", "D\u00e1niel Varr\u00f3"], "DOIs": ["https://doi.org/10.1145/3180155.3180186", "http://ieeexplore.ieee.org/document/8453176"], "tag": ["Software tools and environments"], "abstract": "ABSTRACTMany testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy."}, {"id": "conf/icse/ChowdhuryMMGJC18", "title": "Automatically finding bugs in a commercial cyber-physical system development tool chain with SLforge.", "authors": ["Shafiul Azam Chowdhury", "Soumik Mohian", "Sidharth Mehra", "Siddhant Gawsane", "Taylor T. Johnson", "Christoph Csallner"], "DOIs": ["https://doi.org/10.1145/3180155.3180231", "http://ieeexplore.ieee.org/document/8453177"], "tag": ["Software tools and environments"], "abstract": "ABSTRACTCyber-physical system (CPS) development tool chains are widely used in the design, simulation, and verification of CPS data-flow models. Commercial CPS tool chains such as MathWorks' Simulink generate artifacts such as code binaries that are widely deployed in embedded systems. Hardening such tool chains by testing is crucial since formally verifying them is currently infeasible. Existing differential testing frameworks such as CyFuzz can not generate models rich in language features, partly because these tool chains do not leverage the available informal Simulink specifications. Furthermore, no study of existing Simulink models is available, which could guide CyFuzz to generate realistic models.To address these shortcomings, we created the first large collection of public Simulink models and used the collected models' properties to guide random model generation. To further guide model generation we systematically collected semi-formal Simulink specifications. In our experiments on several hundred models, the resulting SLforge generator was more effective and efficient than the state-of-the-art tool CyFuzz. SLforge also found 8 new confirmed bugs in Simulink."}, {"id": "conf/icse/BradleyFH18", "title": "Context-aware conversational developer assistants.", "authors": ["Nick C. Bradley", "Thomas Fritz", "Reid Holmes"], "DOIs": ["https://doi.org/10.1145/3180155.3180238", "http://ieeexplore.ieee.org/document/8453178"], "tag": ["Software tools and environments"], "abstract": "ABSTRACTBuilding and maintaining modern software systems requires developers to perform a variety of tasks that span various tools and information sources. The crosscutting nature of these development tasks requires developers to maintain complex mental models and forces them (a) to manually split their high-level tasks into low-level commands that are supported by the various tools, and (b) to (re)establish their current context in each tool. In this paper we present Devy, a Conversational Developer Assistant (CDA) that enables developers to focus on their high-level development tasks. Devy reduces the number of manual, often complex, low-level commands that developers need to perform, freeing them to focus on their high-level tasks. Specifically, Devy infers high-level intent from developer's voice commands and combines this with an automatically-generated context model to determine appropriate workflows for invoking low-level tool actions; where needed, Devy can also prompt the developer for additional information. Through a mixed methods evaluation with 21 industrial developers, we found that Devy provided an intuitive interface that was able to support many development tasks while helping developers stay focused within their development environment. While industrial developers were largely supportive of the automation Devy enabled, they also provided insights into several other tasks and workflows CDAs could support to enable them to better focus on the important parts of their development tasks."}, {"id": "conf/icse/MendezPSHHHSPSB18", "title": "Open source barriers to entry, revisited: a sociotechnical perspective.", "authors": ["Christopher J. Mendez", "Hema Susmita Padala", "Zoe Steine-Hanson", "Claudia Hilderbrand", "Amber Horvath", "Charles Hill", "Logan Simpson", "Nupoor Patil", "Anita Sarma", "Margaret M. Burnett"], "DOIs": ["https://doi.org/10.1145/3180155.3180241", "http://ieeexplore.ieee.org/document/8453179"], "tag": ["Software tools and environments"], "abstract": "ABSTRACTResearch has revealed that significant barriers exist when entering Open-Source Software (OSS) communities and that women disproportionately experience such barriers. However, this research has focused mainly on social/cultural factors, ignoring the environment itself --- the tools and infrastructure. To shed some light onto how tools and infrastructure might somehow factor into OSS barriers to entry, we conducted a field study with five teams of software professionals, who worked through five use-cases to analyze the tools and infrastructure used in their OSS projects. These software professionals found tool/infrastructure barriers in 7% to 71% of the use-case steps that they analyzed, most of which are tied to newcomer barriers that have been established in the literature. Further, over 80% of the barrier types they found include attributes that are biased against women."}, {"id": "conf/icse/AbdessalemNBS18", "title": "Testing vision-based control systems using learnable evolutionary algorithms.", "authors": ["Raja Ben Abdessalem", "Shiva Nejati", "Lionel C. Briand", "Thomas Stifter"], "DOIs": ["https://doi.org/10.1145/3180155.3180160", "http://ieeexplore.ieee.org/document/8453180"], "tag": ["Search-based software engineering I"], "abstract": "ABSTRACTVision-based control systems are key enablers of many autonomous vehicular systems, including self-driving cars. Testing such systems is complicated by complex and multidimensional input spaces. We propose an automated testing algorithm that builds on learnable evolutionary algorithms. These algorithms rely on machine learning or a combination of machine learning and Darwinian genetic operators to guide the generation of new solutions (test scenarios in our context). Our approach combines multiobjective population-based search algorithms and decision tree classification models to achieve the following goals: First, classification models guide the search-based generation of tests faster towards critical test scenarios (i.e., test scenarios leading to failures). Second, search algorithms refine classification models so that the models can accurately characterize critical regions (i.e., the regions of a test input space that are likely to contain most critical test scenarios). Our evaluation performed on an industrial automotive automotive system shows that: (1) Our algorithm outperforms a baseline evolutionary search algorithm and generates 78% more distinct, critical test scenarios compared to the baseline algorithm. (2) Our algorithm accurately characterizes critical regions of the system under test, thus identifying the conditions that are likely to lead to system failures."}, {"id": "conf/icse/GuoS18", "title": "To preserve or not to preserve invalid solutions in search-based software engineering: a case study in software product lines.", "authors": ["Jianmei Guo", "Kai Shi"], "DOIs": ["https://doi.org/10.1145/3180155.3180163", "http://ieeexplore.ieee.org/document/8453181"], "tag": ["Search-based software engineering I"], "abstract": "ABSTRACTMulti-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way."}, {"id": "conf/icse/LinJGM18", "title": "Nemo: multi-criteria test-suite minimization with integer nonlinear programming.", "authors": ["Jun-Wei Lin", "Reyhaneh Jabbarvand", "Joshua Garcia", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3180155.3180174", "http://ieeexplore.ieee.org/document/8453182"], "tag": ["Search-based software engineering I"], "abstract": "ABSTRACTMulti-criteria test-suite minimization aims to remove redundant test cases from a test suite based on some criteria such as code coverage, while trying to optimally maintain the capability of the reduced suite based on other criteria such as fault-detection effectiveness. Existing techniques addressing this problem with integer linear programming claim to produce optimal solutions. However, the multi-criteria test-suite minimization problem is inherently nonlinear, due to the fact that test cases are often dependent on each other in terms of test-case criteria. In this paper, we propose a framework that formulates the multi-criteria test-suite minimization problem as an integer nonlinear programming problem. To solve this problem optimally, we programmatically transform this nonlinear problem into a linear one and then solve the problem using modern linear solvers. We have implemented our framework as a tool, called Nemo, that supports a number of modern linear and nonlinear solvers. We have evaluated Nemo with a publicly available dataset and minimization problems involving multiple criteria including statement coverage, fault-revealing capability, and test execution time. The experimental results show that Nemo can be used to efficiently find an optimal solution for multi-criteria test-suite minimization problems with modern solvers, and the optimal solutions outperform the suboptimal ones by up to 164.29% in terms of the criteria considered in the problem."}, {"id": "conf/icse/AgrawalM18", "title": "Is \"better data\" better than \"better data miners\"?: on the benefits of tuning SMOTE for defect prediction.", "authors": ["Amritanshu Agrawal", "Tim Menzies"], "DOIs": ["https://doi.org/10.1145/3180155.3180197", "http://ieeexplore.ieee.org/document/8453183"], "tag": ["Search-based software engineering I"], "abstract": "ABSTRACTWe report and fix an important systematic error in prior studies that ranked classifiers for software analytics. Those studies did not (a) assess classifiers on multiple criteria and they did not (b) study how variations in the data affect the results. Hence, this paper applies (a) multi-performance criteria while (b) fixing the weaker regions of the training data (using SMOTUNED, which is an auto-tuning version of SMOTE). This approach leads to dramatically large increases in software defect predictions when applied in a 5*5 cross-validation study for 3,681 JAVA classes (containing over a million lines of code) from open source systems, SMOTUNED increased AUC and recall by 60% and 20% respectively. These improvements are independent of the classifier used to predict for defects. Same kind of pattern (improvement) was observed when a comparative analysis of SMOTE and SMOTUNED was done against the most recent class imbalance technique.In conclusion, for software analytic tasks like defect prediction, (1) data pre-processing can be more important than classifier choice, (2) ranking studies are incomplete without such pre-processing, and (3) SMOTUNED is a promising candidate for pre-processing."}, {"id": "conf/icse/BorleFSGH18", "title": "Analyzing the effects of test driven development in GitHub.", "authors": ["Neil C. Borle", "Meysam Feghhi", "Eleni Stroulia", "Russell Greiner", "Abram Hindle"], "DOIs": ["https://doi.org/10.1145/3180155.3182535", "http://ieeexplore.ieee.org/document/8453184"], "tag": ["Testing IV"], "abstract": "ABSTRACTTesting is an integral part of the software development lifecycle, approached with varying degrees of rigor by different process models. Agile process models recommend Test Driven Development (TDD) as a key practice for reducing costs and improving code quality. The objective of this work is to perform a cost-benefit analysis of this practice. Previous work by Fucci et al. [2, 3] engaged in laboratory studies of developers actively engaged in test-driven development practices. Fucci et al. found little difference between test-first behaviour of TDD and test-later behaviour. To that end, we opted to conduct a study about TDD behaviours in the \"wild\" rather than in the laboratory. Thus we have conducted a comparative analysis of GitHub repositories that adopts TDD to a lesser or greater extent, in order to determine how TDD affects software development productivity and software quality. We classified GitHub repositories archived in 2015 in terms of how rigorously they practiced TDD, thus creating a TDD spectrum. We then matched and compared various subsets of these repositories on this TDD spectrum with control sets of equal size. The control sets were samples from all GitHub repositories that matched certain characteristics, and that contained at least one test file. We compared how the TDD sets differed from the control sets on the following characteristics: number of test files, average commit velocity, number of bug-referencing commits, number of issues recorded, usage of continuous integration, number of pull requests, and distribution of commits per author. We found that Java TDD projects were relatively rare. In addition, there were very few significant differences in any of the metrics we used to compare TDD-like and non-TDD projects; therefore, our results do not reveal any observable benefits from using TDD."}, {"id": "conf/icse/HerboldTG18", "title": "A comparative study to benchmark cross-project defect prediction approaches.", "authors": ["Steffen Herbold", "Alexander Trautsch", "Jens Grabowski"], "DOIs": ["https://doi.org/10.1145/3180155.3182542", "http://ieeexplore.ieee.org/document/8453185"], "tag": ["Testing IV"], "abstract": "ABSTRACTCross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article [2, 3], we provide a benchmark for CPDP. Our benchmark replicates 24 CPDP approaches proposed by researchers between 2008 and 2015. Through our benchmark, we answer the following research questions:\u2022 RQ1: Which CPDP approaches perform best in terms of F-measure, G-measure, AUC, and MCC?\u2022 RQ2: Does any CPDP approach consistently fulfill the performance criteria for successful predictions postulated by Zimmermann et al. [4], i.e., have at least 0.75 recall, 0.75 precision, and 0.75 accuracy?\u2022 RQ3: What is the impact of using only larger products (> 100 instances) with a certain balance (at least 5% defective instances and at least 5% non-defective instances) on the benchmark results?\u2022 RQ4: What is the impact of using a relatively small subset of a larger data set on the benchmark results?We identified 5 public data sets, which contain defect data about 86 software products that we used to answer these research question. The advantage of using multiple data sets was that we could increase the number of software products and, thereby, increase the external validity of our results. Moreover, we wanted to use multiple performance criteria for the evaluation of the CPDP approaches. Therefore, RQ1 ranks approaches not just using a single criterion, but using the four performance metrics AUC, F-measure, G-measure, and MCC. Existing approaches for the ranking of statistically different approaches neither account for software products from different data sets, nor multiple performance metrics. Therefore, we defined a new approach for the combination of separate rankings for the performance criteria and data sets, into one common ranking.Figure 1 depicts the results for RQ1. The results show that an approach proposed by Camargo Cruz and Ochimizu [1] performs best and even outperforms cross-validation. Moreover, our results show that only 6 of the 24 approaches outperform one of our baselines, i.e., using all data for training without any transfer learning. Regarding RQ2, we determined that predictions only seldomly achieve a high performance of 0.75 recall, precision, and accuracy. The best CPDP approaches only fulfill the criterion for 4 of the 86 products, i.e., 4.6% of the time. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice.RQ3 and RQ4 were used to see if results are affected by subsetting data, as is often done for defect prediction experiments. For RQ3, i.e., using a large subset, we determined no difference between using all data and using the subset. For RQ4, i.e., using a small subset of of data, we found that there are statistically signifcant differences in reported performances of up to 5%. Thus, the use of small subsets should be avoided."}, {"id": "conf/icse/GaoW18", "title": "MSeer: an advanced technique for locating multiple bugs in parallel.", "authors": ["Ruizhi Gao", "W. Eric Wong"], "DOIs": ["https://doi.org/10.1145/3180155.3182552", "http://ieeexplore.ieee.org/document/8453186"], "tag": ["Testing IV"], "abstract": "ABSTRACTIn practice, a program may contain multiple bugs. The simultaneous presence of these bugs may deteriorate the effectiveness of existing fault-localization techniques to locate program bugs. While it is acceptable to use all failed and successful tests to identify suspicious code for programs with exactly one bug, it is not appropriate to use the same approach for programs with multiple bugs because the due-to relationship between failed tests and underlying bugs cannot be easily identified. One solution is to generate fault-focused clusters by grouping failed tests caused by the same bug into the same clusters. We propose MSeer - an advanced fault localization technique for locating multiple bugs in parallel. Our major contributions include the use of (1) a revised Kendall tau distance to measure the distance between two failed tests, (2) an innovative approach to simultaneously estimate the number of clusters and assign initial medoids to these clusters, and (3) an improved K-medoids clustering algorithm to better identify the due-to relationship between failed tests and their corresponding bugs. Case studies on 840 multiple-bug versions of seven programs suggest that MSeer performs better in terms of effectiveness and efficiency than two other techniques for locating multiple bugs in parallel."}, {"id": "conf/icse/Arcuri18a", "title": "Journal first presentation of an experience report on applying software testing academic results in industry: we need usable automated test generation.", "authors": ["Andre Arcuri"], "DOIs": ["https://doi.org/10.1145/3180155.3182555", "http://ieeexplore.ieee.org/document/8453187"], "tag": ["Testing IV"], "abstract": "ABSTRACTWhat is the impact of software engineering research on current practices in industry? In this paper, I report on my direct experience as a PhD/post-doc working in software engineering research projects, and then spending the following five years as an engineer in two different companies (the first one being the same I worked in collaboration with during my post-doc). Given a background in software engineering research, what cutting-edge techniques and tools from academia did I use in my daily work when developing and testing the systems of these companies? Regarding validation and verification (my main area of research), the answer is rather short: as far as I can tell, only FindBugs. In this paper, I report on why this was the case, and discuss all the challenging, complex open problems we face in industry and which somehow are \"neglected\" in the academic circles. In particular, I will first discuss what actual tools I could use in my daily work, such as JaCoCo and Selenium. Then, I will discuss the main open problems I faced, particularly related to environment simulators, unit and web testing. After that, popular topics in academia are presented, such as UML, regression and mutation testing. Their lack of impact on the type of projects I worked on in industry is then discussed. Finally, from this industrial experience, I provide my opinions about how this situation can be improved, in particular related to how academics are evaluated, and advocate for a greater involvement into open-source projects."}, {"id": "conf/icse/WangSWXR18", "title": "CCAligner: a token based large-gap clone detector.", "authors": ["Pengcheng Wang", "Jeffrey Svajlenko", "Yanzhao Wu", "Yun Xu", "Chanchal K. Roy"], "DOIs": ["https://doi.org/10.1145/3180155.3180179", "http://ieeexplore.ieee.org/document/8453188"], "tag": ["Software evolution and maintenance II"], "abstract": "ABSTRACTCopying code and then pasting with large number of edits is a common activity in software development, and the pasted code is a kind of complicated Type-3 clone. Due to large number of edits, we consider the clone as a large-gap clone. Large-gap clone can reflect the extension of code, such as change and improvement. The existing state-of-the-art clone detectors suffer from several limitations in detecting large-gap clones. In this paper, we propose a tool, CCAligner, using code window that considers e edit distance for matching to detect large-gap clones. In our approach, a novel e-mismatch index is designed and the asymmetric similarity coefficient is used for similarity measure. We thoroughly evaluate CCAligner both for large-gap clone detection, and for general Type-1, Type-2 and Type-3 clone detection. The results show that CCAligner performs better than other competing tools in large-gap clone detection, and has the best execution time for 10MLOC input with good precision and recall in general Type-1 to Type-3 clone detection. Compared with existing state-of-the-art tools, CCAligner is the best performing large-gap clone detection tool, and remains competitive with the best clone detectors in general Type-1, Type-2 and Type-3 clone detection."}, {"id": "conf/icse/HassanW18", "title": "HireBuild: an automatic approach to history-driven repair of build scripts.", "authors": ["Foyzul Hassan", "Xiaoyin Wang"], "DOIs": ["https://doi.org/10.1145/3180155.3180181", "http://ieeexplore.ieee.org/document/8453189"], "tag": ["Software evolution and maintenance II"], "abstract": "ABSTRACTAdvancements in software build tools such as Maven reduce build management effort, but developers still need specialized knowledge and long time to maintain build scripts and resolve build failures. More recent build tools such as Gradle give developers greater extent of customization flexibility, but can be even more difficult to maintain. According to the TravisTorrent dataset of open-source software continuous integration, 22% of code commits include changes in build script files to maintain build scripts or to resolve build failures. Automated program repair techniques have great potential to reduce cost of resolving software failures, but the existing techniques mostly focus on repairing source code so that they cannot directly help resolving software build failures. To address this limitation, we propose HireBuild: <u>Hi</u>story-Driven <u>Rep</u>air of <u>Build</u> Scripts, the first approach to automatic patch generation for build scripts, using fix patterns automatically generated from existing build script fixes and recommending fix patterns based on build log similarity. From TravisTorrent dataset, we extracted 175 build failures and their corresponding fixes which revise Gradle build scripts. Among these 175 build failures, we used the 135 earlier build fixes for automatic fix-pattern generation and the more recent 40 build failures (fixes) for evaluation of our approach. Our experiment shows that our approach can fix 11 of 24 reproducible build failures, or 45% of the reproducible build failures, within comparable time of manual fixes."}, {"id": "conf/icse/KubelkaRB18", "title": "The road to live programming: insights from the practice.", "authors": ["Juraj Kubelka", "Romain Robbes", "Alexandre Bergel"], "DOIs": ["https://doi.org/10.1145/3180155.3180200", "http://ieeexplore.ieee.org/document/8453190"], "tag": ["Software evolution and maintenance II"], "abstract": "ABSTRACTLive Programming environments allow programmers to get feedback instantly while changing software. Liveness is gaining attention among industrial and open-source communities; several IDEs offer high degrees of liveness. While several studies looked at how programmers work during software evolution tasks, none of them consider live environments. We conduct such a study based on an analysis of 17 programming sessions of practitioners using Pharo, a mature Live Programming environment. The study is complemented by a survey and subsequent analysis of 16 programming sessions in additional languages, e.g., JavaScript. We document the approaches taken by developers during their work. We find that some liveness features are extensively used, and have an impact on the way developers navigate source code and objects in their work."}, {"id": "conf/icse/HoraSVR18", "title": "Assessing the threat of untracked changes in software evolution.", "authors": ["Andr\u00e9 C. Hora", "Danilo Silva", "Marco Tulio Valente", "Romain Robbes"], "DOIs": ["https://doi.org/10.1145/3180155.3180212", "http://ieeexplore.ieee.org/document/8453191"], "tag": ["Software evolution and maintenance II"], "abstract": "ABSTRACTWhile refactoring is extensively performed by practitioners, many Mining Software Repositories (MSR) approaches do not detect nor keep track of refactorings when performing source code evolution analysis. In the best case, keeping track of refactorings could be unnecessary work; in the worst case, these untracked changes could significantly affect the performance of MSR approaches. Since the extent of the threat is unknown, the goal of this paper is to assess whether it is significant. Based on an extensive empirical study, we answer positively: we found that between 10 and 21% of changes at the method level in 15 large Java systems are untracked. This results in a large proportion (25%) of entities that may have their histories split by these changes, and a measurable effect on at least two MSR approaches. We conclude that handling untracked changes should be systematically considered by MSR studies."}, {"id": "conf/icse/PelegSY18", "title": "Programming not only by example.", "authors": ["Hila Peleg", "Sharon Shoham", "Eran Yahav"], "DOIs": ["https://doi.org/10.1145/3180155.3180189", "http://ieeexplore.ieee.org/document/8453192"], "tag": ["Models and modeling II"], "abstract": "ABSTRACTRecent years have seen great progress in automated synthesis techniques that can automatically generate code based on some intent expressed by the programmer, but communicating this intent remains a major challenge. When the expressed intent is coarse-grained (for example, restriction on the expected type of an expression), the synthesizer often produces a long list of results for the programmer to choose from, shifting the heavy-lifting to the user. An alternative approach, successfully used in end-user synthesis, is programming by example (PBE), where the user leverages examples to interactively and iteratively refine the intent. However, using only examples is not expressive enough for programmers, who can observe the generated program and refine the intent by directly relating to parts of the generated program.We present a novel approach to interacting with a synthesizer using a granular interaction model. Our approach employs a rich interaction model where (i) the synthesizer decorates a candidate program with debug information that assists in understanding the program and identifying good or bad parts, and (ii) the user is allowed to provide feedback not only on the expected output of a program but also on the program itself. After identifying a program as (partially) correct or incorrect, the user can also explicitly indicate the good or bad parts, to allow the synthesizer to accept or discard parts of the program instead of discarding the program as a whole.We show the value of our approach in a controlled user study. Our study shows that participants have a strong preference for granular feedback instead of examples and can provide granular feedback much faster."}, {"id": "conf/icse/DegiovanniCARAF18", "title": "Goal-conflict likelihood assessment based on model counting.", "authors": ["Renzo Degiovanni", "Pablo F. Castro", "Marcelo Arroyo", "Marcelo Ruiz", "Nazareno Aguirre", "Marcelo F. Frias"], "DOIs": ["https://doi.org/10.1145/3180155.3180261", "http://ieeexplore.ieee.org/document/8453193"], "tag": ["Models and modeling II"], "abstract": "ABSTRACTIn goal-oriented requirements engineering approaches, conflict analysis has been proposed as an abstraction for risk analysis. Intuitively, given a set of expected goals to be achieved by the system-to-be, a conflict represents a subtle situation that makes goals diverge, i.e., not be satisfiable as a whole. Conflict analysis is typically driven by the identify-assess-control cycle, aimed at identifying, assessing and resolving conflicts that may obstruct the satisfaction of the expected goals. In particular, the assessment step is concerned with evaluating how likely the identified conflicts are, and how likely and severe are their consequences.So far, existing assessment approaches restrict their analysis to obstacles (conflicts that prevent the satisfaction of a single goal), and assume that certain probabilistic information on the domain is provided, that needs to be previously elicited from experienced users, statistical data or simulations. In this paper, we present a novel automated approach to assess how likely a conflict is, that applies to general conflicts (not only obstacles) without requiring probabilistic information on the domain. Intuitively, given the LTL formulation of the domain and of a set of goals to be achieved, we compute goal conflicts, and exploit string model counting techniques to estimate the likelihood of the occurrence of the corresponding conflicting situations and the severity in which these affect the satisfaction of the goals. This information can then be used to prioritize conflicts to be resolved, and suggest which goals to drive attention to for refinements."}, {"id": "conf/icse/LaraG18", "title": "A posteriori typing for model-driven engineering: concepts, analysis, and applications.", "authors": ["Juan de Lara", "Esther Guerra"], "DOIs": ["https://doi.org/10.1145/3180155.3182545", "http://ieeexplore.ieee.org/document/8453194"], "tag": ["Models and modeling II"], "abstract": "ABSTRACTModel-Driven Engineering (MDE) is a software engineering paradigm where models are actively used to specify, test, simulate, analyse and maintain the systems to be built, among other activities. Models can be defined using general-purpose modelling languages like the UML, but for particular domains, the use of domain-specific languages is pervasive. Either way, models must conform to a meta-model which defines their abstract syntax.In MDE, the definition of model management operations - often typed over project-specific meta-models - is recurrent. However, even if two operations are similar, they must be developed from scratch whenever they are applied to instances of different meta-models. This is so as operations defined (i.e., typed) over a meta-model cannot be directly reused for another. Part of this difficulty of reuse is because classes in meta-models are used in two ways: as templates to create objects and as static classifiers for them. These two aspects are inherently tied in most meta-modelling approaches, which results in unnecessarily rigid systems and hinders reusability of MDE artefacts.To enhance flexibility and reuse in MDE, we propose an approach to decouple object creation from typing [1]. The approach relies on standard mechanisms for object creation, and proposes the notion of a posteriori typing as a means to retype objects and enable multiple, partial, dynamic typings.A posteriori typing enhances flexibility because it allows models to be retyped with respect to other meta-models. Hence, we distinguish between creation meta-models used to construct models, and role meta-models into which models are retyped. This permits unanticipated reuse, as a model management operation defined for a role meta-model can be reused as-is with models built using a different creation meta-model, once such models are reclassified. Moreover our approach permits expressing some types of bidirectional model transformations by reclassification. The transformations defined as reclassifications have better performance than the equivalent ones defined with traditional transformation languages, because reclassification does not require creating new objects.In [1], we propose two mechanisms to define a posteriori typings: type-level (mappings between meta-models) and instance-level (set of model queries). The paper presents the underlying theory and type correctness criteria of both mechanisms, defines some analysis methods, identifies practical restrictions for retyping specifications, and demonstrates the feasibility of the approach by an implementation atop our meta-modelling tool MetaDepth. We also explore application scenarios of a posteriori typing (to define transformations, for model transformation reuse, and to improve transformation expressiveness by dynamic type change), and present an experiment showing the potential performance gains when expressing transformations as retypings.The tool is available at http://metadepth.org/. A catalogue of transformations expressed as retypings, and retypings bridging recurring meta-model heterogeneities, are available at http://miso.es/aposteriori/."}, {"id": "conf/icse/LangeNTY18", "title": "A static verification framework for message passing in Go using behavioural types.", "authors": ["Julien Lange", "Nicholas Ng", "Bernardo Toninho", "Nobuko Yoshida"], "DOIs": ["https://doi.org/10.1145/3180155.3180157", "http://ieeexplore.ieee.org/document/8453195"], "tag": ["Models and modeling II"], "abstract": "ABSTRACTThe Go programming language has been heavily adopted in industry as a language that efficiently combines systems programming with concurrency. Go's concurrency primitives, inspired by process calculi such as CCS and CSP, feature channel-based communication and lightweight threads, providing a distinct means of structuring concurrent software. Despite its popularity, the Go programming ecosystem offers little to no support for guaranteeing the correctness of message-passing concurrent programs.This work proposes a practical verification framework for message passing concurrency in Go by developing a robust static analysis that infers an abstract model of a program's communication behaviour in the form of a behavioural type, a powerful process calculi typing discipline. We make use of our analysis to deploy a model and termination checking based verification of the inferred behavioural type that is suitable for a range of safety and liveness properties of Go programs, providing several improvements over existing approaches. We evaluate our framework and its implementation on publicly available real-world Go code."}, {"id": "conf/icse/GrantCB18", "title": "Inferring and asserting distributed system invariants.", "authors": ["Stewart Grant", "Hendrik Cech", "Ivan Beschastnikh"], "DOIs": ["https://doi.org/10.1145/3180155.3180199", "http://ieeexplore.ieee.org/document/8453196"], "tag": ["Inference and invariants"], "abstract": "ABSTRACTDistributed systems are difficult to debug and understand. A key reason for this is distributed state, which is not easily accessible and must be pieced together from the states of the individual nodes in the system.We propose Dinv, an automatic approach to help developers of distributed systems uncover the runtime distributed state properties of their systems. Dinv uses static and dynamic program analyses to infer relations between variables at different nodes. For example, in a leader election algorithm, Dinv can relate the variable leader at different nodes to derive the invariant \u2200 nodes i, j, leaderi = leaderj. This can increase the developer's confidence in the correctness of their system. The developer can also use Dinv to convert an inferred invariant into a distributed runtime assertion on distributed state.We applied Dinv to several popular distributed systems, such as etcd Raft, Hashicorp Serf, and Taipei-Torrent, which have between 1.7K and 144K LOC and are widely used. Dinv derived useful invariants for these systems, including invariants that capture the correctness of distributed routing strategies, leadership, and key hash distribution. We also used Dinv to assert correctness of the inferred etcd Raft invariants at runtime, using these asserts to detect injected silent bugs."}, {"id": "conf/icse/RadhakrishnaLMM18", "title": "DroidStar: callback typestates for Android classes.", "authors": ["Arjun Radhakrishna", "Nicholas V. Lewchenko", "Shawn Meier", "Sergio Mover", "Krishna Chaitanya Sripada", "Damien Zufferey", "Bor-Yuh Evan Chang", "Pavol Cern\u00fd"], "DOIs": ["https://doi.org/10.1145/3180155.3180232", "http://ieeexplore.ieee.org/document/8453197"], "tag": ["Inference and invariants"], "abstract": "ABSTRACTEvent-driven programming frameworks, such as Android, are based on components with asynchronous interfaces. The protocols for interacting with these components can often be described by finite-state machines we dub callback typestates. Callback typestates are akin to classical typestates, with the difference that their outputs (callbacks) are produced asynchronously. While useful, these specifications are not commonly available, because writing them is difficult and error-prone.Our goal is to make the task of producing callback typestates significantly easier. We present a callback typestate assistant tool, DroidStar, that requires only limited user interaction to produce a callback typestate. Our approach is based on an active learning algorithm, L*. We improved the scalability of equivalence queries (a key component of L*), thus making active learning tractable on the Android system.We use DroidStar to learn callback typestates for Android classes both for cases where one is already provided by the documentation, and for cases where the documentation is unclear. The results show that DROIDSTAR learns callback typestates accurately and efficiently. Moreover, in several cases, the synthesized callback typestates uncovered surprising and undocumented behaviors."}, {"id": "conf/icse/XuMZZX18", "title": "Debugging with intelligence via probabilistic inference.", "authors": ["Zhaogui Xu", "Shiqing Ma", "Xiangyu Zhang", "Shuofei Zhu", "Baowen Xu"], "DOIs": ["https://doi.org/10.1145/3180155.3180237", "http://ieeexplore.ieee.org/document/8453198"], "tag": ["Inference and invariants"], "abstract": "ABSTRACTWe aim to debug a single failing execution without the assistance from other passing/failing runs. In our context, debugging is a process with substantial uncertainty - lots of decisions have to be made such as what variables shall be inspected first. To deal with such uncertainty, we propose to equip machines with human-like intelligence. Specifically, we develop a highly automated debugging technique that aims to couple human-like reasoning (e.g., dealing with uncertainty and fusing knowledge) with program semantics based analysis, to achieve benefits from the two and mitigate their limitations. We model debugging as a probabilistic inference problem, in which the likelihood of each executed statement instance and variable being correct/faulty is modeled by a random variable. Human knowledge, human-like reasoning rules and program semantics are modeled as conditional probability distributions, also called probabilistic constraints. Solving these constraints identifies the most likely faulty statements. Our results show that the technique is highly effective. It can precisely identify root causes for a set of real-world bugs in a very small number of interactions with developers, much smaller than a recent proposal that does not encode human intelligence. Our user study also confirms that it substantially improves human productivity."}, {"id": "conf/icse/BeyerJLW18", "title": "Reducer-based construction of conditional verifiers.", "authors": ["Dirk Beyer", "Marie-Christine Jakobs", "Thomas Lemberger", "Heike Wehrheim"], "DOIs": ["https://doi.org/10.1145/3180155.3180259", "http://ieeexplore.ieee.org/document/8453199"], "tag": ["Inference and invariants"], "abstract": "ABSTRACTDespite recent advances, software verification remains challenging. To solve hard verification tasks, we need to leverage not just one but several different verifiers employing different technologies. To this end, we need to exchange information between verifiers. Conditional model checking was proposed as a solution to exactly this problem: The idea is to let the first verifier output a condition which describes the state space that it successfully verified and to instruct the second verifier to verify the yet unverified state space using this condition. However, most verifiers do not understand conditions as input.In this paper, we propose the usage of an off-the-shelf construction of a conditional verifier from a given traditional verifier and a reducer. The reducer takes as input the program to be verified and the condition, and outputs a residual program whose paths cover the unverified state space described by the condition. As a proof of concept, we designed and implemented one particular reducer and composed three conditional model checkers from the three best verifiers at SV-COMP 2017. We defined a set of claims and experimentally evaluated their validity. All experimental data and results are available for replication."}, {"id": "conf/icse/RibeiroMT18", "title": "Challenges and pitfalls on surveying evidence in the software engineering technical literature: an exploratory study with novices.", "authors": ["Talita Vieira Ribeiro", "Jobson L. Massollar", "Guilherme Horta Travassos"], "DOIs": ["https://doi.org/10.1145/3180155.3182557", "http://ieeexplore.ieee.org/document/8453200"], "tag": ["Surveys and reviews"], "abstract": "ABSTRACTThe evidence-based software engineering approach advocates the use of scientific evidence by software engineers to support the adoption of software technologies in industrial software development and maintenance projects. Aside from the unavailability of scientific knowledge in industrial settings and the time required to acquire evidence in the software engineering (SE) field, additional challenges prevent practitioners, mainly those that are not experienced in research, from collecting knowledge from scientific sources to support the decisionmaking throughout their software projects."}, {"id": "conf/icse/ReyesDCJ18", "title": "Statistical errors in software engineering experiments: a preliminary literature review.", "authors": ["Rolando P. Reyes", "Oscar Dieste", "Efra\u00edn R. Fonseca C.", "Natalia Juristo"], "DOIs": ["https://doi.org/10.1145/3180155.3180161", "http://ieeexplore.ieee.org/document/8453201"], "tag": ["Surveys and reviews"], "abstract": "ABSTRACTBackground: Statistical concepts and techniques are often applied incorrectly, even in mature disciplines such as medicine or psychology. Surprisingly, there are very few works that study statistical problems in software engineering (SE). Aim: Assess the existence of statistical errors in SE experiments. Method: Compile the most common statistical errors in experimental disciplines. Survey experiments published in ICSE to assess whether errors occur in high quality SE publications. Results: The same errors as identified in others disciplines were found in ICSE experiments, where 30% of the reviewed papers included several error types such as: a) missing statistical hypotheses, b) missing sample size calculation, c) failure to assess statistical test assumptions, and d) uncorrected multiple testing. This rather large error rate is greater for research papers where experiments are confined to the validation section. The origin of the errors can be traced back to: a) researchers not having sufficient statistical training, and, b) a profusion of exploratory research. Conclusions: This paper provides preliminary evidence that SE research suffers from the same statistical problems as other experimental disciplines. However, the SE community appears to be unaware of any shortcomings in its experiments, whereas other disciplines work hard to avoid these threats. Further research is necessary to find the underlying causes and set up corrective measures, but there are some potentially effective actions and are a priori easy to implement: a) improve the statistical training of SE researchers, and b) enforce quality assessment and reporting guidelines in SE publications."}, {"id": "conf/icse/HuangZZBY18", "title": "Synthesizing qualitative research in software engineering: a critical review.", "authors": ["Xin Huang", "He Zhang", "Xin Zhou", "Muhammad Ali Babar", "Song Yang"], "DOIs": ["https://doi.org/10.1145/3180155.3180235", "http://ieeexplore.ieee.org/document/8453202"], "tag": ["Surveys and reviews"], "abstract": "ABSTRACTSynthesizing data extracted from primary studies is an integral component of the methodologies in support of Evidence Based Software Engineering (EBSE) such as System Literature Review (SLR). Since a large and increasing number of studies in Software Engineering (SE) incorporate qualitative data, it is important to systematically review and understand different aspects of the Qualitative Research Synthesis (QRS) being used in SE. We have reviewed the use of QRS methods in 328 SLRs published between 2005 and 2015. We also inquired the authors of 274 SLRs to confirm whether or not any QRS methods were used in their respective reviews. 116 of them provided the responses, which were included in our analysis. We found eight QRS methods applied in SE research, two of which, narrative synthesis and thematic synthesis, have been predominantly adopted by SE researchers for synthesizing qualitative data. Our study determines that a significant amount of missing knowledge and incomplete understanding of the defined QRS methods in the community. Our effort also identifies an initial set factors that may influence the selection and use of appropriate QRS methods in SE."}, {"id": "conf/icse/GazzolaMM18", "title": "Automatic software repair: a survey.", "authors": ["Luca Gazzola", "Daniela Micucci", "Leonardo Mariani"], "DOIs": ["https://doi.org/10.1145/3180155.3182526", "http://ieeexplore.ieee.org/document/8453203"], "tag": ["Search-based software engineering II"], "abstract": "ABSTRACTDebugging software failures is still a painful, time consuming, and expensive process. For instance, recent studies showed that debugging activities often account for about 50% of the overall development cost of software products [3]. There are many factors contributing to the cost of debugging, but the most impacting one is the extensive manual effort that is still required to identify and remove faults. So far, the automation of debugging activities essentially resulted in the development of techniques that provide useful insights about the possible locations of faults, the inputs and states of the application responsible for the failures, as well as the anomalous operations executed during failures. However, developers must still put a relevant effort on the analysis of the failed executions to exactly identify the faults that must be fixed. In addition, these techniques do not help the developers with the synthesis of an appropriate fix."}, {"id": "conf/icse/CasteleinASPD18", "title": "Search-based test data generation for SQL queries.", "authors": ["Jeroen Castelein", "Maur\u00edcio Finavaro Aniche", "Mozhan Soltani", "Annibale Panichella", "Arie van Deursen"], "DOIs": ["https://doi.org/10.1145/3180155.3180202", "http://ieeexplore.ieee.org/document/8453204"], "tag": ["Search-based software engineering II"], "abstract": "ABSTRACTDatabase-centric systems strongly rely on SQL queries to manage and manipulate their data. These SQL commands can range from very simple selections to queries that involve several tables, sub-queries, and grouping operations. And, as with any important piece of code, developers should properly test SQL queries. In order to completely test a SQL query, developers need to create test data that exercise all possible coverage targets in a query, e.g., JOINs and WHERE predicates. And indeed, this task can be challenging and time-consuming for complex queries. Previous studies have modeled the problem of generating test data as a constraint satisfaction problem and, with the help of SAT solvers, generate the required data. However, such approaches have strong limitations, such as partial support for queries with JOINs, subqueries, and strings (which are commonly used in SQL queries). In this paper, we model test data generation for SQL queries as a search-based problem. Then, we devise and evaluate three different approaches based on random search, biased random search, and genetic algorithms (GAs). The GA, in particular, uses a fitness function based on information extracted from the physical query plan of a database engine as search guidance. We then evaluate each approach in 2,135 queries extracted from three open source software and one industrial software system. Our results show that GA is able to completely cover 98.6% of all queries in the dataset, requiring only a few seconds per query. Moreover, it does not suffer from the limitations affecting state-of-the art techniques."}, {"id": "conf/icse/XueL18", "title": "Multi-objective integer programming approaches for solving optimal feature selection problem: a new perspective on multi-objective optimization problems in SBSE.", "authors": ["Yinxing Xue", "Yan-Fu Li"], "DOIs": ["https://doi.org/10.1145/3180155.3180257", "http://ieeexplore.ieee.org/document/8453205"], "tag": ["Search-based software engineering II"], "abstract": "ABSTRACTThe optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study we first expose the mathematical nature of this problem --- multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution)."}, {"id": "conf/icse/LuWY0N18", "title": "Automated refactoring of OCL constraints with search.", "authors": ["Hong Lu", "Shuai Wang", "Tao Yue", "Shaukat Ali", "Jan F. Nyg\u00e5rd"], "DOIs": ["https://doi.org/10.1145/3180155.3182546", "http://ieeexplore.ieee.org/document/8453206"], "tag": ["Search-based software engineering II"], "abstract": "ABSTRACTObject Constraint Language (OCL) constraints are typically used for providing precise semantics to models developed with the Unified Modeling Language (UML). When OCL constraints evolve in a regular basis, it is essential that they are easy to understand and maintain. For instance, in cancer registries, to ensure the quality of cancer data, more than one thousand medical rules are defined and evolve regularly. Such rules can be specified with OCL. It is, therefore, important to ensure the understandability and maintainability of medical rules specified with OCL.To tackle such a challenge, we propose an automated search-<u>b</u>ased <u>O</u>CL constraint <u>r</u>efactoring <u>a</u>pproach (SBORA) by defining and applying three OCL quality metrics (Complexity, Coupling, and Cohesion) and four semantics-preserving refactoring operators (i.e., Context Change, Swap, Split and Merge) which are encoded as potential solutions for search algorithms. A solution is therefore an optimal sequence of refactoring operators, which are sequentially applied to the original set of OCL constraints to automatically obtain a semantically equivalent set of OCL constraints with better understandability and maintainability in terms of Complexity, Coupling, and Cohesion.We evaluate SBORA along with six commonly used multi-objective search algorithms (e.g., Indicator-Based Evolutionary Algorithm (IBEA)) by employing four case studies from different domains: healthcare (i.e., cancer registry system from Cancer Registry of Norway (CRN)), Oil&Gas (i.e., subsea production systems), warehouse (i.e., handling systems), and an open source case study named SEPA. Results show: 1) IBEA achieves the best performance among all the search algorithms and 2) the refactoring approach along with IBEA can manage to reduce on average 29.25% Complexity and 39% Coupling and improve 47.75% Cohesion, as compared to the original OCL constraint set from CRN. To further test the performance of SBORA, we also applied it to refactor an OCL constraint set specified on the UML 2.3 metamodel and we obtained encouraging results. Furthermore, we conducted a controlled experiment with 96 subjects and results show that the understandability and maintainability of the original constraint set can be improved significantly from the perspectives of the 96 participants of the controlled experiment."}, {"id": "conf/icse/ChaHLO18", "title": "Automatically generating search heuristics for concolic testing.", "authors": ["Sooyoung Cha", "Seongjoon Hong", "Junhee Lee", "Hakjoo Oh"], "DOIs": ["https://doi.org/10.1145/3180155.3180166", "http://ieeexplore.ieee.org/document/8453207"], "tag": ["Search-based software engineering II"], "abstract": "ABSTRACTWe present a technique to automatically generate search heuristics for concolic testing. A key challenge in concolic testing is how to effectively explore the program's execution paths to achieve high code coverage in a limited time budget. Concolic testing employs a search heuristic to address this challenge, which favors exploring particular types of paths that are most likely to maximize the final coverage. However, manually designing a good search heuristic is nontrivial and typically ends up with suboptimal and unstable outcomes. The goal of this paper is to overcome this shortcoming of concolic testing by automatically generating search heuristics. We define a class of search heuristics, namely a parameterized heuristic, and present an algorithm that efficiently finds an optimal heuristic for each subject program. Experimental results with open-source C programs show that our technique successfully generates search heuristics that significantly outperform existing manually-crafted heuristics in terms of branch coverage and bug-finding."}], "2019": [{"id": "conf/icse/Liu0BKKKKT19", "title": "Learning to spot and refactor inconsistent method names.", "authors": ["Kui Liu", "Dongsun Kim", "Tegawend\u00e9 F. Bissyand\u00e9", "Tae-young Kim", "Kisub Kim", "Anil Koyuncu", "Suntae Kim", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00019", "https://dl.acm.org/citation.cfm?id=3339507"], "tag": ["Automated program repair 1"], "abstract": "To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild."}, {"id": "conf/icse/SahaSP19", "title": "Harnessing evolution for multi-hunk program repair.", "authors": ["Seemanta Saha", "Ripon K. Saha", "Mukul R. Prasad"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00020", "https://dl.acm.org/citation.cfm?id=3339508"], "tag": ["Automated program repair 1"], "abstract": "Despite significant advances in automatic program repair (APR) techniques over the past decade, practical deployment remains an elusive goal. One of the important challenges in this regard is the general inability of current APR techniques to produce patches that require edits in multiple locations, i.e., multi-hunk patches. In this work, we present a novel APR technique that generalizes single-hunk repair techniques to include an important class of multi-hunk bugs, namely bugs that may require applying a substantially similar patch at a number of locations. We term such sets of repair locations as evolutionary siblings - similar looking code, instantiated in similar contexts, that are expected to undergo similar changes. At the heart of our proposed method is an analysis to accurately identify a set of evolutionary siblings, for a given bug. This analysis leverages three distinct sources of information, namely the test-suite spectrum, a novel code similarity analysis, and the revision history of the project. The discovered siblings are then simultaneously repaired in a similar fashion. We instantiate this technique in a tool called HERCULES and demonstrate that it is able to correctly fix 46 bugs in the Defects4J dataset, the highest of any individual APR technique to date. This includes 15 multi-hunk bugs and overall 11 bugs which have not been fixed by any other technique so far."}, {"id": "conf/icse/TufanoPWBP19", "title": "On learning meaningful code changes via neural machine translation.", "authors": ["Michele Tufano", "Jevgenija Pantiuchina", "Cody Watson", "Gabriele Bavota", "Denys Poshyvanyk"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00021", "https://dl.acm.org/citation.cfm?id=3339509"], "tag": ["Automated program repair 1"], "abstract": "Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring."}, {"id": "conf/icse/RahmanPR19", "title": "Natural software revisited.", "authors": ["Musfiqur Rahman", "Dharani Palani", "Peter C. Rigby"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00022", "https://dl.acm.org/citation.cfm?id=3339511"], "tag": ["Mining of software properties and patterns"], "abstract": "Recent works have concluded that software code is more repetitive and predictable, i.e. more natural, than English texts. On re-examination, we find that much of the apparent \"naturalness\" of source code is due to the presence of language specific syntax, especially separators, such as semi-colons and brackets. For example, separators account for 44% of all tokens in our Java corpus. When we follow the NLP practices of eliminating punctuation (e.g., separators) and stopwords (e.g., keywords), we find that code is still repetitive and predictable, but to a lesser degree than previously thought. We suggest that SyntaxTokens be filtered to reduce noise in code recommenders. Unlike the code written for a particular project, API code usage is similar across projects: a file is opened and closed in the same manner regardless of domain. When we restrict our n-grams to those contained in the Java API, we find that API usages are highly repetitive. Since API calls are common across programs, researchers have made reliable statistical models to recommend sophisticated API call sequences. Sequential n-gram models were developed for natural languages. Code is usually represented by an AST which contains control and data flow, making n-grams models a poor representation of code. Comparing n-grams to statistical graph representations of the same codebase, we find that graphs are more repetitive and contain higherlevel patterns than n-grams. We suggest that future work focus on statistical code graphs models that accurately capture complex coding patterns. Our replication package makes our scripts and data available to future researchers[1]."}, {"id": "conf/icse/SainiFLY0SBL19", "title": "Towards automating precision studies of clone detectors.", "authors": ["Vaibhav Saini", "Farima Farmahinifarahani", "Yadong Lu", "Di Yang", "Pedro Martins", "Hitesh Sajnani", "Pierre Baldi", "Cristina V. Lopes"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00023", "https://dl.acm.org/citation.cfm?id=3339512"], "tag": ["Mining of software properties and patterns"], "abstract": "Current research in clone detection suffers from poor ecosystems for evaluating precision of clone detection tools. Corpora of labeled clones are scarce and incomplete, making evaluation labor intensive and idiosyncratic, and limiting intertool comparison. Precision-assessment tools are simply lacking. We present a semiautomated approach to facilitate precision studies of clone detection tools. The approach merges automatic mechanisms of clone classification with manual validation of clone pairs. We demonstrate that the proposed automatic approach has a very high precision and it significantly reduces the number of clone pairs that need human validation during precision experiments. Moreover, we aggregate the individual effort of multiple teams into a single evolving dataset of labeled clone pairs, creating an important asset for software clone research."}, {"id": "conf/icse/Du0LGZLJ19", "title": "Leopard: identifying vulnerable code for vulnerability assessment through program metrics.", "authors": ["Xiaoning Du", "Bihuan Chen", "Yuekang Li", "Jianmin Guo", "Yaqin Zhou", "Yang Liu", "Yu Jiang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00024", "https://dl.acm.org/citation.cfm?id=3339514"], "tag": ["Security 1"], "abstract": "Identifying potentially vulnerable locations in a code base is critical as a pre-step for effective vulnerability assessment; i.e., it can greatly help security experts put their time and effort to where it is needed most. Metric-based and pattern-based methods have been presented for identifying vulnerable code. The former relies on machine learning and cannot work well due to the severe imbalance between non-vulnerable and vulnerable code or lack of features to characterize vulnerabilities. The latter needs the prior knowledge of known vulnerabilities and can only identify similar but not new types of vulnerabilities. In this paper, we propose and implement a generic, lightweight and extensible framework, LEOPARD, to identify potentially vulnerable functions through program metrics. LEOPARD requires no prior knowledge about known vulnerabilities. It has two steps by combining two sets of systematically derived metrics. First, it uses complexity metrics to group the functions in a target application into a set of bins. Then, it uses vulnerability metrics to rank the functions in each bin and identifies the top ones as potentially vulnerable. Our experimental results on 11 real-world projects have demonstrated that, LEOPARD can cover 74.0% of vulnerable functions by identifying 20% of functions as vulnerable and outperform machine learning-based and static analysis-based techniques. We further propose three applications of LEOPARD for manual code review and fuzzing, through which we discovered 22 new bugs in real applications like PHP, radare2 and FFmpeg, and eight of them are new vulnerabilities."}, {"id": "conf/icse/FanWS0ZZ19", "title": "Smoke: scalable path-sensitive memory leak detection for millions of lines of code.", "authors": ["Gang Fan", "Rongxin Wu", "Qingkai Shi", "Xiao Xiao", "Jinguo Zhou", "Charles Zhang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00025", "https://dl.acm.org/citation.cfm?id=3339516"], "tag": ["Static analysis"], "abstract": "Detecting memory leak at industrial scale is still not well addressed, in spite of the tremendous effort from both industry and academia in the past decades. Existing work suffers from an unresolved paradox - a highly precise analysis limits its scalability and an imprecise one seriously hurts its precision or recall. In this work, we present SMOKE, a staged approach to resolve this paradox. In the first stage, instead of using a uniform precise analysis for all paths, we use a scalable but imprecise analysis to compute a succinct set of candidate memory leak paths. In the second stage, we leverage a more precise analysis to verify the feasibility of those candidates. The first stage is scalable, due to the design of a new sparse program representation, the use-flow graph (UFG), that models the problem as a polynomial-time state analysis. The second stage analysis is both precise and efficient, due to the smaller number of candidates and the design of a dedicated constraint solver. Experimental results show that SMOKE can finish checking industrial-sized projects, up to 8MLoC, in forty minutes with an average false positive rate of 24.4%. Besides, SMOKE is significantly faster than the state-of-the-art research techniques as well as the industrial tools, with the speedup ranging from 5.2X to 22.8X. In the twenty-nine mature and extensively checked benchmark projects, SMOKE has discovered thirty previously unknown memory leaks which were con?rmed by developers, and one even assigned a CVE ID."}, {"id": "conf/icse/KristensenM19", "title": "Reasonably-most-general clients for JavaScript library analysis.", "authors": ["Erik Krogh Kristensen", "Anders M\u00f8ller"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00026", "https://dl.acm.org/citation.cfm?id=3339517"], "tag": ["Static analysis"], "abstract": "A well-known approach to statically analyze libraries without having access to their client code is to model all possible clients abstractly using a most-general client. In dynamic languages, however, a most-general client would be too general: it may interact with the library in ways that are not intended by the library developer and are not realistic in actual clients, resulting in useless analysis results. In this work, we explore the concept of a reasonably-most-general client, in the context of a new static analysis tool REAGENT that aims to detect errors in TypeScript declaration files for JavaScript libraries. By incorporating different variations of reasonably-most-general clients into an existing static analyzer for JavaScript, we use REAGENT to study how different assumptions of client behavior affect the analysis results. We also show how REAGENT is able to find type errors in real-world TypeScript declaration files, and, once the errors have been corrected, to guarantee that no remaining errors exist relative to the selected assumptions."}, {"id": "conf/icse/HeoOY19", "title": "Resource-aware program analysis via online abstraction coarsening.", "authors": ["Kihong Heo", "Hakjoo Oh", "Hongseok Yang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00027", "https://dl.acm.org/citation.cfm?id=3339518"], "tag": ["Static analysis"], "abstract": "We present a new technique for developing a resource-aware program analysis. Such an analysis is aware of constraints on available physical resources, such as memory size, tracks its resource use, and adjusts its behaviors during fixpoint computation in order to meet the constraint and achieve high precision. Our resource-aware analysis adjusts behaviors by coarsening program abstraction, which usually makes the analysis consume less memory and time until completion. It does so multiple times during the analysis, under the direction of what we call a controller. The controller constantly intervenes in the fixpoint computation of the analysis and decides how much the analysis should coarsen the abstraction. We present an algorithm for learning a good controller automatically from benchmark programs. We applied our technique to a static analysis for C programs, where we control the degree of flow-sensitivity to meet a constraint on peak memory consumption. The experimental results with 18 real-world programs show that our algorithm can learn a good controller and the analysis with this controller meets the constraint and utilizes available memory effectively."}, {"id": "conf/icse/VassalloPGP19", "title": "Automated reporting of anti-patterns and decay in continuous integration.", "authors": ["Carmine Vassallo", "Sebastian Proksch", "Harald C. Gall", "Massimiliano Di Penta"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00028", "https://dl.acm.org/citation.cfm?id=3339520"], "tag": ["Continuous integration"], "abstract": "Continuous Integration (CI) is a widely-used software engineering practice. The software is continuously built so that changes can be easily integrated and issues such as unmet quality goals or style inconsistencies get detected early. Unfortunately, it is not only hard to introduce CI into an existing project, but it is also challenging to live up to the CI principles when facing tough deadlines or business decisions. Previous work has identified common anti-patterns that reduce the promised benefits of CI. Typically, these anti-patterns slowly creep into a project over time before they are identified. We argue that automated detection can help with early identification and prevent such a process decay. In this work, we further analyze this assumption and survey 124 developers about CI anti-patterns. From the results, we build CI-Odor, a reporting tool for CI processes that detects the existence of four relevant anti-patterns by analyzing regular build logs and repository information. In a study on the 18,474 build logs of 36 popular JAVA projects, we reveal the presence of 3,823 high-severity warnings spread across projects. We validate our reports in a survey among 13 original developers of these projects and through general feedback from 42 developers that confirm the relevance of our reports."}, {"id": "conf/icse/HeCHWPY19", "title": "A system identification based Oracle for control-CPS software fault localization.", "authors": ["Zhijian He", "Yao Chen", "Enyan Huang", "Qixin Wang", "Yu Pei", "Haidong Yuan"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00029", "https://dl.acm.org/citation.cfm?id=3339522"], "tag": ["Debugging and fault localization"], "abstract": "Control-CPS software fault localization (SFL, aka bug localization) is of critical importance as bugs may cause major failures, even injuries/deaths. To locate the bugs in control-CPSs, SFL tools often demand many labeled (\"correct\"/\"incorrect\") source code execution traces as inputs. To label the correctness of these traces, we must judge the corresponding control-CPS physical trajectories' correctness. However, unlike discrete outputs, the boundaries between correct and incorrect physical trajectories are often vague. The mechanism (aka oracle) to judge the physical trajectories' correctness thus becomes a major challenge. So far, the ad hoc practice of ``human oracles'' is still widely used, whose qualities heavily depend on the human experts' expertise and availability. This paper proposes an oracle based on the well adopted autoregressive system identification (AR-SI). With proven success for controlling black-box physical systems, AR-SI is adapted by us to identify the buggy control-CPS as a black-box. We use this identification result as an oracle to judge the control-CPS's behaviors, and propose a methodology to prepare traces for control-CPS debugging. Comprehensive evaluations on classic control-CPSs with injected real-life and artificial bugs show that our proposed approach significantly outperforms the human oracle approach in SFL accuracy (recall) and latency, and in oracle false positive/negative rates. Our approach also helps discover a new real-life bug in a consumer-grade control-CPS."}, {"id": "conf/icse/ZhaoYSLZZH19", "title": "ReCDroid: automatically reproducing Android application crashes from bug reports.", "authors": ["Yu Zhao", "Tingting Yu", "Ting Su", "Yang Liu", "Wei Zheng", "Jingzhi Zhang", "William G. J. Halfond"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00030", "https://dl.acm.org/citation.cfm?id=3339523"], "tag": ["Debugging and fault localization"], "abstract": "The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers heavily rely on bug reports in issue tracking systems to reproduce failures (e.g., crashes). However, the process of crash reproduction is often manually done by developers, making the resolution of bugs inefficient, especially that bug reports are often written in natural language. To improve the productivity of developers in resolving bug reports, in this paper, we introduce a novel approach, called ReCDroid, that can automatically reproduce crashes from bug reports for Android apps. ReCDroid uses a combination of natural language processing (NLP) and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash. We have evaluated ReCDroid on 51 original bug reports from 33 Android apps. The results show that ReCDroid successfully reproduced 33 crashes (63.5% success rate) directly from the textual description of bug reports. A user study involving 12 participants demonstrates that ReCDroid can improve the productivity of developers when resolving crash bug reports."}, {"id": "conf/icse/AmarR19", "title": "Mining historical test logs to predict bugs and localize faults in the test logs.", "authors": ["Anunay Amar", "Peter C. Rigby"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00031", "https://dl.acm.org/citation.cfm?id=3339525"], "tag": ["DevOps and logging"], "abstract": "Software testing is an integral part of modern software development. However, test runs can produce thousands of lines of logged output that make it difficult to find the cause of a fault in the logs. This problem is exacerbated by environmental failures that distract from product faults. In this paper we present techniques with the goal of capturing the maximum number of product faults, while flagging the minimum number of log lines for inspection. We observe that the location of a fault in a log should be contained in the lines of a failing test log. In contrast, a passing test log should not contain the lines related to a failure. Lines that occur in both a passing and failing log introduce noise when attempting to find the fault in a failing log. We introduce an approach where we remove the lines that occur in the passing log from the failing log. After removing these lines, we use information retrieval techniques to flag the most probable lines for investigation. We modify TF-IDF to identify the most relevant log lines related to past product failures. We then vectorize the logs and develop an exclusive version of KNN to identify which logs are likely to lead to product faults and which lines are the most probable indication of the failure. Our best approach, LogFaultFlagger finds 89% of the total faults and flags less than 1% of the total failed log lines for inspection. LogFaultFlagger drastically outperforms the previous work CAM. We implemented LogFaultFlagger as a tool at Ericsson where it presents fault prediction summaries to base station testers."}, {"id": "conf/icse/LiC0S19", "title": "Dlfinder: characterizing and detecting duplicate logging code smells.", "authors": ["Zhenhao Li", "Tse-Hsun (Peter) Chen", "Jinqiu Yang", "Weiyi Shang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00032", "https://dl.acm.org/citation.cfm?id=3339526"], "tag": ["DevOps and logging"], "abstract": "Developers rely on software logs for a wide variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncovered five patterns of duplicate logging code smells. For each instance of the code smell, we further manually identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the four manually studied systems and two additional systems: Camel and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 82 problematic code smell instances to developers and all of them have been fixed."}, {"id": "conf/icse/RahmanPW19", "title": "The seven sins: security smells in infrastructure as code scripts.", "authors": ["Akond Rahman", "Chris Parnin", "Laurie Williams"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00033", "https://dl.acm.org/citation.cfm?id=3339528"], "tag": ["Security 2"], "abstract": "Practitioners use infrastructure as code (IaC) scripts to provision servers and development environments. While developing IaC scripts, practitioners may inadvertently introduce security smells. Security smells are recurring coding patterns that are indicative of security weakness and can potentially lead to security breaches. The goal of this paper is to help practitioners avoid insecure coding practices while developing infrastructure as code (IaC) scripts through an empirical study of security smells in IaC scripts. We apply qualitative analysis on 1,726 IaC scripts to identify seven security smells. Next, we implement and validate a static analysis tool called Security Linter for Infrastructure as Code scripts (SLIC) to identify the occurrence of each smell in 15,232 IaC scripts collected from 293 open source repositories. We identify 21,201 occurrences of security smells that include 1,326 occurrences of hard-coded passwords. We submitted bug reports for 1,000 randomly-selected security smell occurrences. We obtain 212 responses to these bug reports, of which 148 occurrences were accepted by the development teams to be fixed. We observe security smells can have a long lifetime, e.g., a hard-coded secret can persist for as long as 98 months, with a median lifetime of 20 months."}, {"id": "conf/icse/NilizadehNP19", "title": "DifFuzz: differential fuzzing for side-channel analysis.", "authors": ["Shirin Nilizadeh", "Yannic Noller", "Corina S. Pasareanu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00034", "https://dl.acm.org/citation.cfm?id=3339529"], "tag": ["Security 2"], "abstract": "Side-channel attacks allow an adversary to uncover secret program data by observing the behavior of a program with respect to a resource, such as execution time, consumed memory or response size. Side-channel vulnerabilities are difficult to reason about as they involve analyzing the correlations between resource usage over multiple program paths. We present DifFuzz, a fuzzing-based approach for detecting side-channel vulnerabilities related to time and space. DifFuzz automatically detects these vulnerabilities by analyzing two versions of the program and using resource-guided heuristics to find inputs that maximize the difference in resource consumption between secret-dependent paths. The methodology of DifFuzz is general and can be applied to programs written in any language. For this paper, we present an implementation that targets analysis of Java programs, and uses and extends the Kelinci and AFL fuzzers. We evaluate DifFuzz on a large number of Java programs and demonstrate that it can reveal unknown side-channel vulnerabilities in popular applications. We also show that DifFuzz compares favorably against Blazer and Themis, two state-of-the-art analysis tools for finding side-channels in Java programs."}, {"id": "conf/icse/MotwaniB19", "title": "Automatically generating precise Oracles from structured natural language specifications.", "authors": ["Manish Motwani", "Yuriy Brun"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00035", "https://dl.acm.org/citation.cfm?id=3339531"], "tag": ["Test generation"], "abstract": "Software specifications often use natural language to describe the desired behavior, but such specifications are difficult to verify automatically. We present Swami, an automated technique that extracts test oracles and generates executable tests from structured natural language specifications. Swami focuses on exceptional behavior and boundary conditions that often cause field failures but that developers often fail to manually write tests for. Evaluated on the official JavaScript specification (ECMA-262), 98.4% of the tests Swami generated were precise to the specification. Using Swami to augment developer-written test suites improved coverage and identified 1 previously unknown defect and 15 missing JavaScript features in Rhino, 1 previously unknown defect in Node.js, and 18 semantic ambiguities in the ECMA-262 specification."}, {"id": "conf/icse/SedanoRP19", "title": "The product backlog.", "authors": ["Todd Sedano", "Paul Ralph", "C\u00e9cile P\u00e9raire"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00036", "https://dl.acm.org/citation.cfm?id=3339533"], "tag": ["Agile development"], "abstract": "Context: One of the most common artifacts in contemporary software projects is a product backlog comprising user stories, bugs, chores or other work items. However, little research has investigated how the backlog is generated or the precise role it plays in a project. Objective: The purpose of this paper is to determine what is a product backlog, what is its role, and how does it emerge? Method: Following Constructivist Grounded Theory, we conducted a two-year, five-month participant-observation study of eight software development projects at Pivotal, a large, international software company. We interviewed 56 software engineers, product designers, and product managers.We conducted a survey of 27 product designers. We alternated between analysis and theoretical sampling until achieving theoretical saturation. Results: We observed 13 practices and 6 obstacles related to product backlog generation. Limitations: Grounded Theory does not support statistical generalization. While the proposed theory of product backlogs appears widely applicable, organizations with different software development cultures may use different practices. Conclusion: The product backlog is simultaneously a model of work to be done and a boundary object that helps bridge the gap between the processes of generating user stories and realizing them in working code. It emerges from sensemaking (the team making sense of the project context) and coevolution (a cognitive process where the team simultaneously refines its understanding of the problematic context and fledgling solution concepts)."}, {"id": "conf/icse/PanCP0L19", "title": "Easy modelling and verification of unpredictable and preemptive interrupt-driven systems.", "authors": ["Minxue Pan", "Shouyu Chen", "Yu Pei", "Tian Zhang", "Xuandong Li"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00037", "https://dl.acm.org/citation.cfm?id=3339535"], "tag": ["Analysis and verification"], "abstract": "The widespread real-time and embedded systems are mostly interrupt-driven because their heavy interaction with the environment is often initiated by interrupts. With the interrupt arrival being unpredictable and the interrupt handling being preemptive, a large number of possible system behaviours are generated, which makes the correctness assurance of such systems difficult and costly. Model checking is considered to be one of the effective methods for exhausting behavioural state space for correctness. However, existing modelling approaches for interrupt-driven systems are based on either calculus or automata theory, and have a steep learning curve. To address this problem, we propose a new modelling language called interrupt sequence diagram (ISD). By extending the popular UML sequence diagram notations, the ISD supports the modelling of interrupts' essential features visually and concisely. We also propose an automata-based semantics for ISD, based on which ISD can be transformed to a subset of hybrid automata so as to leverage the abundant off-the-shelf checkers. Experiments on examples from both real-world and existing literature were conducted, and the results demonstrate our approach's usability and effectiveness."}, {"id": "conf/icse/BaeLR19", "title": "Towards understanding and reasoning about Android interoperations.", "authors": ["Sora Bae", "Sungho Lee", "Sukyoung Ryu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00038", "https://dl.acm.org/citation.cfm?id=3339536"], "tag": ["Analysis and verification"], "abstract": "Hybrid applications (apps) have become one of the most attractive options for mobile app developers thanks to its support for portability and device-specific features. Android hybrid apps, for example, support portability via JavaScript, device-specific features via Android Java, and seamless interactions between them. However, their interoperation semantics is often under-documented and unintuitive, which makes hybrid apps vulnerable to errors. While recent research has addressed such vulnerabilities, none of them are based on any formal grounds. In this paper, we present the first formal specification of Android interoperability to establish a firm ground for understanding and reasoning about the interoperations. We identify its semantics via extensive testing and thorough inspection of Android source code. We extend an existing multi-language semantics to formally express the key features of hybrid mechanisms, dynamic and indistinguishable interoperability. Based on the extensions, we incrementally define a formal interoperation semantics and disclose its numerous unintuitive and inconsistent behaviors. Moreover, on top of the formal semantics, we devise a lightweight type system that can detect bugs due to the unintuitive inter-language communication. We show that it detects more bugs more efficiently than HybriDroid, the state-of-the-art analyzer of Android hybrid apps, in real-world Android hybrid apps."}, {"id": "conf/icse/RutledgePKOPZ19", "title": "Zero-overhead path prediction with progressive symbolic execution.", "authors": ["Richard Rutledge", "Sunjae Park", "Haider A. Khan", "Alessandro Orso", "Milos Prvulovic", "Alenka G. Zajic"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00039", "https://dl.acm.org/citation.cfm?id=3339537"], "tag": ["Analysis and verification"], "abstract": "In previous work, we introduced zero-overhead profiling (ZOP), a technique that leverages the electromagnetic emissions generated by the computer hardware to profile a program without instrumenting it. Although effective, ZOP has several shortcomings: it requires test inputs that achieve extensive code coverage for its training phase; it predicts path profiles instead of complete execution traces; and its predictions can suffer unrecoverable accuracy losses. In this paper, we present zero-overhead path prediction (ZOP-2), an approach that extends ZOP and addresses its limitations. First, ZOP-2 achieves high coverage during training through progressive symbolic execution (PSE)-symbolic execution of increasingly small program fragments. Second, ZOP-2 predicts complete execution traces, rather than path profiles. Finally, ZOP-2 mitigates the problem of path mispredictions by using a stateless approach that can recover from prediction errors. We evaluated our approach on a set of benchmarks with promising results; for the cases considered, (1) ZOP-2 achieved over 90% path prediction accuracy, and (2) PSE covered feasible paths missed by traditional symbolic execution, thus boosting ZOP-2's accuracy."}, {"id": "conf/icse/KiPDKZ19", "title": "Mimic: UI compatibility testing system for Android apps.", "authors": ["Taeyeon Ki", "Chang Min Park", "Karthik Dantu", "Steven Y. Ko", "Lukasz Ziarek"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00040", "https://dl.acm.org/citation.cfm?id=3339539"], "tag": ["Mobile apps"], "abstract": "This paper proposes Mimic, an automated UI compatibility testing system for Android apps. Mimic is designed specifically for comparing the UI behavior of an app across different devices, different Android versions, and different app versions. This design choice stems from a common problem that Android developers and researchers face-how to test whether or not an app behaves consistently across different environments or internal changes. Mimic allows Android app developers to easily perform backward and forward compatibility testing for their apps. It also enables a clear comparison between a stable version of app and a newer version of app. In doing so, Mimic allows multiple testing strategies to be used, such as randomized or sequential testing. Finally, Mimic programming model allows such tests to be scripted with much less developer effort than other comparable systems. Additionally, Mimic allows parallel testing with multiple testing devices and thereby speeds up testing time. To demonstrate these capabilities, we perform extensive tests for each of the scenarios described above. Our results show that Mimic is effective in detecting forward and backward compatibility issues, and verify runtime behavior of apps. Our evaluation also shows that Mimic significantly reduces the development burden for developers."}, {"id": "conf/icse/XiaoWCWG19", "title": "IconIntent: automatic identification of sensitive UI widgets based on icon classification for Android apps.", "authors": ["Xusheng Xiao", "Xiaoyin Wang", "Zhihao Cao", "Hanlin Wang", "Peng Gao"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00041", "https://dl.acm.org/citation.cfm?id=3339540"], "tag": ["Mobile apps"], "abstract": "Many mobile applications (i.e., apps) include UI widgets to use or collect users' sensitive data. Thus, to identify suspicious sensitive data usage such as UI-permission mismatch, it is crucial to understand the intentions of UI widgets. However, many UI widgets leverage icons of specific shapes (object icons) and icons embedded with text (text icons) to express their intentions, posing challenges for existing detection techniques that analyze only textual data to identify sensitive UI widgets. In this work, we propose a novel app analysis framework, ICONINTENT, that synergistically combines program analysis and icon classification to identify sensitive UI widgets in Android apps. ICONINTENT automatically associates UI widgets and icons via static analysis on app's UI layout files and code, and then adapts computer vision techniques to classify the associated icons into eight categories of sensitive data. Our evaluations of ICONINTENT on 150 apps from Google Play show that ICONINTENT can detect 248 sensitive UI widgets in 97 apps, achieving a precision of 82.4%. When combined with SUPOR, the state-of-the-art sensitive UI widget identification technique based on text analysis, SUPOR +ICONINTENT can detect 487 sensitive UI widgets (101.2% improvement over SUPOR only), and reduces suspicious permissions to be inspected by 50.7% (129.4% improvement over SUPOR only)."}, {"id": "conf/icse/GuSMC0YZLS19", "title": "Practical GUI testing of Android applications via model abstraction and refinement.", "authors": ["Tianxiao Gu", "Chengnian Sun", "Xiaoxing Ma", "Chun Cao", "Chang Xu", "Yuan Yao", "Qirun Zhang", "Jian Lu", "Zhendong Su"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00042", "https://dl.acm.org/citation.cfm?id=3339542"], "tag": ["Model-based software engineering"], "abstract": "This paper introduces a new, fully automated modelbased approach for effective testing of Android apps. Different from existing model-based approaches that guide testing with a static GUI model (i.e., the model does not evolve its abstraction during testing, and is thus often imprecise), our approach dynamically optimizes the model by leveraging the runtime information during testing. This capability of model evolution significantly improves model precision, and thus dramatically enhances the testing effectiveness compared to existing approaches, which our evaluation confirms.We have realized our technique in a practical tool, APE. On 15 large, widely-used apps from the Google Play Store, APE outperforms the state-of-the-art Android GUI testing tools in terms of both testing coverage and the number of detected unique crashes. To further demonstrate APE's effectiveness and usability, we conduct another evaluation of APE on 1,316 popular apps, where it found 537 unique crashes. Out of the 38 reported crashes, 13 have been fixed and 5 have been confirmed."}, {"id": "conf/icse/ZhangHMBLU19", "title": "AutoTap: synthesizing and repairing trigger-action programs using LTL properties.", "authors": ["Lefan Zhang", "Weijia He", "Jesse Martinez", "Noah Brackenbury", "Shan Lu", "Blase Ur"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00043", "https://dl.acm.org/citation.cfm?id=3339543"], "tag": ["Model-based software engineering"], "abstract": "End-user programming, particularly trigger-action programming (TAP), is a popular method of letting users express their intent for how smart devices and cloud services interact. Unfortunately, sometimes it can be challenging for users to correctly express their desires through TAP. This paper presents AutoTap, a system that lets novice users easily specify desired properties for devices and services. AutoTap translates these properties to linear temporal logic (LTL) and both automatically synthesizes property-satisfying TAP rules from scratch and repairs existing TAP rules. We designed AutoTap based on a user study about properties users wish to express. Through a second user study, we show that novice users made significantly fewer mistakes when expressing desired behaviors using AutoTap than using TAP rules. Our experiments show that AutoTap is a simple and effective option for expressive end-user programming."}, {"id": "conf/icse/Sivaraman0BK19", "title": "Active inductive logic programming for code search.", "authors": ["Aishwarya Sivaraman", "Tianyi Zhang", "Guy Van den Broeck", "Miryung Kim"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00044", "https://dl.acm.org/citation.cfm?id=3339545"], "tag": ["Program comprehension and reuse"], "abstract": "Modern search techniques either cannot efficiently incorporate human feedback to refine search results or cannot express structural or semantic properties of desired code. The key insight of our interactive code search technique ALICE is that user feedback can be actively incorporated to allow users to easily express and refine search queries. We design a query language to model the structure and semantics of code as logic facts. Given a code example with user annotations, ALICE automatically extracts a logic query from code features that are tagged as important. Users can refine the search query by labeling one or more examples as desired (positive) or irrelevant (negative). ALICE then infers a new logic query that separates positive examples from negative examples via active inductive logic programming. Our comprehensive simulation experiment shows that ALICE removes a large number of false positives quickly by actively incorporating user feedback. Its search algorithm is also robust to user labeling mistakes. Our choice of leveraging both positive and negative examples and using nested program structure as an inductive bias is effective in refining search queries. Compared with an existing interactive code search technique, ALICE does not require a user to manually construct a search pattern and yet achieves comparable precision and recall with much fewer search iterations. A case study with real developers shows that ALICE is easy to use and helps express complex code patterns."}, {"id": "conf/icse/MalikPP19", "title": "NL2Type: inferring JavaScript function types from natural language information.", "authors": ["Rabee Sohail Malik", "Jibesh Patra", "Michael Pradel"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00045", "https://dl.acm.org/citation.cfm?id=3339546"], "tag": ["Program comprehension and reuse"], "abstract": "JavaScript is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal IDE support, difficult to understand APIs, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers to annotate code with types. This paper presents NL2Type, a learning-based approach for predicting likely type signatures of JavaScript functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, LSTM-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 JavaScript files from real-world projects. NL2Type predicts types with a precision of 84.1% and a recall of 78.9% when considering only the top-most suggestion, and with a precision of 95.5% and a recall of 89.6% when considering the top-5 suggestions. The approach outperforms both JSNice, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and DeepTyper, a recent type prediction approach that is also based on deep learning. Beyond predicting types, NL2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations."}, {"id": "conf/icse/0001YLK19", "title": "Analyzing and supporting adaptation of online code examples.", "authors": ["Tianyi Zhang", "Di Yang", "Crista Lopes", "Miryung Kim"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00046", "https://dl.acm.org/citation.cfm?id=3339547"], "tag": ["Program comprehension and reuse"], "abstract": "Developers often resort to online Q&A forums such as Stack Overflow (SO) for filling their programming needs. Although code examples on those forums are good starting points, they are often incomplete and inadequate for developers' local program contexts; adaptation of those examples is necessary to integrate them to production code. As a consequence, the process of adapting online code examples is done over and over again, by multiple developers independently. Our work extensively studies these adaptations and variations, serving as the basis for a tool that helps integrate these online code examples in a target context in an interactive manner. We perform a large-scale empirical study about the nature and extent of adaptations and variations of SO snippets. We construct a comprehensive dataset linking SO posts to GitHub counterparts based on clone detection, time stamp analysis, and explicit URL references. We then qualitatively inspect 400 SO examples and their GitHub counterparts and develop a taxonomy of 24 adaptation types. Using this taxonomy, we build an automated adaptation analysis technique on top of GumTree to classify the entire dataset into these types. We build a Chrome extension called ExampleStack that automatically lifts an adaptation-aware template from each SO example and its GitHub counterparts to identify hot spots where most changes happen. A user study with sixteen programmers shows that seeing the commonalities and variations in similar GitHub counterparts increases their confidence about the given SO example, and helps them grasp a more comprehensive view about how to reuse the example differently and avoid common pitfalls."}, {"id": "conf/icse/HortonP19", "title": "DockerizeMe: automatic inference of environment dependencies for python code snippets.", "authors": ["Eric Horton", "Chris Parnin"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00047", "https://dl.acm.org/citation.cfm?id=3339548"], "tag": ["Program comprehension and reuse"], "abstract": "Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50% of public Python gists fail due to an import error for a missing library. We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies."}, {"id": "conf/icse/DmeiriTWBLDVR19", "title": "BugSwarm: mining and continuously growing a dataset of reproducible failures and fixes.", "authors": ["David A. Tomassi", "Naji Dmeiri", "Yichen Wang", "Antara Bhowmick", "Yen-Chuan Liu", "Premkumar T. Devanbu", "Bogdan Vasilescu", "Cindy Rubio-Gonz\u00e1lez"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00048", "https://dl.acm.org/citation.cfm?id=3339550"], "tag": ["SE datasets, research infrastructure, and methodology"], "abstract": "Fault-detection, localization, and repair methods are vital to software quality; but it is difficult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and fixes are vital to good experimental evaluation of approaches to software quality, but they are difficult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully configurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually."}, {"id": "conf/icse/ZhaoXC0019", "title": "ActionNet: vision-based workflow action recognition from programming screencasts.", "authors": ["Dehai Zhao", "Zhenchang Xing", "Chunyang Chen", "Xin Xia", "Guoqiang Li"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00049", "https://dl.acm.org/citation.cfm?id=3339551"], "tag": ["SE datasets, research infrastructure, and methodology"], "abstract": "Programming screencasts have two important applications in software engineering context: study developer behaviors, information needs and disseminate software engineering knowledge. Although programming screencasts are easy to produce, they are not easy to analyze or index due to the image nature of the data. Existing techniques extract only content from screencasts, but ignore workflow actions by which developers accomplish programming tasks. This significantly limits the effective use of programming screencasts in downstream applications. In this paper, we are the first to present a novel technique for recognizing workflow actions in programming screencasts. Our technique exploits image differencing and Convolutional Neural Network (CNN) to analyze the correspondence and change of consecutive frames, based on which nine classes of frequent developer actions can be recognized from programming screencasts. Using programming screencasts from Youtube, we evaluate different configurations of our CNN model and the performance of our technique for developer action recognition across developers, working environments and programming languages. Using screencasts of developers' real work, we demonstrate the usefulness of our technique in a practical application for actionaware extraction of key-code frames in developers' work."}, {"id": "conf/icse/Eyolfson019", "title": "How C++ developers use immutability declarations: an empirical study.", "authors": ["Jonathan Eyolfson", "Patrick Lam"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00050", "https://dl.acm.org/citation.cfm?id=3339553"], "tag": ["Studying developers"], "abstract": "Best practices for developers, as encoded in recent programming language designs, recommend the use of immutability whenever practical. However, there is a lack of empirical evidence about the uptake of this advice. Our goal is to understand the usage of immutability by C++ developers in practice. This work investigates how C++ developers use immutability by analyzing their use of the C++ immutability qualifier, const, and by analyzing the code itself. We answer the following broad questions about const usage: 1) do developers actually write non-trivial (more than 3 methods) immutable classes and immutable methods? 2) do developers label their immutable classes and methods? We analyzed 7 medium-to-large open source projects and collected two sources of empirical data: 1) const annotations by developers, indicating an intent to write immutable code; and 2) the results of a simple static analysis which identified easily const-able methods---those that clearly did not mutate state. We estimate that 5% of non-trivial classes (median) are immutable. We found the vast majority of classes do carry immutability labels on methods: surprisingly, developers const-annotate 46% of methods, and we estimate that at least 51% of methods could be const-annotated. Furthermore, developers missed immutability labels on at least 6% of unannotated methods. We provide an in-depth discussion on how developers use const and the results of our analyses."}, {"id": "conf/icse/Chattopadhyay0G19", "title": "Latent patterns in activities: a field study of how developers manage context.", "authors": ["Souti Chattopadhyay", "Nicholas Nelson", "Yenifer Ramirez Gonzalez", "Annel Amelia Leon", "Rahul Pandita", "Anita Sarma"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00051", "https://dl.acm.org/citation.cfm?id=3339554"], "tag": ["Studying developers"], "abstract": "In order to build efficient tools that support complex programming tasks, it is imperative that we understand how developers program. We know that developers create a context around their programming task by gathering relevant information. We also know that developers decompose their tasks recursively into smaller units. However, important gaps exist in our knowledge about: (1) the role that context plays in supporting smaller units of tasks, (2) the relationship that exists among these smaller units, and (3) how context flows across them. The goal of this research is to gain a better understanding of how developers structure their tasks and manage context through a field study of ten professional developers in an industrial setting. Our analysis reveals that developers decompose their tasks into smaller units with distinct goals, that specific patterns exist in how they sequence these smaller units, and that developers may maintain context between those smaller units with related goals."}, {"id": "conf/icse/AbidSDAM19", "title": "Developer reading behavior while summarizing Java methods: size and context matters.", "authors": ["Nahla J. Abid", "Bonita Sharif", "Natalia Dragan", "Hend Alrasheed", "Jonathan I. Maletic"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00052", "https://dl.acm.org/citation.cfm?id=3339555"], "tag": ["Studying developers"], "abstract": "An eye-tracking study of 18 developers reading and summarizing Java methods is presented. The developers provide a written summary for methods assigned to them. In total, 63 methods are used from five different systems. Previous studies on this topic use only short methods presented in isolation usually as images. In contrast, this work presents the study in the Eclipse IDE allowing access to all the source code in the system. The developer can navigate via scrolling and switching files while writing the summary. New eye-tracking infrastructure allows for this improvement in the study environment. Data collected includes eye gazes on source code, written summaries, and time to complete each summary. Unlike prior work that concluded developers focus on the signature the most, these results indicate that they tend to focus on the method body more than the signature. Moreover, both experts and novices tend to revisit control flow terms rather than reading them for a long period. They also spend a significant amount of gaze time and have higher gaze visits when they read call terms. Experts tend to revisit the body of the method significantly more frequently than its signature as the size of the method increases. Moreover, experts tend to write their summaries from source code lines that they read the most."}, {"id": "conf/icse/HuangLKSHLW19", "title": "Distilling neural representations of data structure manipulation using fMRI and fNIRS.", "authors": ["Yu Huang", "Xinyu Liu", "Ryan Krueger", "Tyler Santander", "Xiaosu Hu", "Kevin Leach", "Westley Weimer"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00053", "https://dl.acm.org/citation.cfm?id=3339556"], "tag": ["Studying developers"], "abstract": "Data structures permeate many aspects of software engineering, but their associated human cognitive processes are not thoroughly understood. We leverage medical imaging and insights from the psychological notion of spatial ability to decode the neural representations of several fundamental data structures and their manipulations. In a human study involving 76 participants, we examine list, array, tree, and mental rotation tasks using both functional near-infrared spectroscopy (fNIRS) and functional magnetic resonance imaging (fMRI). We find a nuanced relationship: data structure and spatial operations use the same focal regions of the brain but to different degrees. They are related but distinct neural tasks. In addition, more difficult computer science problems induce higher cognitive load than do problems of pure spatial reasoning. Finally, while fNIRS is less expensive and more permissive, there are some computing-relevant brain regions that only fMRI can reach."}, {"id": "conf/icse/PhilipBKMN19", "title": "FastLane: test minimization for rapidly deployed large-scale online services.", "authors": ["Adithya Abraham Philip", "Ranjita Bhagwan", "Rahul Kumar", "Chandra Shekhar Maddila", "Nachiappan Nagappan"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00054", "https://dl.acm.org/citation.cfm?id=3339558"], "tag": ["Test selection and prioritization"], "abstract": "Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases. This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that our techniques can save 18.04%, i.e., almost a fifth of test-time while obtaining a test outcome accuracy of 99.99%."}, {"id": "conf/icse/CrucianiMVB19", "title": "Scalable approaches for test suite reduction.", "authors": ["Emilio Cruciani", "Breno Miranda", "Roberto Verdecchia", "Antonia Bertolino"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00055", "https://dl.acm.org/citation.cfm?id=3339559"], "tag": ["Test selection and prioritization"], "abstract": "Test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large-size test suites. Most existing techniques are too expensive for handling modern massive systems and moreover depend on artifacts, such as code coverage metrics or specification models, that are not commonly available at large scale. We present a family of novel very efficient approaches for similarity-based test suite reduction that apply algorithms borrowed from the big data domain together with smart heuristics for finding an evenly spread subset of test cases. The approaches are very general since they only use as input the test cases themselves (test source code or command line input). We evaluate four approaches in a version that selects a fixed budget B of test cases, and also in an adequate version that does the reduction guaranteeing some fixed coverage. The results show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efficiency. When applied to a suite of more than 500K real world test cases, the most efficient of the four approaches could select B test cases (for varying B values) in less than 10 seconds."}, {"id": "conf/icse/ZhuLSG19", "title": "A framework for checking regression test selection tools.", "authors": ["Chenguang Zhu", "Owolabi Legunsen", "August Shi", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00056", "https://dl.acm.org/citation.cfm?id=3339560"], "tag": ["Test selection and prioritization"], "abstract": "Regression test selection (RTS) reduces regression testing costs by re-running only tests that can change behavior due to code changes. Researchers and large software organizations recently developed and adopted several RTS tools to deal with the rapidly growing costs of regression testing. As RTS tools gain adoption, it becomes critical to check that they are correct and efficient. Unfortunately, checking RTS tools currently relies solely on limited tests that RTS tool developers manually write. We present RTSCheck, the first framework for checking RTS tools. RTSCheck feeds evolving programs (i.e., sequences of program revisions) to an RTS tool and checks the output against rules inspired by existing RTS test suites. Violations of these rules are likely due to deviations from expected RTS tool behavior, and indicative of bugs in the tool. RTSCheck uses three components to obtain evolving programs: (1) AutoEP automatically generates evolving programs and corresponding tests, (2) DefectsEP uses buggy and fixed program revisions from bug databases, and (3) EvoEP uses sequences of program revisions from actual open-source projects' histories. We used RTSCheck to check three recently developed RTS tools for Java: Clover, Ekstazi, and STARTS. RTSCheck discovered 27 bugs in these three tools."}, {"id": "conf/icse/AbadGZF19", "title": "Supporting analysts by dynamic extraction and classification of requirements-related knowledge.", "authors": ["Zahra Shakeri Hossein Abad", "Vincenzo Gervasi", "Didar Zowghi", "Behrouz H. Far"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00057", "https://dl.acm.org/citation.cfm?id=3339562"], "tag": ["Requirements"], "abstract": "In many software development projects, analysts are required to deal with systems' requirements from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents. Accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario. Our approach is to mine real-time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation. We propose to use both generative and discriminating methods. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modelling of natural language processing tasks. We used an extended version of Support Vector Machines (SVMs) with variable-sized feature vectors to efficiently and dynamically extract and classify requirements-related knowledge from the existing documents. To evaluate the performance of our approach intuitively and quantitatively, we used edit distance and precision/recall metrics. We show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate. Furthermore, we found that statistical and linguistic parameters such as smoothing methods, and words contiguity and order features can impact the performance of both extraction and classification tasks."}, {"id": "conf/icse/AryaWGC19", "title": "Analysis and detection of information types of open source software issue discussions.", "authors": ["Deeksha Arya", "Wenting Wang", "Jin L. C. Guo", "Jinghui Cheng"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00058", "https://dl.acm.org/citation.cfm?id=3339564"], "tag": ["Software analytics"], "abstract": "Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders."}, {"id": "conf/icse/Murphy-HillSSJW19", "title": "Do developers discover new tools on the toilet?", "authors": ["Emerson R. Murphy-Hill", "Edward K. Smith", "Caitlin Sadowski", "Ciera Jaspan", "Collin Winter", "Matthew Jorde", "Andrea Knight", "Andrew Trenk", "Steve Gross"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00059", "https://dl.acm.org/citation.cfm?id=3339566"], "tag": ["Tool taxonomy and adoption"], "abstract": "Maintaining awareness of useful tools is a substantial challenge for developers. Physical newsletters are a simple technique to inform developers about tools. In this paper, we evaluate such a technique, called Testing on the Toilet, by performing a mixed-methods case study. We first quantitatively evaluate how effective this technique is by applying statistical causal inference over six years of data about tools used by thousands of developers. We then qualitatively contextualize these results by interviewing and surveying 382 developers, from authors to editors to readers. We found that the technique was generally effective at increasing software development tool use, although the increase varied depending on factors such as the breadth of applicability of the tool, the extent to which the tool has reached saturation, and the memorability of the tool name."}, {"id": "conf/icse/KavalerTVF19", "title": "Tool choice matters: JavaScript quality assurance tools and usage outcomes in GitHub projects.", "authors": ["David Kavaler", "Asher Trockman", "Bogdan Vasilescu", "Vladimir Filkov"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00060", "https://dl.acm.org/citation.cfm?id=3339567"], "tag": ["Tool taxonomy and adoption"], "abstract": "Quality assurance automation is essential in modern software development. In practice, this automation is supported by a multitude of tools that fit different needs and require developers to make decisions about which tool to choose in a given context. Data and analytics of the pros and cons can inform these decisions. Yet, in most cases, there is a dearth of empirical evidence on the effectiveness of existing practices and tool choices. We propose a general methodology to model the time-dependent effect of automation tool choice on four outcomes of interest: prevalence of issues, code churn, number of pull requests, and number of contributors, all with a multitude of controls. On a large data set of npm JavaScript projects, we extract the adoption events for popular tools in three task classes: linters, dependency managers, and coverage reporters. Using mixed methods approaches, we study the reasons for the adoptions and compare the adoption effects within each class, and sequential tool adoptions across classes. We find that some tools within each group are associated with more beneficial outcomes than others, providing an empirical perspective for the benefits of each. We also find that the order in which some tools are implemented is associated with varying outcomes."}, {"id": "conf/icse/YangZSS00X19", "title": "Hunting for bugs in code coverage tools via randomized differential testing.", "authors": ["Yibiao Yang", "Yuming Zhou", "Hao Sun", "Zhendong Su", "Zhiqiang Zuo", "Lei Xu", "Baowen Xu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00061", "https://dl.acm.org/citation.cfm?id=3339569"], "tag": ["Unit testing"], "abstract": "Reliable code coverage tools are critically important as it is heavily used to facilitate many quality assurance activities, such as software testing, fuzzing, and debugging. However, little attention has been devoted to assessing the reliability of code coverage tools. In this study, we propose a randomized differential testing approach to hunting for bugs in the most widely used C code coverage tools. Specifically, by generating random input programs, our approach seeks for inconsistencies in code coverage reports produced by different code coverage tools, and then identifies inconsistencies as potential code coverage bugs. To effectively report code coverage bugs, we addressed three specific challenges: (1) How to filter out duplicate test programs as many of them triggering the same bugs in code coverage tools; (2) how to automatically reduce large test programs to much smaller ones that have the same properties; and (3) how to determine which code coverage tools have bugs? The extensive evaluations validate the effectiveness of our approach, resulting in 42 and 28 confirmed/fixed bugs for gcov and llvm-cov, respectively. This case study indicates that code coverage tools are not as reliable as it might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of code coverage tools. This work opens up a new direction in code coverage validation which calls for more attention in this area."}, {"id": "conf/icse/DelplanqueDPBE19", "title": "Rotten green tests.", "authors": ["Julien Delplanque", "St\u00e9phane Ducasse", "Guillermo Polito", "Andrew P. Black", "Anne Etien"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00062", "https://dl.acm.org/citation.cfm?id=3339570"], "tag": ["Unit testing"], "abstract": "Unit tests are a tenant of agile programming methodologies, and are widely used to improve code quality and prevent code regression. A green (passing) test is usually taken as a robust sign that the code under test is valid. However, some green tests contain assertions that are never executed. We call such tests Rotten Green Tests. Rotten Green Tests represent a case worse than a broken test: they report that the code under test is valid, but in fact do not test that validity. We describe an approach to identify rotten green tests by combining simple static and dynamic call-site analyses. Our approach takes into account test helper methods, inherited helpers, and trait compositions, and has been implemented in a tool called DrTest. DrTest reports no false negatives, yet it still reports some false positives due to conditional use or multiple test contexts. Using DrTest we conducted an empirical evaluation of 19,905 real test cases in mature projects of the Pharo ecosystem. The results of the evaluation show that the tool is effective; it detected 294 tests as rotten-green tests that contain assertions that are not executed. Some rotten tests have been \u201csleeping\u201d in Pharo for at least 5 years."}, {"id": "conf/icse/XuSYX19", "title": "VFix: value-flow-guided precise program repair for null pointer dereferences.", "authors": ["Xuezheng Xu", "Yulei Sui", "Hua Yan", "Jingling Xue"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00063", "https://dl.acm.org/citation.cfm?id=3339572"], "tag": ["Automated repair 2"], "abstract": "Automated Program Repair (APR) faces a key challenge in efficiently generating correct patches from a potentially infinite solution space. Existing approaches, which attempt to reason about the entire solution space, can be ineffective (by often producing no plausible patches at all) and imprecise (by often producing plausible but incorrect patches). We present VFIX, a new value-flow-guided APR approach, to fix null pointer exception (NPE) bugs by considering a substantially reduced solution space in order to greatly increase the number of correct patches generated. By reasoning about the data and control dependences in the program, VFIX can identify bug-relevant repair statements more accurately and generate more correct repairs than before. VFIX outperforms a set of 8 state-of-the-art APR tools in fixing the NPE bugs in Defects4j in terms of both precision (by correctly fixing 3 times as many bugs as the most precise one and 50% more than all the bugs correctly fixed by these 8 tools altogether) and efficiency (by producing a correct patch in minutes instead of hours)."}, {"id": "conf/icse/LeB00LP19", "title": "On reliability of patch correctness assessment.", "authors": ["Xuan-Bach D. Le", "Lingfeng Bao", "David Lo", "Xin Xia", "Shanping Li", "Corina S. Pasareanu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00064", "https://dl.acm.org/citation.cfm?id=3339573"], "tag": ["Automated repair 2"], "abstract": "Current state-of-the-art automatic software repair (ASR) techniques rely heavily on incomplete specifications, or test suites, to generate repairs. This, however, may cause ASR tools to generate repairs that are incorrect and hard to generalize. To assess patch correctness, researchers have been following two methods separately: (1) Automated annotation, wherein patches are automatically labeled by an independent test suite (ITS) - a patch passing the ITS is regarded as correct or generalizable, and incorrect otherwise, (2) Author annotation, wherein authors of ASR techniques manually annotate the correctness labels of patches generated by their and competing tools. While automated annotation cannot ascertain that a patch is actually correct, author annotation is prone to subjectivity. This concern has caused an on-going debate on the appropriate ways to assess the effectiveness of numerous ASR techniques proposed recently. In this work, we propose to assess reliability of author and automated annotations on patch correctness assessment. We do this by first constructing a gold set of correctness labels for 189 randomly selected patches generated by 8 state-of-the-art ASR techniques through a user study involving 35 professional developers as independent annotators. By measuring inter-rater agreement as a proxy for annotation quality - as commonly done in the literature - we demonstrate that our constructed gold set is on par with other high-quality gold sets. We then compare labels generated by author and automated annotations with this gold set to assess reliability of the patch assessment methodologies. We subsequently report several findings and highlight implications for future studies."}, {"id": "conf/icse/ChenFMWG19", "title": "How reliable is the crowdsourced knowledge of security implementation?", "authors": ["Mengsu Chen", "Felix Fischer", "Na Meng", "Xiaoyin Wang", "Jens Grossklags"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00065", "https://dl.acm.org/citation.cfm?id=3339575"], "tag": ["Crowdsourced knowledge and feedback"], "abstract": "Stack Overflow (SO) is the most popular online Q&A site for developers to share their expertise in solving programming issues. Given multiple answers to a certain question, developers may take the accepted answer, the answer from a person with high reputation, or the one frequently suggested. However, researchers recently observed that SO contains exploitable security vulnerabilities in the suggested code of popular answers, which found their way into security-sensitive high-profile applications that millions of users install every day. This observation inspires us to explore the following questions: How much can we trust the security implementation suggestions on SO? If suggested answers are vulnerable, can developers rely on the community's dynamics to infer the vulnerability and identify a secure counterpart? To answer these highly important questions, we conducted a comprehensive study on security-related SO posts by contrasting secure and insecure advice with the community-given content evaluation. Thereby, we investigated whether SO's gamification approach on incentivizing users is effective in improving security properties of distributed code examples. Moreover, we traced the distribution of duplicated samples over given answers to test whether the community behavior facilitates or prevents propagation of secure and insecure code suggestions within SO. We compiled 953 different groups of similar security-related code examples and labeled their security, identifying 785 secure answer posts and 644 insecure answer posts. Compared with secure suggestions, insecure ones had higher view counts (36,508 vs. 18,713), received a higher score (14 vs. 5), and had significantly more duplicates (3.8 vs. 3.0) on average. 34% of the posts provided by highly reputable so-called trusted users were insecure. Our findings show that based on the distribution of secure and insecure code on SO, users being laymen in security rely on additional advice and guidance. However, the community-given feedback does not allow differentiating secure from insecure choices. The reputation mechanism fails in indicating trustworthy users with respect to security questions, ultimately leaving other users wandering around alone in a software security minefield."}, {"id": "conf/icse/0008ZBPL19", "title": "Pattern-based mining of opinions in Q&A websites.", "authors": ["Bin Lin", "Fiorella Zampetti", "Gabriele Bavota", "Massimiliano Di Penta", "Michele Lanza"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00066", "https://dl.acm.org/citation.cfm?id=3339576"], "tag": ["Crowdsourced knowledge and feedback"], "abstract": "Informal documentation contained in resources such as Q&A websites (e.g., Stack Overflow) is a precious resource for developers, who can find there examples on how to use certain APIs, as well as opinions about pros and cons of such APIs. Automatically identifying and classifying such opinions can alleviate developers' burden in performing manual searches, and can be used to recommend APIs that are good from some points of view (e.g., performance), or highlight those less ideal from other perspectives (e.g., compatibility). We propose POME (Pattern-based Opinion MinEr), an approach that leverages natural language parsing and pattern-matching to classify Stack Overflow sentences referring to APIs according to seven aspects (e.g., performance, usability), and to determine their polarity (positive vs negative). The patterns have been inferred by manually analyzing 4,346 sentences from Stack Overflow linked to a total of 30 APIs. We evaluated POME by (i) comparing the pattern-matching approach with machine learners leveraging the patterns themselves as well as n-grams extracted from Stack Overflow posts; (ii) assessing the ability of POME to detect the polarity of sentences, as compared to sentiment-analysis tools; (iii) comparing POME with the state-of-the-art Stack Overflow opinion mining approach, Opiner, through a study involving 24 human evaluators. Our study shows that POME exhibits a higher precision than a state-of-the-art technique (Opiner), in terms of both opinion aspect identification and polarity assessment."}, {"id": "conf/icse/GhorbaniGM19", "title": "Detection and repair of architectural inconsistencies in Java.", "authors": ["Negar Ghorbani", "Joshua Garcia", "Sam Malek"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00067", "https://dl.acm.org/citation.cfm?id=3339578"], "tag": ["Dependencies"], "abstract": "Java is one of the most widely used programming languages. However, the absence of explicit support for architectural constructs, such as software components, in the programming language itself has prevented software developers from achieving the many benefits that come with architecture-based development. To address this issue, Java 9 has introduced the Java Platform Module System (JPMS), resulting in the first instance of encapsulation of modules with rich software architectural interfaces added to a mainstream programming language. The primary goal of JPMS is to construct and maintain large applications efficiently-as well as improve the encapsulation, security, and maintainability of Java applications in general and the JDK itself. A challenge, however, is that module declarations do not necessarily reflect actual usage of modules in an application, allowing developers to mistakenly specify inconsistent dependencies among the modules. In this paper, we formally define 8 inconsistent modular dependencies that may arise in Java-9 applications. We also present DARCY, an approach that leverages these definitions and static program analyses to automatically (1) detect the specified inconsistent dependencies within Java applications and (2) repair those identified inconsistencies. The results of our experiments, conducted over 38 open-source Java-9 applications, indicate that architectural inconsistencies are widespread and demonstrate the benefits of DARCY in automated detection and repair of these inconsistencies."}, {"id": "conf/icse/WangWWLTZYC19", "title": "Could I have a stack trace to examine the dependency conflict issue?", "authors": ["Ying Wang", "Ming Wen", "Rongxin Wu", "Zhenwei Liu", "Shin Hwei Tan", "Zhiliang Zhu", "Hai Yu", "Shing-Chi Cheung"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00068", "https://dl.acm.org/citation.cfm?id=3339579"], "tag": ["Dependencies"], "abstract": "Intensive use of libraries in Java projects brings potential risk of dependency conflicts, which occur when a project directly or indirectly depends on multiple versions of the same library or class. When this happens, JVM loads one version and shadows the others. Runtime exceptions can occur when methods in the shadowed versions are referenced. Although project management tools such as Maven are able to give warnings of potential dependency conflicts when a project is built, developers often ask for crashing stack traces before examining these warnings. It motivates us to develop Riddle, an automated approach that generates tests and collects crashing stack traces for projects subject to risk of dependency conflicts. Riddle, built on top of Asm and Evosuite, combines condition mutation, search strategies and condition restoration. We applied Riddle on 19 real-world Java projects with duplicate libraries or classes. We reported 20 identified dependency conflicts including their induced crashing stack traces and the details of generated tests. Among them, 15 conflicts were confirmed by developers as real issues, and 10 were readily fixed. The evaluation results demonstrate the effectiveness and usefulness of Riddle."}, {"id": "conf/icse/CuiLCZFJGQ19", "title": "Investigating the impact of multiple dependency structures on software defects.", "authors": ["Di Cui", "Ting Liu", "Yuanfang Cai", "Qinghua Zheng", "Qiong Feng", "Wuxia Jin", "Jiaqi Guo", "Yu Qu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00069", "https://dl.acm.org/citation.cfm?id=3339580"], "tag": ["Dependencies"], "abstract": "Over the past decades, numerous approaches were proposed to help practitioner to predict or locate defective files. These techniques often use syntactic dependency, history co-change relation, or semantic similarity. The problem is that, it remains unclear whether these different dependency relations will present similar accuracy in terms of defect prediction and localization. In this paper, we present our systematic investigation of this question from the perspective of software architecture. Considering files involved in each dependency type as an individual design space, we model such a design space using one DRSpace. We derived 3 DRSpaces for each of the 117 Apache open source projects, with 643,079 revision commits and 101,364 bug reports in total, and calculated their interactions with defective files. The experiment results are surprising: the three dependency types present significantly different architectural views, and their interactions with defective files are also drastically different. Intuitively, they play completely different roles when used for defect prediction/localization. The good news is that the combination of these structures has the potential to improve the accuracy of defect prediction/localization. In summary, our work provides a new perspective regarding to which type(s) of relations should be used for the task of defect prediction/localization. These quantitative and qualitative results also advance our knowledge of the relationship between software quality and architectural views formed using different dependency types."}, {"id": "conf/icse/ChenFCSLLX19", "title": "StoryDroid: automated generation of storyboard for Android apps.", "authors": ["Sen Chen", "Lingling Fan", "Chunyang Chen", "Ting Su", "Wenhe Li", "Yang Liu", "Lihua Xu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00070", "https://dl.acm.org/citation.cfm?id=3339582"], "tag": ["Requirements engineering for mass-market software"], "abstract": "Mobile apps are now ubiquitous. Before developing a new app, the development team usually endeavors painstaking efforts to review many existing apps with similar purposes. The review process is crucial in the sense that it reduces market risks and provides inspiration for app development. However, manual exploration of hundreds of existing apps by different roles (e.g., product manager, UI/UX designer, developer) in a development team can be ineffective. For example, it is difficult to completely explore all the functionalities of the app in a short period of time. Inspired by the conception of storyboard in movie production, we propose a system, StoryDroid, to automatically generate the storyboard for Android apps, and assist different roles to review apps efficiently. Specifically, StoryDroid extracts the activity transition graph and leverages static analysis techniques to render UI pages to visualize the storyboard with the rendered pages. The mapping relations between UI pages and the corresponding implementation code (e.g., layout code, activity code, and method hierarchy) are also provided to users. Our comprehensive experiments unveil that StoryDroid is effective and indeed useful to assist app development. The outputs of StoryDroid enable several potential applications, such as the recommendation of UI design and layout code."}, {"id": "conf/icse/JoshiFM19", "title": "Statistical algorithmic profiling for randomized approximate programs.", "authors": ["Keyur Joshi", "Vimuth Fernando", "Sasa Misailovic"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00071", "https://dl.acm.org/citation.cfm?id=3339584"], "tag": ["Trends and challenges in SE"], "abstract": "Many modern applications require low-latency processing of large data sets, often by using approximate algorithms that trade accuracy of the results for faster execution or reduced memory consumption. Although the algorithms provide probabilistic accuracy and performance guarantees, a software developer who implements these algorithms has little support from existing tools. Standard profilers do not consider accuracy of the computation and do not check whether the outputs of these programs satisfy their accuracy specifications. We present AXPROF, an algorithmic profiling framework for analyzing randomized approximate programs. The developer provides the accuracy specification as a formula in a mathematical notation, using probability or expected value predicates. AXPROF automatically generates statistical reasoning code. It first constructs the empirical models of accuracy, time, and memory consumption. It then selects and runs appropriate statistical tests that can, with high confidence, determine if the implementation satisfies the specification. We used AXPROF to profile 15 approximate applications from three domains - data analytics, numerical linear algebra, and approximate computing. AXPROF was effective in finding bugs and identifying various performance optimizations. In particular, we discovered five previously unknown bugs in the implementations of the algorithms and created fixes, guided by AXPROF."}, {"id": "conf/icse/KhatchadourianT19", "title": "Safe automated refactoring for intelligent parallelization of Java 8 streams.", "authors": ["Raffi Khatchadourian", "Yiming Tang", "Mehdi Bagherzadeh", "Syed Ahmed"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00072", "https://dl.acm.org/citation.cfm?id=3339586"], "tag": ["Concurrency"], "abstract": "Streaming APIs are becoming more pervasive in mainstream Object-Oriented programming languages. For example, the Stream API introduced in Java 8 allows for functional-like, MapReduce-style operations in processing both finite and infinite data structures. However, using this API efficiently involves subtle considerations like determining when it is best for stream operations to run in parallel, when running operations in parallel can be less efficient, and when it is safe to run in parallel due to possible lambda expression side-effects. In this paper, we present an automated refactoring approach that assists developers in writing efficient stream code in a semantics-preserving fashion. The approach, based on a novel data ordering and typestate analysis, consists of preconditions for automatically determining when it is safe and possibly advantageous to convert sequential streams to parallel and unorder or de-parallelize already parallel streams. The approach was implemented as a plug-in to the Eclipse IDE, uses the WALA and SAFE analysis frameworks, and was evaluated on 11 Java projects consisting of ?642K lines of code. We found that 57 of 157 candidate streams (36.31%) were refactorable, and an average speedup of 3.49 on performance tests was observed. The results indicate that the approach is useful in optimizing stream code to their full potential."}, {"id": "conf/icse/ChangDGW0H19", "title": "Detecting atomicity violations for event-driven Node.js applications.", "authors": ["Xiaoning Chang", "Wensheng Dou", "Yu Gao", "Jie Wang", "Jun Wei", "Tao Huang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00073", "https://dl.acm.org/citation.cfm?id=3339587"], "tag": ["Concurrency"], "abstract": "Node.js has been widely-used as an event-driven server-side architecture. To improve performance, a task in a Node.js application is usually divided into a group of events, which are non-deterministically scheduled by Node.js. Developers may assume that the group of events (named atomic event group) should be atomically processed, without interruption. However, the atomicity of an atomic event group is not guaranteed by Node.js, and thus other events may interrupt the execution of the atomic event group, break down the atomicity and cause unexpected results. Existing approaches mainly focus on event race among two events, and cannot detect high-level atomicity violations among a group of events. In this paper, we propose NodeAV, which can predictively detect atomicity violations in Node.js applications based on an execution trace. Based on happens-before relations among events in an execution trace, we automatically identify a pair of events that should be atomically processed, and use predefined atomicity violation patterns to detect atomicity violations. We have evaluated NodeAV on real-world Node.js applications. The experimental results show that NodeAV can effectively detect atomicity violations in these Node.js applications."}, {"id": "conf/icse/Yin0L019", "title": "Parallel refinement for multi-threaded program verification.", "authors": ["Liangze Yin", "Wei Dong", "Wanwei Liu", "Ji Wang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00074", "https://dl.acm.org/citation.cfm?id=3339588"], "tag": ["Concurrency"], "abstract": "Program verification is one of the most important methods to ensuring the correctness of concurrent programs. However, due to the path explosion problem, concurrent program verification is usually time consuming, which hinders its scalability to industrial programs. Parallel processing is a mainstream technique to deal with those problems which require mass computing. Hence, designing parallel algorithms to improve the performance of concurrent program verification is highly desired. This paper focuses on parallelization of the abstraction refinement technique, one of the most efficient techniques for concurrent program verification. We present a parallel refinement framework which employs multiple engines to refine the abstraction in parallel. Different from existing work which parallelizes the search process, our method achieves the effect of parallelization by refinement constraint and learnt clause sharing, so that the number of required iterations can be significantly reduced. We have implemented this framework on the scheduling constraint based abstraction refinement method, one of the best methods for concurrent program verification. Experiments on SV-COMP 2018 show the encouraging results of our method. For those complex programs requiring a large number of iterations, our method can obtain a linear reduction of the iteration number and significantly improve the verification performance."}, {"id": "conf/icse/YatishJTT19", "title": "Mining software defects: should we consider affected releases?", "authors": ["Suraj Yatish", "Jirayus Jiarpakdee", "Patanamon Thongtanunam", "Chakkrit Tantithamthavorn"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00075", "https://dl.acm.org/citation.cfm?id=3339590"], "tag": ["Defect prediction"], "abstract": "With the rise of the Mining Software Repositories (MSR) field, defect datasets extracted from software repositories play a foundational role in many empirical studies related to software quality. At the core of defect data preparation is the identification of post-release defects. Prior studies leverage many heuristics (e.g., keywords and issue IDs) to identify post-release defects. However, such the heuristic approach is based on several assumptions, which pose common threats to the validity of many studies. In this paper, we set out to investigate the nature of the difference of defect datasets generated by the heuristic approach and the realistic approach that leverages the earliest affected release that is realistically estimated by a software development team for a given defect. In addition, we investigate the impact of defect identification approaches on the predictive accuracy and the ranking of defective modules that are produced by defect models. Through a case study of defect datasets of 32 releases, we find that that the heuristic approach has a large impact on both defect count datasets and binary defect datasets. Surprisingly, we find that the heuristic approach has a minimal impact on defect count models, suggesting that future work should not be too concerned about defect count models that are constructed using heuristic defect datasets. On the other hand, using defect datasets generated by the realistic approach lead to an improvement in the predictive accuracy of defect classification models."}, {"id": "conf/icse/CabralMSM19", "title": "Class imbalance evolution and verification latency in just-in-time software defect prediction.", "authors": ["George G. Cabral", "Leandro L. Minku", "Emad Shihab", "Suhaib Mujahid"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00076", "https://dl.acm.org/citation.cfm?id=3339591"], "tag": ["Defect prediction"], "abstract": "Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that re-build classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency -- the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP."}, {"id": "conf/icse/LeeC19", "title": "FLOSS participants' perceptions about gender and inclusiveness: a survey.", "authors": ["Amanda Lee", "Jeffrey C. Carver"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00077", "https://dl.acm.org/citation.cfm?id=3339593"], "tag": ["Developer biases and trust"], "abstract": "Background: While FLOSS projects espouse openness and acceptance for all, in practice, female contributors often face discriminatory barriers to contribution. Aims: In this paper, we examine the extent to which these problems still exist. We also study male and female contributors' perceptions of other contributors. Method: We surveyed participants from 15 FLOSS projects, asking a series of open-ended, closed-ended, and behavioral scale questions to gather information about the issue of gender in FLOSS projects. Results: Though many of those we surveyed expressed a positive sentiment towards females who participate in FLOSS projects, some were still strongly against their inclusion. Often, the respondents who were against inclusiveness also believed their own sentiments were the prevailing belief in the community, contrary to our findings. Others did not see the purpose of attempting to be inclusive, expressing the sentiment that a discussion of gender has no place in FLOSS. Conclusions: FLOSS projects have started to move forwards in terms of gender acceptance. However, there is still a need for more progress in the inclusion of gender-diverse contributors."}, {"id": "conf/icse/QiuNBSV19", "title": "Going farther together: the impact of social capital on sustained participation in open source.", "authors": ["Huilian Sophie Qiu", "Alexander Nolte", "Anita Brown", "Alexander Serebrenik", "Bogdan Vasilescu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00078", "https://dl.acm.org/citation.cfm?id=3339594"], "tag": ["Developer biases and trust"], "abstract": "Sustained participation by contributors in opensource software is critical to the survival of open-source projects and can provide career advancement benefits to individual contributors. However, not all contributors reap the benefits of open-source participation fully, with prior work showing that women are particularly underrepresented and at higher risk of disengagement. While many barriers to participation in open-source have been documented in the literature, relatively little is known about how the social networks that open-source contributors form impact their chances of long-term engagement. In this paper we report on a mixed-methods empirical study of the role of social capital (i.e., the resources people can gain from their social connections) for sustained participation by women and men in open-source GitHub projects. After combining survival analysis on a large, longitudinal data set with insights derived from a user survey, we confirm that while social capital is beneficial for prolonged engagement for both genders, women are at disadvantage in teams lacking diversity in expertise."}, {"id": "conf/icse/ImtiazMCRBM19", "title": "Investigating the effects of gender bias on GitHub.", "authors": ["Nasif Imtiaz", "Justin Middleton", "Joymallya Chakraborty", "Neill Robson", "Gina Bai", "Emerson R. Murphy-Hill"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00079", "https://dl.acm.org/citation.cfm?id=3339595"], "tag": ["Developer biases and trust"], "abstract": "Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature.We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub,then evaluate those hypotheses quantitatively. While our results how that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men."}, {"id": "conf/icse/YouLMPZ019", "title": "SLF: fuzzing without valid seed inputs.", "authors": ["Wei You", "Xuwei Liu", "Shiqing Ma", "David Mitchel Perry", "Xiangyu Zhang", "Bin Liang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00080", "https://dl.acm.org/citation.cfm?id=3339597"], "tag": ["Fuzzing"], "abstract": "Fuzzing is an important technique to detect software bugs and vulnerabilities. It works by mutating a small set of seed inputs to generate a large number of new inputs. Fuzzers' performance often substantially degrades when valid seed inputs are not available. Although existing techniques such as symbolic execution can generate seed inputs from scratch, they have various limitations hindering their applications in real-world complex software. In this paper, we propose a novel fuzzing technique that features the capability of generating valid seed inputs. It piggy-backs on AFL to identify input validity checks and the input fields that have impact on such checks. It further classifies these checks according to their relations to the input. Such classes include arithmetic relation, object offset, data structure length and so on. A multi-goal search algorithm is developed to apply class-specific mutations in order to satisfy inter-dependent checks all together. We evaluate our technique on 20 popular benchmark programs collected from other fuzzing projects and the Google fuzzer test suite, and compare it with existing fuzzers AFL and AFLFast, symbolic execution engines KLEE and S2E, and a hybrid tool Driller that combines fuzzing with symbolic execution. The results show that our technique is highly effective and efficient, out-performing the other tools."}, {"id": "conf/icse/Wang0WL19", "title": "Superion: grammar-aware greybox fuzzing.", "authors": ["Junjie Wang", "Bihuan Chen", "Lei Wei", "Yang Liu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00081", "https://dl.acm.org/citation.cfm?id=3339598"], "tag": ["Fuzzing"], "abstract": "In recent years, coverage-based greybox fuzzing has proven itself to be one of the most effective techniques for finding security bugs in practice. Particularly, American Fuzzy Lop (AFL for short) is deemed to be a great success in fuzzing relatively simple test inputs. Unfortunately, when it meets structured test inputs such as XML and JavaScript, those grammar-blind trimming and mutation strategies in AFL hinder the effectiveness and efficiency. To this end, we propose a grammar-aware coverage-based greybox fuzzing approach to fuzz programs that process structured inputs. Given the grammar (which is often publicly available) of test inputs, we introduce a grammar-aware trimming strategy to trim test inputs at the tree level using the abstract syntax trees (ASTs) of parsed test inputs. Further, we introduce two grammar-aware mutation strategies (i.e., enhanced dictionary-based mutation and tree-based mutation). Specifically, tree-based mutation works via replacing subtrees using the ASTs of parsed test inputs. Equipped with grammar-awareness, our approach can carry the fuzzing exploration into width and depth. We implemented our approach as an extension to AFL, named Superion; and evaluated the effectiveness of Superion using large- scale programs (i.e., an XML engine libplist and three JavaScript engines WebKit, Jerryscript and ChakraCore). Our results have demonstrated that Superion can improve the code coverage (i.e., 16.7% and 8.8% in line and function coverage) and bug-finding capability (i.e., 34 new bugs, among which we discovered 22 new vulnerabilities with 19 CVEs assigned and 3.2K USD bug bounty rewards received) over AFL and jsfunfuzz."}, {"id": "conf/icse/0002JHC19", "title": "Grey-box concolic testing on binary code.", "authors": ["Jaeseung Choi", "Joonun Jang", "Choongwoo Han", "Sang Kil Cha"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00082", "https://dl.acm.org/citation.cfm?id=3339599"], "tag": ["Fuzzing"], "abstract": "We present grey-box concolic testing, a novel path-based test case generation method that combines the best of both white-box and grey-box fuzzing. At a high level, our technique systematically explores execution paths of a program under test as in white-box fuzzing, a.k.a. concolic testing, while not giving up the simplicity of grey-box fuzzing: it only uses a lightweight instrumentation, and it does not rely on an SMT solver. We implemented our technique in a system called Eclipser, and compared it to the state-of-the-art grey-box fuzzers (including AFLFast, LAF-intel, Steelix, and VUzzer) as well as a symbolic executor (KLEE). In our experiments, we achieved higher code coverage and found more bugs than the other tools."}, {"id": "conf/icse/AtlidakisGP19", "title": "RESTler: stateful REST API fuzzing.", "authors": ["Vaggelis Atlidakis", "Patrice Godefroid", "Marina Polishchuk"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00083", "https://dl.acm.org/citation.cfm?id=3339600"], "tag": ["Fuzzing"], "abstract": "This paper introduces RESTler, the first stateful REST API fuzzer. RESTler analyzes the API specification of a cloud service and generates sequences of requests that automatically test the service through its API. RESTler generates test sequences by (1) inferring producer-consumer dependencies among request types declared in the specification (e.g., inferring that \"a request B should be executed after request A\" because B takes as an input a resource-id x produced by A) and by (2) analyzing dynamic feedback from responses observed during prior test executions in order to generate new tests (e.g., learning that \"a request C after a request sequence A;B is refused by the service\" and therefore avoiding this combination in the future). We present experimental results showing that these two techniques are necessary to thoroughly exercise a service under test while pruning the large search space of possible request sequences. We used RESTler to test GitLab, an open-source Git service, as well as several Microsoft Azure and Office365 cloud services. RESTler found 28 bugs in GitLab and several bugs in each of the Azure and Office365 cloud services tested so far. These bugs have been confirmed and fixed by the service owners."}, {"id": "conf/icse/MolinaDPRAF19", "title": "Training binary classifiers as data structure invariants.", "authors": ["Facundo Molina", "Renzo Degiovanni", "Pablo Ponzio", "Germ\u00e1n Regis", "Nazareno Aguirre", "Marcelo F. Frias"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00084", "https://dl.acm.org/citation.cfm?id=3339602"], "tag": ["Machine learning in static analysis"], "abstract": "We present a technique to distinguish valid from invalid data structure objects. The technique is based on building an artificial neural network, more precisely a binary classifier, and training it to identify valid and invalid instances of a data structure. The obtained classifier can then be used in place of the data structure's invariant, in order to attempt to identify (in)correct behaviors in programs manipulating the structure. In order to produce the valid objects to train the network, an assumed-correct set of object building routines is randomly executed. Invalid instances are produced by generating values for object fields that \"break\" the collected valid values, i.e., that assign values to object fields that have not been observed as feasible in the assumed-correct executions that led to the collected valid instances. We experimentally assess this approach, over a benchmark of data structures. We show that this learning technique produces classifiers that achieve significantly better accuracy in classifying valid/invalid objects compared to a technique for dynamic invariant detection, and leads to improved bug finding."}, {"id": "conf/icse/FanLLWNZL19", "title": "Graph embedding based familial analysis of Android malware using unsupervised learning.", "authors": ["Ming Fan", "Xiapu Luo", "Jun Liu", "Meng Wang", "Chunyin Nong", "Qinghua Zheng", "Ting Liu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00085", "https://dl.acm.org/citation.cfm?id=3339603"], "tag": ["Machine learning in static analysis"], "abstract": "The rapid growth of Android malware has posed severe security threats to smartphone users. On the basis of the familial trait of Android malware observed by previous work, the familial analysis is a promising way to help analysts better focus on the commonalities of malware samples within the same families, thus reducing the analytical workload and accelerating malware analysis. The majority of existing approaches rely on supervised learning and face three main challenges, i.e., low accuracy, low efficiency, and the lack of labeled dataset. To address these challenges, we first construct a fine-grained behavior model by abstracting the program semantics into a set of subgraphs. Then, we propose SRA, a novel feature that depicts the similarity relationships between the Structural Roles of sensitive API call nodes in subgraphs. An SRA is obtained based on graph embedding techniques and represented as a vector, thus we can effectively reduce the high complexity of graph matching. After that, instead of training a classifier with labeled samples, we construct malware link network based on SRAs and apply community detection algorithms on it to group the unlabeled samples into groups. We implement these ideas in a system called GefDroid that performs Graph embedding based familial analysis of AnDroid malware using unsupervised learning. Moreover, we conduct extensive experiments to evaluate GefDroid on three datasets with ground truth. The results show that GefDroid can achieve high agreements (0.707-0.883 in term of NMI) between the clustering results and the ground truth. Furthermore, GefDroid requires only linear run-time overhead and takes around 8.6s to analyze a sample on average, which is considerably faster than the previous work."}, {"id": "conf/icse/ZhangWZ0WL19", "title": "A novel neural source code representation based on abstract syntax tree.", "authors": ["Jian Zhang", "Xu Wang", "Hongyu Zhang", "Hailong Sun", "Kaixuan Wang", "Xudong Liu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00086", "https://dl.acm.org/citation.cfm?id=3339604"], "tag": ["Machine learning in static analysis"], "abstract": "Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches."}, {"id": "conf/icse/LeClairJM19", "title": "A neural model for generating natural language summaries of program subroutines.", "authors": ["Alexander LeClair", "Siyuan Jiang", "Collin McMillan"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00087", "https://dl.acm.org/citation.cfm?id=3339605"], "tag": ["Machine learning in static analysis"], "abstract": "Source code summarization -- creating natural language descriptions of source code behavior -- is a rapidly-growing research topic with applications to automatic documentation generation, program comprehension, and software maintenance. Traditional techniques relied on heuristics and templates built manually by human experts. Recently, data-driven approaches based on neural machine translation have largely overtaken template-based systems. But nearly all of these techniques rely almost entirely on programs having good internal documentation; without clear identifier names, the models fail to create good summaries. In this paper, we present a neural model that combines words from code with code structure from an AST. Unlike previous approaches, our model processes each data source as a separate input, which allows the model to learn code structure independent of the text in code. This process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided. We evaluate our technique with a dataset we created from 2.1m Java methods. We find improvement over two baseline techniques from SE literature and one from NLP literature."}, {"id": "conf/icse/RamsauerLM19", "title": "The list is the process: reliable pre-integration tracking of commits on mailing lists.", "authors": ["Ralf Ramsauer", "Daniel Lohmann", "Wolfgang Mauerer"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00088", "https://dl.acm.org/citation.cfm?id=3339607"], "tag": ["Mining software changes and patterns"], "abstract": "A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history. We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth. Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements."}, {"id": "conf/icse/NguyenNDNTH19", "title": "Graph-based mining of in-the-wild, fine-grained, semantic code change patterns.", "authors": ["Hoan Anh Nguyen", "Tien N. Nguyen", "Danny Dig", "Son Nguyen", "Hieu Tran", "Michael Hilton"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00089", "https://dl.acm.org/citation.cfm?id=3339608"], "tag": ["Mining software changes and patterns"], "abstract": "Prior research exploited the repetitiveness of code changes to enable several tasks such as code completion, bug-fix recommendation, library adaption, etc. These and other novel applications require accurate detection of semantic changes, but the state-of-the-art methods are limited to algorithms that detect specific kinds of changes at the syntactic level. Existing algorithms relying on syntactic similarity have lower accuracy, and cannot effectively detect semantic change patterns. We introduce a novel graph-based mining approach, CPatMiner, to detect previously unknown repetitive changes in the wild, by mining fine-grained semantic code change patterns from a large number of repositories. To overcome unique challenges such as detecting meaningful change patterns and scaling to large repositories, we rely on fine-grained change graphs to capture program dependencies. We evaluate CPatMiner by mining change patterns in a diverse corpus of 5,000+ open-source projects from GitHub across a population of 170,000+ developers. We use three complementary methods. First, we sent the mined patterns to 108 open-source developers. We found that 70% of respondents recognized those patterns as their meaningful frequent changes. Moreover, 79% of respondents even named the patterns, and 44% wanted future IDEs to automate such repetitive changes. We found that the mined change patterns belong to various development activities: adaptive (9%), perfective (20%), corrective (35%) and preventive (36%, including refactorings). Second, we compared our tool with the state-of-the-art, AST-based technique, and reported that it detects 2.1x more meaningful patterns. Third, we use CPatMiner to search for patterns in a corpus of 88 GitHub projects with longer histories consisting of 164M SLOCs. It constructed 322K fine-grained change graphs containing 3M nodes, and detected 17K instances of change patterns from which we provide unique insights on the practice of change patterns among individuals and teams. We found that a large percentage (75%) of the change patterns from individual developers are commonly shared with others, and this holds true for teams. Moreover, we found that the patterns are not intermittent but spread widely over time. Thus, we call for a community-based change pattern database to provide important resources in novel applications."}, {"id": "conf/icse/LillackSHBW19", "title": "Intention-based integration of software variants.", "authors": ["Max Lillack", "Stefan Stanciulescu", "Wilhelm Hedman", "Thorsten Berger", "Andrzej Wasowski"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00090", "https://dl.acm.org/citation.cfm?id=3339610"], "tag": ["Software product lines"], "abstract": "Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional soft- ware merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants. In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions-domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the pro- posed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration."}, {"id": "conf/icse/HeradioFME19", "title": "Supporting the statistical analysis of variability models.", "authors": ["Ruben Heradio", "David Fern\u00e1ndez-Amor\u00f3s", "Christoph Mayr-Dorn", "Alexander Egyed"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00091", "https://dl.acm.org/citation.cfm?id=3339611"], "tag": ["Software product lines"], "abstract": "Variability models are broadly used to specify the configurable features of highly customizable software. In practice, they can be large, defining thousands of features with their dependencies and conflicts. In such cases, visualization techniques and automated analysis support are crucial for understanding the models. This paper contributes to this line of research by presenting a novel, probabilistic foundation for statistical reasoning about variability models. Our approach not only provides a new way to visualize, describe and interpret variability models, but it also supports the improvement of additional state-of-the-art methods for software product lines; for instance, providing exact computations where only approximations were available before, and increasing the sensitivity of existing analysis operations for variability models. We demonstrate the benefits of our approach using real case studies with up to 17,365 features, and written in two different languages (KConfig and feature models)."}, {"id": "conf/icse/LazregCCH019", "title": "Multifaceted automated analyses for variability-intensive embedded systems.", "authors": ["Sami Lazreg", "Maxime Cordy", "Philippe Collet", "Patrick Heymans", "S\u00e9bastien Mosser"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00092", "https://dl.acm.org/citation.cfm?id=3339612"], "tag": ["Software product lines"], "abstract": "Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry."}, {"id": "conf/icse/WenLWXCS19", "title": "Exposing library API misuses via mutation analysis.", "authors": ["Ming Wen", "Yepang Liu", "Rongxin Wu", "Xuan Xie", "Shing-Chi Cheung", "Zhendong Su"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00093", "https://dl.acm.org/citation.cfm?id=3339614"], "tag": ["API analysis"], "abstract": "Misuses of library APIs are pervasive and often lead to software crashes and vulnerability issues. Various static analysis tools have been proposed to detect library API misuses. They often involve mining frequent patterns from a large number of correct API usage examples, which can be hard to obtain in practice. They also suffer from low precision due to an over-simplified assumption that a deviation from frequent usage patterns indicates a misuse. We make two observations on the discovery of API misuse patterns. First, API misuses can be represented as mutants of the corresponding correct usages. Second, whether a mutant will introduce a misuse can be validated via executing it against a test suite and analyzing the execution information. Based on these observations, we propose MutApi, the first approach to discovering API misuse patterns via mutation analysis. To effectively mimic API misuses based on correct usages, we first design eight effective mutation operators inspired by the common characteristics of API misuses. MutApi generates mutants by applying these mutation operators on a set of client projects and collects mutant-killing tests as well as the associated stack traces. Misuse patterns are discovered from the killed mutants that are prioritized according to their likelihood of causing API misuses based on the collected information. We applied MutApi on 16 client projects with respect to 73 popular Java APIs. The results show that MutApi is able to discover substantial API misuse patterns with a high precision of 0.78. It also achieves a recall of $0.49$ on the MuBench benchmark, which outperforms the state-of-the-art techniques."}, {"id": "conf/icse/WeiLC19", "title": "Pivot: learning API-device correlations to facilitate Android compatibility issue detection.", "authors": ["Lili Wei", "Yepang Liu", "Shing-Chi Cheung"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00094", "https://dl.acm.org/citation.cfm?id=3339615"], "tag": ["API analysis"], "abstract": "The heavily fragmented Android ecosystem has induced various compatibility issues in Android apps. The search space for such fragmentation-induced compatibility issues (FIC issues) is huge, comprising three dimensions: device models, Android OS versions, and Android APIs. FIC issues, especially those arising from device models, evolve quickly with the frequent release of new device models to the market. As a result, an automated technique is desired to maintain timely knowledge of such FIC issues, which are mostly undocumented. In this paper, we propose such a technique, PIVOT, that automatically learns API-device correlations of FIC issues from existing Android apps. PIVOT extracts and prioritizes API-device correlations from a given corpus of Android apps. We evaluated PIVOT with popular Android apps on Google Play. Evaluation results show that PIVOT can effectively prioritize valid API-device correlations for app corpora collected at different time. Leveraging the knowledge in the learned API-device correlations, we further conducted a case study and successfully uncovered ten previously-undetected FIC issues in open-source Android apps."}, {"id": "conf/icse/HuangGLLQC019", "title": "SafeCheck: safety enhancement of Java unsafe API.", "authors": ["Shiyou Huang", "Jianmei Guo", "Sanhong Li", "Xiang Li", "Yumin Qi", "Kingsum Chow", "Jeff Huang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00095", "https://dl.acm.org/citation.cfm?id=3339616"], "tag": ["API analysis"], "abstract": "Java is a safe programming language by providing bytecode verification and enforcing memory protection. For instance, programmers cannot directly access the memory but have to use object references. Yet, the Java runtime provides an Unsafe API as a backdoor for the developers to access the low- level system code. Whereas the Unsafe API is designed to be used by the Java core library, a growing community of third-party libraries use it to achieve high performance. The Unsafe API is powerful, but dangerous, which leads to data corruption, resource leaks and difficult-to-diagnose JVM crash if used improperly. In this work, we study the Unsafe crash patterns and propose a memory checker to enforce memory safety, thus avoiding the JVM crash caused by the misuse of the Unsafe API at the bytecode level. We evaluate our technique on real crash cases from the openJDK bug system and real-world applications from AJDK. Our tool reduces the efforts from several days to a few minutes for the developers to diagnose the Unsafe related crashes. We also evaluate the runtime overhead of our tool on projects using intensive Unsafe operations, and the result shows that our tool causes a negligible perturbation to the execution of the applications."}, {"id": "conf/icse/HaoFJL019", "title": "CTRAS: crowdsourced test report aggregation and summarization.", "authors": ["Rui Hao", "Yang Feng", "James A. Jones", "Yuying Li", "Zhenyu Chen"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00096", "https://dl.acm.org/citation.cfm?id=3339618"], "tag": ["Crowdsourcing in software engineering"], "abstract": "Crowdsourced testing has been widely adopted to improve the quality of various software products. Crowdsourced workers typically perform testing tasks and report their experiences through test reports. While the crowdsourced test reports provide feedbacks from real usage scenarios, inspecting such a large number of reports becomes a time-consuming yet inevitable task. To improve the efficiency of this task, existing widely used issue-tracking systems, such as JIRA, Bugzilla, and Mantis, have provided keyword-search-based methods to assist users in identifying duplicate test reports. However, on mobile devices (such as mobile phones), where the crowdsourced test reports often contain insufficient text descriptions but instead rich screenshots, these text-analysis-based methods become less effective because the data has fundamentally changed. In this paper, instead of focusing on only detecting duplicates based on textual descriptions, we present CTRAS: a novel approach to leveraging duplicates to enrich the content of bug descriptions and improve the efficiency of inspecting these reports. CTRAS is capable of automatically aggregating duplicates based on both textual information and screenshots, and further summarizes the duplicate test reports into a comprehensive and comprehensible report. To validate CTRAS, we conducted quantitative studies using more than 5000 test reports, collected from 12 industrial crowdsourced projects. The experimental results reveal that CTRAS can reach an accuracy of 0.87, on average, regarding automatically detecting duplicate reports, and it outperforms the classic Max-Coverage-based and MMR summarization methods under Jensen Shannon divergence metric. Moreover, we conducted a task-based user study with 30 participants, whose result indicates that CTRAS can save nearly 30% time cost on average without loss of correctness."}, {"id": "conf/icse/WangYKMW19", "title": "iSENSE: completion-aware crowdtesting management.", "authors": ["Junjie Wang", "Ye Yang", "Rahul Krishna", "Tim Menzies", "Qing Wang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00097", "https://dl.acm.org/citation.cfm?id=3339619"], "tag": ["Crowdsourcing in software engineering"], "abstract": "Crowdtesting has become an effective alternative to traditional testing, especially for mobile applications. However, crowdtesting is hard to manage in nature. Given the complexity of mobile applications and unpredictability of distributed crowdtesting processes, it is difficult to estimate (a) remaining number of bugs yet to be detected or (b) required cost to find those bugs. Experience-based decisions may result in ineffective crowdtesting processes, e.g., there is an average of 32% wasteful spending in current crowdtesting practices. This paper aims at exploring automated decision support to effectively manage crowdtesting processes. It proposes an approach named ISENSE which applies incremental sampling technique to process crowdtesting reports arriving in chronological order, organizes them into fixed-size groups as dynamic inputs, and predicts two test completion indicators in an incremental manner. The two indicators are: 1) total number of bugs predicted with Capture-ReCapture model, and 2) required test cost for achieving certain test objectives predicted with AutoRegressive Integrated Moving Average model. The evaluation of ISENSE is conducted on 46,434 reports of 218 crowdtesting tasks from one of the largest crowdtesting platforms in China. Its effectiveness is demonstrated through two application studies for automating crowdtesting management and semi-automation of task closing trade-off analysis. The results show that ISENSE can provide managers with greater awareness of testing progress to achieve cost-effectiveness gains of crowdtesting. Specifically, a median of 100% bugs can be detected with 30% saved cost based on the automated close prediction."}, {"id": "conf/icse/0001WK019", "title": "How practitioners perceive coding proficiency.", "authors": ["Xin Xia", "Zhiyuan Wan", "Pavneet Singh Kochhar", "David Lo"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00098", "https://dl.acm.org/citation.cfm?id=3339621"], "tag": ["Human factors"], "abstract": "Coding proficiency is essential to software practitioners. Unfortunately, our understanding on coding proficiency often translates to vague stereotypes, e.g., \"able to write good code\". The lack of specificity hinders employers from measuring a software engineer's coding proficiency, and software engineers from improving their coding proficiency skills. This raises an important question: what skills matter to improve one's coding proficiency. To answer this question, we perform an empirical study by surveying 340 software practitioners from 33 countries across 5 continents. We first identify 38 coding proficiency skills grouped into nine categories by interviewing 15 developers from three companies. We then ask our survey respondents to rate the level of importance for these skills, and provide rationales of their ratings. Our study highlights a total of 21 important skills that receive an average rating of 4.0 and above (important and very important), along with rationales given by proponents and dissenters. We discuss implications of our findings to researchers, educators, and practitioners."}, {"id": "conf/icse/SarkerVBF19", "title": "Socio-technical work-rate increase associates with changes in work patterns in online projects.", "authors": ["Farhana Sarker", "Bogdan Vasilescu", "Kelly Blincoe", "Vladimir Filkov"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00099", "https://dl.acm.org/citation.cfm?id=3339622"], "tag": ["Human factors"], "abstract": "Software developers work on a variety of tasks ranging from the technical, e.g., writing code, to the social, e.g., participating in issue resolution discussions. The amount of work developers perform per week (their work-rate) also varies and depends on project needs and developer schedules. Prior work has shown that while moderate levels of increased technical work and multitasking lead to higher productivity, beyond a certain threshold, they can lead to lowered performance. Here, we study how increases in the short-term work-rate along both the technical and social dimensions are associated with changes in developers' work patterns, in particular communication sentiment, technical productivity, and social productivity. We surveyed active and prolific developers on GitHub to understand the causes and impacts of increased work-rates. Guided by the responses, we developed regression models to study how communication and committing patterns change with increased work-rates and fit those models to large-scale data gathered from traces left by thousands of GitHub developers. From our survey and models, we find that most developers do experience work-rate-increase-related changes in behavior. Most notably, our models show that there is a sizable effect when developers comment much more than their average: the negative sentiment in their comments increases, suggesting an increased level of stress. Our models also show that committing patterns do not change with increased commenting, and vice versa, suggesting that technical and social activities tend not to be multitasked."}, {"id": "conf/icse/BarcombSR019", "title": "Why do episodic volunteers stay in FLOSS communities?", "authors": ["Ann Barcomb", "Klaas-Jan Stol", "Dirk Riehle", "Brian Fitzgerald"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00100", "https://dl.acm.org/citation.cfm?id=3339623"], "tag": ["Human factors"], "abstract": "Successful Free/Libre and Open Source Software (FLOSS) projects incorporate both habitual and infrequent, or episodic, contributors. Using the concept of episodic volunteering (EV) from the general volunteering literature, we derive a model consisting of five key constructs that we hypothesize affect episodic volunteers' retention in FLOSS communities. To evaluate the model we conducted a survey with over 100 FLOSS episodic volunteers. We observe that three of our model constructs (social norms, satisfaction and community commitment) are all positively associated with volunteers' intention to remain, while the two other constructs (psychological sense of community and contributor benefit motivations) are not. Furthermore, exploratory clustering on unobserved heterogeneity suggests that there are four distinct categories of volunteers: satisfied, classic, social and obligated. Based on our findings, we offer suggestions for projects to incorporate and manage episodic volunteers, so as to better leverage this type of contributors and potentially improve projects' sustainability."}, {"id": "conf/icse/HellendoornPGB19", "title": "When code completion fails: a case study on real-world completions.", "authors": ["Vincent J. Hellendoorn", "Sebastian Proksch", "Harald C. Gall", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00101", "https://dl.acm.org/citation.cfm?id=3339625"], "tag": ["IDEs"], "abstract": "Code completion is commonly used by software developers and is integrated into all major IDE's. Good completion tools can not only save time and effort but may also help avoid incorrect API usage. Many proposed completion tools have shown promising results on synthetic benchmarks, but these benchmarks make no claims about the realism of the completions they test. This lack of grounding in real-world data could hinder our scientific understanding of developer needs and of the efficacy of completion models. This paper presents a case study on 15,000 code completions that were applied by 66 real developers, which we study and contrast with artificial completions to inform future research and tools in this area. We find that synthetic benchmarks misrepresent many aspects of real-world completions; tested completion tools were far less accurate on real-world data. Worse, on the few completions that consumed most of the developers' time, prediction accuracy was less than 20% -- an effect that is invisible in synthetic benchmarks. Our findings have ramifications for future benchmarks, tool design and real-world efficacy: Benchmarks must account for completions that developers use most, such as intra-project APIs; models should be designed to be amenable to intra-project data; and real-world developer trials are essential to quantifying performance on the least predictable completions, which are both most time-consuming and far more typical than artificial data suggests. We publicly release our preprint [https://doi.org/10.5281/zenodo.2565673] and replication data and materials [https://doi.org/10.5281/zenodo.2562249]."}, {"id": "conf/icse/CitoLRG19", "title": "Interactive production performance feedback in the IDE.", "authors": ["J\u00fcrgen Cito", "Philipp Leitner", "Martin Rinard", "Harald C. Gall"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00102", "https://dl.acm.org/citation.cfm?id=3339626"], "tag": ["IDEs"], "abstract": "Because of differences between development and production environments, many software performance problems are detected only after software enters production. We present PerformanceHat, a new system that uses profiling information from production executions to develop a global performance model suitable for integration into interactive development environments. PerformanceHat's ability to incrementally update this global model as the software is changed in the development environment enables it to deliver near real-time predictions of performance consequences reflecting the impact on the production environment. We implement PerformanceHat as an Eclipse plugin and evaluate it in a controlled experiment with 20 professional software developers implementing several software maintenance tasks using our approach and a representative baseline (Kibana). Our results indicate that developers using PerformanceHat were significantly faster in (1) detecting the performance problem, and (2) finding the root-cause of the problem. These results provide encouraging evidence that our approach helps developers detect, prevent, and debug production performance problems during development before the problem manifests in production."}, {"id": "conf/icse/SuWYC019", "title": "Redundant loads: a software inefficiency indicator.", "authors": ["Pengfei Su", "Shasha Wen", "Hailong Yang", "Milind Chabbi", "Xu Liu"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00103", "https://dl.acm.org/citation.cfm?id=3339628"], "tag": ["Performance"], "abstract": "Modern software packages have become increasingly complex with millions of lines of code and references to many external libraries. Redundant operations are a common performance limiter in these code bases. Missed compiler optimization opportunities, inappropriate data structure and algorithm choices, and developers' inattention to performance are some common reasons for the existence of redundant operations. Developers mainly depend on compilers to eliminate redundant operations. However, compilers' static analysis often misses optimization opportunities due to ambiguities and limited analysis scope; automatic optimizations to algorithmic and data structural problems are out of scope. We develop LoadSpy, a whole-program profiler to pinpoint redundant memory load operations, which are often a symptom of many redundant operations. The strength of LoadSpy exists in identifying and quantifying redundant load operations in programs and associating the redundancies with program execution contexts and scopes to focus developers' attention on problematic code. LoadSpy works on fully optimized binaries, adopts various optimization techniques to reduce its overhead, and provides a rich graphic user interface, which make it a complete developer tool. Applying LoadSpy showed that a large fraction of redundant loads is common in modern software packages despite highest levels of automatic compiler optimizations. Guided by LoadSpy, we optimize several well-known benchmarks and real-world applications, yielding significant speedups."}, {"id": "conf/icse/YangYWLC19", "title": "View-centric performance optimization for database-backed web applications.", "authors": ["Junwen Yang", "Cong Yan", "Chengcheng Wan", "Shan Lu", "Alvin Cheung"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00104", "https://dl.acm.org/citation.cfm?id=3339629"], "tag": ["Performance"], "abstract": "Web developers face the stringent task of designing informative web pages while keeping the page-load time low. This task has become increasingly challenging as most web contents are now generated by processing ever-growing amount of user data stored in back-end databases. It is difficult for developers to understand the cost of generating every web-page element, not to mention explore and pick the web design with the best trade-off between performance and functionality. In this paper, we present Panorama, a view-centric and database-aware development environment for web developers. Using database-aware program analysis and novel IDE design, Panorama provides developers with intuitive information about the cost and the performance-enhancing opportunities behind every HTML element, as well as suggesting various global code refactorings that enable developers to easily explore a wide spectrum of performance and functionality trade-offs."}, {"id": "conf/icse/WangKZ19", "title": "Adjust: runtime mitigation of resource abusing third-party online ads.", "authors": ["Weihang Wang", "I Luk Kim", "Yunhui Zheng"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00105", "https://dl.acm.org/citation.cfm?id=3339630"], "tag": ["Performance"], "abstract": "Online advertising is the most critical revenue stream for many Internet companies. However, showing ads on websites comes with a price tag. Since website contents and third-party ads are blended together, third-party ads may compete with the publisher contents, delaying or even breaking the rendering of first-party contents. In addition, dynamically including scripts from ad networks all over the world may introduce buggy scripts that slow down page loads and even freeze the browser. The resulting poor usability problems lead to bad user experience and lower profits. The problems caused by such resource abusing ads are originated from two root causes: First, content publishers have no control over third-party ads. Second, publishers cannot differentiate resource consumed by ads from that consumed by their own contents. To address these challenges, we propose an effective technique, AdJust, that allows publishers to specify constraints on events associated with third-party ads (e.g., URL requests, HTML element creations, and timers), so that they can mitigate user experience degradations and enforce consistent ads experience to all users. We report on a series of experiments over the Alexa top 200 news websites. The results point to the efficacy of our proposed techniques: AdJust effectively mitigated degradations that freeze web browsers (on 36 websites), reduced the load time of publisher contents (on 61 websites), prioritized publisher contents (on 166 websites) and ensured consistent rendering orders among top ads (on 68 websites)."}, {"id": "conf/icse/MaozRS19", "title": "Symbolic repairs for GR(1) specifications.", "authors": ["Shahar Maoz", "Jan Oliver Ringert", "Rafi Shalom"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00106", "https://dl.acm.org/citation.cfm?id=3339632"], "tag": ["Specifications and models"], "abstract": "Unrealizability is a major challenge for GR(1), an expressive assume-guarantee fragment of LTL that enables efficient synthesis. Some works attempt to help engineers deal with unrealizability by generating counter-strategies or computing an unrealizable core. Other works propose to repair the unrealizable specification by suggesting repairs in the form of automatically generated assumptions. In this work we present two novel symbolic algorithms for repairing unrealizable GR(1) specifications. The first algorithm infers new assumptions based on the recently introduced JVTS. The second algorithm infers new assumptions directly from the specification. Both algorithms are sound. The first is incomplete but can be used to suggest many different repairs. The second is complete but suggests a single repair. Both are symbolic and therefore efficient. We implemented our work, validated its correctness, and evaluated it on benchmarks from the literature. The evaluation shows the strength of our algorithms, in their ability to suggest repairs and in their performance and scalability compared to previous solutions."}, {"id": "conf/icse/PhamLQT19", "title": "CRADLE: cross-backend validation to detect and localize bugs in deep learning libraries.", "authors": ["Hung Viet Pham", "Thibaud Lutellier", "Weizhen Qi", "Lin Tan"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00107", "https://dl.acm.org/citation.cfm?id=3339633"], "tag": ["Specifications and models"], "abstract": "Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies."}, {"id": "conf/icse/KimFY19", "title": "Guiding deep learning system testing using surprise adequacy.", "authors": ["Jinhan Kim", "Robert Feldt", "Shin Yoo"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00108", "https://dl.acm.org/citation.cfm?id=3339634"], "tag": ["Specifications and models"], "abstract": "Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining."}, {"id": "conf/icse/NguyenRRODP19", "title": "FOCUS: a recommender system for mining API function calls and usage patterns.", "authors": ["Phuong T. Nguyen", "Juri Di Rocco", "Davide Di Ruscio", "Lina Ochoa", "Thomas Degueule", "Massimiliano Di Penta"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00109", "https://dl.acm.org/citation.cfm?id=3339636"], "tag": ["APIs"], "abstract": "Software developers interact with APIs on a daily basis and, therefore, often face the need to learn how to use new APIs suitable for their purposes. Previous work has shown that recommending usage patterns to developers facilitates the learning process. Current approaches to usage pattern recommendation, however, still suffer from high redundancy and poor run-time performance. In this paper, we reformulate the problem of usage pattern recommendation in terms of a collaborative-filtering recommender system. We present a new tool, FOCUS, which mines open-source project repositories to recommend API method invocations and usage patterns by analyzing how APIs are used in projects similar to the current project. We evaluate FOCUS on a large number of Java projects extracted from GitHub and Maven Central and find that it outperforms the state-of-the-art approach PAM with regards to success rate, accuracy, and execution time. Results indicate the suitability of context-aware collaborative-filtering recommender systems to provide API usage patterns."}, {"id": "conf/icse/SpadiniPBHBB19", "title": "Test-driven code review: an empirical study.", "authors": ["Davide Spadini", "Fabio Palomba", "Tobias Baum", "Stefan Hanenberg", "Magiel Bruntink", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00110", "https://dl.acm.org/citation.cfm?id=3339638"], "tag": ["Code reviews"], "abstract": "Test-Driven Code Review (TDR) is a code review practice in which a reviewer inspects a patch by examining the changed test code before the changed production code. Although this practice has been mentioned positively by practitioners in informal literature and interviews, there is no systematic knowledge of its effects, prevalence, problems, and advantages. In this paper, we aim at empirically understanding whether this practice has an effect on code review effectiveness and how developers' perceive TDR. We conduct (i) a controlled experiment with 93 developers that perform more than 150 reviews, and (ii) 9 semi-structured interviews and a survey with 103 respondents to gather information on how TDR is perceived. Key results from the experiment show that developers adopting TDR find the same proportion of defects in production code, but more in test code, at the expenses of fewer maintainability issues in production code. Furthermore, we found that most developers prefer to review production code as they deem it more critical and tests should follow from it. Moreover, general poor test code quality and no tool support hinder the adoption of TDR. Public preprint: [https: //doi.org/10.5281/zenodo.2551217], data and materials: [https:// doi.org/10.5281/zenodo.2553139]."}, {"id": "conf/icse/AlamiCW19", "title": "Why does code review work for open source software communities?", "authors": ["Adam Alami", "Marisa Leavitt Cohn", "Andrzej Wasowski"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00111", "https://dl.acm.org/citation.cfm?id=3339639"], "tag": ["Code reviews"], "abstract": "Open source software communities have demonstrated that they can produce high quality results. The overall success of peer code review, commonly used in open source projects, has likely contributed strongly to this success. Code review is an emotionally loaded practice, with public exposure of reputation and ample opportunities for conflict. We set off to ask why code review works for open source communities, despite this inherent challenge. We interviewed 21 open source contributors from four communities and participated in meetings of ROS community devoted to implementation of the code review process. It appears that the hacker ethic is a key reason behind the success of code review in FOSS communities. It is built around the ethic of passion and the ethic of caring. Furthermore, we observed that tasks of code review are performed with strong intrinsic motivation, supported by many non-material extrinsic motivation mechanisms, such as desire to learn, to grow reputation, or to improve one's positioning on the job market. In the paper, we describe the study design, analyze the collected data and formulate 20 proposals for how what we know about hacker ethics and human and social aspects of code review, could be exploited to improve the effectiveness of the practice in software projects."}, {"id": "conf/icse/KalteneckerGSGA19", "title": "Distance-based sampling of software configuration spaces.", "authors": ["Christian Kaltenecker", "Alexander Grebhahn", "Norbert Siegmund", "Jianmei Guo", "Sven Apel"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00112", "https://dl.acm.org/citation.cfm?id=3339641"], "tag": ["Configuration and optimization"], "abstract": "Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets."}, {"id": "conf/icse/HaZ19", "title": "DeepPerf: performance prediction for configurable software with deep sparse neural network.", "authors": ["Huong Ha", "Hongyu Zhang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00113", "https://dl.acm.org/citation.cfm?id=3339642"], "tag": ["Configuration and optimization"], "abstract": "Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach."}, {"id": "conf/icse/ChowdhuryHKSMK19", "title": "GreenBundle: an empirical study on the energy impact of bundled processing.", "authors": ["Shaiful Alam Chowdhury", "Abram Hindle", "Rick Kazman", "Takumi Shuto", "Ken Matsui", "Yasutaka Kamei"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00114", "https://dl.acm.org/citation.cfm?id=3339644"], "tag": ["Energy consumption in mobile apps"], "abstract": "Energy consumption is a concern in the data-center and at the edge, on mobile devices such as smartphones. Software that consumes too much energy threatens the utility of the end-user's mobile device. Energy consumption is fundamentally a systemic kind of performance and hence it should be addressed at design time via a software architecture that supports it, rather than after release, via some form of refactoring. Unfortunately developers often lack knowledge of what kinds of designs and architectures can help address software energy consumption. In this paper we show that some simple design choices can have significant effects on energy consumption. In particular we examine the Model-View-Controller architectural pattern and demonstrate how converting to Model-View-Presenter with bundling can improve the energy performance of both benchmark systems and real world applications. We show the relationship between energy consumption and bundled and delayed view updates: bundling events in the presenter can often reduce energy consumption by 30%."}, {"id": "conf/icse/JabbarvandLM19", "title": "Search-based energy testing of Android.", "authors": ["Reyhaneh Jabbarvand", "Jun-Wei Lin", "Sam Malek"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00115", "https://dl.acm.org/citation.cfm?id=3339645"], "tag": ["Energy consumption in mobile apps"], "abstract": "The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efficiently use the device's battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app's energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present COBWEB, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app's energy behavior, COBWEB generates a test suite that can effectively find energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efficiently test energy behavior of apps, but also its superiority over prior techniques by finding a wider and more diverse set of energy defects."}, {"id": "conf/icse/WangWSTCS0WZL19", "title": "Global optimization of numerical programs via prioritized stochastic algebraic transformations.", "authors": ["Xie Wang", "Huaijin Wang", "Zhendong Su", "Enyi Tang", "Xin Chen", "Weijun Shen", "Zhenyu Chen", "Linzhang Wang", "Xianpei Zhang", "Xuandong Li"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00116", "https://dl.acm.org/citation.cfm?id=3339647"], "tag": ["Program transformations"], "abstract": "Numerical code is often applied in the safety-critical, but resource-limited areas. Hence, it is crucial for it to be correct and efficient, both of which are difficult to ensure. On one hand, accumulated rounding errors in numerical programs can cause system failures. On the other hand, arbitrary/infinite-precision arithmetic, although accurate, is infeasible in practice and especially in resource-limited scenarios because it performs thousands of times slower than floating-point arithmetic. Thus, it has been a significant challenge to obtain high-precision, easy-to-maintain, and efficient numerical code. This paper introduces a novel global optimization framework to tackle this challenge. Using our framework, a developer simply writes the infinite-precision numerical program directly following the problem's mathematical requirement specification. The resulting code is correct and easy-to-maintain, but inefficient. Our framework then optimizes the program in a global fashion (i.e., considering the whole program, rather than individual expressions or statements as in prior work), the key technical difficulty this work solves. To this end, it analyzes the program's numerical value flows across different statements through a symbolic trace extraction algorithm, and generates optimized traces via stochastic algebraic transformations guided by effective rule selection. We first evaluate our technique on numerical benchmarks from the literature; results show that our global optimization achieves significantly higher worst-case accuracy than the state-of-the-art numerical optimization tool. Second, we show that our framework is also effective on benchmarks having complicated program structures, which are challenging for numerical optimization. Finally, we apply our framework on real-world code to successfully detect numerical bugs that have been confirmed by developers."}, {"id": "conf/icse/Ketkar0MDA19", "title": "Type migration in ultra-large-scale codebases.", "authors": ["Ameya Ketkar", "Ali Mesbah", "Davood Mazinanian", "Danny Dig", "Edward Aftandilian"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00117", "https://dl.acm.org/citation.cfm?id=3339648"], "tag": ["Program transformations"], "abstract": "Type migration is a refactoring activity in which an existing type is replaced with another one throughout the source code. Manually performing type migration is tedious as programmers need to find all instances of the type to be migrated, along with its dependencies that propagate over assignment operations, method hierarchies, and subtypes. Existing automated approaches for type migration are not adequate for ultra-large-codebases - they perform an intensive whole-program analysis that does not scale. If we could represent the type structure of the program as graphs, then we could employ a MAPREDUCE parallel and distributed process that scales to hundreds of millions of LOC. We implemented this approach as an IDE-independent tool called T2R, which integrates with most build systems. We evaluated T2R's accuracy, usefulness and scalability on seven open source projects and one proprietary codebase of 300M LOC. T2R generated 130 type migration patches, of which the original developers accepted 98%."}, {"id": "conf/icse/AzimAN019", "title": "Dynamic slicing for Android.", "authors": ["Tanzirul Azim", "Arash Alavi", "Iulian Neamtiu", "Rajiv Gupta"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00118", "https://dl.acm.org/citation.cfm?id=3339649"], "tag": ["Program transformations"], "abstract": "Dynamic program slicing is useful for a variety of tasks, from testing to debugging to security. Prior slicing approaches have targeted traditional desktop/server platforms, rather than mobile platforms such as Android. Slicing mobile, event-based systems is challenging due to their asynchronous callback construction and the IPC (interprocess communication)- heavy, sensor-driven, timing-sensitive nature of the platform. To address these problems, we introduce AndroidSlicer1, the first slicing approach for Android. AndroidSlicer combines a novel asynchronous slicing approach for modeling data and control dependences in the presence of callbacks with lightweight and precise instrumentation; this allows slicing for apps running on actual phones, and without requiring the app's source code. Our slicer is capable of handling a wide array of inputs that Android supports without adding any noticeable overhead. Experiments on 60 apps from Google Play show that AndroidSlicer is effective (reducing the number of instructions to be examined to 0.3% of executed instructions) and efficient (app instrumentation and post-processing combined takes 31 seconds); all while imposing a runtime overhead of just 4%. We present three applications of AndroidSlicer that are particularly relevant in the mobile domain: (1) finding and tracking input parts responsible for an error/crash, (2) fault localization, i.e., finding the instructions responsible for an error/crash, and (3) reducing the regression test suite. Experiments with these applications on an additional set of 18 popular apps indicate that AndroidSlicer is effective for Android testing and debugging."}, {"id": "conf/icse/TranTNNN19", "title": "Recovering variable names for minified code with usage contexts.", "authors": ["Hieu Tran", "Ngoc M. Tran", "Son Nguyen", "Hoan Nguyen", "Tien N. Nguyen"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00119", "https://dl.acm.org/citation.cfm?id=3339651"], "tag": ["Reverse engineering"], "abstract": "To avoid the exposure of original source code in a Web application, the variable names in JS code deployed in the wild are often replaced by short, meaningless names, thus making the code extremely difficult to manually understand and analysis. This paper presents JSNeat, an information retrieval (IR)-based approach to recover the variable names in minified JS code. JSNeat follows a data-driven approach to recover names by searching for them in a large corpus of open-source JS code. We use three types of contexts to match a variable in given minified code against the corpus including the context of the properties and roles of the variable, the context of that variable and relations with other variables under recovery, and the context of the task of the function to which the variable contributes. We performed several empirical experiments to evaluate JSNeat on the dataset of more than 322K JS files with 1M functions, and 3.5M variables with 176K unique variable names. We found that JSNeat achieves a high accuracy of 69.1%, which is the relative improvements of 66.1% and 43% over two state-of-the-art approaches JSNice and JSNaughty, respectively. The time to recover for a file or a variable with JSNeat is twice as fast as with JSNice and 4x as fast as with JNaughty, respectively."}, {"id": "conf/icse/GrechBSS19", "title": "Gigahorse: thorough, declarative decompilation of smart contracts.", "authors": ["Neville Grech", "Lexi Brent", "Bernhard Scholz", "Yannis Smaragdakis"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00120", "https://dl.acm.org/citation.cfm?id=3339652"], "tag": ["Reverse engineering"], "abstract": "The rise of smart contracts - autonomous applications running on blockchains - has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a highlevel 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts - e.g., Gigahorse can decompile over 99.98% of deployed contracts, compared to 88% for the recently-published Vandal decompiler and under 50% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a \u201cbatteries included\u201d approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation."}, {"id": "conf/icse/MillerKSZZL19", "title": "Probabilistic disassembly.", "authors": ["Kenneth A. Miller", "Yonghwi Kwon", "Yi Sun", "Zhuo Zhang", "Xiangyu Zhang", "Zhiqiang Lin"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00121", "https://dl.acm.org/citation.cfm?id=3339653"], "tag": ["Reverse engineering"], "abstract": "Disassembling stripped binaries is a prominent challenge for binary analysis, due to the interleaving of code segments and data, and the difficulties of resolving control transfer targets of indirect calls and jumps. As a result, most existing disassemblers have both false positives (FP) and false negatives (FN). We observe that uncertainty is inevitable in disassembly due to the information loss during compilation and code generation. Therefore, we propose to model such uncertainty using probabilities and propose a novel disassembly technique, which computes a probability for each address in the code space, indicating its likelihood of being a true positive instruction. The probability is computed from a set of features that are reachable to an address, including control flow and data flow features. Our experiments with more than two thousands binaries show that our technique does not have any FN and has only 3.7% FP. In comparison, a state-of-the-art superset disassembly technique has 85% FP. A rewriter built on our disassembly can generate binaries that are only half of the size of those by superset disassembly and run 3% faster. While many widely-used disassemblers such as IDA and BAP suffer from missing function entries, our experiment also shows that even without any function entry information, our disassembler can still achieve 0 FN and 6.8% FP."}, {"id": "conf/icse/AghajaniNVLMBL19", "title": "Software documentation issues unveiled.", "authors": ["Emad Aghajani", "Csaba Nagy", "Olga Lucero Vega-M\u00e1rquez", "Mario Linares-V\u00e1squez", "Laura Moreno", "Gabriele Bavota", "Michele Lanza"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00122", "https://dl.acm.org/citation.cfm?id=3339655"], "tag": ["Software documentation"], "abstract": "(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavourable take on documentation. Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners."}, {"id": "conf/icse/HataTKI19", "title": "9.6 million links in source code comments: purpose, evolution, and decay.", "authors": ["Hideaki Hata", "Christoph Treude", "Raula Gaikovina Kula", "Takashi Ishio"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00123", "https://dl.acm.org/citation.cfm?id=3339656"], "tag": ["Software documentation"], "abstract": "Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10% of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems."}, {"id": "conf/icse/AgrawalKVRCL19", "title": "Leveraging artifact trees to evolve and reuse safety cases.", "authors": ["Ankit Agrawal", "Seyedehzahra Khoshmanesh", "Michael Vierhauser", "Mona Rahimi", "Jane Cleland-Huang", "Robyn R. Lutz"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00124", "https://dl.acm.org/citation.cfm?id=3339658"], "tag": ["Software quality"], "abstract": "Safety Assurance Cases (SACs) are increasingly used to guide and evaluate the safety of software-intensive systems. They are used to construct a hierarchically organized set of claims, arguments, and evidence in order to provide a structured argument that a system is safe for use. However, as the system evolves and grows in size, a SAC can be difficult to maintain. In this paper we utilize design science to develop a novel solution for identifying areas of a SAC that are affected by changes to the system. Moreover, we generate actionable recommendations for updating the SAC, including its underlying artifacts and trace links, in order to evolve an existing safety case for use in a new version of the system. Our approach, Safety Artifact Forest Analysis (SAFA), leverages traceability to automatically compare software artifacts from a previously approved or certified version with a new version of the system. We identify, visualize, and explain changes in a Delta Tree. We evaluate our approach using the Dronology system for monitoring and coordinating the actions of cooperating, small Unmanned Aerial Vehicles. Results from a user study show that SAFA helped users to identify changes that potentially impacted system safety and provided information that could be used to help maintain and evolve a SAC."}, {"id": "conf/icse/LickerR19", "title": "Detecting incorrect build rules.", "authors": ["N\u00e1ndor Licker", "Andrew Rice"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00125", "https://dl.acm.org/citation.cfm?id=3339660"], "tag": ["Testing and analysis:\ndomain-specific approaches"], "abstract": "Automated build systems are routinely used by software engineers to minimize the number of objects that need to be recompiled after incremental changes to the source files of a project. In order to achieve efficient and correct builds, developers must provide the build tools with dependency information between the files and modules of a project, usually expressed in a macro language specific to each build tool. In order to guarantee correctness, the authors of these tools are responsible for enumerating all the files whose contents an output depends on. Unfortunately, this is a tedious process and not all dependencies are captured in practice, which leads to incorrect builds. We automatically uncover such missing dependencies through a novel method that we call build fuzzing. The correctness of build definitions is verified by modifying files in a project, triggering incremental builds and comparing the set of changed files to the set of expected changes. These sets are determined using a dependency graph inferred by tracing the system calls executed during a clean build. We evaluate our method by exhaustively testing build rules of open-source projects, uncovering issues leading to race conditions and faulty builds in 31 of them. We provide a discussion of the bugs we detect, identifying anti-patterns in the use of the macro languages. We fix some of the issues in projects where the features of build systems allow a clean solution."}, {"id": "conf/icse/WangD00Z19", "title": "Adversarial sample detection for deep neural network through model mutation testing.", "authors": ["Jingyi Wang", "Guoliang Dong", "Jun Sun", "Xinyu Wang", "Peixin Zhang"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00126", "https://dl.acm.org/citation.cfm?id=3339661"], "tag": ["Testing and analysis:\ndomain-specific approaches"], "abstract": "Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of 'sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity. We evaluated our approach on the MNIST and CIFAR10 datasets. The results show that our approach detects adversarial samples generated by state-of-the-art attacking methods efficiently and accurately."}, {"id": "conf/icse/ChenSS19", "title": "Deep differential testing of JVM implementations.", "authors": ["Yuting Chen", "Ting Su", "Zhendong Su"], "DOIs": ["https://doi.org/10.1109/ICSE.2019.00127", "https://dl.acm.org/citation.cfm?id=3339662"], "tag": ["Testing and analysis:\ndomain-specific approaches"], "abstract": "The Java Virtual Machine (JVM) is the cornerstone of the widely-used Java platform. Thus, it is critical to ensure the reliability and robustness of popular JVM implementations. However, little research exists on validating production JVMs. One notable effort is classfuzz, which mutates Java bytecode syntactically to stress-test different JVMs. It is shown that classfuzz mainly produces illegal bytecode files and uncovers defects in JVMs' startup processes. It remains a challenge to effectively test JVMs' bytecode verifiers and execution engines to expose deeper bugs. This paper tackles this challenge by introducing classming, a novel, effective approach to performing deep, differential JVM testing. The key of classming is a technique, live bytecode mutation, to generate, from a seed bytecode file f, likely valid, executable (live) bytecode files: (1) capture the seed f's live bytecode, the sequence of its executed bytecode instructions; (2) repeatedly manipulate the control- and data-flow in f's live bytecode to generate semantically different mutants; and (3) selectively accept the generated mutants to steer the mutation process toward live, diverse mutants. The generated mutants are then employed to differentially test JVMs. We have evaluated classming on mainstream JVM implementations, including OpenJDK's HotSpot and IBM's J9, by mutating the DaCapo benchmarks. Our results show that classming is very effective in uncovering deep JVM differences. More than 1,800 of the generated classes exposed JVM differences, and more than 30 triggered JVM crashes. We analyzed and reported the JVM runtime differences and crashes, of which 14 have already been confirmed/fixed, including a highly critical security vulnerability in J9 that allowed untrusted code to disable the security manager and elevate its privileges (CVE-2017-1376)."}]}, "icst/icst": {"2017": [{"id": "conf/icst/GuoML17", "title": "Localizing Faults in SQL Predicates.", "authors": ["Yun Guo", "Amihai Motro", "Nan Li"], "DOIs": ["https://doi.org/10.1109/ICST.2017.8"], "tag": ["Fault Localization and Injection"], "abstract": "Fault localization techniques have been applied to database and data-centric applications that use SQL or SQL-based languages. However, existing techniques can only identify the SQL statements that have faults, but not determine the precise location of the faults within SQL statements. Since SQL statements can be rather complex, programmers are still left with a difficult repair chore. We propose a novel fault localization method to localize multiple types of faults in SQL predicates, that is based on row-based dynamic slicing and delta debugging. Our method was implemented in a tool called ALTAR, and experiments were performed on two publicly available databases. Our method can be compared with existing fault localization techniques when these are applied to \"drill-down\" in SQL statements. The results showed that ALTAR can discover more types of faults. Moreover, for the type of faults discovered by current methods, ALTAR is more precise."}, {"id": "conf/icst/PerezAd17", "title": "Prevalence of Single-Fault Fixes and Its Impact on Fault Localization.", "authors": ["Alexandre Perez", "Rui Abreu", "Marcelo d'Amorim"], "DOIs": ["https://doi.org/10.1109/ICST.2017.9"], "tag": ["Fault Localization and Injection"], "abstract": "Several fault predictors were proposed in the context of Spectrum-based Fault Localization approaches to rank software components in order of suspiciousness of being the root-cause of observed failures. Previous work has also shown that some of the fault predictors (near-)optimally rank software components, provided that there is one fault in the system. Despite this, further work is being spent on creating more complex, computationally expensive, model-based techniques that can handle multiple-faulted scenarios accurately. However, our hypothesis is that when software is being developed, bugs arise one-at-a-time and therefore can be considered as single-faulted scenarios. We describe an approach to mine repositories, find bug-fixes, and catalog them according to the number of faults they fix, to assess the prevalence of single-fault fixes. Our empirical study using 279 open-source projects reveals that there is a prevalence of single-fault fixes, with over 82% of all fixes only eliminating one bug from the system, enabling the use of simpler, (near-)optimal, fault predictors. Moreover, we draw on the practical implications of our findings to influence and set direction for future research."}, {"id": "conf/icst/JeongLKKH17", "title": "FIFA: A Kernel-Level Fault Injection Framework for ARM-Based Embedded Linux System.", "authors": ["Eunjin Jeong", "Namgoo Lee", "Jinhan Kim", "Duseok Kang", "Soonhoi Ha"], "DOIs": ["https://doi.org/10.1109/ICST.2017.10"], "tag": ["Fault Localization and Injection"], "abstract": "Emulating fault scenarios by injecting faults intentionally is commonly used to test and verify the robustness of a system. As the number of hardware devices integrated into an embedded system tends to increase consistently and the chance of hardware failure is expected to increase in an SoC, it becomes important to emulate fault scenarios caused by hardware-related errors. To this end, we present a kernel-level fault injection framework for ARM-based embedded Linux systems, called FIFA, aiming to investigate the effect of an individual hardware error in a real hardware platform rather than performing statistical analysis by random experiments. FIFA consists of two complementary fault injection techniques, one is based on the Kernel GNU Debugger and the other on hardware breakpoints. Compared with the previous work that emulates bit-flip errors only, FIFA supports other types of errors such as time delay and device failure. The viability of the proposed framework is proved by real-life experiments with an ODROID-XU4 system."}, {"id": "conf/icst/XuLC17", "title": "Using Delta Debugging to Minimize Stress Tests for Concurrent Data Structures.", "authors": ["Jing Xu", "Yu Lei", "Richard H. Carver"], "DOIs": ["https://doi.org/10.1109/ICST.2017.11"], "tag": ["Debugging, Composite Faults, and Complexity Analysis"], "abstract": "Concurrent data structures are often tested under stress to detect bugs that can only be exposed by some rare interleavings of instructions. A typical stress test for a concurrent data structure creates a number of threads that repeatedly invoke methods of the target data structure. After a failure is detected by a stress test, developers need to localize the fault causing the failure. However, the execution trace of a failed stress test may be very long, making it time-consuming to replay the failure and localize the fault. In this paper, we present an approach to minimizing stress tests for concurrent data structures. Our approach is to create a smaller test that still produces the same failure by removing some of the threads and/or method invocations in the original stress test. We apply delta debugging to identify the threads and method invocations that are essential for causing the failure. Other threads and method invocations are removed to create a smaller stress test. To increase the chance of triggering the original failure during the execution of the new stress test, we force the new execution to replay the original failed execution trace when possible, and try to guide the execution back to the failed trace when the execution diverges. We describe a tool called TestMinimizer and report the results of an empirical study in which TestMinimizer was applied to 16 real-life concurrent data structures. The results of our evaluation showed that TestMinimizer can effectively and efficiently minimize the stress tests for these concurrent data structures."}, {"id": "conf/icst/GopinathJG17", "title": "The Theory of Composite Faults.", "authors": ["Rahul Gopinath", "Carlos Jensen", "Alex Groce"], "DOIs": ["https://doi.org/10.1109/ICST.2017.12"], "tag": ["Debugging, Composite Faults, and Complexity Analysis"], "abstract": "Fault masking happens when the effect of one fault serves to mask that of another fault for particular test inputs. The coupling effect is relied upon by testing practitioners to ensure that fault masking is rare. It states that complex faults are coupled to simple faults in such a way that a test data set that detects all simple faults in a program will detect a high percentage of the complex faults. While this effect has been empirically evaluated, our theoretical understanding of the coupling effect is as yet incomplete. Wah proposed a theory of the coupling effect on finite bijective (or near bijective) functions with the same domain and co-domain and assuming a uniform distribution for candidate functions. This model, however, was criticized as being too simple to model real systems, as it did not account for differing domain and co-domain in real programs, or for the syntactic neighborhood. We propose a new theory of fault coupling for general functions (with certain constraints). We show that there are two kinds of fault interactions, of which only the weak interaction can be modeled by the theory of the coupling effect. The strong interaction can produce faults that are semantically different from the original faults. These faults should hence be considered as independent atomic faults. Our analysis shows that the theory holds even when the effect of the syntactic neighborhood of the program is considered. We analyze numerous real-world programs with real faults to validate our hypothesis."}, {"id": "conf/icst/LuckowKP17", "title": "Symbolic Complexity Analysis Using Context-Preserving Histories.", "authors": ["Kasper S\u00f8e Luckow", "Rody Kersten", "Corina S. Pasareanu"], "DOIs": ["https://doi.org/10.1109/ICST.2017.13"], "tag": ["Debugging, Composite Faults, and Complexity Analysis"], "abstract": "We propose a technique based on symbolic execution for analyzing the algorithmic complexity of programs. The technique uses an efficient guided analysis to compute bounds on the worst-case complexity (for increasing input sizes) and to generate test values that trigger the worst-case behaviors. The resulting bounds are fitted to a function to obtain a prediction of the worst-case program behavior at any input sizes. Comparing these predictions to the programmers' expectations or to theoretical asymptotic bounds can reveal vulnerabilities or confirm that a program behaves as expected. To achieve scalability we use path policies to guide the symbolic execution towards worst-case paths. The policies are learned from the worst-case results obtained with exhaustive exploration at small input sizes and are applied to guide exploration at larger input sizes, where un-guided exhaustive exploration is no longer possible. To achieve precision we use path policies that take into account the history of choices made along the path when deciding which branch to execute next in the program. Furthermore, the history computation is context-preserving, meaning that the decision for each branch depends on the history computed with respect to the enclosing method. We implemented the technique in the Symbolic PathFinder tool. We show experimentally that it can find vulnerabilities in complex Java programs and can outperform established symbolic techniques."}, {"id": "conf/icst/KleineS17", "title": "Coveringcerts: Combinatorial Methods for X.509 Certificate Testing.", "authors": ["Kristoffer Kleine", "Dimitris E. Simos"], "DOIs": ["https://doi.org/10.1109/ICST.2017.14"], "tag": ["Security Testing"], "abstract": "Correct behaviour of X.509 certificate validation code in SSL/TLS implementations is crucial to ensure secure communication channels. Recently, there have been major efforts in testing these implementations, namely frankencerts and mucerts, which provide new ways to generate test certificates which are likely to reveal errors in the implementations of X.509 validation logic. However, it remains a significant challenge to generate effective test certificates. In this paper, we explore the applicability of a prominent combinatorial method, namely combinatorial testing, for testing of X.509 certificates. We demonstrate that combinatorial testing provides the theoretical guarantees for revealing errors in the certificate validation logic of SSL/TLS implementations. Our findings indicate that the introduced combinatorial testing constructs, coveringcerts, compare favorably to existing testing methods by encapsulating the semantics of the validation logic in the input model and employing combinatorial strategies that significantly reduce the number of tests needed. Besides the foundations of our approach, we also report on experiments that indicate its practical use."}, {"id": "conf/icst/Al-QahtaniER17", "title": "Recovering Semantic Traceability Links between APIs and Security Vulnerabilities: An Ontological Modeling Approach.", "authors": ["Sultan S. Al-Qahtani", "Ellis E. Eghan", "Juergen Rilling"], "DOIs": ["https://doi.org/10.1109/ICST.2017.15"], "tag": ["Security Testing"], "abstract": "Over the last decade, a globalization of the software industry took place, which facilitated the sharing and reuse of code across existing project boundaries. At the same time, such global reuse also introduces new challenges to the software engineering community, with not only components but also their problems and vulnerabilities being now shared. For example, vulnerabilities found in APIs no longer affect only individual projects but instead might spread across projects and even global software ecosystem borders. Tracing these vulnerabilities at a global scale becomes an inherently difficult task since many of the existing resources required for such analysis still rely on proprietary knowledge representation. In this research, we introduce an ontology-based knowledge modeling approach that can eliminate such information silos. More specifically, we focus on linking security knowledge with other software knowledge to improve traceability and trust in software products (APIs). Our approach takes advantage of the Semantic Web and its reasoning services, to trace and assess the impact of security vulnerabilities across project boundaries. We present a case study, to illustrate the applicability and flexibility of our ontological modeling approach by tracing vulnerabilities across project and resource boundaries."}, {"id": "conf/icst/WanLXCL17", "title": "Mining Sandboxes for Linux Containers.", "authors": ["Zhiyuan Wan", "David Lo", "Xin Xia", "Liang Cai", "Shanping Li"], "DOIs": ["https://doi.org/10.1109/ICST.2017.16", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.16"], "tag": ["Security Testing"], "abstract": "A container is a group of processes isolated from other groups via distinct kernel namespaces and resource allocation quota. Attacks against containers often leverage kernel exploits through system call interface. In this paper, we present an approach that mines sandboxes for containers. We first explore the behaviors of a container by leveraging automatic testing, and extract the set of system calls accessed during testing. The set of system calls then results as a sandbox of the container. The mined sandbox restricts the container's access to system calls which are not seen during testing and thus reduces the attack surface. In the experiment, our approach requires less than eleven minutes to mine sandbox for each of the containers. The enforcement of mined sandboxes does not impact the regular functionality of a container and incurs low performance overhead."}, {"id": "conf/icst/OliveiraFDHS17", "title": "Perphecy: Performance Regression Test Selection Made Simple but Effective.", "authors": ["Augusto Born de Oliveira", "Sebastian Fischmeister", "Amer Diwan", "Matthias Hauswirth", "Peter F. Sweeney"], "DOIs": ["https://doi.org/10.1109/ICST.2017.17"], "tag": ["Regression Testing"], "abstract": "Developers of performance sensitive production software are in a dilemma: performance regression tests are too costly to run at each commit, but skipping the tests delays and complicates performance regression detection. Ideally, developers would have a system that predicts whether a given commit is likely to impact performance and suggests which tests to run to detect a potential performance regression. Prior approaches towards this problem require static or dynamic analyses that limit their generality and applicability. This paper presents an approach that is simple and general, and that works surprisingly well for real applications."}, {"id": "conf/icst/AlagozHG17", "title": "A Selection Method for Black Box Regression Testing with a Statistically Defined Quality Level.", "authors": ["Ibrahim Alag\u00f6z", "Thomas Herpel", "Reinhard German"], "DOIs": ["https://doi.org/10.1109/ICST.2017.18"], "tag": ["Regression Testing"], "abstract": "We consider regression testing of safety-critical systems consisting of black-box components. This scenario is common for automotive electronic systems where testing time is expensive and should be reduced without uncontrolled reduction of reliability. This requires a methodology to select test cases from a larger test suite for which a defined quality level of the resulting regression test cycle can be provided. With this in mind, we propose a method to select test cases based on a stochastic model. We are modeling test case failure probabilities as dependent random variables, therefore already observed test results have an influence on the estimation of further test case failure probabilities. Based on an information theoretical approach, we validate the mutual differential information degree between test case failure probabilities and compute a function which returns the risk for not selecting a test case, i.e., the probability of not selecting a failing test case. Depending on the mutual differential information significant reductions of testing time can be achieved while testing reliability is preserved at a quantifiable high level. We will validate theoretically our results and will show in an industrial case study the benefits of our method."}, {"id": "conf/icst/ArcuriFJ17", "title": "Private API Access and Functional Mocking in Automated Unit Test Generation.", "authors": ["Andrea Arcuri", "Gordon Fraser", "Ren\u00e9 Just"], "DOIs": ["https://doi.org/10.1109/ICST.2017.19"], "tag": ["Regression Testing"], "abstract": "Not all object oriented code is easily testable: Dependency objects might be difficult or even impossible to instantiate, and object-oriented encapsulation makes testing potentially simple code difficult if it cannot easily be accessed. When this happens, then developers can resort to mock objects that simulate the complex dependencies, or circumvent object-oriented encapsulation and access private APIs directly through the use of, for example, Java reflection. Can automated unit test generation benefit from these techniques as well? In this paper we investigate this question by extending the EvoSuite unit test generation tool with the ability to directly access private APIs and to create mock objects using the popular Mockito framework. However, care needs to be taken that this does not impact the usefulness of the generated tests: For example, a test accessing a private field could later fail if that field is renamed, even if that renaming is part of a semantics-preserving refactoring. Such a failure would not be revealing a true regression bug, but is a false positive, which wastes the developer's time for investigating and fixing the test. Our experiments on the SF110 and Defects4J benchmarks confirm the anticipated improvements in terms of code coverage and bug finding, but also confirm the existence of false positives. However, by ensuring the test generator only uses mocking and reflection if there is no other way to reach some part of the code, their number remains small."}, {"id": "conf/icst/LinWC17", "title": "Using Semantic Similarity in Crawling-Based Web Application Testing.", "authors": ["Jun-Wei Lin", "Farn Wang", "Paul Chu"], "DOIs": ["https://doi.org/10.1109/ICST.2017.20", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.20"], "tag": ["Web and Mobile Applications"], "abstract": "To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, their broad use is limited by the required manual configurations for input value selection, GUI state comparison and clickable detection. In existing crawlers, the configurations are usually string-matching based rules looking for tags or attributes of DOM elements, and often application-specific. Moreover, in input topic identification, it can be difficult to determine which rule suggests a better match when several rules match an input field to more than one topic. This paper presents a natural-language approach based on semantic similarity to address the above issues. The proposed approach represents DOM elements as vectors in a vector space formed by the words used in the elements. The topics of encountered input fields during crawling can then be inferred by their similarities with ones in a labeled corpus. Semantic similarity can also be applied to suggest if a GUI state is newly discovered and a DOM element is clickable under an unsupervised learning paradigm. We evaluated the proposed approach in input topic identification with 100 real-world forms and GUI state comparison with real data from industry. Our evaluation shows that the proposed approach has comparable or better performance to the conventional techniques. Experiments in input topic identification also show that the accuracy of the rule-based approach can be improved by up to 22% when integrated with our approach."}, {"id": "conf/icst/FazziniFCO17", "title": "Barista: A Technique for Recording, Encoding, and Running Platform Independent Android Tests.", "authors": ["Mattia Fazzini", "Eduardo Noronha de A. Freitas", "Shauvik Roy Choudhary", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1109/ICST.2017.21"], "tag": ["Web and Mobile Applications"], "abstract": "Because mobile apps are extremely popular and often mission critical nowadays, companies invest a great deal of resources in testing the apps they provide to their customers. Testing is particularly important for Android apps, which must run on a multitude of devices and operating system versions. Unfortunately, as we confirmed in many interviews with quality assurance professionals, app testing is today a very human intensive, and therefore tedious and error prone, activity. To address this problem, and better support testing of Android apps, we propose a new technique that allows testers to easily create platform independent test scripts for an app and automatically run the generated test scripts on multiple devices and operating system versions. The technique does so without modifying the app under test or the runtime system, by (1) intercepting the interactions of the tester with the app and (2) providing the tester with an intuitive way to specify expected results that it then encode as test oracles. We implemented our technique in a tool named Barista and used the tool to evaluate the practical usefulness and applicability of our approach. Our results show that Barista (1) can faithfully encode user defined test cases as test scripts with built-in oracles that can run on multiple platforms and (2) outperforms two popular tools with similar functionality. Barista and our experimental infrastructure are publicly available."}, {"id": "conf/icst/LiCWH0WL17", "title": "ATOM: Automatic Maintenance of GUI Test Scripts for Evolving Mobile Applications.", "authors": ["Xiao Li", "Nana Chang", "Yan Wang", "Haohua Huang", "Yu Pei", "Linzhang Wang", "Xuandong Li"], "DOIs": ["https://doi.org/10.1109/ICST.2017.22"], "tag": ["Web and Mobile Applications"], "abstract": "The importance of regression testing in assuring the integrity of a program after changes is well recognized. One major obstacle in practicing regression testing is in maintaining tests that become obsolete due to evolved program behavior or specification. For mobile apps, the problem of maintaining obsolete GUI test scripts for regression testing is even more pressing. Mobile apps rely heavily on the correct functioning of their GUIs to compete on the market and provide good user experiences. But on the one hand, GUI tests break easily when changes happen to the GUI, On the other hand, mobile app developers often need to fight for a tight feedback loop and are left with limited time for test maintenance. In this paper, we propose a novel approach, called ATOM, to automatically maintain GUI test scripts of mobile apps for regression testing. ATOM uses an event sequence model to abstract possible event sequences on a GUI and a delta ESM to abstract the changes made to the GUI. Given both models as input, ATOM automatically updates the test scripts written for a base version app to reflect the changes. In an experiment with 22 versions from 11 production Android apps, ATOM updated all the test scripts affected by the version change, the updated scripts achieve over 80% of the coverage by the original scripts on the base version app, all except one set of updated scripts preserve over 60% of the actions in the original test scripts."}, {"id": "conf/icst/ZhangYZLCHL17", "title": "Automated Testing of Definition-Use Data Flow for Multithreaded Programs.", "authors": ["Xiaodong Zhang", "Zijiang Yang", "Qinghua Zheng", "Pei Liu", "Jialiang Chang", "Yu Hao", "Ting Liu"], "DOIs": ["https://doi.org/10.1109/ICST.2017.23"], "tag": ["Parallel Systems and Concurrency"], "abstract": "With the advent of multicore processors, there is a trend towards multithreading to take advantage of parallel computing resources. Due to greatly increased complexity, programmers need effective testing methodology that can thoroughly test multithreaded programs. There has been significant progress based on symbolic execution that attempts to exhaustively explore all the intra-thread paths and inter-thread interleavings. However, such testing approach faces two insuperable challenges. Firstly, exploring an astronomically large number of paths and interleavings limits its scalability. Secondly, a path itself does not directly help programmers understand program behavior. In this paper, we propose an alternate testing methodology that focuses on definition-use data flow instead of paths/interleavings. Such approach not only leads to orders of magnitude reduction in testing complexity, but also gives programmers direct help on examining the shared variable usage in a multithreaded program."}, {"id": "conf/icst/ChanWSPS17", "title": "IPA: Error Propagation Analysis of Multi-Threaded Programs Using Likely Invariants.", "authors": ["Abraham Chan", "Stefan Winter", "Habib Saissi", "Karthik Pattabiraman", "Neeraj Suri"], "DOIs": ["https://doi.org/10.1109/ICST.2017.24"], "tag": ["Parallel Systems and Concurrency"], "abstract": "Error Propagation Analysis (EPA) is a technique forunderstanding how errors affect a program's execution and resultin program failures. For this purpose, EPA usually compares thetraces of a fault-free (golden) run with those from a faulty run ofthe program. This makes existing EPA approaches brittle for multithreadedprograms, which do not typically have a deterministicgolden run. In this paper, we study the use of likely invariantsgenerated by automated approaches as alternatives for goldenrun based EPA in multithreaded programs. We present InvariantPropagation Analysis (IPA), an approach and a framework forautomatically deriving invariants for multithreaded programs, and using the invariants for EPA. We evaluate the invariantsderived by IPA in terms of their coverage for different faulttypes across six representative programs through fault injectionexperiments. We find that stable invariants can be inferred in allsix programs, although their coverage of faults depends on theapplication and the fault type."}, {"id": "conf/icst/DiasFFLSSV17", "title": "Verifying Concurrent Programs Using Contracts.", "authors": ["Ricardo J. Dias", "Carla Ferreira", "Jan Fiedor", "Jo\u00e3o M. Louren\u00e7o", "Ales Smrcka", "Diogo G. Sousa", "Tom\u00e1s Vojnar"], "DOIs": ["https://doi.org/10.1109/ICST.2017.25"], "tag": ["Parallel Systems and Concurrency"], "abstract": "The central notion of this paper is that of contracts for concurrency, allowing one to capture the expected atomicity of sequences of method or service calls in a concurrent program. The contracts may be either extracted automatically from the source code, or provided by developers of libraries or software modules to reflect their expected usage in a concurrent setting. We start by extending the so-far considered notion of contracts for concurrency in several ways, improving their expressiveness and enhancing their applicability in practice. Then, we propose two complementary analyses - a static and a dynamic one - to verify programs against the extended contracts. We have implemented both approaches and present promising experimental results from their application on various programs, including real-world ones where our approach unveiled previously unknown errors."}, {"id": "conf/icst/TrautschG17", "title": "Are There Any Unit Tests? An Empirical Study on Unit Testing in Open Source Python Projects.", "authors": ["Fabian Trautsch", "Jens Grabowski"], "DOIs": ["https://doi.org/10.1109/ICST.2017.26"], "tag": ["Empirics on Testing"], "abstract": "Unit testing is an essential practice in Extreme Programming (XP) and Test-driven Development (TDD) and used in many software lifecycle models. Additionally, a lot of literature deals with this topic. Therefore, it can be expected that it is widely used among developers. Despite its importance, there is no empirical study which investigates, whether unit tests are used by developers in real life projects at all. This paper presents such a study, where we collected and analyzed data from over 70K revisions of 10 different Python projects. Based on two different definitions of unit testing, we calculated the actual number of unit tests and compared it with the expected number (as inferred from the intentions of the developers), had a look at the mocking behavior of developers, and at the evolution of the number of unit tests. Our main findings show, (i) that developers believe that they are developing more unit tests than they actually do, (ii) most projects have a very small amount of unit tests, (iii) developers make use of mocks, but these do not have a significant influence on the number of unit tests, (iv) four different patterns for the evolution of the number of unit tests could be detected, and (v) the used unit test definition has an influence on the results."}, {"id": "conf/icst/ArmstrongKA17", "title": "Broadcast vs. Unicast Review Technology: Does It Matter?", "authors": ["Foundjem Armstrong", "Foutse Khomh", "Bram Adams"], "DOIs": ["https://doi.org/10.1109/ICST.2017.27"], "tag": ["Empirics on Testing"], "abstract": "Code review is the process of having other team members examine changes to a software system in order to evaluate their technical content and quality. Over the years, multiple tools have been proposed to help software developers conduct and manage code reviews. Some software organizations have been migrating from broadcast review technology to a more advanced unicast review approach such as Jira, but it is unclear if these unicast review technology leads to better code reviews. This paper empirically studies review data of five Apache projects that switched from broadcast based code review to unicast based, to understand the impact of review technology on review effectiveness and quality. Results suggest that broadcast based review is twice faster than review done with unicast based review technology. However, unicast's review quality seems to be better than that of the broadcast based. Our findings suggest that the medium (i.e., broadcast or unicast) technology used for code reviews can relate to the effectiveness and quality of reviews activities."}, {"id": "conf/icst/Fard017", "title": "JavaScript: The (Un)Covered Parts.", "authors": ["Amin Milani Fard", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1109/ICST.2017.28"], "tag": ["Empirics on Testing"], "abstract": "Testing JavaScript code is important. JavaScript has grown to be among the most popular programming languages and it is extensively used to create web applications both on the client and server. We present the first empirical study of JavaScript tests to characterize their prevalence, quality metrics (e.g. code coverage), and shortcomings. We perform our study across a representative corpus of 373 JavaScript projects, with over 5.4 million lines of JavaScript code. Our results show that 22% of the studied subjects do not have test code. About 40% of projects with JavaScript at client-side do not have a test, while this is only about 3% for the purely server-side JavaScript projects. Also tests for server-side code have high quality (in terms of code coverage, test code ratio, test commit ratio, and average number of assertions per test), while tests for client-side code have moderate to low quality. In general, tests written in Mocha, Tape, Tap, and Nodeunit frameworks have high quality and those written without using any framework have low quality. We scrutinize the (un)covered parts of the code under test to find out root causes for the uncovered code. Our results show that JavaScript tests lack proper coverage for event-dependent callbacks (36%), asynchronous callbacks (53%), and DOM-related code (63%). We believe that it is worthwhile for the developer and research community to focus on testing techniques and tools to achieve better coverage for difficult to cover JavaScript code."}, {"id": "conf/icst/ConverseOK17", "title": "Non-Semantics-Preserving Transformations for Higher-Coverage Test Generation Using Symbolic Execution.", "authors": ["Hayes Converse", "Oswaldo Olivo", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ICST.2017.29"], "tag": ["Model-Based Testing I"], "abstract": "Symbolic execution is a well-studied method that has a number of useful applications, including generation of high-quality test suites that find many bugs. However, scaling it to real-world applications is a significant challenge, as it depends on the often expensive process of solving constraints on program inputs. Our insight is that when the goal of symbolic execution is test generation, non-semantics-preserving program transformations can reduce the cost of symbolic execution and the tests generated for the transformed programs can still serve as quality suites for the original program. We present five such transformations based on a few different program simplification heuristics that are designed to lower the cost of symbolic execution for input generation. As enabling technology we use the KLEE symbolic execution engine and the LLVM compiler infrastructure. We evaluate our transformations using a suite of small subjects as well as a subset of the well-studied Unix Coreutils. In a majority of cases, our approach reduces the time for symbolic execution for input generation and increases code coverage of the resultant suite."}, {"id": "conf/icst/WalkinshawF17", "title": "Uncertainty-Driven Black-Box Test Data Generation.", "authors": ["Neil Walkinshaw", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1109/ICST.2017.30"], "tag": ["Model-Based Testing I"], "abstract": "We can never be certain that a software system is correct simply by testing it, but with every additional successful test we become less uncertain about its correctness. In absence of source code or elaborate specifications and models, tests are usually generated or chosen randomly. However, rather than randomly choosing tests, it would be preferable to choose those tests that decrease our uncertainty about correctness the most. In order to guide test generation, we apply what is referred to in Machine Learning as \"Query Strategy Framework\": We infer a behavioural model of the system under test and select those tests which the inferred model is \"least certain\" about. Running these tests on the system under test thus directly targets those parts about which tests so far have failed to inform the model. We provide an implementation that uses a genetic programming engine for model inference in order to enable an uncertainty sampling technique known as \"query by committee\", and evaluate it on eight subject systems from the Apache Commons Math framework and JodaTime. The results indicate that test generation using uncertainty sampling outperforms conventional and Adaptive Random Testing."}, {"id": "conf/icst/SullivanWZK17", "title": "Automated Test Generation and Mutation Testing for Alloy.", "authors": ["Allison Sullivan", "Kaiyuan Wang", "Razieh Nokhbeh Zaeem", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ICST.2017.31"], "tag": ["Model-Based Testing I"], "abstract": "We present two novel approaches for automated testing of models written in Alloy - a well-known declarative, first-order language that is supported by a fully automatic SAT-based analysis engine. The first approach introduces automated test generation for Alloy and is embodied by three techniques that create test suites in the traditional spirit of black-box, white-box, and mutation-based testing. The second approach introduces mutation testing for Alloy and defines how to create mutants of Alloy models, compute mutation testing results, and check for equivalent mutants using SAT. The two approaches build on the theoretical foundation defined previously by our AUnit framework, which introduced the idea of unit testing for Alloy in the spirit of unit testing for imperative languages. While test generation and mutation testing are heavily studied problems with many solutions in the context of imperative languages, the key novelty of our work is to introduce and address these problems for the declarative programming paradigm, specifically for the Alloy language. Experimental results using several Alloy subjects, including those with real faults, demonstrate the efficacy of our framework."}, {"id": "conf/icst/TapplerAB17", "title": "Model-Based Testing IoT Communication via Active Automata Learning.", "authors": ["Martin Tappler", "Bernhard K. Aichernig", "Roderick Bloem"], "DOIs": ["https://doi.org/10.1109/ICST.2017.32"], "tag": ["Model-Based Testing II"], "abstract": "This paper presents a learning-based approach to detecting failures in reactive systems. The technique is based on inferring models of multiple implementations of a common specification which are pair-wise cross-checked for equivalence. Any counterexample to equivalence is flagged as suspicious and has to be analysed manually. Hence, it is possible to find possible failures in a semi-automatic way without prior modelling. We show that the approach is effective by means of a case study. For this case study, we carried out experiments in which we learned models of five implementations of MQTT brokers/servers, a protocol used in the Internet of Things. Examining these models, we found several violations of the MQTT specification. All but one of the considered implementations showed faulty behaviour. In the analysis, we discuss effectiveness and also issues we faced."}, {"id": "conf/icst/ArthoGRBMKHTY17", "title": "Model-Based API Testing of Apache ZooKeeper.", "authors": ["Cyrille Artho", "Quentin Gros", "Guillaume Rousset", "Kazuaki Banzai", "Lei Ma", "Takashi Kitamura", "Masami Hagiya", "Yoshinori Tanabe", "Mitsuharu Yamamoto"], "DOIs": ["https://doi.org/10.1109/ICST.2017.33", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.33"], "tag": ["Model-Based Testing II"], "abstract": "Apache ZooKeeper is a distributed data storage that is highly concurrent and asynchronous due to network communication, testing such a system is very challenging. Our solution using the tool \"Modbat\" generates test cases for concurrent client sessions, and processes results from synchronous and asynchronous callbacks. We use an embedded model checker to compute the test oracle for non-deterministic outcomes, the oracle model evolves dynamically with each new test step. Our work has detected multiple previously unknown defects in ZooKeeper. Finally, a thorough coverage evaluation of the core classes show how code and branch coverage strongly relate to feature coverage in the model, and hence modeling effort."}, {"id": "conf/icst/WangPB17", "title": "System Testing of Timing Requirements Based on Use Cases and Timed Automata.", "authors": ["Chunhui Wang", "Fabrizio Pastore", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1109/ICST.2017.34"], "tag": ["Model-Based Testing II"], "abstract": "In the context of use-case centric development and requirements-driven testing, this paper addresses the problem of automatically deriving system test cases to verify timing requirements. Inspired by engineering practice in an automotive software development context, we rely on an analyzable form of use case specifications and augment such functional descriptions with timed automata, capturing timing requirements, following a methodology aiming at minimizing modeling overhead. We automate the generation of executable test cases using a test strategy based on maximizing test suite diversity and building over the UPPAAL model checker. Initial empirical results based on an industrial case study provide evidence of the effectiveness of the approach."}, {"id": "conf/icst/Gyori0PM17", "title": "Efficient Incrementalized Runtime Checking of Linear Measures on Lists.", "authors": ["Alex Gyori", "Pranav Garg", "Edgar Pek", "P. Madhusudan"], "DOIs": ["https://doi.org/10.1109/ICST.2017.35"], "tag": ["Automated and Run-Time Testing"], "abstract": "We present mechanisms to specify and efficiently check, at runtime, assertions that express structural properties and aggregate measures of dynamically manipulated linkedlist data structures. Checking assertions involving the structure, disjointness, and aggregation measures on lists and list segments typically requires linear or quadratic time in the size of the heap. Our main contribution is an incrementalization instrumentation that tracks properties of data structures dynamically as the program executes and leads to orders of magnitude speedup in assertion checking in many scenarios. Our incrementalization incurs a constant overhead on updates to list structures but enables checking assertions in constant time, independent of the size of the heap. We define a general class of functions on lists, called linear measures, which are amenable to our incrementalization technique. We demonstrate the effectiveness of our technique by showing orders of magnitude speedup in two scenarios: one scenario stemming from assertions at the level of APIs of list-manipulating libraries and the other scenario stemming from providing dynamic detection of security attacks caused by malicious rootkits."}, {"id": "conf/icst/WangBO17", "title": "Behavioral Execution Comparison: Are Tests Representative of Field Behavior?", "authors": ["Qianqian Wang", "Yuriy Brun", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1109/ICST.2017.36", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.36"], "tag": ["Automated and Run-Time Testing"], "abstract": "Software testing is the most widely used approach for assessing and improving software quality, but it is inherently incomplete and may not be representative of how the software is used in the field. This paper addresses the questions of to what extent tests represent how real users use software, and how to measure behavioral differences between test and field executions. We study four real-world systems, one used by endusers and three used by other (client) software, and compare test suites written by the systems' developers to field executions using four models of behavior: statement coverage, method coverage, mutation score, and a temporal-invariant-based model we developed. We find that developer-written test suites fail to accurately represent field executions: the tests, on average, miss 6.2% of the statements and 7.7% of the methods exercised in the field, the behavior exercised only in the field kills an extra 8.6% of the mutants, finally, the tests miss 52.6% of the behavioral invariants that occur in the field. In addition, augmenting the in-house test suites with automatically-generated tests by a tool targeting high code coverage only marginally improves the tests' behavioral representativeness. These differences between field and test executions - and in particular the finer-grained and more sophisticated ones that we measured using our invariantbased model - can provide insight for developers and suggest a better method for measuring test suite quality."}, {"id": "conf/icst/PouldingF17a", "title": "Automated Random Testing in Multiple Dispatch Languages.", "authors": ["Simon M. Poulding", "Robert Feldt"], "DOIs": ["https://doi.org/10.1109/ICST.2017.37"], "tag": ["Automated and Run-Time Testing"], "abstract": "In programming languages that use multiple dispatch, a single function can have multiple implementations, each of which may specialise the function's operation. Which one of these implementations to execute is determined by the data types of all the arguments to the function. Effective testing of functions that use multiple dispatch therefore requires diverse test inputs in terms of the data types of the input's arguments as well as their values. In this paper we describe an approach for generating test inputs where both the values and types are chosen probabilistically. The approach uses reflection to automatically determine how to create inputs with the desired types, and dynamically updates the probability distribution from which types are sampled in order to improve both the test efficiency and efficacy. We evaluate the technique on 247 methods across 9 built-in functions of Julia, a technical computing language that applies multiple dispatch at runtime. In the process, we identify three real faults in these widely-used functions."}, {"id": "conf/icst/Gay17", "title": "The Fitness Function for the Job: Search-Based Generation of Test Suites That Detect Real Faults.", "authors": ["Gregory Gay"], "DOIs": ["https://doi.org/10.1109/ICST.2017.38"], "tag": ["Search-Based Testing"], "abstract": "Search-based test generation, if effective at fault detection, can lower the cost of testing. Such techniques rely on fitness functions to guide the search. Ultimately, such functions represent test goals that approximate - but do not ensure - fault detection. The need to rely on approximations leads to two questions - can fitness functions produce effective tests and, if so, which should be used to generate tests? To answer these questions, we have assessed the fault-detection capabilities of the EvoSuite framework and eight of its fitness functions on 353 real faults from the Defects4J database. Our analysis has found that the strongest indicator of effectiveness is a high level of code coverage. Consequently, the branch coverage fitness function is the most effective. Our findings indicate that fitness functions that thoroughly explore system structure should be used as primary generation objectives - supported by secondary fitness functions that vary the scenarios explored."}, {"id": "conf/icst/JanNAB17", "title": "A Search-Based Testing Approach for XML Injection Vulnerabilities in Web Applications.", "authors": ["Sadeeq Jan", "Cu D. Nguyen", "Andrea Arcuri", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1109/ICST.2017.39"], "tag": ["Search-Based Testing"], "abstract": "In most cases, web applications communicate with web services (SOAP and RESTful). The former act as a front-end to the latter, which contain the business logic. A hacker might not have direct access to those web services (e.g., they are not on public networks), but can still provide malicious inputs to the web application, thus potentially compromising related services. Typical examples are XML injection attacks that target SOAP communications. In this paper, we present a novel, search-based approach used to generate test data for a web application in an attempt to deliver malicious XML messages to web services. Our goal is thus to detect XML injection vulnerabilities in web applications. The proposed approach is evaluated on two studies, including an industrial web application with millions of users. Results show that we are able to effectively generate test data (e.g., input values in an HTML form) that detect such vulnerabilities."}, {"id": "conf/icst/PradhanWAYL17", "title": "CBGA-ES: A Cluster-Based Genetic Algorithm with Elitist Selection for Supporting Multi-Objective Test Optimization.", "authors": ["Dipesh Pradhan", "Shuai Wang", "Shaukat Ali", "Tao Yue", "Marius Liaaen"], "DOIs": ["https://doi.org/10.1109/ICST.2017.40"], "tag": ["Search-Based Testing"], "abstract": "Multi-objective search algorithms (e.g., non-dominated sorting genetic algorithm II (NSGA-II)) have been frequently applied to address various testing problems requiring multi-objective optimization such as test case selection. However, existing multi-objective search algorithms have certain randomness when selecting parent solutions for producing offspring solutions. In the worse case, suboptimal parent solutions may result in offspring solutions with bad quality, and thus affect the overall quality of the next generation. To address such a challenge, we propose a cluster-based genetic algorithm with elitist selection (CBGA-ES) with the aim to reduce such randomness for supporting multi-objective test optimization. We empirically compared CBGA-ES with random search, greedy (as baselines) and four commonly used multi-objective search algorithms (e.g., NSGA-II) using two industrial and one real world test optimization problem, i.e., test suite minimization, test case prioritization, and test case selection. The results showed that CBGA-ES significantly outperformed the baseline algorithms (e.g., greedy), and the four selected search algorithms for all the three test optimization problems. CBGA-ES managed to outperform more than 75% of the objectives for all the four algorithms in each test optimization problem. Moreover, CBGA-ES was able to improve the quality of the solutions for an average of 32.5% for each objective as compared to the four algorithms for the three test optimization problems."}, {"id": "conf/icst/ChengT17", "title": "Incremental Deductive Verification for Relational Model Transformations.", "authors": ["Zheng Cheng", "Massimo Tisi"], "DOIs": ["https://doi.org/10.1109/ICST.2017.41"], "tag": ["Model Checking and Verification"], "abstract": "In contract-based development of model transformations, continuous deductive verification may help the transformation developer in early bug detection. However, because of the execution performance of current verification systems, re-verifying from scratch after a change has been made would introduce impractical delays. We address this problem by proposing an incremental verification approach for the ATL model-transformation language. Our approach is based on decomposing each OCL contract into sub-goals, and caching the sub-goal verification results. At each change we exploit the semantics of relational model transformation to determine whether a cached verification result may be impacted. Consequently, less postconditions/sub-goals need to be re-verified. When a change forces the re-verification of a postcondition, we use the cached verification results of sub-goals to construct a simplified version of the postcondition to verify. We prove the soundness of our approach and show its effectiveness by mutation analysis. Our case study presents an approximate 50% reuse of verification results for postconditions, and 70% reuse of verification results for sub-goals. The user perceives about 56% reduction of verification time for postconditions, and 51% for sub-goals."}, {"id": "conf/icst/AichernigS17", "title": "Statistical Model Checking Meets Property-Based Testing.", "authors": ["Bernhard K. Aichernig", "Richard Schumi"], "DOIs": ["https://doi.org/10.1109/ICST.2017.42"], "tag": ["Model Checking and Verification"], "abstract": "In recent years, statistical model checking (SMC) has become increasingly popular, because it scales well to larger stochastic models and is relatively simple to implement. SMC solves the model checking problem by simulating the model for finitely many executions and uses hypothesis testing to infer if the samples provide statistical evidence for or against a property. Being based on simulation and statistics, SMC avoids the state-space explosion problem well-known from other model checking algorithms. In this paper we show how SMC can be easily integrated into a property-based testing framework, like FsCheck for C#. As a result we obtain a very flexible testing and simulation environment, where a programmer can define models and properties in a familiar programming language. The advantages: no external modelling language is needed and both stochastic models and implementations can be checked. In addition, we have access to the powerful test-data generators of a property-based testing tool. We demonstrate the feasibility of our approach by repeating three experiments from the SMC literature."}, {"id": "conf/icst/PastoreMM17", "title": "Timed k-Tail: Automatic Inference of Timed Automata.", "authors": ["Fabrizio Pastore", "Daniela Micucci", "Leonardo Mariani"], "DOIs": ["https://doi.org/10.1109/ICST.2017.43"], "tag": ["Model Checking and Verification"], "abstract": "Accurate and up-to-date models describing the behavior of software systems are seldom available in practice. To address this issue, software engineers may use specification mining techniques, which can automatically derive models that capture the behavior of the system under analysis. So far, most specification mining techniques focused on the functional behavior of the systems, with specific emphasis on models that represent the ordering of operations, such as temporal rules and finite state models. Although useful, these models are inherently partial. For instance, they miss the timing behavior, which is extremely relevant for many classes of systems and components, such as shared libraries and user-driven applications. Mining specifications that include both the functional and the timing aspects can improve the applicability of many testing and analysis solutions. This paper addresses this challenge by presenting the Timed k-Tail (TkT) specification mining technique that can mine timed automata from program traces. Since timed automata can effectively represent the interplay between the functional and the timing behavior of a system, TkT could be exploited in those contexts where time-related information is relevant. Our empirical evaluation shows that TkT can efficiently and effectively mine accurate models. The mined models have been used to identify executions with anomalous timing. The evaluation shows that most of the anomalous executions have been correctly identified while producing few false positives."}, {"id": "conf/icst/EnoiuSCP17", "title": "A Comparative Study of Manual and Automated Testing for Industrial Control Software.", "authors": ["Eduard Paul Enoiu", "Daniel Sundmark", "Adnan Causevic", "Paul Pettersson"], "DOIs": ["https://doi.org/10.1109/ICST.2017.44"], "tag": ["New Methods and Empirical Results (Short Papers)"], "abstract": "Automated test generation has been suggested as a way of creating tests at a lower cost. Nonetheless, it is not very well studied how such tests compare to manually written ones in terms of cost and effectiveness. This is particularly true for industrial control software, where strict requirements on both specification-based testing and code coverage typically are met with rigorous manual testing. To address this issue, we conducted a case study in which we compared manually and automatically created tests. We used recently developed real-world industrial programs written in the IEC 61131-3, a popular programming language for developing industrial control systems using programmable logic controllers. The results show that automatically generated tests achieve similar code coverage as manually created tests, but in a fraction of the time (an average improvement of roughly 90%). We also found that the use of an automated test generation tool does not result in better fault detection in terms of mutation score compared to manual testing. Specifically, manual tests more effectively detect logical, timer and negation type of faults, compared to automatically generated tests. The results underscore the need to further study how manual testing is performed in industrial practice and the extent to which automated test generation can be used in the development of reliable systems."}, {"id": "conf/icst/ChenBHZZX17", "title": "How Do Assertions Impact Coverage-Based Test-Suite Reduction?", "authors": ["Junjie Chen", "Yanwei Bai", "Dan Hao", "Lingming Zhang", "Lu Zhang", "Bing Xie"], "DOIs": ["https://doi.org/10.1109/ICST.2017.45"], "tag": ["New Methods and Empirical Results (Short Papers)"], "abstract": "Code coverage is the dominant criterion in test-suite reduction. Typically, most test-suite reduction techniques repeatedly remove tests covering code that has been covered by other tests from the test suite. However, test-suite reduction based on code coverage alone may incur fault-detection capability loss, because a test detects faults if and only if its execution covers buggy code and its test oracle catches the buggy state. In other words, test oracles may also affect test-suite reduction, However, to our knowledge, their impacts have never been studied before. In this paper, we conduct the first empirical study on such impacts by using 10 real-world GitHub Java projects, and find that assertions (i.e., a typical type of test oracles) are significantly correlated with coverage-based test-suite reduction. Based on our preliminary study results, we also proposed an assertion-aware test-suite reduction technique which outperforms traditional test-suite reduction in terms of cost-effectiveness."}, {"id": "conf/icst/DevroeyPPLSH17", "title": "Automata Language Equivalence vs. Simulations for Model-Based Mutant Equivalence: An Empirical Evaluation.", "authors": ["Xavier Devroey", "Gilles Perrouin", "Mike Papadakis", "Axel Legay", "Pierre-Yves Schobbens", "Patrick Heymans"], "DOIs": ["https://doi.org/10.1109/ICST.2017.46", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.46"], "tag": ["New Methods and Empirical Results (Short Papers)"], "abstract": "Mutation analysis is a popular test assessment method. It relies on the mutation score, which indicates how many mutants are revealed by a test suite. Yet, there are mutants whose behaviour is equivalent to the original system, wasting analysis resources and preventing the satisfaction of the full (100%) mutation score. For finite behavioural models, the Equivalent Mutant Problem (EMP) can be addressed through language equivalence of non-deterministic finite automata, which is a well-studied, yet computationally expensive, problem in automata theory. In this paper, we report on our preliminary assessment of a state-of-the-art exact language equivalence tool to handle the EMP against 3 models of size up to 15,000 states on 1170 mutants. We introduce random and mutation-biased simulation heuristics as baselines for comparison. Results show that the exact approach is often more than ten times faster in the weak mutation scenario. For strong mutation, our biased simulations are faster for models larger than 300 states. They can be up to 1,000 times faster while limiting the error of misclassifying non-equivalent mutants as equivalent to 10% on average. We therefore conclude that the approaches can be combined for improved efficiency."}, {"id": "conf/icst/LaurentPKHTV17", "title": "Assessing and Improving the Mutation Testing Practice of PIT.", "authors": ["Thomas Laurent", "Mike Papadakis", "Marinos Kintis", "Christopher Henard", "Yves Le Traon", "Anthony Ventresque"], "DOIs": ["https://doi.org/10.1109/ICST.2017.47", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.47"], "tag": ["New Methods and Empirical Results (Short Papers)"], "abstract": "Mutation testing is extensively used in software testing studies. However, popular mutation testing tools use a restrictive set of mutants which does not conform to the community standards and mutation testing literature. This can be problematic since the effectiveness of mutation strongly depends on the used mutants. To investigate this issue we form an extended set of mutants and implement it on a popular mutation testing tool named PIT. We then show that in real-world projects the original mutants of PIT are easier to kill and lead to tests that score statistically lower than those of the extended set of mutants for a range of 35% to 70% of the studied classes. These results raise serious concerns regarding the validity of mutation-based experiments that use PIT. To further show the strengths of the extended mutants we also performed an analysis using a benchmark with mutation-adequate test cases and identified equivalent mutants. Our results confirmed that the extended mutants are more effective than a) the original version of PIT and b) two other popular mutation testing tools (major and muJava). In particular, our results demonstrate that the extended mutants are more effective by 23%, 12% and 7% than the mutants of the original PIT, major and muJava. They also show that the extended mutants are at least as strong as the mutants of all the other three tools together. To support future research, we make the new version of PIT, which is equipped with the extended mutants, publicly available."}, {"id": "conf/icst/MarcozziDBKP17", "title": "Generic and Effective Specification of Structural Test Objectives.", "authors": ["Micha\u00ebl Marcozzi", "Micka\u00ebl Delahaye", "S\u00e9bastien Bardin", "Nikolai Kosmatov", "Virgile Prevosto"], "DOIs": ["https://doi.org/10.1109/ICST.2017.48"], "tag": ["New Theories and Tools (Short Papers)"], "abstract": "A large amount of research has been carried out to automate white-box testing. While a wide range of different and sometimes heterogeneous code-coverage criteria have been proposed, there exists no generic formalism to describe them all, and available test automation tools usually support only a small subset of them. We introduce a new specification language, called HTOL (Hyperlabel Test Objectives Language), providing a powerful generic mechanism to define a wide range of test objectives. HTOL comes with a formal semantics, and can encode all standard criteria but full mutations. Besides specification, HTOL is appealing in the context of test automation as it allows handling criteria in a unified way."}, {"id": "conf/icst/MilewiczP17", "title": "Ariadne: Hybridizing Directed Model Checking and Static Analysis.", "authors": ["Reed Milewicz", "Peter Pirkelbauer"], "DOIs": ["https://doi.org/10.1109/ICST.2017.49"], "tag": ["New Theories and Tools (Short Papers)"], "abstract": "While directed model checking has proven to be a powerful tool in the fight against concurrency bugs, scalability remains a concern due to the combinatorial explosion in size of the state space. Overcoming that combinatorial explosion requires the selection and/or parameterization of meta*-heuristics, but we are left with a persistent problem of having to provide or compute specialized knowledge of the program under consideration, and this limits the practical value of the technique. To circumvent that, this paper investigates directed model checking as a platform for the synthesis of results from other analyses. We introduce an open-source tool, Ariadne, which translates reports of suspected race conditions of a static analyzer (Petablox) to instrumentation using a source-to-source compiler (ROSE) that can be exploited by a model checker (Java Pathfinder). We detail the algorithm used, present experimental results, and outline directions for future research."}, {"id": "conf/icst/PatrickDG17", "title": "A Toolkit for Testing Stochastic Simulations against Statistical Oracles.", "authors": ["Matthew Patrick", "Ruairi Donnelly", "Christopher A. Gilligan"], "DOIs": ["https://doi.org/10.1109/ICST.2017.50"], "tag": ["New Theories and Tools (Short Papers)"], "abstract": "Stochastic simulations are developed and employed across many fields, to advise governmental policy decisions and direct future research. Faulty simulation software can have serious consequences, but its correctness is difficult to determine due to complexity and random behaviour. Stochastic simulations may output a different result each time they are run, whereas most testing techniques are designed for programs which (for a given set of inputs) always produce the same behaviour. In this paper, we introduce a new approach towards testing stochastic simulations using statistical oracles and transition probabilities. Our approach was implemented as a toolkit, which allows the frequency of state transitions to be tested, along with their final output distribution. We evaluated our toolkit on eight simulation programs from a variety fields and found it can detect errors at least three times smaller (and in one case, over 1000 times smaller) than a conventional (tolerance threshold) approach."}, {"id": "conf/icst/GambiGZ17", "title": "O!Snap: Cost-Efficient Testing in the Cloud.", "authors": ["Alessio Gambi", "Alessandra Gorla", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1109/ICST.2017.51", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.51"], "tag": ["New Theories and Tools (Short Papers)"], "abstract": "Porting a testing environment to a cloud infrastructure is not straightforward. This paper presents O!Snap, an approach to generate test plans to cost-efficiently execute tests in the cloud. O!Snap automatically maximizes reuse of existing virtual machines, and interleaves the creation of updated test images with the execution of tests to minimize overall test execution time and/or cost. In an evaluation involving 2,600+ packages and 24,900+ test jobs of the Debian continuous integration environment, O!Snap reduces test setup time by up to 88% and test execution time by up to 43.3% without additional costs."}, {"id": "conf/icst/DwarakanathEPDP17", "title": "Accelerating Test Automation through a Domain Specific Language.", "authors": ["Anurag Dwarakanath", "Dipin Era", "Aditya Priyadarshi", "Neville Dubash", "Sanjay Podder"], "DOIs": ["https://doi.org/10.1109/ICST.2017.52"], "tag": ["Model Checking and DSL-Based Testing"], "abstract": "Test automation involves the automatic execution of test scripts instead of being manually run. This significantly reduces the amount of manual effort needed and thus is of great interest to the software testing industry. There are two key problems in the existing tools & methods for test automation - a) Creating an automation test script is essentially a code development task, which most testers are not trained on, and b) the automation test script is seldom readable, making the task of maintenance an effort intensive process. We present the Accelerating Test Automation Platform (ATAP) which is aimed at making test automation accessible to non-programmers. ATAP allows the creation of an automation test script through a domain specific language based on English. The English-like test scripts are automatically converted to machine executable code using Selenium WebDriver. ATAP's English-like test script makes it easy for non-programmers to author. The functional flow of an ATAP script is easy to understand as well thus making maintenance simpler (you can understand the flow of the test script when you revisit it many months later). ATAP has been built around the Eclipse ecosystem and has been used in a real-life testing project. We present the details of the implementation of ATAP and the results from its usage in practice."}, {"id": "conf/icst/DarkeCCV17", "title": "Efficient Safety Proofs for Industry-Scale Code Using Abstractions and Bounded Model Checking.", "authors": ["Priyanka Darke", "Bharti Chimdyalwar", "Avriti Chauhan", "R. Venkatesh"], "DOIs": ["https://doi.org/10.1109/ICST.2017.53"], "tag": ["Model Checking and DSL-Based Testing"], "abstract": "Loop Abstraction followed by Bounded Model Checking, or LABMC in short, is a promising recent technique for proving safety of large programs. In an experimental setup proposed last year [14], LABMC was combined with slicing and Iterative Context Extension (ICE) with the aim of achieving scalability over industrial code. In this paper, we address two major limitations of that set-up, namely (i) the inability of ICE to prune redundant code in a verification context, and (ii) the unavailability of a tool that implements the set-up. We propose an improvement over ICE called Iterative Function Level Slicing (IFLS) and incorporate it in our tool called ELABMC, to offer an efficient implementation of [14]. We substantiate our claim with two sets of experiments over industrial applications as well as academic benchmarks. Quantifying the benefits of IFLS over traditional ICE in one, our results report that IFLS leads to 34.9% increase in efficiency, 17.7% improvement in precision, and scales in 14.2% more cases. With the second experiment, we show that ELABMC outperforms state-of-the-art verification techniques in the task of identifying static analysis warnings as false alarms."}, {"id": "conf/icst/ArcainiGR17a", "title": "NuSeen: A Tool Framework for the NuSMV Model Checker.", "authors": ["Paolo Arcaini", "Angelo Gargantini", "Elvinia Riccobene"], "DOIs": ["https://doi.org/10.1109/ICST.2017.54", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.54"], "tag": ["Model Checking and DSL-Based Testing"], "abstract": "NuSMV is a well-known tool for system verification that permits to verify both CTL and LTL properties. Although the tool is very powerful, it offers a minimal support for the editing and validation (e.g., by simulation) of models and of requirements specified as temporal properties. In this paper, we propose NuSeen, a framework that assists a designer during the modeling and V&V activities when using NuSMV. In addition to an editor furnished with syntax highlighting, autocompletion, and outline, NuSeen also provides some tools for visualizing the variable dependencies, and graphically visualizing the counterexamples. It helps the designer in validating the model by checking certain qualities like minimality and completeness. Moreover, the framework also provides facilities for model-based testing by means of a test suite generator that is able to generate tests achieving value and decision coverage for NuSMV models."}, {"id": "conf/icst/HollandSK17", "title": "Transferring State-of-the-Art Immutability Analyses: Experimentation Toolbox and Accuracy Benchmark.", "authors": ["Benjamin Holland", "Ganesh Ram Santhanam", "Suresh Kothari"], "DOIs": ["https://doi.org/10.1109/ICST.2017.55"], "tag": ["Code Analysis and White Box Testing"], "abstract": "Immutability analysis is important to software testing, verification and validation (V&V) because it can be used to identify independently testable functions without side-effects. Existing tools for immutability analysis are largely academic prototypes that have not been rigorously tested for accuracy or have not been maintained and are unable to analyze programs written in later versions of Java. In this paper, we re-implement two prominent approaches to inferring the immutability of an object: one that leverages a points-to analysis and another that uses a type-system. In addition to supporting Java 8 source programs, our re-implementations support the analysis of compiled Java bytecode. In order to evaluate the relative accuracy, we create a benchmark that rigorously tests the accuracy boundaries of the respective approaches. We report results of experiments on analyzing the benchmark with the two approaches and compare their scalability to real world applications. Our results from the benchmark reveal that points-to based approach is more accurate than the type inference based approach in certain cases. However, experiments with real world applications show that the points-to based approach does not scale well to very large applications and a type inference based approach may offer a scalable alternative."}, {"id": "conf/icst/TangCZGXHBM17", "title": "NIVAnalyzer: A Tool for Automatically Detecting and Verifying Next-Intent Vulnerabilities in Android Apps.", "authors": ["Junjie Tang", "Xingmin Cui", "Ziming Zhao", "Shanqing Guo", "Xin-Shun Xu", "Chengyu Hu", "Tao Ban", "Bing Mao"], "DOIs": ["https://doi.org/10.1109/ICST.2017.56"], "tag": ["Code Analysis and White Box Testing"], "abstract": "In the Android system design, any app can start another app's public components to facilitate code reuse by sending an asynchronous message called Intent. In addition, Android also allows an app to have private components that should only be visible to the app itself. However, malicious apps can bypass this system protection and directly invoke private components in vulnerable apps through a class of newly discovered vulnerability, which is called next-intent vulnerability. In this paper, we design an intent flow analysis strategy which accurately tracks the intent in smali code to statically detect next-intent vulnerabilities efficiently and effectively on a large scale. We further propose an automated approach to dynamically verify the discovered vulnerabilities by generating exploit apps. Then we implement a tool named NIVAnalyzer and evaluate it on 20,000 apps downloaded from Google Play. As the result, we successfully confirms 190 vulnerable apps, some of which even have millions of downloads. We also confirmed that an open-source project and a third-party SDK, which are still used by other apps, have next intent vulnerabilities."}, {"id": "conf/icst/MarcozziBDKP17", "title": "Taming Coverage Criteria Heterogeneity with LTest.", "authors": ["Micha\u00ebl Marcozzi", "S\u00e9bastien Bardin", "Micka\u00ebl Delahaye", "Nikolai Kosmatov", "Virgile Prevosto"], "DOIs": ["https://doi.org/10.1109/ICST.2017.57"], "tag": ["Code Analysis and White Box Testing"], "abstract": "Automated white-box testing is a major issue in software engineering. In previous work, we introduced LTest, a generic and integrated toolkit for automated white-box testing of C programs. LTest supports a broad class of coverage criteria in a unified way (through the label specification mechanism) and covers most major parts of the testing process - including coverage measurement, test generation and detection of infeasible test objectives. However, the original version of LTest was unable to handle several major classes of coverage criteria, such as MCDC or dataflow criteria. Moreover, its practical applicability remained barely assessed. In this work, we present a significantly extended version of LTest that supports almost all existing testing criteria, including MCDC and some software security properties, through a native support of recently proposed hyperlabels. We also provide a more realistic view on the practical applicability of the extended tool, with experiments assessing its efficiency and scalability on real-world programs."}, {"id": "conf/icst/UkaiQ17", "title": "Test Design as Code: JCUnit.", "authors": ["Hiroshi Ukai", "Xiao Qu"], "DOIs": ["https://doi.org/10.1109/ICST.2017.58", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.58"], "tag": ["Code Analysis and White Box Testing"], "abstract": "In a development process where testing is highly automated, there is a major challenge to cope with issues such as huge test size and test stability. In this paper, we propose a model-based testing (MBT) tool called JCUnit, which generates a test suite from a model given as a Java class. Unlike other tools, it is designed to generate small and stable test suites and supports various popular models. With this tool, developers can apply MBT approach to their products without learning domain-specific language of proprietary MBT tools. Moreover, features such as portability and pluggability make it useful in a wide range of phases from unit testing to system testing. As a result, the efforts required in practical software testing will be reduced."}, {"id": "conf/icst/FlemstromGK17", "title": "SAGA Toolbox: Interactive Testing of Guarded Assertions.", "authors": ["Daniel Flemstr\u00f6m", "Thomas Gustafsson", "Avenir Kobetski"], "DOIs": ["https://doi.org/10.1109/ICST.2017.59"], "tag": ["Dynamic Analysis"], "abstract": "This paper presents the SAGA toolbox. It centers around development of tests, and analysis of test results, on Guarded Assertions (GA) format. Such a test defines when to test, and what to expect in such a state. The SAGA toolbox lets the user describe the test, and at the same time get immediate feedback on the test result based on a trace from the System Under Test (SUT). The feedback is visual using plots of the trace. This enables the test engineer to play around with the data and use an agile development method, since the data is already there. Moreover, the SAGA toolbox also enables the test engineer to change test stimuli plots to study the effect they have on a test. It can later generate computer programs that can feed these test stimuli to the SUT. This enables an interactive feedback loop, where immediate feedback on changes to the test, or to the test stimuli, indicate whether the test is correct and it passed or failed."}, {"id": "conf/icst/MarijanLGSI17", "title": "TITAN: Test Suite Optimization for Highly Configurable Software.", "authors": ["Dusica Marijan", "Marius Liaaen", "Arnaud Gotlieb", "Sagar Sen", "Carlo Ieva"], "DOIs": ["https://doi.org/10.1109/ICST.2017.60"], "tag": ["Dynamic Analysis"], "abstract": "Exhaustive testing of highly configurable software developed in continuous integration is rarely feasible in practice due to the configuration space of exponential size on the one hand, and strict time constraints on the other. This entails using selective testing techniques to determine the most failure-inducing test cases, conforming to highly-constrained time budget. These challenges have been well recognized by researchers, such that many different techniques have been proposed. In practice, however, there is a lack of efficient tools able to reduce high testing effort, without compromising software quality. In this paper we propose a test suite optimization technology TITAN, which increases the time-and cost-efficiency of testing highly configurable software developed in continuous integration. The technology implements practical test prioritization and minimization techniques, and provides test traceability and visualization for improving the quality of testing. We present the TITAN tool and discuss a set of methodological and technological challenges we have faced during TITAN development. We evaluate TITAN in testing of Cisco's highly configurable software with frequent high quality releases, and demonstrate the benefit of the approach in such a complex industry domain."}, {"id": "conf/icst/SunRJB17", "title": "ADRENALIN-RV: Android Runtime Verification Using Load-Time Weaving.", "authors": ["Haiyang Sun", "Andrea Ros\u00e0", "Omar Javed", "Walter Binder"], "DOIs": ["https://doi.org/10.1109/ICST.2017.61"], "tag": ["Dynamic Analysis"], "abstract": "Android has become one of the most popular operating systems for mobile devices. As the number of applications for the Android ecosystem grows, so is their complexity, increasing the need for runtime verification on the Android platform. Unfortunately, despite the presence of several runtime verification frameworks for Java bytecode, DEX bytecode used in Android does not benefit from such a wide support. While a few runtime verification tools support applications developed for Android, such tools offer only limited bytecode coverage and may not be able to detect property violations in certain classes. In this paper, we show that ADRENALIN-RV, our new runtime verification tool for Android, overcomes this limitation. In contrast to other frameworks, ADRENALIN-RV weaves monitoring code at load time and is able to instrument all loaded classes. In addition to the default classes inside the application package (APK), ADRENALIN-RV covers both the Android class library and libraries dynamically loaded from the storage, network, or generated dynamically, which related tools cannot verify. Evaluation results demonstrate the increased code coverage of ADRENALIN-RV with respect to other runtime validation tools for Android. Thanks to ADRENALIN-RV, we were able to detect violations that cannot be detected by other tools."}, {"id": "conf/icst/FowlerCSB17", "title": "Towards a Testbed for Automotive Cybersecurity.", "authors": ["Daniel S. Fowler", "Madeline Cheah", "Siraj Ahmed Shaikh", "Jeremy W. Bryans"], "DOIs": ["https://doi.org/10.1109/ICST.2017.62", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.62"], "tag": ["Industry Track"], "abstract": "Modern automotive platforms are cyber-physical in nature and increasingly connected. Cybersecurity testing of such platforms is expensive and carries safety concerns, making it challenging to perform tests for vulnerabilities and refine test methodologies. We propose a testbed, built over a Controller Area Network (CAN) simulator, and validate it against a real-world demonstration of a weakness in a test vehicle using aftermarket On Board Diagnostic (OBD) scanners (dongles)."}, {"id": "conf/icst/RamlerH17", "title": "How to Test in Sixteen Languages? Automation Support for Localization Testing.", "authors": ["Rudolf Ramler", "Robert Hoschek"], "DOIs": ["https://doi.org/10.1109/ICST.2017.63", "http://doi.ieeecomputersociety.org/10.1109/ICST.2017.63"], "tag": ["Industry Track"], "abstract": "Developing for a global market requires the internationalization of software products and their localization to different countries, regions, and cultures. Localization testing verifies that the localized software variants work, look and feel as expected. Localization testing is a perfect candidate for automation. It has a high potential to reduce the manual effort in testing of multiple language variants and to speed-up release cycles. However, localization testing is rarely investigated in scientific work. There are only a few reports on automation approaches for localization testing providing very little empirical results or practical advice. In this paper we describe the approach we applied for automated testing of the different localized variants of a large industrial software system, we report on the various bugs found, and we discuss our experiences and lessons learned."}, {"id": "conf/icst/Al-NayeemOPRZ17", "title": "Information Needs for Validating Evolving Software Systems: An Exploratory Study at Google.", "authors": ["Abdullah Al-Nayeem", "Krzysztof Ostrowski", "Sebastian Pueblas", "Christophe Restif", "Sai Zhang"], "DOIs": ["https://doi.org/10.1109/ICST.2017.64"], "tag": ["Industry Track"], "abstract": "Software evolves continuously. Its behavior must be validated by engineers who perform daily development and maintenance tasks. However, despite its high importance, information needs for validating evolving software has not been systematically studied in an industrial setting. Such lack of empirical knowledge hinders attempts to understand this fundamental practice and improve the corresponding tool support. This paper presents a study on 194 Site Reliability Engineers (SREs) at Google to explore their information needs. The results suggest several directions where software engineering researchers may consider putting effort to develop new techniques and tool support that matter to practitioners."}, {"id": "conf/icst/DarwishGT17", "title": "A Controlled Experiment on Coverage Maximization of Automated Model-Based Software Test Cases in the Automotive Industry.", "authors": ["Rashid Darwish", "Lynnie Nakyanzi Gwosuta", "Richard Torkar"], "DOIs": ["https://doi.org/10.1109/ICST.2017.65"], "tag": ["Industry Track"], "abstract": "In the automotive industry, as the complexity of electronic control units (ECUs) increase, there is a need for the creation of models that facilitate early tests to ensure functionality, but there is little guidance on how to write these tests in order to achieve maximum coverage. Our prototype CANoe+, which builds on the CANoe and GraphWalker tools, was evaluated against CANoe with regard to coverage maximization of generated test cases from the viewpoint of both software developers and software testers."}, {"id": "conf/icst/YangHHLC17", "title": "An Industrial Study of Natural Language Processing Based Test Case Prioritization.", "authors": ["Yilin Yang", "Xinhai Huang", "Xuefei Hao", "Zicong Liu", "Zhenyu Chen"], "DOIs": ["https://doi.org/10.1109/ICST.2017.66"], "tag": ["Industry Track"], "abstract": "In mobile application development, the frequentsoftware release limits the testing time resource. In order todetect bugs in early phases, researchers proposed various testcase prioritization (TCP) techniques in past decades. In practice, considering that some test case is described or contains text, theresearchers also employed Natural Language Processing (NLP)to assist the TCP techniques. This paper conducted an extensiveempirical study to analyze the performance of three NLP basedTCP technologies, which is based on 15059 test cases from 30industrial projects. The result shows that all of these threestrategies can help to improve the efficiency of software testing, and the Risk strategy achieved the best performance across thesubject programs."}, {"id": "conf/icst/AlegrothMVA17", "title": "Overview of the ICST International Software Testing Contest.", "authors": ["Emil Al\u00e9groth", "Shinsuke Matsuki", "Tanja E. J. Vos", "Kinji Akemine"], "DOIs": ["https://doi.org/10.1109/ICST.2017.67"], "tag": ["International Software Testing Contest"], "abstract": "In the software testing contest, practitioners and researcher's are invited to test their test approaches against similar approaches to evaluate pros and cons and which is perceivably the best. The 2017 iteration of the contest focused on Graphical User Interface-driven testing, which was evaluated on the testing tool TESTONA. The winner of the competition was announced at the closing ceremony of the international conference on software testing (ICST), 2017."}], "2018": [{"id": "conf/icst/GambiBZ18", "title": "Practical Test Dependency Detection.", "authors": ["Alessio Gambi", "Jonathan Bell", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00011", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00011"], "tag": ["Research 1:\nTesting & Debugging 1"], "abstract": "Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies."}, {"id": "conf/icst/AlsharifKM18", "title": "DOMINO: Fast and Effective Test Data Generation for Relational Database Schemas.", "authors": ["Abdullah Alsharif", "Gregory M. Kapfhammer", "Phil McMinn"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00012"], "tag": ["Research 1:\nTesting & Debugging 1"], "abstract": "An organization's databases are often one of its most valuable assets. Data engineers commonly use a relational database because its schema ensures the validity and consistency of the stored data through the specification and enforcement of integrity constraints. To ensure their correct specification, industry advice recommends the testing of the integrity constraints in a relational schema. Since manual schema testing is labor-intensive and error-prone, this paper presents DOMINO, a new automated technique that generates test data according to a coverage criterion for integrity constraint testing. In contrast to more generalized search-based approaches, which represent the current state of the art for this task, DOMINO uses tailored, domain-specific operators to efficiently generate test data for relational database schemas. In an empirical study incorporating 34 relational database schemas hosted by three different database management systems, the results show that DOMINO can not only generate test suites faster than the state-of-the-art search-based method but that its test suites can also detect more schema faults."}, {"id": "conf/icst/WangPB18", "title": "Automated Generation of Constraints from Use Case Specifications to Support System Testing.", "authors": ["Chunhui Wang", "Fabrizio Pastore", "Lionel C. Briand"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00013"], "tag": ["Research 1:\nTesting & Debugging 1"], "abstract": "System testing plays a crucial role in safety-critical domains, e.g., automotive, where system test cases are used to demonstrate the compliance of software with its functional and safety requirements. Unfortunately, since requirements are typically written in natural language, significant engineering effort is required to derive test cases from requirements. In such a context, automated support for generating system test cases from requirements specifications written in natural language would be highly beneficial. Unfortunately, existing approaches have limited applicability. For example, some of them require that software engineers provide formal specifications that capture some of the software behavior described using natural language. The effort needed to define such specifications is usually a significant deterrent for software developers. This paper proposes an approach, OCLgen, which largely automates the generation of the additional formal specifications required by an existing test generation approach named UMTG. More specifically, OCLgen relies on semantic analysis techniques to automatically derive the pre- and post-conditions of the activities described in use case specifications. The generated conditions are used by UMTG to identify the test inputs that cover all the use case scenarios described in use case specifications. In practice, the proposed approach enables the automated generation of test cases from use case specifications while avoiding most of the additional modeling effort required by UMTG. Results from an industrial case study show that the approach can automatically and correctly generate more than 75% of the pre- and post-conditions characterizing the activities described in use case specifications."}, {"id": "conf/icst/RahmanW18", "title": "Characterizing Defective Configuration Scripts Used for Continuous Deployment.", "authors": ["Akond Rahman", "Laurie Williams"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00014", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00014"], "tag": ["Research 2:\nRegression and Continuous Integration"], "abstract": "In software engineering, validation and verification (V&V) resources are limited and characterization of defective software source files can help in efficiently allocating V&V resources. Similar to software source files, defects occur in the scripts used to automatically manage configurations and software deployment infrastructure, often known as infrastructure as code (IaC) scripts. Defects in IaC scripts can have dire consequences, for example, creating large-scale system outages. Identifying the characteristics of defective IaC scripts can help in mitigating these defects by allocating V&V efforts efficiently based upon these characteristics. The objective of this paper is to help software practitioners to prioritize validation and verification efforts for infrastructure as code (IaC) scripts by identifying the characteristics of defective IaC scripts. Researchers have previously extracted text features to characterize defective software source files written in general purpose programming languages. We investigate if text features can be used to identify properties that characterize defective IaC scripts. We use two text mining techniques to extract text features from IaC scripts: the bag-of-words technique, and the term frequency-inverse document frequency (TF-IDF) technique. Using the extracted features and applying grounded theory, we characterize defective IaC scripts. We also use the text features to build defect prediction models with tuned statistical learners. We mine open source repositories from Mozilla, Openstack, and Wikimedia Commons, to construct three case studies and evaluate our methodology. We identify three properties that characterize defective IaC scripts: filesystem operations, infrastructure provisioning, and managing user accounts. Using the bag-of-word technique, we observe a median F-Measure of 0.74, 0.71, and 0.73, respectively, for Mozilla, Openstack, and Wikimedia Commons. Using the TF-IDF technique, we observe a median F-Measure of 0.72, 0.74, and 0.70, respectively, for Mozilla, Openstack, and Wikimedia Commons."}, {"id": "conf/icst/PradhanW00L18", "title": "REMAP: Using Rule Mining and Multi-objective Search for Dynamic Test Case Prioritization.", "authors": ["Dipesh Pradhan", "Shuai Wang", "Shaukat Ali", "Tao Yue", "Marius Liaaen"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00015", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00015"], "tag": ["Research 2:\nRegression and Continuous Integration"], "abstract": "Test case prioritization (TP) prioritizes test cases into an optimal order for achieving specific criteria (e.g., higher fault detection capability) as early as possible. However, the existing TP techniques usually only produce a static test case order before the execution without taking runtime test case execution results into account. In this paper, we propose an approach for black-box dynamic TP using rule mining and multi-objective search (named as REMAP). REMAP has three key components: 1) Rule Miner, which mines execution relations among test cases from historical execution data; 2) Static Prioritizer, which defines two objectives (i.e., fault detection capability (FDC) and test case reliance score (TRS)) and applies multi-objective search to prioritize test cases statically; and 3) Dynamic Executor and Prioritizer, which executes statically-prioritized test cases and dynamically updates the test case order based on the runtime test case execution results. We empirically evaluated REMAP with random search, greedy based on FDC, greedy based on FDC and TRS, static search-based prioritization, and rule-based prioritization using two industrial and three open source case studies. Results showed that REMAP significantly outperformed the other approaches for 96% of the case studies and managed to achieve on average 18% higher Average Percentage of Faults Detected (APFD)."}, {"id": "conf/icst/ChenZ18", "title": "Speeding up Mutation Testing via Regression Test Selection: An Extensive Study.", "authors": ["Lingchao Chen", "Lingming Zhang"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00016", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00016"], "tag": ["Research 2:\nRegression and Continuous Integration"], "abstract": "Mutation testing is one of the most powerful methodologies to evaluate the quality of test suites, and has also been demonstrated to be effective for various other testing and debugging problems, e.g., test generation, fault localization, and program repair. However, despite various mutation testing optimization techniques, mutation testing is still notoriously time-consuming. Regression Testing Selection (RTS) has been widely used to speed up regression testing. Given a new program revision, RTS techniques only select and rerun the tests that may be affected by code changes, since the other tests should have the same results as the prior revision. To date, various practical RTS tools have been developed and used in practice. Intuitively, such RTS tools may be directly used to speed up mutation testing of evolving software systems, since we can simply recollect the mutation testing results of the affected tests while directly obtaining the mutation testing results for the other tests from the prior revision. However, to our knowledge, there is no such study. Therefore, in this paper, we perform the first extensive study (using 1513 revisions of 20 real-world GitHub Java projects, totalling 83.26 Million LoC) on the effectiveness and efficiency of various RTS techniques in speeding up mutation testing. Our study results demonstrate that both file-level static and dynamic RTS can achieve precise and efficient mutation testing, providing practical guidelines for developers."}, {"id": "conf/icst/LoscherS18", "title": "Automating Targeted Property-Based Testing.", "authors": ["Andreas L\u00f6scher", "Konstantinos Sagonas"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00017", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00017"], "tag": ["Research 3:\nTesting & Debugging 2"], "abstract": "Targeted property-based testing is an enhanced form of property-based testing (PBT) where the input generation is guided by a search strategy instead of being random, thereby combining the strengths of QuickCheck-like and search-based testing techniques. To use it, however, the user currently needs to specify a search strategy and also supply all ingredients that the search strategy requires. This is often a laborious process and makes targeted PBT less attractive than its random counterpart. In this paper, we focus on simulated annealing, the default search strategy of our tool, and present a technique that automatically creates all the ingredients that targeted PBT requires starting from only a random generator. Our experiments, comparing the automatically generated ingredients to fine-tuned manually written ones, show that the performance that one obtains is sufficient and quite competitive in practice."}, {"id": "conf/icst/WangTHHSSP18", "title": "Testing Cloud Applications under Cloud-Uncertainty Performance Effects.", "authors": ["Wei Wang", "Ningjing Tian", "Sunzhou Huang", "Sen He", "Abhijeet Srivastava", "Mary Lou Soffa", "Lori L. Pollock"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00018", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00018"], "tag": ["Research 3:\nTesting & Debugging 2"], "abstract": "The paradigm shift of deploying applications to the cloud has introduced both opportunities and challenges. Although clouds use elasticity to scale resource usage at runtime to help meet an application's performance requirements, developers are still challenged by unpredictable performance, little control of execution environment, and differences among cloud service providers, all while being charged for their cloud usages. Application performance stability is particularly affected by multi-tenancy in which the hardware is shared among varying applications and virtual machines. Developers porting their applications need to meet performance requirements, but testing on the cloud under the effects of performance uncertainty is difficult and expensive, due to high cloud usage costs. This paper presents a first approach to testing an application with typical inputs for how its performance will be affected by performance uncertainty, without incurring undue costs of brute force testing in the cloud. We specify cloud uncertainty testing criteria, design a test-based strategy to characterize the black box cloud's performance distributions using these testing criteria, and support execution of tests to characterize the resource usage and cloud baseline performance of the application to be deployed. Importantly, we developed a smart test oracle that estimates the application's performance with certain confidence levels using the above characterization test results and determines whether it will meet its performance requirements. We evaluated our testing approach on both the Chameleon cloud and Amazon web services; results indicate that this testing strategy shows promise as a cost-effective approach to test for performance effects of cloud uncertainty when porting an application to the cloud."}, {"id": "conf/icst/HendersonP18", "title": "Behavioral Fault Localization by Sampling Suspicious Dynamic Control Flow Subgraphs.", "authors": ["Tim A. D. Henderson", "Andy Podgurski"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00019"], "tag": ["Research 3:\nTesting & Debugging 2"], "abstract": "We present a new algorithm, Score Weighted Random Walks (SWRW), for behavioral fault localization. Behavioral fault localization localizes faults (bugs) in programs to a group of interacting program elements such as basic blocks or functions. SWRW samples suspicious (or discriminative) subgraphs from basic-block level dynamic control flow graphs collected during the execution of passing and failing tests. The suspiciousness of a subgraph may be measured by any one of a family of new metrics adapted from probabilistic formulations of existing coverage-based statistical fault localization metrics. We conducted an empirical evaluation of SWRW with nine subgraph-suspiciousness measures on five real-world subject programs. The results indicate that SWRW outperforms previous fault localization techniques based on discriminative subgraph mining."}, {"id": "conf/icst/KorogluSMMUTD18", "title": "QBE: QLearning-Based Exploration of Android Applications.", "authors": ["Yavuz K\u00f6roglu", "Alper Sen", "Ozlem Muslu", "Yunus Mete", "Ceyda Ulker", "Tolga Tanriverdi", "Yunus Donmez"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00020", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00020"], "tag": ["Research 4:\nMobile and Web Application Testing"], "abstract": "Android applications are used extensively around the world. Many of these applications contain potential crashes. Black-box testing of Android applications has been studied over the last decade to detect these crashes. In this paper, we propose QLearning-Based Exploration (QBE), a fully automated black-box testing methodology, which explores GUI actions using a well-known reinforcement learning technique called QLearning. QBE performs automata learning to obtain a model of the AUT, and generates replayable test suites. Specifically, QBE learns from a set of existing applications the kinds of actions that are most useful in order to reach a particular objective such as detecting crashes or increasing activity coverage. To the best of our knowledge, ours is the first machine learning based approach in Android GUI Testing. We conduct experiments on a test set of 100 AUTs obtained from the commonly used F-Droid benchmarks to show the effectiveness of QBE. We show that QBE performs better than all compared black-box tools in terms of activity coverage and number of distinct detected crashes. We make QBE and our experimental data available online."}, {"id": "conf/icst/ElerRGF18", "title": "Automated Accessibility Testing of Mobile Apps.", "authors": ["Marcelo Medeiros Eler", "Jos\u00e9 Miguel Rojas", "Yan Ge", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00021", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00021"], "tag": ["Research 4:\nMobile and Web Application Testing"], "abstract": "It is important to make mobile apps accessible, so as not to exclude users with common disabilities such as blindness, low vision, or color blindness. Even when developers are aware of these accessibility needs, the lack of tool support makes the development and assessment of accessible apps challenging. Some accessibility properties can be checked statically, but user interface widgets are often created dynamically and are not amenable to static checking. Some accessibility checking frameworks analyze accessibility properties at runtime, but have to rely on existing thorough test suites. In this paper, we introduce the idea of using automated test generation to explore the accessibility of mobile apps. We present the MATE tool (Mobile Accessibility Testing), which automatically explores apps while applying different checks for accessibility issues related to visual impairment. For each issue, MATE generates a detailed report that supports the developer in fixing the issue. Experiments on a sample of 73 apps demonstrate that MATE detects more basic accessibility problems than static analysis, and many additional types of accessibility problems that cannot be detected statically at all. Comparison with existing accessibility testing frameworks demonstrates that the independence of an existing test suite leads to the identification of many more accessibility problems. Even when enabling Android's assistive features like contrast enhancement, MATE can still find many accessibility issues."}, {"id": "conf/icst/WangDGG018", "title": "Context-Based Event Trace Reduction in Client-Side JavaScript Applications.", "authors": ["Jie Wang", "Wensheng Dou", "Chushu Gao", "Yu Gao", "Jun Wei"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00022"], "tag": ["Research 4:\nMobile and Web Application Testing"], "abstract": "Record-replay techniques are developed to facilitate debugging client-side JavaScript application failures. They faithfully record all events that reveal a failure, but record many events irrelevant to the failure. Delta debugging adopts the divide-and-conquer algorithm to generate a minimal event subtrace that still reveals the same failure. However, delta debugging is slow because it may generate lots of syntactically infeasible candidate event subtraces in which some events can trigger syntactical errors (e.g., ReferenceError and TypeError), and thus cannot be replayed as expected. Based on this observation, we propose EvMin, an effective and efficient approach to remove failure-irrelevant events from an event trace. We use the variable usage information (e.g., DOM variable usage) in an event to model the event's context. We require that, each event in an event subtrace has the compatible context with its corresponding one in the original event trace. In this way, we avoid generating syntactically infeasible event subtraces, and dramatically speed up delta debugging. We have implemented EvMin and evaluated it on 10 real-world JavaScript application failures. Our evaluation shows that EvMin generates 72% fewer event subtraces, and takes 84% less time than delta debugging."}, {"id": "conf/icst/DurieuxHYBM18", "title": "Exhaustive Exploration of the Failure-Oblivious Computing Search Space.", "authors": ["Thomas Durieux", "Youssef Hamadi", "Zhongxing Yu", "Benoit Baudry", "Martin Monperrus"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00023"], "tag": ["Research 5:\nProgram Repair and Synthesis"], "abstract": "High-availability of software systems requires automated handling of crashes in presence of errors. Failure-oblivious computing is one technique that aims to achieve high availability. We note that failure-obliviousness has not been studied in depth yet, and there is very few study that helps understand why failure-oblivious techniques work. In order to make failure-oblivious computing to have an impact in practice, we need to deeply understand failure-oblivious behaviors in software. In this paper, we study, design and perform an experiment that analyzes the size and the diversity of the failure-oblivious behaviors. Our experiment consists of exhaustively computing the search space of 16 field failures of large-scale open-source Java software. The outcome of this experiment is a much better understanding of what really happens when failure-oblivious computing is used, and this opens new promising research directions."}, {"id": "conf/icst/SharmaHM18", "title": "Finding Substitutable Binary Code for Reverse Engineering by Synthesizing Adapters.", "authors": ["Vaibhav Sharma", "Kesha Hietala", "Stephen McCamant"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00024"], "tag": ["Research 5:\nProgram Repair and Synthesis"], "abstract": "Independently developed codebases typically contain many segments of code that perform the same or closely related operations. Being able to detect these related segments is helpful to applications such as reverse engineering. In this paper, we tackle the problem of determining whether two segments of binary code perform the same operation by asking whether one segment can be substituted by the other. A key insight behind our approach is that because these segments often have different interfaces, some glue code (an adapter) will be needed to perform the substitution. Here we present an algorithm that searches for substitutable code segments by attempting to synthesize an adapter between them. We implement our technique using concrete adapter enumeration and binary symbolic execution to explore the relation between size of adapter search space and total search time. Then, using more than 61,000 fragments of binary code extracted from a ARM image built for the iPod Nano 2g device and functions from the VLC media player, we evaluate our adapter synthesis implementation on more than one million synthesis tasks. Our tool finds dozens of instances of VLC functions in the firmware image. These results confirm that instances of adaptably substitutable binary functions exist in real-world code, and suggest that adapter synthesis has promising reverse engineering applications."}, {"id": "conf/icst/YangHWK18", "title": "EdSynth: Synthesizing API Sequences with Conditionals and Loops.", "authors": ["Zijiang Yang", "Jinru Hua", "Kaiyuan Wang", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00025", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00025"], "tag": ["Research 5:\nProgram Repair and Synthesis"], "abstract": "Good API design enables many clients to effectively use the core functionality implemented by the APIs. For real-world applications however, correctly using the APIs and identifying what methods to use and how to invoke them appropriately can be challenging. Researchers have developed a number of API synthesis approaches that enable a semantically rich form of API completion where the client provides a description of desired functionality, e.g., in the form of test suites, and the automatic tools create method sequences using the desired APIs based on the given correctness criteria (e.g., all given tests pass). However, existing API synthesis approaches are largely limited to creating single basic blocks of code and do not readily handle multiple blocks in the presence of loops (or recursion) and complex test executions. A key issue with handling multiple blocks is the very large space of possible method sequences and their combinations. This paper introduces EdSynth, an API synthesis approach that explores the sequence spaces on-demand during the test execution; that is, the given tests not only provide a validation mechanism - as is common in test-driven API synthesis - but also play a vital role in guiding the space exploration by helping prune much of it. EdSynth follows the spirit of recent work on test-execution-driven synthesis and lazily initializes candidates during the execution of given tests where the part of the candidate completion that is actually executed directly determines the generation of future candidates. To further optimize the space exploration, EdSynth ranks API candidates based on a set of pre-defined heuristics. We evaluate EdSynth's ability to synthesize complex APIs in the presence of conditional statements, loops and multiple basic blocks. The experimental results show that EdSynth is effective at handling synthesis tasks with multiple API sequences in both the conditions and bodies of loops/branches; moreover, when applied to synthesis of straight-line code, EdSynth compares well with a state-of-the-art API synthesis tool that only handles straight-line code. The experiments show that EdSynth's ranking strategies help reduce synthesis time by 43%."}, {"id": "conf/icst/AlegrothKR18", "title": "Continuous Integration and Visual GUI Testing: Benefits and Drawbacks in Industrial Practice.", "authors": ["Emil Al\u00e9groth", "Arvid Karlsson", "Alexander Radway"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00026"], "tag": ["Research 6:\nVisual Testing"], "abstract": "Continuous integration (CI) is growing in industrial popularity, spurred on by market trends towards faster delivery and higher quality software. A key facilitator of CI is automated testing that should be executed, automatically, on several levels of system abstraction. However, many systems lack the interfaces required for automated testing. Others lack test automation coverage of the system under test's (SUT) graphical user interface (GUI) as it is shown to the user. One technique that shows promise to solve these challenges is Visual GUI Testing (VGT), which uses image recognition to stimulate and assert the SUT's behavior. Research has presented the technique's applicability and feasibility in industry but only limited support, from an academic setting, that the technique is applicable in a CI environment. In this paper we presents a study from an industrial design research study with the objective to help bridge the gap in knowledge regarding VGT's applicability in a CI environment in industry. Results, acquired from interviews, observations and quantitative analysis of 17.567 test executions, collected over 16 weeks, show that VGT provides similar benefits to other automated test techniques for CI. However, several significant drawbacks, such as high costs, are also identified. The study concludes that, although VGT is applicable in an industrial CI environment, its severe challenges require more research and development before the technique becomes efficient in practice."}, {"id": "conf/icst/RyouR18", "title": "Automatic Detection of Visibility Faults by Layout Changes in HTML5 Web Pages.", "authors": ["Yeonhee Ryou", "Sukyoung Ryu"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00027"], "tag": ["Research 6:\nVisual Testing"], "abstract": "Modern HTML5 web pages (pages) often change various elements of their documents dynamically to provide rich functionality to users interactively. As users interact with a document via events, the combination of HTML, CSS, and JavaScript dynamically changes the document layout, which is the arrangement of the document elements visualized to the users. Web pages change their layouts not only to support user interaction but also to react to different screen sizes being used to run the pages. To support diverse devices with different screen sizes using a single web page document, developers use Responsive Web Design, which enables web page layouts to change when the sizes of the underlying devices change. While such dynamic features of web pages provide powerful experiences to users, they also make development of web pages more difficult. Even expert developers find it difficult to write HTML5 web pages correctly. In this paper, we first define the problem that functionalities of HTML5 web pages may become unusable due to layout changes, and propose a technique to detect the problem automatically. We show that our implementation detects such problems in real-world HTML5 web pages."}, {"id": "conf/icst/BajammalM18", "title": "Web Canvas Testing Through Visual Inference.", "authors": ["Mohammad Bajammal", "Ali Mesbah"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00028"], "tag": ["Research 6:\nVisual Testing"], "abstract": "Canvas elements are one of the major web technologies for creating high-performance graphics and visualizations in the browser. The canvas provides APIs for directly painting on the screen, but does not have a DOM state. As such, common web testing techniques that rely on the DOM cannot be applied to canvas elements. Furthermore, there has been little to no research in the literature for testing canvas elements. We propose an automated approach for testing canvas elements and their properties. Our approach performs a visual analysis of the screenshots of canvas elements and infers visual objects, their attributes, and their hierarchical relationships present on the canvas. Each inferred object is then represented as an augmented element inside the canvas element on the DOM tree. Finally, tests are generated from the augmented canvas DOM with assertions that check the inferred objects. We implement this approach in a tool, CanvaSure, and evaluate its accuracy and effectiveness for testing canvas-based applications. Our evaluation results show that CanvaSure has an accuracy of 91% for visually inferring the contents of the canvas, and is capable of correctly detecting 93% of injected visual faults on canvas applications."}, {"id": "conf/icst/JiLCPZ0YL18", "title": "Uncovering Unknown System Behaviors in Uncertain Networks with Model and Search-Based Testing.", "authors": ["Ruihua Ji", "Zhong Li", "Shouyu Chen", "Minxue Pan", "Tian Zhang", "Shaukat Ali", "Tao Yue", "Xuandong Li"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00029", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00029"], "tag": ["Research 7:\nSearch Based Approaches"], "abstract": "Modern software systems rely on information networks for communication. Such information networks are inherently unpredictable and unreliable. Consequently, software systems behave in an unstipulated manner in uncertain network conditions. Discovering unknown behaviors of these software systems in uncertain network conditions is essential to ensure their correct behaviors. Such discovery requires the development of systematic and automated methods. We propose an online and iterative model-based testing approach to evolve test models with search algorithms. Our ultimate aim is to discover unknown expected behaviors that can only be observed in uncertain network conditions. Also, we have implemented an adaptive search-based test case generation strategy to generate test cases that are executed on the system under test. We evaluated our approach with an open source video conference application-Jitsi with three search algorithms in comparison with random search. Results show that our approach is efficient in discovering unknown system behaviors. In particular, (1+1) Evolutionary Algorithm outperformed the other algorithms."}, {"id": "conf/icst/MahajanAMH18", "title": "Automated Repair of Internationalization Presentation Failures in Web Pages Using Style Similarity Clustering and Search-Based Techniques.", "authors": ["Sonal Mahajan", "Abdulmajeed Alameer", "Phil McMinn", "William G. J. Halfond"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00030"], "tag": ["Research 7:\nSearch Based Approaches"], "abstract": "Internationalization enables companies to reach a global audience by adapting their websites to locale specific language and content. However, such translations can often introduce Internationalization Presentation Failures (IPFs) - distortions in the intended appearance of a website. It is challenging for developers to design websites that can inherently adapt to varying lengths of text from different languages. Debugging and repairing IPFs is complicated by the large number of HTML elements and CSS properties that define a web page's appearance. Tool support is also limited as existing techniques can only detect IPFs, with the repair remaining a labor intensive manual task. To address this problem, we propose a search-based technique for automatically repairing IPFs in web applications. Our empirical evaluation showed that our approach was able to successfully resolve 98% of the reported IPFs for 23 real-world web pages. In a user study, participants rated the visual quality of our fixes significantly higher than the unfixed versions."}, {"id": "conf/icst/MehneYPSGK18", "title": "Accelerating Search-Based Program Repair.", "authors": ["Ben Mehne", "Hiroaki Yoshida", "Mukul R. Prasad", "Koushik Sen", "Divya Gopinath", "Sarfraz Khurshid"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00031"], "tag": ["Research 7:\nSearch Based Approaches"], "abstract": "Automatic program repair techniques offer the possibility of reducing, or even eliminating, the substantial manual effort that currently goes into the patching of software defects. However, current repair techniques take minutes or hours, to generate rather simple repairs, severely limiting their practical applicability. Search-based program repair represents a popular class of automatic repair techniques. Patch compilation and test case execution are the dominant contributors to runtime in this class of repair techniques. In this work we propose two complementary techniques, namely Location Selection and Test-Case Pruning, to improve the efficiency of search-based repair techniques. Location Selection reduces the number of repair candidates examined in arriving at a repair, thereby reducing the number of patch compilations as well as the overall number of test case evaluations during the repair process. Test-Case Pruning, on the other hand, optimizes the number of test cases executed per examined candidate. We implement the proposed techniques in the context of SPR, a state-of-the-art search-based repair tool, evaluate them on the GenProg benchmarks and observe that the proposed techniques provide a 3.9X speed-up, on average, without any degradation in repair quality."}, {"id": "conf/icst/KimHKPK18", "title": "Invasive Software Testing: Mutating Target Programs to Diversify Test Exploration for High Test Coverage.", "authors": ["Yunho Kim", "Shin Hong", "Bongseok Ko", "Duy Loc Phan", "Moonzoo Kim"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00032"], "tag": ["Research 8:\nTesting & Debugging 3"], "abstract": "Software testing techniques have advanced significantly over several decades; however, most of current techniques still test a target program as it is, and fail to utilize valuable information of diverse test executions on many variants of the original program in test generation. This paper proposes a new direction for software testing - Invasive Software Testing (IST). IST first generates a set of target program variants m1, \u22ef, mn from an original target program p by applying mutation operations 1, \u22ef, \u03bcn. Second, given a test suite T, IST executes m1, \u22ef, mn with T and records the test runs which increase test coverage compared to p with T. Based on the recorded information, IST generates guideposts for automated test generation on p toward high test coverage. Finally, IST generates test inputs on p with the guideposts and achieves higher test coverage. We developed DEMINER which implements IST for C programs through software mutation and concolic testing. Further, we showed the effectiveness of DEMINER on three real-world target programs Busybox-ls, Busybox-printf, and GNU-find. The experiment results show that the amount of the improved branch coverage by DEMINER is 24.7% relatively larger than those of the conventional concolic testing techniques on average."}, {"id": "conf/icst/ShamshiriRGWF18", "title": "How Do Automatically Generated Unit Tests Influence Software Maintenance?", "authors": ["Sina Shamshiri", "Jos\u00e9 Miguel Rojas", "Juan Pablo Galeotti", "Neil Walkinshaw", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00033", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00033"], "tag": ["Research 8:\nTesting & Debugging 3"], "abstract": "Generating unit tests automatically saves time over writing tests manually and can lead to higher code coverage. However, automatically generated tests are usually not based on realistic scenarios, and are therefore generally considered to be less readable. This places a question mark over their practical value: Every time a test fails, a developer has to decide whether this failure has revealed a regression fault in the program under test, or whether the test itself needs to be updated. Does the fact that automatically generated tests are harder to read outweigh the time-savings gained by their automated generation, and render them more of a hindrance than a help for software maintenance? In order to answer this question, we performed an empirical study in which participants were presented with an automatically generated or manually written failing test, and were asked to identify and fix the cause of the failure. Our experiment and two replications resulted in a total of 150 data points based on 75 participants. Whilst maintenance activities take longer when working with automatically generated tests, we found developers to be equally effective with manually written and automatically generated tests. This has implications on how automated test generation is best used in practice, and it indicates a need for research into the generation of more realistic tests."}, {"id": "conf/icst/MarianiMPRX18", "title": "Localizing Faults in Cloud Systems.", "authors": ["Leonardo Mariani", "Cristina Monni", "Mauro Pezz\u00e8", "Oliviero Riganelli", "Rui Xin"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00034"], "tag": ["Research 8:\nTesting & Debugging 3"], "abstract": "By leveraging large clusters of commodity hardware, the Cloud offers great opportunities to optimize the operative costs of software systems, but impacts significantly on the reliability of software applications. The lack of control of applications over Cloud execution environments largely limits the applicability of state-of-the-art approaches that address reliability issues by relying on heavyweight training with injected faults. In this paper, we propose LOUD, a lightweight fault localization approach that relies on positive training only, and can thus operate within the constraints of Cloud systems. LOUD relies on machine learning and graph theory. It trains machine learning models with correct executions only, and compensates the inaccuracy that derives from training with positive samples, by elaborating the outcome of machine learning techniques with graph theory algorithms. The experimental results reported in this paper confirm that LOUD can localize faults with high precision, by relying only on a lightweight positive training."}, {"id": "conf/icst/ZhuPZ18", "title": "An Investigation of Compression Techniques to Speed up Mutation Testing.", "authors": ["Qianqian Zhu", "Annibale Panichella", "Andy Zaidman"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00035"], "tag": ["Research 9:\nMutation Analysis"], "abstract": "Mutation testing is widely considered as a high-end test coverage criterion due to the vast number of mutants it generates. Although many efforts have been made to reduce the computational cost of mutation testing, in practice, the scalability issue remains. In this paper, we explore whether we can use compression techniques to improve the efficiency of strong mutation based on weak mutation information. Our investigation is centred around six mutation compression strategies that we have devised. More specifically, we adopt overlapped grouping and Formal Concept Analysis (FCA) to cluster mutants and test cases based on the reachability (code coverage) and necessity (weak mutation) conditions. Moreover, we leverage mutation knowledge (mutation locations and mutation operator types) during compression. To evaluate our method, we conducted a study on 20 open source Java projects using manually written tests. We also compare our method with pure random sampling and weak mutation. The overall results show that mutant compression techniques are a better choice than random sampling and weak mutation in practice: they can effectively speed up strong mutation 6.3 to 94.3 times with an accuracy of >90%."}, {"id": "conf/icst/HaririSLGKM18", "title": "Approximate Transformations as Mutation Operators.", "authors": ["Farah Hariri", "August Shi", "Owolabi Legunsen", "Milos Gligoric", "Sarfraz Khurshid", "Sasa Misailovic"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00036"], "tag": ["Research 9:\nMutation Analysis"], "abstract": "Mutation testing is a well-established approach for evaluating test-suite quality by modifying code using syntax-changing (and potentially semantics-changing) transformations, called mutation operators. This paper proposes approximate transformations as new mutation operators that can give novel insights about the code and tests. Approximate transformations are semantics-changing transformations used in the emerging area of approximate computing, but so far they were not evaluated for mutation testing. We found that approximate transformations can be effective mutation operators. We compared three approximate transformations with a set of conventional mutation operators from the literature, on nine open-source Java subjects. The results showed that approximate transformations change program behavior differently from conventional mutation operators. Our analysis uncovered code patterns in which approximate mutants survivedand showed the practical value of approximate transformations for both understanding code amenable to approximations and discovering bad tests. We submitted 11 pull requests to fix bad tests. Seven have already been integrated by the developers."}, {"id": "conf/icst/KhosrowjerdiMR18", "title": "Virtualized-Fault Injection Testing: A Machine Learning Approach.", "authors": ["Hojat Khosrowjerdi", "Karl Meinke", "Andreas Rasmusson"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00037"], "tag": ["Research 9:\nMutation Analysis"], "abstract": "We introduce a new methodology for virtualized fault injection testing of safety critical embedded systems. This approach fully automates the key steps of test case generation, fault injection and verdict construction. We use machine learning to reverse engineer models of the system under test. We use model checking to generate test verdicts with respect to safety requirements formalised in temporal logic. We exemplify our approach by implementing a tool chain based on integrating the QEMU hardware emulator, the GNU debugger GDB and the LBTest requirements testing tool. This tool chain is then evaluated on two industrial safety critical applications from the automotive sector."}, {"id": "conf/icst/HemmatiS18", "title": "Investigating NLP-Based Approaches for Predicting Manual Test Case Failure.", "authors": ["Hadi Hemmati", "Fatemeh Sharifi"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00038"], "tag": ["Research 10:\nDefect Analysis"], "abstract": "System-level manual acceptance testing is one of the most expensive testing activities. In manual testing, typically, a human tester is given an instruction to follow on the software. The results as \"passed\" or \"failed\" will be recorded by the tester, according to the instructions. Since this is a labourintensive task, any attempt in reducing the amount of this type of expensive testing is essential, in practice. Unfortunately, most of the existing heuristics for reducing test executions (e.g., test selection, prioritization, and reduction) are either based on source code or specification of the software under test, which are typically not being accessed during manual acceptance testing. In this paper, we propose a test case failure prediction approach for manual testing that can be used as a noncode/ specifcation-based heuristic for test selection, prioritization, and reduction. The approach uses basic Information Retrieval (IR) methods on the test case descriptions, written in natural language. The IR-based measure is based on the frequency of terms in the manual test scripts. We show that a simple linear regression model using the extracted natural language/IR-based feature together with a typical history-based feature (previous test execution results) can accurately predict the test cases' failure in new releases. We have conducted an extensive empirical study on manual test suites of 41 releases of Mozilla Firefox over three projects (Mobile, Tablet, Desktop). Our comparison of several proposed approaches for predicting failure shows that a) we can accurately predict the test case failure and b) the NLP-based feature can improve the prediction models."}, {"id": "conf/icst/FengJ0F18", "title": "An Empirical Study on Software Failure Classification with Multi-label and Problem-Transformation Techniques.", "authors": ["Yang Feng", "James A. Jones", "Zhenyu Chen", "Chunrong Fang"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00039", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00039"], "tag": ["Research 10:\nDefect Analysis"], "abstract": "Classification techniques have been used in software-engineering research to perform tasks such as categorizing software executions. Traditionally, existing work has proposed single-label failure classification techniques, in which the training and subsequent executions are labeled with a singular fault attribution. Although such approaches have received substantial attention in research on automated software engineering, in reality, recent work shows that the assumption of such a single attribution is often unrealistic: in practice, the inherent characteristics of software behavior, such as multiple faults that contribute to failures and fault interactions, may negatively influence the effectiveness of these techniques. To relax this unrealistic assumption, in the machine learning field, researchers have proposed new approaches for multi-label classification. However, the effectiveness and efficiency of such approaches varies widely based upon application domains. In this paper, we empirically investigate the performance of these new approaches on the failure classification task under different application settings. We conducted experiments using eight classification techniques on five subject programs with more than 8,000 faulty versions to investigate how each such technique accounts for the intricacies of software behavior. Our experimental results show that multi-label techniques provide improved accuracy over single-label. We also evaluated the efficiency of the training and prediction phases of each technique, and offer guidance as to the applicability for each technique for different usage contexts."}, {"id": "conf/icst/TimperleyAKHG18", "title": "Crashing Simulated Planes is Cheap: Can Simulation Detect Robotics Bugs Early?", "authors": ["Christopher Steven Timperley", "Afsoon Afzal", "Deborah S. Katz", "Jam Marcos Hernandez", "Claire Le Goues"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00040"], "tag": ["Research 10:\nDefect Analysis"], "abstract": "Robotics and autonomy systems are becoming increasingly important, moving from specialised factory domains to increasingly general and consumer-focused applications. As such systems grow ubiquitous, there is a commensurate need to protect against potentially catastrophic harm. System-level testing in simulation is a particularly promising approach for assuring robotics systems, allowing for more extensive testing in realistic scenarios and seeking bugs that may not manifest at the unit-level. Ideally, such testing could find critical bugs well before expensive field-testing is required. However, simulations can only model coarse environmental abstractions, contributing to a common perception that robotics bugs can only be found in live deployment. To address this gap, we conduct an empirical study on bugs that have been fixed in the widely used, open-source ArduPilot system. We identify bug-fixing commits by exploiting commenting conventions in the version-control history. We provide a quantitative and qualitative evaluation of the bugs, focusing on characterising how the bugs are triggered and how they can be detected, with a goal of identifying how they can be best identified in simulation, well before field testing. To our surprise, we find that the majority of bugs manifest under simple conditions that can be easily reproduced in software-based simulation. Conversely, we find that system configurations and forms of input play an important role in triggering bugs. We use these results to inform a novel framework for testing for these and other bugs in simulation, consistently and reproducibly. These contributions can inform the construction of techniques for automated testing of robotics systems, with the goal of finding bugs early and cheaply, without incurring the costs of physically testing for bugs in live systems."}, {"id": "conf/icst/PrauseGG18", "title": "Evaluating Automated Software Verification Tools.", "authors": ["Christian Prause", "Rainer Gerlich", "Ralf Gerlich"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00041"], "tag": ["Industry 1"], "abstract": "Automated software verification tools support developers in detecting faults that may lead to runtime errors. A fault in critical software that slips into the field, e.g., into a spacecraft, may have fatal consequences. However, there is an enormous variety of free and commercial tools available. Suppliers and customers of software need to have a clear understanding what tools suit the needs and expectations in their domain. We selected six tools (Polyspace, QA C, Klocwork, and others) and applied them to real-world spacecraft software. We collected reports from all the tools and manually verified whether they were justified. In particular, we clocked the time needed to confirm or disprove each report. The result is a profile of true and false positive and negative reports for each tool. We investigate questions regarding effectiveness and efficiency of different tools and their combinations, what the best tool is, if it makes sense at all to apply automated software verification to well-tested software, and whether tools with many or few reports are preferable."}, {"id": "conf/icst/DebroyBYE18", "title": "Automating Web Application Testing from the Ground Up: Experiences and Lessons Learned in an Industrial Setting.", "authors": ["Vidroha Debroy", "Lance Brimble", "Matthew Yost", "Archana Erry"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00042", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00042"], "tag": ["Industry 1"], "abstract": "Automating web application testing is a very tricky process due to inherent complexity, dynamic behavior(s) in web pages, differences in the way browsers render the same content (especially on different form factors), and so on. Yet manual testing is not a practical option, and such automation is a must in the interests of effectiveness and efficiency, because of the large number of browsers/devices that users can choose from, and also given the rapid software development cycles of today. This paper discusses our efforts at Varidesk to automate web tests against our main website - which offers many features and content, but is also a true eCommerce site where users from around the globe can purchase a very broad variety of active workspace solutions that we offer. Our solution was developed in-house, from the ground up, and leveraged and extended freely available automation and test libraries such as Selenium WebDriver and NUnit respectively. We talk about the challenges we faced and how we overcame them, as well as provide technical insights on real-world concerns such as managing test brittleness, and integrating the web tests into an existing Continuous Integration and Continuous Deployment (CI/CD) pipeline. Part of the novelty of this paper is that we are also transparent on the rationale behind our decision to build versus buy, and how we managed resources, especially in terms of cost. We also present lessons learned, and encouraged by the success that we have observed, hope that the results will be beneficial to academia and practitioners alike."}, {"id": "conf/icst/WalterSPR18", "title": "Improving Test Execution Efficiency Through Clustering and Reordering of Independent Test Steps.", "authors": ["Benedikt Walter", "Maximilian Schilling", "Marco Piechotta", "Stephan Rudolph"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00043", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00043"], "tag": ["Industry 1"], "abstract": "We observe an inefficient execution order of tests in the way that for a given set of test cases, a significant number of test steps occur in more than one test case. During test executions these duplicates increase the test load while providing none or limited additional test results. Removing such test steps interrupts the initial execution chain of steps inside a test case. To solve this issue, we propose a test case synthesis. After removing redundant steps, all (non-redundant) test steps are rearranged into a new set of test cases. This is achieved by using a clustering technique to group similar test steps into new test cases. A Path finding algorithm is used to find an optimized test step execution order for each test case. By applying this method in a case study at Mercedes-Benz Passenger Car Development, we observe a test load reduction of 15% due to removing redundant test steps and an additional reduction of at least 3% for rearranging test steps. This totals up to at least 18% overall test load reduction for the proposed method including removing redundant elements. We see this as strong indication for the usefulness of our approach."}, {"id": "conf/icst/SchwarzlH18", "title": "Systematic Test Platform Selection: Reducing Costs for Testing Software-Based Automotive E/E Systems.", "authors": ["Christian Schwarzl", "Jens Herrmann"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00044", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00044"], "tag": ["Industry 2"], "abstract": "In systematic testing software-based automotive electric/electronic (E/E) systems and functions the test case specification is one of the most important bases to steer and decide about the quality and amount of the whole testing process. After the test cases have been specified, it has to be decided on which test platforms the test cases should be executed. The quality of this test case distribution can vary from the points of view of test effectiveness and efficiency. The distribution can be done manually, based on experience. Or, alternatively, it should be possible to calculate a distribution on the basis of data available for the planned tests and thereby to replace manual work by calculation. In this paper we describe the results of a development work accomplished at Daimler on the possibilities to calculate a test case distribution to test platforms in the context of testing software-based automotive E/E systems. It shows which data are necessary to determine this distribution and how this kind of calculation can be performed. The automotive function Exterior Light Controller has been used to demonstrate and evaluate this distribution approach. First results are promising as they show that test efficiency can be improved applying the distribution approach maintaining predefined test effectivity goals."}, {"id": "conf/icst/RayROMO18", "title": "Bluetooth Low Energy Devices Security Testing Framework.", "authors": ["Apala Ray", "Vipin Raj", "Manuel Oriol", "Aurelien Monot", "Sebastian Obermeier"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00045"], "tag": ["Industry 2"], "abstract": "Bluetooth Low Energy (BLE), a low-power wireless protocol, is widely used in industrial automation for monitoring field devices. Although the BLE standard defines advanced security mechanisms, there are known security attacks for BLE and BLE-enabled field devices must be tested thoroughly against these attacks. This article identifies the possible attacks for BLE-enabled field devices relevant for industrial automation. It also presents a framework for defining and executing BLE security attacks and evaluates it on three BLE devices. All tested devices are vulnerable and this confirms that there is a need for better security testing tools as well as for additional defense mechanisms for BLE devices."}, {"id": "conf/icst/Arcuri18", "title": "EvoMaster: Evolutionary Multi-context Automated System Test Generation.", "authors": ["Andrea Arcuri"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00046", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00046"], "tag": ["Tool 1"], "abstract": "This paper presents EVOMASTER, an open-source tool that is able to automatically generate system level test cases using evolutionary algorithms. Currently, EVOMASTER targets RESTful web services running on JVM technology, and has been used to find several faults in existing open-source projects. We discuss some of the architectural decisions made for its implementation, and future work."}, {"id": "conf/icst/SullivanWK18", "title": "AUnit: A Test Automation Tool for Alloy.", "authors": ["Allison Sullivan", "Kaiyuan Wang", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ICST.2018.00047", "http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00047"], "tag": ["Tool 1"], "abstract": "Software models help improve the reliability of software systems: models can convey requirements, and can analyze design and implementation properties. A key strength of Alloy, a commonly used first-order modeling language, is the Alloy Analyzer tool-set. The Analyzer allows users to execute commands over models by leveraging a fully automatic SAT-based analysis engine. However, prior to the introduction of AUnit - a testing framework for Alloy - users had to rely on ad-hoc practices to validate their models. In this paper, we present our efforts to establish a formal testing environment in the Alloy Analyzer by creating an AUnit extension. We present additional grammar to support test case creation, as well as the details for executing test suites, calculating test suite coverage, and automatically generating test suites. The tool is available as a stand-alone executable at the following URL (https://sites.google.com/view/aunitanalyzer)."}, {"id": "conf/icst/RibeiroSACK18", "title": "Jaguar: A Spectrum-Based Fault Localization Tool for Real-World Software.", "authors": ["Henrique Lemos Ribeiro", "Higor Amario de Souza", "Roberto Paulo Andrioli de Araujo", "Marcos Lordello Chaim", "Fabio Kon"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00048"], "tag": ["Tool 1"], "abstract": "We present the Jaguar fault localization tool, which uses Spectrum-based Fault Localization techniques (SFL) to indicate faulty code excerpts. Jaguar supports both control-flow and data-flow spectra. It also provides visualization of lists containing suspicious program elements that are more likely to be faulty. These lists can be used to inspect suspicious methods, lines, and variables to locate bugs. Although data-flow spectrum provides more comprehensive information than control-flow, it is rarely used in SFL due to its high execution costs. Jaguar uses the ba-dua coverage tool, which gathers data-flow spectrum using a lightweight approach. Thus, Jaguar can be used in large programs at affordable execution costs. Jaguar is an open source tool for Java programs, which is available as an Eclipse plug-in and as a command line tool."}, {"id": "conf/icst/RaposC18", "title": "SimEvo: A Toolset for Simulink Test Evolution & Maintenance.", "authors": ["Eric J. Rapos", "James R. Cordy"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00049"], "tag": ["Tool 2"], "abstract": "As Simulink models evolve and change during development, test evolution and maintenance can often be overlooked. SimEvo provides a toolset to assist Simulink developers in coevolving test harnesses and test cases alongside their source models. Primarily a collection of testing tools, SimEvo combines the impact analysis features of the SimPact impact analysis tool to identify instances of necessary test case changes and potentially affected blocks, with the SimTH test harness generator to automatically determine if changes need to be made to the test harness model, and automatically generate a new one if necessary. This paper examines the implementation of SimTH, its integration with SimPact into the workbench SimEvo, and an overall analysis of the contributions of the toolset."}, {"id": "conf/icst/HodovanK18", "title": "Fuzzinator: An Open-Source Modular Random Testing Framework.", "authors": ["Ren\u00e1ta Hodov\u00e1n", "\u00c1kos Kiss"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00050"], "tag": ["Tool 2"], "abstract": "There is no fuzzing without a random test generator, but to be useful in practice, it needs to be accompanied by some similarly important components: solutions to feed test cases to the target system, unique error detectors, automatic test case reducers, or issue reporters. Unfortunately, many projects either still focus only on test case generation and give no support for the rest of the tasks, or they tightly couple several components into a monolithic artifact. In this paper, we introduce the Fuzzinator open-source random testing framework that supports connecting all, potentially independently developed components of a fuzzing setup in a modular way."}, {"id": "conf/icst/Rajaram18", "title": "Taxonomy Based Testing Using SW91, a Medical Device Software Defect Taxonomy.", "authors": ["Hamsini Ketheswarasarma Rajaram"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00051"], "tag": ["Doctoral Symposium"], "abstract": "This paper presents a summary of research undertaken to investigate and assess the use of a taxonomy based testing approach to improve medical device software (MDS) quality."}, {"id": "conf/icst/Wang18", "title": "Test Automation Maturity Assessment.", "authors": ["Yuqing Wang"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00052"], "tag": ["Doctoral Symposium"], "abstract": "Test automation is becoming critical in software development process. Though it has been widely applied, many are not surprised to find there is the long journey to a mature test automation process. To get continues improvement and achieve or sustain test automation benefits, organizations need to know what factors can lead to mature test automation and how to assess the current maturity level of test automation in order to identify improvement steps. However, the contemporary test maturity models are likely to emphasize more on general testing but fewer details for test automation, and also lack empirical evidence from the industry to validate the statements that indicate maturity levels. To address the above issues, this study aims to examine what factors lead to a mature test automation process and how to assess the maturity level against them."}, {"id": "conf/icst/Balogh18", "title": "Software Systems, Their Engineers and Their Testers.", "authors": ["Gerg\u00f5 Balogh"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00053"], "tag": ["Doctoral Symposium"], "abstract": "My overall research goal is to provide meaningful insights, methods and practical tools to help the work of stakeholders during various phases of software development to explore the human factor of IT. My current work mainly considers the analyses of package structure and its relation to the grouping of test cases. With this I aim to help senior developers to find problematic or hard to understand parts of software. During my research I introduced an elaborated test coverage based method to find and classify discrepancies between dynamic and static production and test code grouping. This technique helps both testers and developers to find stray test cases or code elements in Java package structure. In the second to last section I briefly introduce my other two research topics and their possible applications in the field of software testing."}, {"id": "conf/icst/Listenmaa18", "title": "Testing Natural Language Grammars.", "authors": ["Inari Listenmaa"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00054"], "tag": ["Doctoral Symposium"], "abstract": "Testing grammars has one big difference from testing software: natural language has no formal specification, so ultimately we must involve a human oracle. However, we can automate many useful subtasks: detect ambiguous constructions and contradictory grammar rules, as well as generate minimal and representative set of examples that cover all the constructions. Think of the whole grammar as a haystack, and we suspect there are a few needles-we cannot promise automatic needle-removal, but instead we help the human oracle to narrow down the search."}, {"id": "conf/icst/Suarez-Otero18", "title": "Analysis of the Logical Consistency in Cassandra.", "authors": ["Pablo Su\u00e1rez-Otero"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00055"], "tag": ["Doctoral Symposium"], "abstract": "Most methods to test the consistency of data in relational databases are based on checking the integrity of the data through testing of the schema. However, in NoSQL databases, there is no schema and the data is denormalized, so we need new methods to test the consistency of the database. One of these NoSQL databases is Cassandra, where each table is created to satisfy one query. As the same information could be retrieved by several queries, this information may be found in several tables. In these cases, whenever there is a change of the values of a row in a table, the consistency could be affected. In this thesis, we propose a preventive approach to this problem, where we test the consistency of the data by doing a static analysis of the Cassandra tables when there is a change of the data that could affect the consistency. To achieve this, we will use a conceptual model to help us in the static analysis."}, {"id": "conf/icst/PiresA18", "title": "Knowledge Discovery Metamodel-Based Unit Test Cases Generation.", "authors": ["Joao Paulo Pires", "Fernando Brito e Abreu"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00056"], "tag": ["Doctoral Symposium"], "abstract": "Existing unit test cases generative approaches are language-dependent. In this document we propose a novel approach, dubbed KDM2xUnit that will allow the generation of test suites matching the xUnit framework, using several transformations and Knowledge Discovery Metamodel (KDM) compliant models as a common intermediate representation for existing software systems and their operating environments."}, {"id": "conf/icst/Rahman18", "title": "Anti-Patterns in Infrastructure as Code.", "authors": ["Akond Rahman"], "DOIs": ["http://doi.ieeecomputersociety.org/10.1109/ICST.2018.00057"], "tag": ["Doctoral Symposium"], "abstract": "In DevOps, infrastructure as code (IaC) scripts are used by practitioners to create and manage an automated deployment pipeline that enables IT organizations to release their software changes rapidly at scale. Low quality IaC scripts can have serious consequences, potentially leading to wide-spread system outages and service discrepancies. The goal of this research is to help practitioners increase the quality of infrastructure as code (IaC) scripts by identifying anti-patterns in IaC scripts and development of IaC scripts. Using open source repositories, we conduct three initial studies to (i) quantify the frequency and categorize the defects in IaC scripts; and (ii) identify operations that characterize defective IaC scripts. Based on our empirical analysis we observe (i) the dominant defect defect categories to be related to syntax and configuration assignments, and (ii) three operations that characterize defective IaC scripts. The above-mentioned findings motivate us to identify anti-patterns in IaC scripts and IaC development. To this end, we propose three studies that identify (i) process anti-patterns; and (ii) security-related anti-patterns in IaC."}], "2019": [{"id": "conf/icst/SaumyaK0B19", "title": "XSTRESSOR : Automatic Generation of Large-Scale Worst-Case Test Inputs by Inferring Path Conditions.", "authors": ["Charitha Saumya", "Jinkyu Koo", "Milind Kulkarni", "Saurabh Bagchi"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00011"], "tag": ["Test Generation"], "abstract": "An important part of software testing is generation of worst-case test inputs, which exercise a program under extreme loads. For such a task, symbolic execution is a useful tool with its capability to reason about all possible execution paths of a program, including the one with the worst case behavior. However, symbolic execution suffers from the path explosion problem and frequent calls to a constraint solver, which make it impractical to be used at a large scale. To address the issue, this paper presents XSTRESSOR that is able to generate test inputs that can run specific loops in a program with the worst-case complexity in a large scale. XSTRESSOR synthetically generates the path condition for the large-scale, worst-case execution from a predictive model that is built from a set of small scale tests. XSTRESSOR avoids the scaling problem of prior techniques by limiting full-blown symbolic execution and run-time calls to constraint solver to small scale tests only. We evaluate XSTRESSOR against WISE and SPF-WCA, the most closely related tools to generate worst-case test inputs. Results show that XSTRESSOR can generate the test inputs faster than WISE and SPF-WCA, and also scale to much larger input sizes."}, {"id": "conf/icst/IwamaF19", "title": "Automated Testing of Basic Recognition Capability for Speech Recognition Systems.", "authors": ["Futoshi Iwama", "Takashi Fukuda"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00012"], "tag": ["Test Generation"], "abstract": "Automatic speech recognition systems transform speech audio data into text data, i.e., word sequences, as the recognition results. These word sequences are generally defined by the language model of the speech recognition system. Therefore, the capability of the speech recognition system to translate audio data obtained by typically pronouncing word sequences that are accepted by the language model into word sequences that are equivalent to the original ones can be regarded as a basic capability of the speech recognition systems. This work describes a testing method that checks whether speech recognition systems have this basic recognition capability. The method can verify the basic capability by performing the testing separately from recognition robustness testing. It can also be fully automated. We constructed a test automation system and evaluated though several experiments whether it could detect defects in speech recognition systems. The results demonstrate that the test automation system can effectively detect basic defects at an early phase of speech recognition development or refinement."}, {"id": "conf/icst/DiniYGK19", "title": "Extension-Aware Automated Testing Based on Imperative Predicates.", "authors": ["Nima Dini", "Cagdas Yelen", "Milos Gligoric", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00013"], "tag": ["Test Generation"], "abstract": "Bounded exhaustive testing (BET) techniques have been shown to be effective for detecting faults in software. BET techniques based on imperative predicates, enumerate all test inputs up to the given bounds such that each test input satisfies the properties encoded by the predicate. The search space is bounded by the user, who specifies the number of objects of each type and the list of values for each field of each type. To optimize the search, existing techniques detect isomorphic instances and record accessed fields during the execution of a predicate. However, these optimizations are extension-unaware, i.e., they do not speed up the search when the predicate is modified, say due to a fix or additional properties. We present a technique, named iGen, that speeds up test generation when imperative predicates are extended. iGen memoizes intermediate results of a test generation and reuses the results in a future search - even when the new search space differs from the old space. We integrated our technique in two BET tools (one for Java and one for Python) and evaluated these implementations with several data structure pairs, including two pairs from the Standard Java Library. Our results show that iGen speeds up test generation by up to 46.59x for the Java tool and up to 49.47x for the Python tool. Additionally, we show that the speedup obtained by iGen increases for larger test instances."}, {"id": "conf/icst/BaderCF19", "title": "Parallel Many-Objective Search for Unit Tests.", "authors": ["Verena Bader", "Jos\u00e9 Campos", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00014"], "tag": ["Test Generation"], "abstract": "Meta-heuristic search algorithms such as genetic algorithms have been applied successfully to generate unit tests, but typically take long to produce reasonable results, achieve sub-optimal code coverage, and have large variance due to their stochastic nature. Parallel genetic algorithms have been shown to be an effective improvement over sequential algorithms in many domains, but have seen little exploration in the context of unit test generation to date. In this paper, we describe a parallelised version of the many-objective sorting algorithm (MOSA) for test generation. Through the use of island models, where individuals can migrate between independently evolving populations, this algorithm not only reduces the necessary search time, but produces overall better results. Experiments with an implementation of parallel MOSA on the EvoSuite test generation tool using a large corpus of complex open source Java classes confirm that the parallelised MOSA algorithm achieves on average 84% code coverage, compared to 79% achieved by a standard sequential version."}, {"id": "conf/icst/CoppikSS19", "title": "MemFuzz: Using Memory Accesses to Guide Fuzzing.", "authors": ["Nicolas Coppik", "Oliver Schwahn", "Neeraj Suri"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00015"], "tag": ["Fuzzing and Security"], "abstract": "Fuzzing is a form of random testing that is widely used for finding bugs and vulnerabilities. State of the art approaches commonly leverage information about the control flow of prior executions of the program under test to decide which inputs to mutate further. By relying solely on control flow information to characterize executions, such approaches may miss relevant differences. We propose augmenting evolutionary fuzzing by additionally leveraging information about memory accesses performed by the target program. The resulting approach can leverage more sophisticated information about the execution of the target program, enhancing the effectiveness of the evolutionary fuzzing. We implement our approach as a modification of the widely used AFL fuzzer and evaluate our implementation on three widely used target applications. We find distinct crashes from those detected by AFL for all three targets in our evaluation."}, {"id": "conf/icst/ZhaoLWSH19", "title": "SeqFuzzer: An Industrial Protocol Fuzzing Framework from a Deep Learning Perspective.", "authors": ["Hui Zhao", "Zhihui Li", "Hansheng Wei", "Jianqi Shi", "Yanhong Huang"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00016"], "tag": ["Fuzzing and Security"], "abstract": "Industrial networks are the cornerstone of modern industrial control systems. Performing security checks of industrial communication processes helps detect unknown risks and vulnerabilities. Fuzz testing is a widely used method for performing security checks that takes advantage of automation. However, there is a big challenge to carry out security checks on industrial network due to the increasing variety and complexity of industrial communication protocols. In this case, existing approaches usually take a long time to model the protocol for generating test cases, which is labor-intensive and time-consuming. This becomes even worse when the target protocol is stateful. To help in addressing this problem, we employed a deep learning model to learn the structures of protocol frames and deal with the temporal features of stateful protocols. We propose a fuzzing framework named SeqFuzzer which automatically learns the protocol frame structures from communication traffic and generates fake but plausible messages as test cases. For proving the usability of our approach, we applied SeqFuzzer to widely-used Ethernet for Control Automation Technology (EtherCAT) devices and successfully detected several security vulnerabilities."}, {"id": "conf/icst/PiantadosiSO19", "title": "Fixing of Security Vulnerabilities in Open Source Projects: A Case Study of Apache HTTP Server and Apache Tomcat.", "authors": ["Valentina Piantadosi", "Simone Scalabrino", "Rocco Oliveto"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00017"], "tag": ["Fuzzing and Security"], "abstract": "Software vulnerabilities are particularly dangerous bugs that may allow an attacker to violate the confidentiality, integrity or availability constraints of a software system. Fixing vulnerabilities soon is of primary importance; besides, it is crucial to release complete patches that do not leave any corner case not covered. In this paper we study the process of vulnerability fixing in Open Source Software. We focus on three dimensions: personal, i.e., who fixes software vulnerabilities; temporal, i.e., how long does it take to release a patch; procedural, i.e., what is the process followed to fix the vulnerability. In the context of our study we analyzed 337 CVE Entries regarding Apache HTTP Server and Apache Tomcat and we manually linked them to the patches written to fix such vulnerabilities and their related commits. The results show that developers who fix software vulnerabilities are much more experienced than the average. Furthermore, we observed that the vulnerabilities are fixed through more than a commit and, surprisingly, that in about 3% of the cases such vulnerabilities show up again in future releases (i.e., they are not actually fixed). In the light of such results, we derived some lessons learned that represent a starting point for future research directions aiming at better supporting developers during the documentation and fixing of vulnerabilities."}, {"id": "conf/icst/SondhiRP19", "title": "Similarities Across Libraries: Making a Case for Leveraging Test Suites.", "authors": ["Devika Sondhi", "Divya Rani", "Rahul Purandare"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00018"], "tag": ["Empirical Studies and Benchmarking"], "abstract": "Developers may choose to implement a library, despite the existence of similar libraries, considering factors such as computational performance, language or platform dependency, and accuracy. As a result, GitHub is a host to several library projects that have overlaps in the functionalities. These overlaps have been of interest to developers from the perspective of code reuse or preferring one implementation over the other. We present an empirical study to explore the extent and nature of existence of these similarities in the library functions. We have further studied whether the similarity among functions across different libraries and their associated test suites can be leveraged to reveal defects in one another. Applying a natural language processing based approach on the documentations associated with functions, we have extracted matching functions across 12 libraries, available on GitHub, over 2 programming languages and 3 themes. Our empirical evaluation indicates existence of a significant number of similar functions across libraries in same as well as different programming languages where a language can influence the extent of existence of similarities. The test suites from another library can serve as an effective source of defect revealing tests. The study resulted in revealing 72 defects in 12 libraries. Further, we analyzed the source of origination of the defect revealing tests. We deduce that issue reports and pull requests can be beneficial in attaining quality test cases not only to test the libraries in which these issues are reported but also for other libraries that are similar in theme."}, {"id": "conf/icst/GyimesiVSMBF019", "title": "BugsJS: a Benchmark of JavaScript Bugs.", "authors": ["P\u00e9ter Gyimesi", "B\u00e9la Vancsics", "Andrea Stocco", "Davood Mazinanian", "\u00c1rp\u00e1d Besz\u00e9des", "Rudolf Ferenc", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00019"], "tag": ["Empirical Studies and Benchmarking"], "abstract": "JavaScript is a popular programming language that is also error-prone due to its asynchronous, dynamic, and loosely-typed nature. In recent years, numerous techniques have been proposed for analyzing and testing JavaScript applications. However, our survey of the literature in this area revealed that the proposed techniques are often evaluated on different datasets of programs and bugs. The lack of a commonly used benchmark limits the ability to perform fair and unbiased comparisons for assessing the efficacy of new techniques. To fill this gap, we propose BugsJS, a benchmark of 453 real, manually validated JavaScript bugs from 10 popular JavaScript server-side programs, comprising 444k LOC in total. Each bug is accompanied by its bug report, the test cases that detect it, as well as the patch that fixes it. BugsJS features a rich interface for accessing the faulty and fixed versions of the programs and executing the corresponding test cases, which facilitates conducting highly-reproducible empirical studies and comparisons of JavaScript analysis and testing tools."}, {"id": "conf/icst/LiuKB0KT19", "title": "You Cannot Fix What You Cannot Find! An Investigation of Fault Localization Bias in Benchmarking Automated Program Repair Systems.", "authors": ["Kui Liu", "Anil Koyuncu", "Tegawend\u00e9 F. Bissyand\u00e9", "Dongsun Kim", "Jacques Klein", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00020"], "tag": ["Empirical Studies and Benchmarking"], "abstract": "Properly benchmarking Automated Program Repair (APR) systems should contribute to the development and adoption of the research outputs by practitioners. To that end, the research community must ensure that it reaches significant milestones by reliably comparing state-of-the-art tools for a better understanding of their strengths and weaknesses. In this work, we identify and investigate a practical bias caused by the fault localization (FL) step in a repair pipeline. We propose to highlight the different fault localization configurations used in the literature, and their impact on APR systems when applied to the Defects4J benchmark. Then, we explore the performance variations that can be achieved by \"tweaking\" the FL step. Eventually, we expect to create a new momentum for (1) full disclosure of APR experimental procedures with respect to FL, (2) realistic expectations of repairing bugs in Defects4J, as well as (3) reliable performance comparison among the state-of-theart APR systems, and against the baseline performance results of our thoroughly assessed kPAR repair tool. Our main findings include: (a) only a subset of Defects4J bugs can be currently localized by commonly-used FL techniques; (b) current practice of comparing state-of-the-art APR systems (i.e., counting the number of fixed bugs) is potentially misleading due to the bias of FL configurations; and (c) APR authors do not properly qualify their performance achievement with respect to the different tuning parameters implemented in APR systems."}, {"id": "conf/icst/HaririSFMM19", "title": "Comparing Mutation Testing at the Levels of Source Code and Compiler Intermediate Representation.", "authors": ["Farah Hariri", "August Shi", "Vimuth Fernando", "Suleman Mahmood", "Darko Marinov"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00021"], "tag": ["Empirical Studies and Benchmarking"], "abstract": "Mutation testing is widely used in research for evaluating the effectiveness of test suites. There are multiple mutation tools that perform mutation at different levels, including traditional mutation testing at the level of source code (SRC) and more recent mutation testing at the level of compiler intermediate representation (IR). This paper presents an extensive comparison of mutation testing at the SRC and IR levels, specifically at the C programming language and the LLVM compiler IR levels. We use a mutation testing tool called SRCIROR that implements conceptually the same mutation operators at both levels. We also employ automated techniques to account for equivalent and duplicated mutants, and to determine minimal and surface mutants. We carry out our study on 15 programs from the Coreutils library. Overall, we find mutation testing to be better at the SRC level: the SRC level produces much fewer mutants and is thus less expensive, but the SRC level still generates a similar number of minimal and surface mutants, and the mutation scores at both levels are very closely correlated. We also perform a case study on the Space program to evaluate which level's mutation score correlates better with the actual fault-detection capability of test suites sampled from Space's test pool. We find the mutation score at both levels to not be very correlated with the actual fault-detection capability of test suites."}, {"id": "conf/icst/SharmaW19", "title": "Testing Machine Learning Algorithms for Balanced Data Usage.", "authors": ["Arnab Sharma", "Heike Wehrheim"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00022"], "tag": ["Machine Learning"], "abstract": "With the increased application of machine learning (ML) algorithms to decision-making processes, the question of fairness of such algorithms came into the focus. Fairness testing aims at checking whether a classifier as \"learned\" by an ML algorithm on some training data is biased in the sense of discriminating against some of the attributes (e.g. gender or age). Fairness testing thus targets the prediction phase in ML, not the learning phase. In this paper, we investigate fairness for the learning phase. Our definition of fairness is based on the idea that the learner should treat all data in the training set equally, disregarding issues like names or orderings of features or orderings of data instances. We term this property balanced data usage. We consequently develop a (metamorphic) testing approach called TiLe for checking balanced data usage. TiLe is applied on 14 ML classifiers taken from the scikit-learn library using 4 artificial and 9 real-world data sets for training, finding 12 of the classifiers to be unbalanced."}, {"id": "conf/icst/KooS0B19", "title": "PySE: Automatic Worst-Case Test Generation by Reinforcement Learning.", "authors": ["Jinkyu Koo", "Charitha Saumya", "Milind Kulkarni", "Saurabh Bagchi"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00023"], "tag": ["Machine Learning"], "abstract": "Stress testing is an important task in software testing, which examines the behavior of a program under a heavy load. Symbolic execution is a useful tool to find out the worst-case input values for the stress testing. However, symbolic execution does not scale to a large program, since the number of paths to search grows exponentially with an input size. So far, such a scalability issue has been mostly managed by pruning out unpromising paths in the middle of searching based on heuristics, but this kind of work easily eliminates the true worst case as well, providing sub-optimal one only. Another way to achieve scalability is to learn a branching policy of worst-case complexity from small scale tests and apply it to a large scale. However, use cases of such a method are restricted to programs whose worst-case branching policy has a simple pattern. To address such limitations, we propose PySE that uses symbolic execution to collect the behaviors of a given branching policy, and updates the policy using a reinforcement learning approach through multiple executions. PySE's branching policy keeps evolving in a way that the length of an execution path increases in the long term, and ultimately reaches the worst-case complexity. PySE can also learn the worst-case branching policy of a complex or irregular pattern, using an artificial neural network in a fully automatic way. Experiment results demonstrate that PySE can effectively find a path of worst-case complexity for various Python benchmark programs and scales."}, {"id": "conf/icst/MonniPP19", "title": "An RBM Anomaly Detector for the Cloud.", "authors": ["Cristina Monni", "Mauro Pezz\u00e8", "Gaetano Prisco"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00024"], "tag": ["Machine Learning"], "abstract": "Failures are unavoidable in complex software systems, and the intrinsic characteristics of cloud systems amplify the problem. Predicting failures before their occurrence by detecting anomalies in system metrics is a viable solution to enable failure preventing or mitigating actions. The most promising approaches for predicting failures exploit statistical analysis or machine learning to reveal anomalies and their correlation with possible failures. Statistical analysis approaches result in far too many false positives, which severely hinder their practical applicability, while accurate machine learning approaches need extensive training with seeded faults, which is often impossible in operative cloud systems. In this paper, we propose EmBeD, Energy-Based anomaly Detection in the cloud, an approach to detect anomalies at runtime based on the free energy of a Restricted Boltzmann Machine (RBM) model. The free energy is a stochastic function that can be used to efficiently score anomalies for detecting outliers. EmBeD analyzes the system behavior from raw metric data, does not require extensive training with seeded faults, and classifies the relation of anomalous behaviors with future failures with very few false positives. The experimental results presented in this paper confirm that EmBeD can precisely predict failure-prone behavior without training with seeded faults, thus overcoming the main limitations of current approaches."}, {"id": "conf/icst/MaoCZ19", "title": "An Extensive Study on Cross-Project Predictive Mutation Testing.", "authors": ["Dongyu Mao", "Lingchao Chen", "Lingming Zhang"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00025"], "tag": ["Machine Learning"], "abstract": "Mutation testing is a powerful technique for evaluating the quality of test suite which plays a key role in ensuring software quality. The concept of mutation testing has also been widely used in other software engineering studies, e.g., test generation, fault localization, and program repair. During the process of mutation testing, large number of mutants may be generated and then executed against the test suite to examine whether they can be killed, making the process extremely computational expensive. Several techniques have been proposed to speed up this process, including selective, weakened, and predictive mutation testing. Among those techniques, Predictive Mutation Testing (PMT) tries to build a classification model based on an amount of mutant execution records to predict whether coming new mutants would be killed or alive without mutant execution, and can achieve significant mutation cost reduction. In PMT, each mutant is represented as a list of features related to the mutant itself and the test suite, transforming the mutation testing problem to a binary classification problem. In this paper, we perform an extensive study on the effectiveness and efficiency of the promising PMT technique under the cross-project setting using a total 654 real world projects with more than 4 Million mutants. Our work also complements the original PMT work by considering more features and the powerful deep learning models. The experimental results show an average of over 0.85 prediction accuracy on 654 projects using cross validation, demonstrating the effectiveness of PMT. Meanwhile, a clear speed up is also observed with an average of 28.7\u00d7 compared to traditional mutation testing with 5 threads. In addition, we analyze the importance of different groups of features in classification model, which provides important implications for the future research."}, {"id": "conf/icst/AlameerCH19", "title": "Efficiently Repairing Internationalization Presentation Failures by Solving Layout Constraints.", "authors": ["Abdulmajeed Alameer", "Paul T. Chiou", "William G. J. Halfond"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00026"], "tag": ["Web and GUI Applications"], "abstract": "Web developers employ internationalization frameworks to automate web page translations and enable their web apps to more easily communicate with a global audience. However, the change of text size in different languages can lead to distortions in the translated web page's layout. These distortions are known as Internationalization Presentation Failures (IPFs). Debugging these IPFs can be a tedious and error-prone process. Previous research efforts to develop an automatic IPF repair technique could compromise the attractiveness and readability of the repaired web page. In this paper, we present a novel approach that can rapidly repair IPFs and maintain the readability and the attractiveness of the web page. Our approach models the correct layout of a web page as a system of constraints. The solution to the system represents the new and correct layout of the web page that resolves its IPFs. In the evaluation, we found that our approach could more quickly produce repairs that were rated as more attractive and more readable than those produced by a prior state-of-the-art technique."}, {"id": "conf/icst/AlthomaliKM19", "title": "Automatic Visual Verification of Layout Failures in Responsively Designed Web Pages.", "authors": ["Ibrahim Althomali", "Gregory M. Kapfhammer", "Phil McMinn"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00027"], "tag": ["Web and GUI Applications"], "abstract": "Responsively designed web pages adjust their layout according to the viewport width of the device in use. Although tools exist to help developers test the layout of a responsive web page, they often rely on humans to flag problems. Yet, the considerable number of web-enabled devices with unique viewport widths makes this manual process both time-consuming and error-prone. Capable of detecting some common responsive layout failures, the ReDeCheck tool partially automates this process. Since ReDeCheck focuses on a web page's document object model (DOM), some of the issues it finds are not observable by humans. This paper presents a tool, called Viser, that renders a ReDeCheck-reported layout issue in a browser, adjusting the opacity of certain elements and checking for a visible difference. Unless Viser classifies an issue as a human-observable layout failure, a web developer can ignore it. This paper's experiments reveal the benefit of using Viser to support automated visual verification of layout failures in responsively designed web pages. Viser automatically classified all of the 117 layout failures that ReDeCheck reported for 20 web pages, each of which had to be manually analyzed in a prior study. Viser's automated manipulation of element opacity also highlighted manual classification's subjectivity: it categorized 28 issues differently to manual analysis, including three correctly reclassified as false positives."}, {"id": "conf/icst/TannoI19", "title": "Suspend-Less Debugging for Interactive and/or Realtime Programs.", "authors": ["Haruto Tanno", "Hideya Iwasaki"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00028"], "tag": ["Web and GUI Applications"], "abstract": "Programs with interactive and/or realtime activities, such as GUI programs, action game programs, network-based programs, and sensor information processing programs, are not suitable for traditional breakpoint-based debugging, in which execution of the target program is suspended, for two reasons. First, since the timings and order of input event occurrences such as user operations are quite important, such programs do not behave as expected if execution is suspended at a breakpoint. Second, suspending a program to observe its internal states significantly degrades the efficiency of debugging. A debugging method is presented that resolves these problems. It keeps track of both the currently executing statement in a program and the changes in value of expressions of interest, and visualizes them in realtime. The proposed method was implemented as SLDSharp, a debugger for C# programs, by means of a program transformation technique. Through a case study of debugging a practical game program created by using the Unity game engine, it is shown in that SLDSharp makes it possible to efficiently debug."}, {"id": "conf/icst/JoffeC19", "title": "Directing a Search Towards Execution Properties with a Learned Fitness Function.", "authors": ["Leonid Joffe", "David Clark"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00029"], "tag": ["SAT and Search Based Testing"], "abstract": "Search based software testing is a popular and successful approach both in academia and industry. SBST methods typically aim to increase coverage whereas searching for executions with specific properties is largely unresearched. Fitness functions for execution properties often possess search landscapes that are difficult or intractable. We demonstrate how machine learning techniques can convert a property that is not searchable, in this case crashes, into one that is. Through experimentation on 6000 C programs drawn from the Codeflaws repository, we demonstrate a strong, program independent correlation between crashing executions and library function call patterns within those executions as discovered by a neural net. We then exploit the correlation to produce a searchable fitness landscape to modify American Fuzzy Lop, a widely used fuzz testing tool. On a test set of previously unseen programs drawn from Codeflaws, a search strategy based on a crash targeting fitness function outperformed a baseline in 80.1% of cases. The experiments were then repeated on three real world programs: the VLC media player, and the libjpeg and mpg321 libraries. The correlation between library call traces and crashes generalises as indicated by ROC AUC scores of 0.91, 0.88 and 0.61. The produced search landscape however is not convenient due to plateaus. This is likely because these programs do not use standard C libraries as often as do those in Codeflaws. This limitation can be overcome by considering a more powerful observation domain and a broader training corpus in future work. Despite limited generalisability of the experimental setup, this research opens new possibilities in the intersection of machine learning, fitness functions, and search based testing in general."}, {"id": "conf/icst/LidO19", "title": "Intent-Preserving Test Repair.", "authors": ["Xiangyu Li", "Marcelo d'Amorim", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00030"], "tag": ["SAT and Search Based Testing"], "abstract": "Repairing broken tests in evolving software systems is an expensive and challenging task. One of the main challenges for test repair, in particular, is preserving the intent of the original tests in the repaired ones. To address this challenge, we propose a technique for test repair that models and considers the intent of a test when repairing it. Our technique first uses a search-based approach to generate repair candidates for the broken test. It then computes, for each candidate, its likelihood of preserving the original test intent. To do so, the technique characterizes such intent using the path conditions generated during a dynamic symbolic execution of the tests. Finally, the technique reports the best candidates to the developer as repair recommendations. We implemented and evaluated our technique on a benchmark of 91 broken tests in 4 open-source programs. Our results are promising, in that the technique was able to generate intentpreserving repair candidates for over 79% of those broken tests and rank the intent-preserving candidates as the first choice of repair recommendations for almost 70% of the broken tests."}, {"id": "conf/icst/WangWZK19", "title": "Learning to Optimize the Alloy Analyzer.", "authors": ["Wenxi Wang", "Kaiyuan Wang", "Mengshi Zhang", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00031"], "tag": ["SAT and Search Based Testing"], "abstract": "Constraint-solving is an expensive phase for scenario finding tools. It has been widely observed that there is no single \"dominant\" SAT solver that always wins in every case; instead, the performance of different solvers varies by cases. Some SAT solvers perform particularly well for certain tasks while other solvers perform well for other tasks. In this paper, we propose an approach that uses machine learning techniques to automatically select a SAT solver for one of the widely used scenario finding tools, i.e. Alloy Analyzer, based on the features extracted from a given model. The goal is to choose the best SAT solver for a given model to minimize the expensive constraint solving time. We extract features from three different levels, i.e. the Alloy source code level, the Kodkod formula level and the boolean formula level. The experimental results show that our portfolio approach outperforms the best SAT solver by 30% as well as the baseline approach by 128% where users randomly select a solver for any given model."}, {"id": "conf/icst/PlazarAPDC19", "title": "Uniform Sampling of SAT Solutions for Configurable Systems: Are We There Yet?", "authors": ["Quentin Plazar", "Mathieu Acher", "Gilles Perrouin", "Xavier Devroey", "Maxime Cordy"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00032"], "tag": ["SAT and Search Based Testing"], "abstract": "Uniform or near-uniform generation of solutions for large satisfiability formulas is a problem of theoretical and practical interest for the testing community. Recent works proposed two algorithms (namely UniGen and QuickSampler) for reaching a good compromise between execution time and uniformity guarantees, with empirical evidence on SAT benchmarks. In the context of highly-configurable software systems (e.g., Linux), it is unclear whether UniGen and QuickSampler can scale and sample uniform software configurations. In this paper, we perform a thorough experiment on 128 real-world feature models. We find that UniGen is unable to produce SAT solutions out of such feature models. Furthermore, we show that QuickSampler does not generate uniform samples and that some features are either never part of the sample or too frequently present. Finally, using a case study, we characterize the impacts of these results on the ability to find bugs in a configurable system. Overall, our results suggest that we are not there: more research is needed to explore the cost-effectiveness of uniform sampling when testing large configurable systems."}, {"id": "conf/icst/ZhangZHWZ19", "title": "Do Pseudo Test Suites Lead to Inflated Correlation in Measuring Test Effectiveness?", "authors": ["Jie M. Zhang", "Lingming Zhang", "Dan Hao", "Meng Wang", "Lu Zhang"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00033"], "tag": ["Coverage"], "abstract": "Code coverage is the most widely adopted criteria for measuring test effectiveness in software quality assurance. The performance of coverage criteria (in indicating test suites' effectiveness) has been widely studied in prior work. Most of the studies use randomly constructed pseudo test suites to facilitate data collection for correlation analysis, yet no previous work has systematically studied whether pseudo test suites would lead to inflated correlation results. This paper focuses on the potentially wide-spread threat with a study over 123 real-world Java projects. Following the typical experimental process of studying coverage criteria, we investigate the correlation between statement/assertion coverage and mutation score using both pseudo and original test suites. Except for direct correlation analysis, we control the number of assertions and the test suite size to conduct partial correlation analysis. The results reveal that 1) the correlation (between coverage criteria and mutation score) derived from pseudo test suites is much higher than from original test suites (from 0.21 to 0.39 higher in Kendall value); 2) contrary to previously reported, statement coverage has a stronger correlation with mutation score than assertion coverage."}, {"id": "conf/icst/TerragniPB19", "title": "Coverage-Driven Test Generation for Thread-Safe Classes via Parallel and Conflict Dependencies.", "authors": ["Valerio Terragni", "Mauro Pezz\u00e8", "Francesco Adalberto Bianchi"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00034"], "tag": ["Coverage"], "abstract": "Thread-safe classes are common in concurrent object-oriented programs. Testing such classes is important to ensure the reliability of the concurrent programs that rely on them. Recently, researchers have proposed the automated generation of concurrent (multi-threaded) tests to expose concurrency faults in thread-safe classes (thread-safety violations). However, generating fault-revealing concurrent tests within an affordable time-budget is difficult due to the huge search space of possible concurrent tests. In this paper, we present DepCon, an approach to effectively reduce the search space of concurrent tests by means of both parallel and conflict dependency analyses. DepCon is based on the intuition that only methods that can both interleave (parallel dependent) and access the same shared memory locations (conflict dependent) can lead to thread-safety violations when concurrently executed. DepCon implements an efficient static analysis to compute the parallel and conflict dependencies among the methods of a class and uses the computed dependencies to steer the generation of tests towards concurrent tests that exhibit the computed dependencies. We evaluated DepCon by experimenting with a prototype implementation for Java programs on a set of thread-safe classes with known concurrency faults. The experimental results show that DepCon is more effective in exposing concurrency faults than state-of-the-art techniques."}, {"id": "conf/icst/WuLSCX19", "title": "Precise Static Happens-Before Analysis for Detecting UAF Order Violations in Android.", "authors": ["Diyu Wu", "Jie Liu", "Yulei Sui", "Shiping Chen", "Jingling Xue"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00035"], "tag": ["Verification and Analysis"], "abstract": "Unlike Java, Android provides a rich set of APIs to support a hybrid concurrency system, which consists of both Java threads and an event queue mechanism for dispatching asynchronous events. In this model, concurrency errors often manifest themselves in the form of order violations. An order violation occurs when two events access the same shared object in an incorrect order, causing unexpected program behaviors (e.g., null pointer dereferences). This paper presents SARD, a static analysis tool for detecting both intra-and inter-thread use-after-free (UAF) order violations, when a pointer is dereferenced (used) after it no longer points to any valid object, through systematic modeling of Android's concurrency mechanism. We propose a new flow-and context-sensitive static happens-before (HB) analysis to reason about the interleavings between two events to effectively identify precise HB relations and eliminate spurious event interleavings. We have evaluated SARD by comparing with NADROID, a state-of-the-art static order violation detection tool for Android. SARD outperforms NADROID in terms of both precision (by reporting three times fewer false alarms than NADROID given the same set of apps used by NADROID) and efficiency (by running two orders of magnitude faster than NADROID)."}, {"id": "conf/icst/KocWFCP19", "title": "An Empirical Assessment of Machine Learning Approaches for Triaging Reports of a Java Static Analysis Tool.", "authors": ["Ugur Koc", "Shiyi Wei", "Jeffrey S. Foster", "Marine Carpuat", "Adam A. Porter"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00036"], "tag": ["Verification and Analysis"], "abstract": "Despite their ability to detect critical bugs in software, developers consider high false positive rates to be a key barrier to using static analysis tools in practice. To improve the usability of these tools, researchers have recently begun to apply machine learning techniques to classify and filter false positive analysis reports. Although initial results have been promising, the long-term potential and best practices for this line of research are unclear due to the lack of detailed, large-scale empirical evaluation. To partially address this knowledge gap, we present a comparative empirical study of four machine learning techniques, namely hand-engineered features, bag of words, recurrent neural networks, and graph neural networks, for classifying false positives, using multiple ground-truth program sets. We also introduce and evaluate new data preparation routines for recurrent neural networks and node representations for graph neural networks, and show that these routines can have a substantial positive impact on classification accuracy. Overall, our results suggest that recurrent neural networks (which learn over a program's source code) outperform the other subject techniques, although interesting tradeoffs are present among all techniques. Our observations provide insight into the future research needed to speed the adoption of machine learning approaches in practice."}, {"id": "conf/icst/LegunsenZHRM19", "title": "Techniques for Evolution-Aware Runtime Verification.", "authors": ["Owolabi Legunsen", "Yi Zhang", "Milica Hadzi-Tanovic", "Grigore Rosu", "Darko Marinov"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00037"], "tag": ["Verification and Analysis"], "abstract": "Runtime Verification (RV) can help find bugs by monitoring program executions against formal properties. Developers should ideally use RV whenever they run tests, to find more bugs earlier. Despite tremendous research progress, RV still incurs high overhead in (1) machine time to monitor properties and (2) developer time to wait for and inspect violations from test executions that do not satisfy the properties. Moreover, all prior RV techniques consider only one program version and wastefully re-monitor unaffected properties and code as software evolves. We present the first evolution-aware RV techniques that reduce RV overhead across multiple program versions. Regression Property Selection (RPS) re-monitors only properties that can be violated in parts of code affected by changes, reducing machine time and developer time. Violation Message Suppression (VMS) simply shows only new violations to reduce developer time; it does not reduce machine time. Regression Property Prioritization (RPP) splits RV in two phases: properties more likely to find bugs are monitored in a critical phase to provide faster feedback to the developers; the rest are monitored in a background phase. We compare our techniques with the evolution-unaware (base) RV when monitoring test executions in 200 versions of 10 open-source projects. RPS and the RPP critical phase reduce the average RV overhead from 9.4\u00d7 (for base RV) to 1.8\u00d7, without missing any new violations. VMS reduces the average number of violations 540\u00d7, from 54 violations per version (for base RV) to one violation per 10 versions."}, {"id": "conf/icst/LamOSM019", "title": "iDFlakies: A Framework for Detecting and Partially Classifying Flaky Tests.", "authors": ["Wing Lam", "Reed Oei", "August Shi", "Darko Marinov", "Tao Xie"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00038"], "tag": ["Evolution and Maintenance"], "abstract": "Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5% order-dependent and 49.5% not. Our study of these flaky tests finds the prevalence of two types of flaky tests, probability of a test-suite run to have at least one failure due to flaky tests, and how different test reorderings affect the number of detected flaky tests. We envision that our work can spur research to alleviate the problem of flaky tests."}, {"id": "conf/icst/FuMG19", "title": "Resurgence of Regression Test Selection for C++.", "authors": ["Ben Fu", "Sasa Misailovic", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00039"], "tag": ["Evolution and Maintenance"], "abstract": "Regression testing - running available tests after each project change - is widely practiced in industry. Despite its widespread use and importance, regression testing is a costly activity. Regression test selection (RTS) optimizes regression testing by selecting only tests affected by project changes. RTS has been extensively studied and several tools have been deployed in large projects. However, work on RTS over the last decade has mostly focused on languages with abstract computing machines (e.g., JVM). Meanwhile development practices (e.g., frequency of commits, testing frameworks, compilers) in C++ projects have dramatically changed and the way we should design and implement RTS tools and the benefits of those tools is unknown. We present a design and implementation of an RTS technique, dubbed RTS++, that targets projects written in C++, which compile to LLVM IR and use the Google Test testing framework. RTS++ uses static analysis of a function call graph to select tests. RTS++ integrates with many existing build systems, including AutoMake, CMake, and Make. We evaluated RTS++ on 11 large open-source projects, totaling 3,811,916 lines of code. To the best of our knowledge, this is the largest evaluation of an RTS technique for C++. We measured the benefits of RTS++ compared to running all available tests (i.e., retest-all). Our results show that RTS++ reduces the number of executed tests and end-to-end testing time by 88% and 61% on average."}, {"id": "conf/icst/RwemalikaKPTL19", "title": "On the Evolution of Keyword-Driven Test Suites.", "authors": ["Renaud Rwemalika", "Marinos Kintis", "Mike Papadakis", "Yves Le Traon", "Pierre Lorrach"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00040"], "tag": ["Evolution and Maintenance"], "abstract": "Many companies rely on software testing to verify that their software products meet their requirements. However, test quality and, in particular, the quality of end-to-end testing is relatively hard to achieve. The problem becomes challenging when software evolves, as end-to-end test suites need to adapt and conform to the evolved software. Unfortunately, end-to-end tests are particularly fragile as any change in the application interface, e.g., application flow, location or name of graphical user interface elements, necessitates a change in the tests. This paper presents an industrial case study on the evolution of Keyword-Driven test suites, also known as Keyword-Driven Testing (KDT). Our aim is to demonstrate the problem of test maintenance, identify the benefits of Keyword-Driven Testing and overall improve the understanding of test code evolution (at the acceptance testing level). This information will support the development of automatic techniques, such as test refactoring and repair, and will motivate future research. To this end, we identify, collect and analyze test code changes across the evolution of industrial KDT test suites for a period of eight months. We show that the problem of test maintenance is largely due to test fragility (most commonly-performed changes are due to locator and synchronization issues) and test clones (over 30% of keywords are duplicated). We also show that the better test design of KDT test suites has the potential for drastically reducing (approximately 70%) the number of test code changes required to support software evolution. To further validate our results, we interview testers from BGL BNP Paribas and report their perceptions on the advantages and challenges of keyword-driven testing."}, {"id": "conf/icst/PatersonCAKFM19", "title": "An Empirical Study on the Use of Defect Prediction for Test Case Prioritization.", "authors": ["David Paterson", "Jos\u00e9 Campos", "Rui Abreu", "Gregory M. Kapfhammer", "Gordon Fraser", "Phil McMinn"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00041"], "tag": ["Evolution and Maintenance"], "abstract": "Test case prioritization has been extensively re-searched as a means for reducing the time taken to discover regressions in software. While many different strategies have been developed and evaluated, prior experiments have shown them to not be effective at prioritizing test suites to find real faults. This paper presents a test case prioritization strategy based on defect prediction, a technique that analyzes code features - such as the number of revisions and authors - to estimate the likelihood that any given Java class will contain a bug. Intuitively, if defect prediction can accurately predict the class that is most likely to be buggy, a tool can prioritize tests to rapidly detect the defects in that class. We investigated how to configure a defect prediction tool, called Schwa, to maximize the likelihood of an accurate prediction, surfacing the link between perfect defect prediction and test case prioritization effectiveness. Using 6 real-world Java programs containing 395 real faults, we conducted an empirical evaluation comparing this paper's strategy, called G-clef, against eight existing test case prioritization strategies. The experiments reveal that using defect prediction to prioritize test cases reduces the number of test cases required to find a fault by on average 9.48% when compared with existing coverage-based strategies, and 10.4% when compared with existing history-based strategies."}, {"id": "conf/icst/PanarinBIZZMRGT19", "title": "Poster: ClearTH Test Automation Framework: A Running Example of a DLT-Based Post-Trade System.", "authors": ["Vladimir Panarin", "Alyona Bulda", "Iosif Itkin", "Alexey Zverev", "Kirill Zagorouiko", "Murad Mamedov", "Alyona Rybakova", "Anna Gromova", "Elena Treshcheva", "Sergey Tishin", "Rostislav Yavorskiy"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00042"], "tag": ["Poster Papers"], "abstract": "The paper presents an overview of a test automation framework aimed at end-to-end functional and non-functional testing of DLT-based hybrid financial software for post-trade. The proposed solution comprises the components designed for testing user-facing parts of the SUT as well as business logic specific for different DLT-based architectures. This combined approach is seen as a viable solution of the problem of the SUT complexity as well the variety of possible DLT architectural decisions."}, {"id": "conf/icst/VuottoNPT19", "title": "Poster: Automatic Consistency Checking of Requirements with ReqV.", "authors": ["Simone Vuotto", "Massimo Narizzano", "Luca Pulina", "Armando Tacchella"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00043"], "tag": ["Poster Papers"], "abstract": "In the context of Requirements Engineering, checking the consistency of functional requirements is an important and still mostly open problem. In case of requirements written in natural language, the corresponding manual review is time consuming and error prone. On the other hand, automated consistency checking most often requires overburdening formalizations. In this paper we introduce ReqV, a tool for formal consistency checking of requirements. The main goal of the tool is to provide an easy-to-use environment for the verification of requirements in Cyber-Physical Systems (CPS). ReqV takes as input a set of requirements expressed in a structured natural language, translates them in a formal language and it checks their inner consistency. In case of failure, ReqV can also extracts a minimal set of conflicting requirements to help designers in correcting the specification."}, {"id": "conf/icst/JebbarSKT19", "title": "Poster: Re-Testing Configured Instances in the Production Environment - A Method for Reducing the Test Suite.", "authors": ["Oussama Jebbar", "Mohamed Aymen Saied", "Ferhat Khendek", "Maria Toeroe"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00044"], "tag": ["Poster Papers"], "abstract": "Configurations play an important role in the behavior and operation of configurable systems. Prior to deployment a configured system is tested in the development environment. However, because of the differences between the development environment and the production environment, the configuration of the system needs to be adapted for the production environment. It is therefore important to re-test the configured system in the production environment. Since the system has already been tested in the development environment one should avoid reapplying all the test cases, it is desirable to reduce the test suite to be used in the production environment as much as possible. This is the goal of the method we propose in this paper. For this, we explore the similarities between the configuration used in the development environment and the configuration for the production environment to eliminate test cases. Indeed, the difference between the two configurations is only at the environment level, i.e. only the configuration parameters that influence the interactions between the system and its environment are changed for the deployment in the production environment. We propose a method that is based on a classification of the configuration parameters (based on their dependency to the environment) and use it to reduce the development time test suite before reapplying it in the production environment."}, {"id": "conf/icst/BaloghHB19", "title": "Poster: Aiding Java Developers with Interactive Fault Localization in Eclipse IDE.", "authors": ["Gerg\u00f5 Balogh", "Ferenc Horv\u00e1th", "\u00c1rp\u00e1d Besz\u00e9des"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00045"], "tag": ["Poster Papers"], "abstract": "Spectrum-Based ones are a popular class of Fault Localization (FL) methods among researchers due to their relative simplicity. However, recent studies highlighted some barriers to the wider adoption of the technique in practical settings. One possibility to increase the practical usefulness of related tools is to involve interactivity between the user and the core FL algorithm. In this setting, the developer interacts with the fault localization algorithm by giving feedback on the elements proposed by the algorithm. This way, the proposed elements can be influenced in the hope to reach the faulty element earlier (we call the proposed approach Interactive Fault Localization, or iFL). With this work, we present our recent achievements in this topic. In particular, we overview the basic approach, our preliminary experimentation with user simulation, and the supporting tool for the actual usage of the method, iFL for Eclipse. Our aim is to provide a basis for the investigation of the feasibility and effectiveness of the technique, before moving on to more comprehensive experiments with actual human subjects. We invite researchers for further discussion on the topic, and for that, the method and tool will be made accessible."}, {"id": "conf/icst/VancsicsGSMBF019", "title": "Poster: Supporting JavaScript Experimentation with BugsJS.", "authors": ["B\u00e9la Vancsics", "P\u00e9ter Gyimesi", "Andrea Stocco", "Davood Mazinanian", "\u00c1rp\u00e1d Besz\u00e9des", "Rudolf Ferenc", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00046"], "tag": ["Poster Papers"], "abstract": "In our recent work, we proposed BUGSJS, a benchmark of several hundred bugs from popular JavaScript server-side programs. In this abstract paper, we report the results of our initial evaluation in adopting BUGSJS to support an experiment in fault localization. First, we describe how BUGSJS facilitated accessing the information required to perform the experiment, namely, test case code, their outcomes, their associated code coverage and related bug information. Second, we illustrate how BUGSJS can be improved to further enable easier application to fault localization research, for instance, by filtering out failing test cases that do not directly contribute to a bug. We hope that our preliminary results will foster researchers in using BUGSJS to enable highly-reproducible empirical studies and comparisons of JavaScript analysis and testing tools."}, {"id": "conf/icst/KahlesTHJ19", "title": "Automating Root Cause Analysis via Machine Learning in Agile Software Testing Environments.", "authors": ["Julen Kahles", "Juha Torronen", "Timo Huuhtanen", "Alexander Jung"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00047"], "tag": ["Industry Track"], "abstract": "We apply machine learning to automate the root cause analysis in agile software testing environments. In particular, we extract relevant features from raw log data after interviewing testing engineers (human experts). Initial efforts are put into clustering the unlabeled data, and despite obtaining weak correlations between several clusters and failure root causes, the vagueness in the rest of the clusters leads to the consideration of labeling. A new round of interviews with the testing engineers leads to the definition of five ground-truth categories. Using manually labeled data, we train artificial neural networks that either classify the data or pre-process it for clustering. The resulting method achieves an accuracy of 88.9%. The methodology of this paper serves as a prototype or baseline approach for the extraction of expert knowledge and its adaptation to machine learning techniques for root cause analysis in agile environments."}, {"id": "conf/icst/LeeHYKKY19", "title": "Classifying False Positive Static Checker Alarms in Continuous Integration Using Convolutional Neural Networks.", "authors": ["Seongmin Lee", "Shin Hong", "Jungbae Yi", "Taeksu Kim", "Chul-Joo Kim", "Shin Yoo"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00048"], "tag": ["Industry Track"], "abstract": "Static code analysis in Continuous Integration (CI) environment can significantly improve the quality of a software system because it enables early detection of defects without any test executions or user interactions. However, being a conservative over-approximation of system behaviours, static analysis also produces a large number of false positive alarms, identification of which takes up valuable developer time. We present an automated classifier based on Convolutional Neural Networks (CNNs). We hypothesise that many false positive alarms can be classified by identifying specific lexical patterns in the parts of the code that raised the alarm: human engineers adopt a similar tactic. We train a CNN based classifier to learn and detect these lexical patterns, using a total of about 10K historical static analysis alarms generated by six static analysis checkers for over 27 million LOC, and their labels assigned by actual developers. The results of our empirical evaluation suggest that our classifier can be highly effective for identifying false positive alarms, with the average precision across all six checkers of 79.72%."}, {"id": "conf/icst/AkramQL19", "title": "VCIPR: Vulnerable Code is Identifiable When a Patch is Released (Hacker's Perspective).", "authors": ["Junaid Akram", "Liang Qi", "Ping Luo"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00049"], "tag": ["Industry Track"], "abstract": "Vulnerable source code fragments remain unfixed for many years and they always propagate to other systems. Unfortunately, this happens often, when patch files are not propagated to all vulnerable code clones. An unpatched bug is a critical security problem, which should be detected and repaired as early as possible. In this paper, we present VCIPR, a scalable system for vulnerability detection in unpatched source code. We present a unique way, that uses a fast, token-based approach to detect vulnerabilities at function level granularity. This approach is language independent, which supports multiple programming languages including Java, C/C++, JavaScript. VCIPR detects most common repair patterns in patch files for the vulnerability code evaluation. We build fingerprint index of top critical CVE's source code, which were retrieved from a reliable source. Then we detect unpatched (vulnerable/non-vulnerable) code fragments in common open source software with high accuracy. A comparison with the state-of-the-art tools proves the effectiveness, efficiency and scalability of our approach. Furthermore, this paper shows that how the hackers can easily identify the vulnerable software whenever a patch file is released."}, {"id": "conf/icst/KingRKWS19", "title": "Automated Function Assessment in Driving Scenarios.", "authors": ["Christian King", "Lennart Ries", "Christopher Kober", "Christoph Wohlfahrt", "Eric Sax"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00050"], "tag": ["Industry Track"], "abstract": "In recent years, numerous innovations in the automotive industry have addressed the field of driver assistance systems and automated driving. Therefore additional required sensors, as well as the need for digital maps and online services, lead to an ever-increasing system space, which must be covered. Established test approaches in the area of Hardware-in-the-Loop (HiL) use predefined and structured test cases to test the systems on the basis of requirements. In the approach of systematic testing, an evaluation is only carried out for a specific test case respectively the duration of a test step. This paper presents a concept for an automated quality assessment of driving scenarios or digital test drives. The aim is the analysis and subsequent evaluation of continuous function behavior during a realistic test drive within a simulated environment. Compared to conventional systematic test approaches, the presented concept allows a continuous evaluation of the test drive, whereby multiple evaluations of systems in similar scenarios with deviating boundary conditions is possible. For the first time, this enables a functional evaluation of a complete test drive comprising numerous scenarios and situations. The presented approach was prototypically implemented and demonstrated on a Hardware-in-the-Loop (HiL) test bench evaluating an adaptive cruise control (ACC) system."}, {"id": "conf/icst/HellhakeSW19", "title": "Using Data Flow-Based Coverage Criteria for Black-Box Integration Testing of Distributed Software Systems.", "authors": ["Dominik Hellhake", "Tobias Schmid", "Stefan Wagner"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00051"], "tag": ["Industry Track"], "abstract": "Modern automotive E/E systems are implemented as distributed real-time software systems. The constantly growing complexity of safety-relevant software functions leads to an increased importance of testing during system integration of such systems. Systematic metrics are required to guide the testing process during system integration by providing coverage measures and stopping criteria but few studied approaches exist. For this purpose, we introduce a data-flow based observation scheme which captures the interplay behavior of involved ECUs during test execution and failure occurrences. In addition, we introduce a data flow-based coverage criterion designed for black box integration. By applying the observation scheme to test cases and associated faults found during execution, we first analyze similarities in data flow coverage. By further analyzing the data flow of failures, that slipped through the phase of system integration testing, we evaluate the usefulness of test gaps identified by using the suggested coverage criterion. We found major differences in the usage of data flow between undetected failures and existing test cases. In addition, we found that for the studied system under test the occurrence of failures is not necessarily a direct consequence of the test execution due to functional dependencies and side effects. Overall, these findings highlight the potential and limitations of data flow-based measures to be formalized as coverage or stopping criteria for the integration testing of distributed software systems."}, {"id": "conf/icst/ZhongZK19", "title": "TestSage: Regression Test Selection for Large-Scale Web Service Testing.", "authors": ["Hua Zhong", "Lingming Zhang", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00052"], "tag": ["Industry Track"], "abstract": "Regression testing is an important but expensive activity in software development. Among various types of tests, web service tests are usually one of the most expensive (due to network communications) but widely adopted types of tests in commercial software development. Regression test selection (RTS) aims to reduce the number of tests which need to be retested by only running tests that are affected by code changes. Although a large number of RTS techniques have been proposed in the past few decades, these techniques have not been adopted on large-scale web service testing. This is because most existing RTS techniques either require direct code dependency between tests and code under test or cannot be applied on large scale systems with enough efficiency. In this paper, we present a novel RTS technique, TestSage, that performs RTS for web service tests on large scale commercial software. With a small overhead, TestSage is able to collect fine grained (function level) dependency between test and service under test that do not directly depend on each other. TestSage has also been successfully applied to large complex systems with over a million functions. We conducted experiments of TestSage on a large scale backend service at Google. Experimental results show that TestSage reduces 34% of testing time when running all AEC (Analysis, Execution and Collection) phases, 50% of testing time while running without collection phase. TestSage has been integrated with internal testing framework at Google and runs day-to-day at the company."}, {"id": "conf/icst/PaivaGB19", "title": "Testing Android Incoming Calls.", "authors": ["Ana C. R. Paiva", "Marco A. Goncalves", "Andre R. Barros"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00053"], "tag": ["Testing Tool Track"], "abstract": "Mobile applications are increasingly present in our daily lives. Being increasingly dependent on apps, we all want to make sure apps work as expected. One way to increase confidence and quality of software is through testing. However, the existing approaches and tools still do not provide sufficient solutions for testing mobile apps with features different from the ones found in desktop or web applications. In particular, there are guidelines that mobile developers should follow and that may be tested automatically but, as far as we know, there are no tools that are able do it. The iMPAcT tool combines exploration, reverse engineering and testing to check if mobile apps follow best practices to implement specific behavior called UI Patterns. Examples of UI Patterns within this catalog are: orientation, background-foreground, side drawer, tab-scroll, among others. For each of these behaviors (UI Patterns), the iMPAcT tool has a corresponding Test Pattern that checks if the UI Pattern implementation follows the guidelines. This paper presents an extension to iMPAcT tool. It enables to test if Android apps work properly after receiving an incoming call, i.e., if the state of the screen after the call is the same as before getting the call. It formalizes the problem, describes the overall approach, describes the architecture of the tool and reports an experiment performed over 61 public mobile apps."}, {"id": "conf/icst/BorgesZ19", "title": "Why Does this App Need this Data? Automatic Tightening of Resource Access.", "authors": ["Nataniel P. Borges Jr.", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00054"], "tag": ["Testing Tool Track"], "abstract": "On mobile operating systems, apps may access resources that are not be needed for their primary functionality. Which are the resources an app actually needs for its core functionality? And what happens if we deny access to other resources? Using a test generator for user interaction, we systematically explore app behavior under varied resource constraints and determine the impact of access restrictions, yielding a minimal set of required privileges for each app and functionality. In our proof of concept on Android apps, our TIARA prototype could block up to 69% of resource accesses while retaining all previously explored functionality."}, {"id": "conf/icst/JendeleSCJR19", "title": "Efficient Automated Decomposition of Build Targets at Large-Scale.", "authors": ["Luk\u00e1s Jendele", "Markus Schwenk", "Diana Cremarenco", "Ivan Janicijevic", "Mikhail Rybalkin"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00055"], "tag": ["Testing Tool Track"], "abstract": "Large monolithic codebases, such as those used at Google and Facebook, enable engineers to easily share code and allow cross-team collaboration. Such codebases are partitioned into a huge number of libraries, binaries, and tests. However, engineers currently usually have to state the build dependencies between those blocks of functionality manually. One of the possible inefficiencies introduced that way are underutilized libraries, i.e. libraries that provide more functionality than required by the dependent code. This results in slow builds and an increased load on the Continuous Integration System. In this paper, we propose a way to automatically find and decompose underutilized libraries into a set of smaller components, where each component is a standalone library. Our work focuses on decompositions at source file level. While prior work already proposed decompositions when the final number of components was given as an input, we introduce an algorithm, AutoDecomposer, that finds the number of components automatically. In contrast to existing work, we analyze how a decomposition would lower the number of tests triggered by the Continuous Integration System in order to select only those decompositions that provide an impact. We evaluate AutoDecomposer's efficiency by comparing its potential impact to the maximum theoretical impact achievable by applying the most granular decomposition. We conclude that applying AutoDecomposer's decompositions generates 95% of the theoretical maximum test triggering frequency reduction, while only generating 4% as many components for large targets and 30% as many components on average compared to the theoretically most efficient approach."}, {"id": "conf/icst/KhaireddineMM19", "title": "Program Repair at Arbitrary Fault Depth.", "authors": ["Besma Khaireddine", "Matias Martinez", "Ali Mili"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00056"], "tag": ["Testing Tool Track"], "abstract": "Program repair has been an active research area for over a decade and has achieved great strides in terms of scalable automated repair tools. In this paper we argue that existing program repair tools lack an important ingredient, which limits their scope and their efficiency: a formal definition of a fault, and a formal characterization of fault removal. To support our conjecture, we consider GenProg, an archetypical program repair tool, and modify it according to our definitions of fault and fault removal; then we show, by means of empirical experiments, the impact that this has on the effectiveness and efficiency of thee tool."}, {"id": "conf/icst/MuscoYN19", "title": "SmokeOut: An Approach for Testing Clustering Implementations.", "authors": ["Vincenzo Musco", "Xin Yin", "Iulian Neamtiu"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00057"], "tag": ["Testing Tool Track"], "abstract": "Clustering is a key Machine Learning technique, used in many high-stakes domains from medicine to self-driving cars. Many clustering algorithms have been proposed, and these algorithms have been implemented in many toolkits. Clustering users assume that clustering implementations are correct, reliable, and for a given algorithm, interchangeable. We challenge these assumptions. We introduce SmokeOut, an approach and tool that pits clustering implementations against each other (and against themselves) while controlling for algorithm and dataset, to find datasets where clustering outcomes differ when they shouldn't, and measure this difference. We ran SmokeOut on 7 clustering algorithms (3 deterministic and 4 nondeterministic) implemented in 7 widely-used toolkits, and run in a variety of scenarios on the Penn Machine Learning Benchmark (162 datasets). SmokeOut has revealed that clustering implementations are fragile: on a given input dataset and using a given clustering algorithm, clustering outcomes and accuracy vary widely between (1) successive runs of the same toolkit; (2) different input parameters for that tool; (3) different toolkits."}, {"id": "conf/icst/Wei19", "title": "AADL-Based Safety Analysis Approaches for Safety-Critical Systems.", "authors": ["Xiaomin Wei"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00058"], "tag": ["Doctoral Symposium"], "abstract": "Ensuring system safety is significant for safety-critical systems. To improve system safety in system architecture models, Architecture Analysis and Design Language (AADL) is used to model safety-critical systems. My thesis provides several safety analysis approaches for AADL models. To make it more effective, model transformation rules from AADL models to target formal models are formulated for the integration of formal methods into safety analysis approaches. The automatic transformation can reduce the degree of application difficulty of formal methods for engineers."}, {"id": "conf/icst/Sondhi19", "title": "Testing for Implicit Inconsistencies in Documentation and Implementation.", "authors": ["Devika Sondhi"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00059"], "tag": ["Doctoral Symposium"], "abstract": "The thesis aims to provide test generation techniques, beyond the consideration of coverage-based criterion, with an objective to highlight inconsistencies in the documentation and the implementation. We leverage the domain knowledge gained from developers' expertise and existing resources to generate test-cases."}, {"id": "conf/icst/Lima19", "title": "Automated Scenario-Based Integration Testing of Time-Constrained Distributed Systems.", "authors": ["Bruno Lima"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00060"], "tag": ["Doctoral Symposium"], "abstract": "In a growing number of domains, such as IoT for e-health and smart cities, the provisioning of end-to-end services to the users depends on the proper interoperation of multiple systems, forming a new distributed system, often subject to timing constraints. To ensure interoperability and integrity, it is important to conduct integration tests that verify the interactions with the environment and between the system components in key scenarios. To solve the test automation challenges, we propose algorithms for decentralized conformance checking and test input generation, and for checking and enforcing the conditions (local observability and controllability) that allow decentralized test execution. With this, we expect to improve the fault detection and localization capabilities and reduce the communication overhead comparatively to other model-based testing approaches. Our approach will be validated using real case studies from industrial partners."}, {"id": "conf/icst/Radavelli19", "title": "Using Testing to Repair Models.", "authors": ["Marco Radavelli"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00061"], "tag": ["Doctoral Symposium"], "abstract": "Software testing is an important phase in the software development process, aiming at locating faults in artifacts, in order to achieve a degree of confidence that the software behaves according to a specification. While most of the techniques in software testing are applied to debugging, fault-localization, and repair of code, to the best of our knowledge there are fewer works regarding the application of software testing to locating faults in models and to the automated repair of such faults. The goal of this PhD project proposal is to study how testing can be applied to repair models. We describe the research approach and discuss the application cases of combinatorial and feature models. We then discuss future work of applying testing to repair models for other scenarios, such as timed automata."}, {"id": "conf/icst/Junior19", "title": "Operational Profile and Software Testing: Aligning User Interest and Test Strategy.", "authors": ["Luiz Cavamura J\u00fanior"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00062"], "tag": ["Doctoral Symposium"], "abstract": "Context: The software's operational profile is a representation of how users use the software, in which the most commonly-used parts of the software are identified. There is evidence of a possible mismatch between the tested parts of the software and the operational profile of the software. Objective: This paper aims to present a doctoral research that investigates the use of software operational profile as a resource to improve software quality from the point of view of users. As one of the expected contributions, the research proposes a strategy in which the operational profile of the software is used to: a) evaluate and adapt a test suite based on the operational profile of the software; b) as a prioritization criterion that allows, given a set of defects, to identify the defects that have the greatest impact on the operation of the software by the users and, thus, to consider this impact in the pricing of the defects. Method: To present the research, this paper describes: a) the problem addressed and the purpose of the research; b) the results obtained by the activities carried out to evaluate the feasibility of the research (systematic mapping of the literature, systematic review of the literature and exploratory study); c) the hypotheses to be investigated; d) expected contributions; e) the current state of the research. Results: The results obtained by the activities carried out were favorable to the accomplishment and continuity of the research, evidencing the originality of the research and the proposed strategy. Conclusion: Once the feasibility of the research and the progress that has been obtained in its execution are verified, the results to be obtained are promising and, consequently, the expected contributions must be obtained."}, {"id": "conf/icst/Tan19", "title": "A Model-Based Approach to Generate Dynamic Synthetic Test Data.", "authors": ["Chao Tan"], "DOIs": ["https://doi.org/10.1109/ICST.2019.00063"], "tag": ["Doctoral Symposium"], "abstract": "Having access to high-quality test data is an important requirement to ensure effective cross-organizational integration testing. The common practice for addressing this need is to generate synthetic data. However, existing approaches cannot generate representative datasets that can evolve to allow the simulation of the dynamics of the systems under test. In this PhD project, and in collaboration with an industrial partner, we investigate the use of machine learning techniques for developing novel solutions that can generate synthetic, dynamic and representative test data."}]}, "sigsoft/fse": {"2017": [{"id": "conf/sigsoft/Williams17", "title": "The rising tide lifts all boats: the advancement of science in cyber security (invited talk).", "authors": ["Laurie Williams"], "DOIs": ["https://doi.org/10.1145/3106237.3121272"], "tag": ["Invited Papers"], "abstract": "ABSTRACT Stolen passwords, compromised medical records, taking the internet out through video cameras\u2013 cybersecurity breaches are in the news every day. Despite all this, the practice of cybersecurity today is generally reactive rather than proactive. That is, rather than improving their defenses in advance, organizations react to attacks once they have occurred by patching the individual vulnerabilities that led to those attacks. Researchers engineer solutions to the latest form of attack. What we need, instead, are scientifically founded design principles for building in security mechanisms from the beginning, giving protection against broad classes of attacks. Through scientific measurement, we can improve our ability to make decisions that are evidence-based, proactive, and long-sighted. Recognizing these needs, the US National Security Agency (NSA) devised a new framework for collaborative research, the \u201cLablet\u201d structure, with the intent to more aggressively advance the science of cybersecurity. A key motivation was to catalyze a shift in relevant areas towards a more organized and cohesive scientific community. The NSA named Carnegie Mellon University, North Carolina State University, and the University of Illinois \u2013 Urbana Champaign its initial Lablets in 2011, and added the University of Maryland in 2014.  This talk will reflect on the structure of the collaborative research efforts of the Lablets, lessons learned in the transition to more scientific concepts to cybersecurity, research results in solving five hard security problems, and methods that are being used for the measurement of scientific progress of the Lablet research."}, {"id": "conf/sigsoft/Easterbrook17", "title": "Verifying the forecast: how climate models are developed and tested (invited talk).", "authors": ["Steve Easterbrook"], "DOIs": ["https://doi.org/10.1145/3106237.3121271"], "tag": ["Invited Papers"], "abstract": "ABSTRACT Stolen passwords, compromised medical records, taking the internet out through video cameras\u2013 cybersecurity breaches are in the news every day. Despite all this, the practice of cybersecurity today is generally reactive rather than proactive. That is, rather than improving their defenses in advance, organizations react to attacks once they have occurred by patching the individual vulnerabilities that led to those attacks. Researchers engineer solutions to the latest form of attack. What we need, instead, are scientifically founded design principles for building in security mechanisms from the beginning, giving protection against broad classes of attacks. Through scientific measurement, we can improve our ability to make decisions that are evidence-based, proactive, and long-sighted. Recognizing these needs, the US National Security Agency (NSA) devised a new framework for collaborative research, the \u201cLablet\u201d structure, with the intent to more aggressively advance the science of cybersecurity. A key motivation was to catalyze a shift in relevant areas towards a more organized and cohesive scientific community. The NSA named Carnegie Mellon University, North Carolina State University, and the University of Illinois \u2013 Urbana Champaign its initial Lablets in 2011, and added the University of Maryland in 2014.  This talk will reflect on the structure of the collaborative research efforts of the Lablets, lessons learned in the transition to more scientific concepts to cybersecurity, research results in solving five hard security problems, and methods that are being used for the measurement of scientific progress of the Lablet research."}, {"id": "conf/sigsoft/Emmerich17", "title": "Software engineering research results in industrial practice: a tale of two projects (invited talk).", "authors": ["Wolfgang Emmerich"], "DOIs": ["https://doi.org/10.1145/3106237.3121273"], "tag": ["Invited Papers"], "abstract": "ABSTRACT In this talk, I will discuss the use of software engineering research results in industrial practice, based on two projects I have been involved with. The first project addressed the challenge that manipulation of financial market data had to be expressed precisely for a large number of different financial markets. The challenge was addressed by defining a functional Domain Specific Language (DSL) that was geared towards expressing these manipulations at a high level of abstraction. An environment that implements the DSL was built using the Eclipse platform together with a compiler that generates a Java-based reference implementation of these manipulations. The implementation is used as a test oracle to generate test cases, which are in turn used to validate a soft real-time system that implements these manipulations. In another project that is still ongoing, I have proposed the use of software product line research to engineer a family of mobile banking applications. I will reflect on the experience of integrating software product line principles and modern Agile development practices. I will then discuss a few areas of software engineering research, that I have personally been involved in, that I have found not to be very useful in practice. I will conclude by outlining some topics where novel research results would be very beneficial from an industrial point of view."}, {"id": "conf/sigsoft/FieldingTEGWKO17", "title": "Reflections on the REST architectural style and \"principled design of the modern web architecture\" (impact paper award).", "authors": ["Roy T. Fielding", "Richard N. Taylor", "Justin R. Erenkrantz", "Michael M. Gorlick", "Jim Whitehead", "Rohit Khare", "Peyman Oreizy"], "DOIs": ["https://doi.org/10.1145/3106237.3121282"], "tag": ["Invited Papers"], "abstract": "ABSTRACT Seventeen years after its initial publication at ICSE 2000, the Representational State Transfer (REST) architectural style continues to hold significance as both a guide for understanding how the World Wide Web is designed to work and an example of how principled design, through the application of architectural styles, can impact the development and understanding of large-scale software architecture. However, REST has also become an industry buzzword: frequently abused to suit a particular argument, confused with the general notion of using HTTP, and denigrated for not being more like a programming methodology or implementation framework.  In this paper, we chart the history, evolution, and shortcomings of REST, as well as several related architectural styles that it inspired, from the perspective of a chain of doctoral dissertations produced by the University of California's Institute for Software Research at UC Irvine. These successive theses share a common theme: extending the insights of REST to new domains and, in their own way, exploring the boundary of software engineering as it applies to decentralized software architectures and architectural design. We conclude with discussion of the circumstances, environment, and organizational characteristics that gave rise to this body of work."}, {"id": "conf/sigsoft/YogaN17", "title": "A fast causal profiler for task parallel programs.", "authors": ["Adarsh Yoga", "Santosh Nagarakatte"], "DOIs": ["https://doi.org/10.1145/3106237.3106254"], "tag": ["Research Papers"], "abstract": "ABSTRACT This paper proposes TASKPROF, a profiler that identifies parallelism bottlenecks in task parallel programs. It leverages the structure of a task parallel execution to perform fine-grained attribution of work to various parts of the program. TASKPROF\u2019s use of hardware performance counters to perform fine-grained measurements minimizes perturbation. TASKPROF\u2019s profile execution runs in parallel using multi-cores. TASKPROF\u2019s causal profile enables users to estimate improvements in parallelism when a region of code is optimized even when concrete optimizations are not yet known. We have used TASKPROF to isolate parallelism bottlenecks in twenty three applications that use the Intel Threading Building Blocks library. We have designed parallelization techniques in five applications to increase parallelism by an order of magnitude using TASKPROF. Our user study indicates that developers are able to isolate performance bottlenecks with ease using TASKPROF."}, {"id": "conf/sigsoft/ZhouCMW17", "title": "On the scalability of Linux kernel maintainers' work.", "authors": ["Minghui Zhou", "Qingying Chen", "Audris Mockus", "Fengguang Wu"], "DOIs": ["https://doi.org/10.1145/3106237.3106287"], "tag": ["Research Papers"], "abstract": "ABSTRACTOpen source software ecosystems evolve ways to balance the workload among groups of participants ranging from core groups to peripheral groups. As ecosystems grow, it is not clear whether the mechanisms that previously made them work will continue to be relevant or whether new mechanisms will need to evolve. The impact of failure for critical ecosystems such as Linux is enormous, yet the understanding of why they function and are effective is limited. We, therefore, aim to understand how the Linux kernel sustains its growth, how to characterize the workload of maintainers, and whether or not the existing mechanisms are scalable. We quantify maintainers' work through the files that are maintained, and the change activity and the numbers of contributors in those files. We find systematic differences among modules; these differences are stable over time, which suggests that certain architectural features, commercial interests, or module-specific practices lead to distinct sustainable equilibria. We find that most of the modules have not grown appreciably over the last decade; most growth has been absorbed by a few modules. We also find that the effort per maintainer does not increase, even though the community has hypothesized that required effort might increase. However, the distribution of work among maintainers is highly unbalanced, suggesting that a few maintainers may experience increasing workload. We find that the practice of assigning multiple maintainers to a file yields only a power of 1/2 increase in productivity. We expect that our proposed framework to quantify maintainer practices will help clarify the factors that allow rapidly growing ecosystems to be sustainable."}, {"id": "conf/sigsoft/TsigkanosKG17", "title": "Modeling and verification of evolving cyber-physical spaces.", "authors": ["Christos Tsigkanos", "Timo Kehrer", "Carlo Ghezzi"], "DOIs": ["https://doi.org/10.1145/3106237.3106299"], "tag": ["Research Papers"], "abstract": "ABSTRACT We increasingly live in cyber-physical spaces -- spaces that are both physical and digital, and where the two aspects are intertwined. Such spaces are highly dynamic and typically undergo continuous change. Software engineering can have a profound impact in this domain, by defining suitable modeling and specification notations as well as supporting design-time formal verification. In this paper, we present a methodology and a technical framework which support modeling of evolving cyber-physical spaces and reasoning about their spatio-temporal properties. We utilize a discrete, graph-based formalism for modeling cyber-physical spaces as well as primitives of change, giving rise to a reactive system consisting of rewriting rules with both local and global application conditions. Formal reasoning facilities are implemented adopting logic-based specification of properties and according model checking procedures, in both spatial and temporal fragments. We evaluate our approach using a case study of a disaster scenario in a smart city."}, {"id": "conf/sigsoft/FuM17", "title": "Easy over hard: a case study on deep learning.", "authors": ["Wei Fu", "Tim Menzies"], "DOIs": ["https://doi.org/10.1145/3106237.3106256"], "tag": ["Research Papers"], "abstract": "ABSTRACT While deep learning is an exciting new technique, the benefits of this method need to be assessed with respect to its computational cost. This is particularly important for deep learning since these learners need hours (to weeks) to train the model. Such long training time limits the ability of (a)~a researcher to test the stability of their conclusion via repeated runs with different random seeds; and (b)~other researchers to repeat, improve, or even refute that original work.  For example, recently, deep learning was used to find which questions in the Stack Overflow programmer discussion forum can be linked together. That deep learning system took 14 hours to execute. We show here that applying a very simple optimizer called DE to fine tune SVM, it can achieve similar (and sometimes better) results. The DE approach terminated in 10 minutes; i.e. 84 times faster hours than deep learning method.  We offer these results as a cautionary tale to the software analytics community and suggest that not every new innovation should be applied without critical analysis. If researchers deploy some new and expensive process, that work should be baselined against some simpler and faster alternatives."}, {"id": "conf/sigsoft/OhBMS17", "title": "Finding near-optimal configurations in product lines by random sampling.", "authors": ["Jeho Oh", "Don S. Batory", "Margaret Myers", "Norbert Siegmund"], "DOIs": ["https://doi.org/10.1145/3106237.3106273"], "tag": ["Research Papers"], "abstract": "ABSTRACT Software Product Lines (SPLs) are highly configurable systems. This raises the challenge to find optimal performing configurations for an anticipated workload. As SPL configuration spaces are huge, it is infeasible to benchmark all configurations to find an optimal one. Prior work focused on building performance models to predict and optimize SPL configurations. Instead, we randomly sample and recursively search a configuration space directly to find near-optimal configurations without constructing a prediction model. Our algorithms are simpler and have higher accuracy and efficiency."}, {"id": "conf/sigsoft/FuM17a", "title": "Revisiting unsupervised learning for defect prediction.", "authors": ["Wei Fu", "Tim Menzies"], "DOIs": ["https://doi.org/10.1145/3106237.3106257"], "tag": ["Research Papers"], "abstract": "ABSTRACTCollecting quality data from software projects can be time-consuming and expensive. Hence, some researchers explore \"unsupervised\" approaches to quality prediction that does not require labelled data. An alternate technique is to use \"supervised\" approaches that learn models from project data labelled with, say, \"defective\" or \"not-defective\". Most researchers use these supervised models since, it is argued, they can exploit more knowledge of the projects. At FSE-16, Yang et al. reported startling results where unsupervised defect predictors outperformed supervised predictors for effort-aware just-in-time defect prediction. If confirmed, these results would lead to a dramatic simplification of a seemingly complex task (data mining) that is widely explored in the software engineering literature. This paper repeats and refutes those results as follows. (1) There is much variability in the efficacy of the Yang et al. predictors so even with their approach, some supervised data is required to prune weaker predictors away. (2) Their findings were grouped across N projects. When we repeat their analysis on a project-by-project basis, supervised predictors are seen to work better. Even though this paper rejects the specific conclusions of Yang et al., we still endorse their general goal. In our our experiments, supervised predictors did not perform outstandingly better than unsupervised ones for effort-aware just-in-time defect prediction. Hence, they may indeed be some combination of unsupervised learners to achieve comparable performance to supervised ones. We therefore encourage others to work in this promising area."}, {"id": "conf/sigsoft/XieCZLLL17", "title": "Loopster: static loop termination analysis.", "authors": ["Xiaofei Xie", "Bihuan Chen", "Liang Zou", "Shang-Wei Lin", "Yang Liu", "Xiaohong Li"], "DOIs": ["https://doi.org/10.1145/3106237.3106260"], "tag": ["Research Papers"], "abstract": "ABSTRACT Loop termination is an important problem for proving the correctness of a system and ensuring that the system always reacts. Existing loop termination analysis techniques mainly depend on the synthesis of ranking functions, which is often expensive. In this paper, we present a novel approach, named Loopster, which performs an efficient static analysis to decide the termination for loops based on path termination analysis and path dependency reasoning. Loopster adopts a divide-and-conquer approach: (1) we extract individual paths from a target multi-path loop and analyze the termination of each path, (2) analyze the dependencies between each two paths, and then (3) determine the overall termination of the target loop based on the relations among paths. We evaluate Loopster by applying it on the loop termination competition benchmark and three real-world projects. The results show that Loopster is effective in a majority of loops with better accuracy and 20 \u00d7+ performance improvement compared to the state-of-the-art tools."}, {"id": "conf/sigsoft/Sidiroglou-Douskos17", "title": "CodeCarbonCopy.", "authors": ["Stelios Sidiroglou-Douskos", "Eric Lahtinen", "Anthony Eden", "Fan Long", "Martin Rinard"], "DOIs": ["https://doi.org/10.1145/3106237.3106269"], "tag": ["Research Papers"], "abstract": "ABSTRACT We present CodeCarbonCopy (CCC), a system for transferring code from a donor application into a recipient application. CCC starts with functionality identified by the developer to transfer into an insertion point (again identified by the developer) in the recipient. CCC uses paired executions of the donor and recipient on the same input file to obtain a translation between the data representation and name space of the recipient and the data representation and name space of the donor. It also implements a static analysis that identifies and removes irrelevant functionality useful in the donor but not in the recipient. We evaluate CCC on eight transfers between six applications. Our results show that CCC can successfully transfer donor functionality into recipient applications."}, {"id": "conf/sigsoft/NelsonDDK17", "title": "The power of \"why\" and \"why not\": enriching scenario exploration with provenance.", "authors": ["Tim Nelson", "Natasha Danas", "Daniel J. Dougherty", "Shriram Krishnamurthi"], "DOIs": ["https://doi.org/10.1145/3106237.3106272"], "tag": ["Research Papers"], "abstract": "ABSTRACT Scenario-finding tools like the Alloy Analyzer are widely used in numerous concrete domains like security, network analysis, UML analysis, and so on. They can help to verify properties and, more generally, aid in exploring a system's behavior.  While scenario finders are valuable for their ability to produce concrete examples, individual scenarios only give insight into what is possible, leaving the user to make their own conclusions about what might be necessary. This paper enriches scenario finding by allowing users to ask ``why?'' and ``why not?'' questions about the examples they are given. We show how to distinguish parts of an example that cannot be consistently removed (or changed) from those that merely reflect underconstraint in the specification. In the former case we show how to determine which elements of the specification and which other components of the example together explain the presence of such facts.  This paper formalizes the act of computing provenance in scenario-finding. We present Amalgam, an extension of the popular Alloy scenario-finder, which implements these foundations and provides interactive exploration of examples. We also evaluate Amalgam's algorithmics on a variety of both textbook and real-world examples."}, {"id": "conf/sigsoft/BohmeS0UZ17", "title": "Where is the bug and how is it fixed? an experiment with practitioners.", "authors": ["Marcel B\u00f6hme", "Ezekiel O. Soremekun", "Sudipta Chattopadhyay", "Emamurho Ugherughe", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1145/3106237.3106255"], "tag": ["Research Papers"], "abstract": "ABSTRACT Research has produced many approaches to automatically locate, explain, and repair software bugs. But do these approaches relate to the way practitioners actually locate, understand, and fix bugs? To help answer this question, we have collected a dataset named DBGBENCH --- the correct fault locations, bug diagnoses, and software patches of 27 real errors in open-source C projects that were consolidated from hundreds of debugging sessions of professional software engineers. Moreover, we shed light on the entire debugging process, from constructing a hypothesis to submitting a patch, and how debugging time, difficulty, and strategies vary across practitioners and types of errors. Most notably, DBGBENCH can serve as reality check for novel automated debugging and repair techniques."}, {"id": "conf/sigsoft/GopsteinIYDZYC17", "title": "Understanding misunderstandings in source code.", "authors": ["Dan Gopstein", "Jake Iannacone", "Yu Yan", "Lois DeLong", "Yanyan Zhuang", "Martin K.-C. Yeh", "Justin Cappos"], "DOIs": ["https://doi.org/10.1145/3106237.3106264"], "tag": ["Research Papers"], "abstract": "ABSTRACT Humans often mistake the meaning of source code, and so misjudge a program's true behavior. These mistakes can be caused by extremely small, isolated patterns in code, which can lead to significant runtime errors. These patterns are used in large, popular software projects and even recommended in style guides. To identify code patterns that may confuse programmers we extracted a preliminary set of `atoms of confusion' from known confusing code. We show empirically in an experiment with 73 participants that these code patterns can lead to a significantly increased rate of misunderstanding versus equivalent code without the patterns. We then go on to take larger confusing programs and measure (in an experiment with 43 participants) the impact, in terms of programmer confusion, of removing these confusing patterns. All of our instruments, analysis code, and data are publicly available online for replication, experimentation, and feedback."}, {"id": "conf/sigsoft/SiegmundPPAHKBB17", "title": "Measuring neural efficiency of program comprehension.", "authors": ["Janet Siegmund", "Norman Peitek", "Chris Parnin", "Sven Apel", "Johannes Hofmeister", "Christian K\u00e4stner", "Andrew Begel", "Anja Bethmann", "Andr\u00e9 Brechmann"], "DOIs": ["https://doi.org/10.1145/3106237.3106268"], "tag": ["Research Papers"], "abstract": "ABSTRACT Most modern software programs cannot be understood in their entirety by a single programmer. Instead, programmers must rely on a set of cognitive processes that aid in seeking, filtering, and shaping relevant information for a given programming task. Several theories have been proposed to explain these processes, such as ``beacons,' for locating relevant code, and ``plans,'' for encoding cognitive models. However, these theories are decades old and lack validation with modern cognitive-neuroscience methods. In this paper, we report on a study using functional magnetic resonance imaging (fMRI) with 11 participants who performed program comprehension tasks. We manipulated experimental conditions related to beacons and layout to isolate specific cognitive processes related to bottom-up comprehension and comprehension based on semantic cues. We found evidence of semantic chunking during bottom-up comprehension and lower activation of brain areas during comprehension based on semantic cues, confirming that beacons ease comprehension."}, {"id": "conf/sigsoft/MuraliCJ17", "title": "Bayesian specification learning for finding API usage errors.", "authors": ["Vijayaraghavan Murali", "Swarat Chaudhuri", "Chris Jermaine"], "DOIs": ["https://doi.org/10.1145/3106237.3106284"], "tag": ["Research Papers"], "abstract": "ABSTRACT We present a Bayesian framework for learning probabilistic specifications from large, unstructured code corpora, and then using these specifications to statically detect anomalous, hence likely buggy, program behavior. Our key insight is to build a statistical model that correlates all specifications hidden inside a corpus with the syntax and observed behavior of programs that implement these specifications. During the analysis of a particular program, this model is conditioned into a posterior distribution that prioritizes specifications that are relevant to the program. The problem of finding anomalies is now framed quantitatively, as a problem of computing a distance between a \"reference distribution\" over program behaviors that our model expects from the program, and the distribution over behaviors that the program actually produces.  We implement our ideas in a system, called Salento, for finding anomalous API usage in Android programs. Salento learns specifications using a combination of a topic model and a neural network model. Our encouraging experimental results show that the system can automatically discover subtle errors in Android applications in the wild, and has high precision and recall compared to competing probabilistic approaches."}, {"id": "conf/sigsoft/VermaR17", "title": "Synergistic debug-repair of heap manipulations.", "authors": ["Sahil Verma", "Subhajit Roy"], "DOIs": ["https://doi.org/10.1145/3106237.3106263"], "tag": ["Research Papers"], "abstract": "ABSTRACT We present Wolverine, an integrated Debug-Repair environment for heap manipulating programs. Wolverine facilitates stepping through a concrete program execution, provides visualizations of the abstract program states (as box-and-arrow diagrams) and integrates a novel, proof-directed repair algorithm to synthesize repair patches. To provide a seamless environment, Wolverine supports \"hot-patching\" of the generated repair patches, enabling the programmer to continue the debug session without requiring an abort-compile-debug cycle. We also propose new debug-repair possibilities, \"specification refinement\" and \"specification slicing\" made possible by Wolverine. We evaluate our framework on 1600 buggy programs (generated using fault injection) on a variety of data-structures like singly, doubly and circular linked-lists, Binary Search Trees, AVL trees, Red-Black trees and Splay trees; Wolverine could repair all the buggy instances within reasonable time (less than 5 sec in most cases). We also evaluate Wolverine on 247 (buggy) student submissions; Wolverine could repair more than 80% of programs where the student had made a reasonable attempt."}, {"id": "conf/sigsoft/FerlesWCD17", "title": "Failure-directed program trimming.", "authors": ["Kostas Ferles", "Valentin W\u00fcstholz", "Maria Christakis", "Isil Dillig"], "DOIs": ["https://doi.org/10.1145/3106237.3106249"], "tag": ["Research Papers"], "abstract": "ABSTRACTThis paper describes a new program simplification technique called program trimming that aims to improve the scalability and precision of safety checking tools. Given a program P, program trimming generates a new program P' such that P and P' are equi-safe (i.e., P' has a bug if and only if P has a bug), but P' has fewer execution paths than P. Since many program analyzers are sensitive to the number of execution paths, program trimming has the potential to improve the effectiveness of safety checking tools. In addition to introducing the concept of program trimming, this paper also presents a lightweight static analysis that can be used as a pre-processing step to remove program paths while retaining equi-safety. We have implemented the proposed technique in a tool called Trimmer and evaluate it in the context of two program analysis techniques, namely abstract interpretation and dynamic symbolic execution. Our experiments show that program trimming significantly improves the effectiveness of both techniques."}, {"id": "conf/sigsoft/CoelhoV17", "title": "Why modern open source projects fail.", "authors": ["Jailton Coelho", "Marco Tulio Valente"], "DOIs": ["https://doi.org/10.1145/3106237.3106246"], "tag": ["Research Papers"], "abstract": "ABSTRACT Open source is experiencing a renaissance period, due to the appearance of modern platforms and workflows for developing and maintaining public code. As a result, developers are creating open source software at speeds never seen before. Consequently, these projects are also facing unprecedented mortality rates. To better understand the reasons for the failure of modern open source projects, this paper describes the results of a survey with the maintainers of 104 popular GitHub systems that have been deprecated. We provide a set of nine reasons for the failure of these open source projects. We also show that some maintenance practices---specifically the adoption of contributing guidelines and continuous integration---have an important association with a project failure or success. Finally, we discuss and reveal the principal strategies developers have tried to overcome the failure of the studied projects."}, {"id": "conf/sigsoft/Hilton0TMD17", "title": "Trade-offs in continuous integration: assurance, security, and flexibility.", "authors": ["Michael Hilton", "Nicholas Nelson", "Timothy Tunnell", "Darko Marinov", "Danny Dig"], "DOIs": ["https://doi.org/10.1145/3106237.3106270"], "tag": ["Research Papers"], "abstract": "ABSTRACT Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI being a widely used activity in software engineering, we do not know what motivates developers to use CI, and what barriers and unmet needs they face. Without such knowledge, developers make easily avoidable errors, tool builders invest in the wrong direction, and researchers miss opportunities for improving the practice of CI. We present a qualitative study of the barriers and needs developers face when using CI. We conduct semi-structured interviews with developers from different industries and development scales. We triangulate our findings by running two surveys. We find that developers face trade-offs between speed and certainty (Assurance), between better access and information security (Security), and between more configuration options and greater ease of use (Flexi- bility). We present implications of these trade-offs for developers, tool builders, and researchers."}, {"id": "conf/sigsoft/JabbarvandM17", "title": "\u00b5Droid: an energy-aware mutation testing framework for Android.", "authors": ["Reyhaneh Jabbarvand", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3106237.3106244"], "tag": ["Research Papers"], "abstract": "ABSTRACT The rising popularity of mobile apps deployed on battery-constrained devices underlines the need for effectively evaluating their energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. As a result, for energy testing, developers are relying on tests intended for evaluating the functional correctness of apps. Such tests may not be adequate for revealing energy defects and inefficiencies in apps. This paper presents an energy-aware mutation testing framework, called \u03bcDROID, that can be used by developers to assess the adequacy of their test suite for revealing energy-related defects. \u03bcDROID implements fifty energy-aware mutation operators and relies on a novel, automatic oracle to determine if a mutant can be killed by a test. Our evaluation on real-world Android apps shows the ability of proposed mutation operators for evaluating the utility of tests in revealing energy defects. Moreover, our automated oracle can detect whether tests kill the energy mutants with an overall accuracy of 94%, thereby making it possible to apply \u03bcDROID automatically."}, {"id": "conf/sigsoft/SadeghiJM17", "title": "PATDroid: permission-aware GUI testing of Android.", "authors": ["Alireza Sadeghi", "Reyhaneh Jabbarvand", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3106237.3106250"], "tag": ["Research Papers"], "abstract": "ABSTRACT Recent introduction of a dynamic permission system in Android, allowing the users to grant and revoke permissions after the installation of an app, has made it harder to properly test apps. Since an app's behavior may change depending on the granted permissions, it needs to be tested under a wide range of permission combinations. At the state-of-the-art, in the absence of any automated tool support, a developer needs to either manually determine the interaction of tests and app permissions, or exhaustively re-execute tests for all possible permission combinations, thereby increasing the time and resources required to test apps. This paper presents an automated approach, called PATDroid, for efficiently testing an Android app while taking the impact of permissions on its behavior into account. PATDroid performs a hybrid program analysis on both an app under test and its test suite to determine which tests should be executed on what permission combinations. Our experimental results show that PATDroid significantly reduces the testing effort, yet achieves comparable code coverage and fault detection capability as exhaustively testing an app under all permission combinations."}, {"id": "conf/sigsoft/VasquezBTMPVBP17", "title": "Enabling mutation testing for Android apps.", "authors": ["Mario Linares V\u00e1squez", "Gabriele Bavota", "Michele Tufano", "Kevin Moran", "Massimiliano Di Penta", "Christopher Vendome", "Carlos Bernal-C\u00e1rdenas", "Denys Poshyvanyk"], "DOIs": ["https://doi.org/10.1145/3106237.3106275"], "tag": ["Research Papers"], "abstract": "ABSTRACT Mutation testing has been widely used to assess the fault-detection effectiveness of a test suite, as well as to guide test case generation or prioritization. Empirical studies have shown that, while mutants are generally representative of real faults, an effective application of mutation testing requires \u201ctraditional\u201d operators designed for programming languages to be augmented with operators specific to an application domain and/or technology. This paper proposes MDroid+, a framework for effective mutation testing of Android apps. First, we systematically devise a taxonomy of 262 types of Android faults grouped in 14 categories by manually analyzing 2,023 so ware artifacts from different sources (e.g., bug reports, commits). Then, we identified a set of 38 mutation operators, and implemented an infrastructure to automatically seed mutations in Android apps with 35 of the identified operators. The taxonomy and the proposed operators have been evaluated in terms of stillborn/trivial mutants generated as compared to well know mutation tools, and their capacity to represent real faults in Android apps"}, {"id": "conf/sigsoft/SuMCWYYPLS17", "title": "Guided, stochastic model-based GUI testing of Android apps.", "authors": ["Ting Su", "Guozhu Meng", "Yuting Chen", "Ke Wu", "Weiming Yang", "Yao Yao", "Geguang Pu", "Yang Liu", "Zhendong Su"], "DOIs": ["https://doi.org/10.1145/3106237.3106298"], "tag": ["Research Papers"], "abstract": "ABSTRACT Mobile apps are ubiquitous, operate in complex environments and are developed under the time-to-market pressure. Ensuring their correctness and reliability thus becomes an important challenge. This paper introduces Stoat, a novel guided approach to perform stochastic model-based testing on Android apps. Stoat operates in two phases: (1) Given an app as input, it uses dynamic analysis enhanced by a weighted UI exploration strategy and static analysis to reverse engineer a stochastic model of the app's GUI interactions; and (2) it adapts Gibbs sampling to iteratively mutate/refine the stochastic model and guides test generation from the mutated models toward achieving high code and model coverage and exhibiting diverse sequences. During testing, system-level events are randomly injected to further enhance the testing effectiveness.  Stoat was evaluated on 93 open-source apps. The results show (1) the models produced by Stoat cover 17~31% more code than those by existing modeling tools; (2) Stoat detects 3X more unique crashes than two state-of-the-art testing tools, Monkey and Sapienz. Furthermore, Stoat tested 1661 most popular Google Play apps, and detected 2110 previously unknown and unique crashes. So far, 43 developers have responded that they are investigating our reports. 20 of reported crashes have been confirmed, and 8 already fixed."}, {"id": "conf/sigsoft/NairMSA17", "title": "Using bad learners to find good configurations.", "authors": ["Vivek Nair", "Tim Menzies", "Norbert Siegmund", "Sven Apel"], "DOIs": ["https://doi.org/10.1145/3106237.3106238"], "tag": ["Research Papers"], "abstract": "ABSTRACT Finding the optimally performing configuration of a software system for a given setting is often challenging. Recent approaches address this challenge by learning performance models based on a sample set of configurations. However, building an accurate performance model can be very expensive (and is often infeasible in practice). The central insight of this paper is that exact performance values (e.g., the response time of a software system) are not required to rank configurations and to identify the optimal one. As shown by our experiments, performance models that are cheap to learn but inaccurate (with respect to the difference between actual and predicted performance) can still be used rank configurations and hence find the optimal configuration. This novel rank-based approach allows us to significantly reduce the cost (in terms of number of measurements of sample configuration) as well as the time required to build performance models. We evaluate our approach with 21 scenarios based on 9 software systems and demonstrate that our approach is beneficial in 16 scenarios; for the remaining 5 scenarios, an accurate model can be built by using very few samples anyway, without the need for a rank-based approach."}, {"id": "conf/sigsoft/SiegmundSA17", "title": "Attributed variability models: outside the comfort zone.", "authors": ["Norbert Siegmund", "Stefan Sobernig", "Sven Apel"], "DOIs": ["https://doi.org/10.1145/3106237.3106251"], "tag": ["Research Papers"], "abstract": "ABSTRACT Variability models are often enriched with attributes, such as performance, that encode the influence of features on the respective attribute. In spite of their importance, there are only few attributed variability models available that have attribute values obtained from empirical, real-world observations and that cover interactions between features. But, what does it mean for research and practice when staying in the comfort zone of developing algorithms and tools in a setting where artificial attribute values are used and where interactions are neglected? This is the central question that we want to answer here. To leave the comfort zone, we use a combination of kernel density estimation and a genetic algorithm to rescale a given (real-world) attribute-value profile to a given variability model. To demonstrate the influence and relevance of realistic attribute values and interactions, we present a replication of a widely recognized, third-party study, into which we introduce realistic attribute values and interactions. We found statistically significant differences between the original study and the replication. We infer lessons learned to conduct experiments that involve attributed variability models. We also provide the accompanying tool Thor for generating attribute values including interactions. Our solution is shown to be agnostic about the given input distribution and to scale to large variability models."}, {"id": "conf/sigsoft/Gazzillo17", "title": "Kmax: finding all configurations of Kbuild makefiles statically.", "authors": ["Paul Gazzillo"], "DOIs": ["https://doi.org/10.1145/3106237.3106283"], "tag": ["Research Papers"], "abstract": "ABSTRACT Feature-oriented software design is a useful paradigm for building and reasoning about highly-configurable software. By making variability explicit, feature-oriented tools and languages make program analysis tasks easier, such as bug-finding, maintenance, and more. But critical software, such as Linux, coreboot, and BusyBox rely instead on brittle tools, such as Makefiles, to encode variability, impeding variability-aware tool development. Summarizing Makefile behavior for all configurations is difficult, because Makefiles have unusual semantics, and exhaustive enumeration of all configurations is intractable in practice. Existing approaches use ad-hoc heuristics, missing much of the encoded variability in Makefiles. We present Kmax, a new static analysis algorithm and tool for Kbuild Makefiles. It is a family-based variability analysis algorithm, where paths are Boolean expressions of configuration options, called reaching configurations, and its abstract state enumerates string values for all configurations. Kmax localizes configuration explosion to the statement level, making precise analysis tractable. The implementation analyzes Makefiles from the Kbuild build system used by several low-level systems projects. Evaluation of Kmax on the Linux and BusyBox build systems shows it to be accurate, precise, and fast. It is the first tool to collect all source files and their configurations from Linux. Compared to previous approaches, Kmax is far more accurate and precise, performs with little overhead, and scales better."}, {"id": "conf/sigsoft/KnuppelTMMS17", "title": "Is there a mismatch between real-world feature models and product-line research?", "authors": ["Alexander Kn\u00fcppel", "Thomas Th\u00fcm", "Stephan Mennicke", "Jens Meinicke", "Ina Schaefer"], "DOIs": ["https://doi.org/10.1145/3106237.3106252"], "tag": ["Research Papers"], "abstract": "ABSTRACT Feature modeling has emerged as the de-facto standard to compactly capture the variability of a software product line. Multiple feature modeling languages have been proposed that evolved over the last decades to manage industrial-size product lines. However, less expressive languages, solely permitting require and exclude constraints, are permanently and carelessly used in product-line research. We address the problem whether those less expressive languages are sufficient for industrial product lines. We developed an algorithm to eliminate complex cross-tree constraints in a feature model, enabling the combination of tools and algorithms working with different feature model dialects in a plug-and-play manner. However, the scope of our algorithm is limited. Our evaluation on large feature models, including the Linux kernel, gives evidence that require and exclude constraints are not sufficient to express real-world feature models. Hence, we promote that research on feature models needs to consider arbitrary propositional formulas as cross-tree constraints prospectively."}, {"id": "conf/sigsoft/CaiCZ17", "title": "Adaptively generating high quality fixes for atomicity violations.", "authors": ["Yan Cai", "Lingwei Cao", "Jing Zhao"], "DOIs": ["https://doi.org/10.1145/3106237.3106239"], "tag": ["Research Papers"], "abstract": "ABSTRACT It is difficult to fix atomicity violations correctly. Existing gate lock algorithm (GLA) simply inserts gate locks to serialize exe-cutions, which may introduce performance bugs and deadlocks. Synthesized context-aware gate locks (by Grail) require complex source code synthesis. We propose \uf061Fixer to adaptively fix ato-micity violations. It firstly analyses the lock acquisitions of an atomicity violation. Then it either adjusts the existing lock scope or inserts a gate lock. The former addresses cases where some locks are used but fail to provide atomic accesses. For the latter, it infers the visibility (being global or a field of a class/struct) of the gate lock such that the lock only protects related accesses. For both cases, \uf061Fixer further eliminates new lock orders to avoid introducing deadlocks. Of course, \uf061Fixer can produce both kinds of fixes on atomicity violations with locks. The experi-mental results on 15 previously used atomicity violations show that: \uf061Fixer correctly fixed all 15 atomicity violations without introducing deadlocks. However, GLA and Grail both intro-duced 5 deadlocks. HFix (that only targets on fixing certain types of atomicity violations) only fixed 2 atomicity violations and introduced 4 deadlocks. \uf061Fixer also provides an alternative way to insert gate locks (by inserting gate locks with proper visibility) considering fix acceptance."}, {"id": "conf/sigsoft/GuoCY17", "title": "AtexRace: across thread and execution sampling for in-house race detection.", "authors": ["Yu Guo", "Yan Cai", "Zijiang Yang"], "DOIs": ["https://doi.org/10.1145/3106237.3106242"], "tag": ["Research Papers"], "abstract": "ABSTRACT Data race is a major source of concurrency bugs. Dynamic data race detection tools (e.g., FastTrack) monitor the execu-tions of a program to report data races occurring in runtime. However, such tools incur significant overhead that slows down and perturbs executions. To address the issue, the state-of-the-art dynamic data race detection tools (e.g., LiteRace) ap-ply sampling techniques to selectively monitor memory access-es. Although they reduce overhead, they also miss many data races as confirmed by existing studies. Thus, practitioners face a dilemma on whether to use FastTrack, which detects more data races but is much slower, or LiteRace, which is faster but detects less data races. In this paper, we propose a new sam-pling approach to address the major limitations of current sampling techniques, which ignore the facts that a data race involves two threads and a program under testing is repeatedly executed. We develop a tool called AtexRace to sample memory accesses across both threads and executions. By selectively monitoring the pairs of memory accesses that have not been frequently observed in current and previous executions, AtexRace detects as many data races as FastTrack at a cost as low as LiteRace. We have compared AtexRace against FastTrack and LiteRace on both Parsec benchmark suite and a large-scale real-world MySQL Server with 223 test cases. The experiments confirm that AtexRace can be a replacement of FastTrack and LiteRace."}, {"id": "conf/sigsoft/GuoWW17", "title": "Symbolic execution of programmable logic controller code.", "authors": ["Shengjian Guo", "Meng Wu", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3106237.3106245"], "tag": ["Research Papers"], "abstract": "ABSTRACT Programmable logic controllers (PLCs) are specialized computers for automating a wide range of cyber-physical systems. Since these systems are often safety-critical, software running on PLCs need to be free of programming errors. However, automated tools for testing PLC software are lacking despite the pervasive use of PLCs in industry. We propose a symbolic execution based method, named SymPLC, for automatically testing PLC software written in programming languages specified in the IEC 61131-3 standard. SymPLC takes the PLC source code as input and translates it into C before applying symbolic execution, to systematically generate test inputs that cover both paths in each periodic task and interleavings of these tasks. Toward this end, we propose a number of PLC-specific reduction techniques for identifying and eliminating redundant interleavings. We have evaluated SymPLC on a large set of benchmark programs with both single and multiple tasks. Our experiments show that SymPLC can handle these programs efficiently, and for multi-task PLC programs, our new reduction techniques outperform the state-of-the-art partial order reduction technique by more than two orders of magnitude."}, {"id": "conf/sigsoft/KusanoW17", "title": "Thread-modular static analysis for relaxed memory models.", "authors": ["Markus Kusano", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3106237.3106243"], "tag": ["Research Papers"], "abstract": "ABSTRACT We propose a memory-model-aware static program analysis method for accurately analyzing the behavior of concurrent software running on processors with weak consistency models such as x86-TSO, SPARC-PSO, and SPARC-RMO. At the center of our method is a unified framework for deciding the feasibility of inter-thread interferences to avoid propagating spurious data flows during static analysis and thus boost the performance of the static analyzer. We formulate the checking of interference feasibility as a set of Datalog rules which are both efficiently solvable and general enough to capture a range of hardware-level memory models. Compared to existing techniques, our method can significantly reduce the number of bogus alarms as well as unsound proofs. We implemented the method and evaluated it on a large set of multithreaded C programs. Our experiments show the method significantly outperforms state-of-the-art techniques in terms of accuracy with only moderate runtime overhead."}, {"id": "conf/sigsoft/AliabadiKGP17", "title": "ARTINALI: dynamic invariant detection for cyber-physical system security.", "authors": ["Maryam Raiyat Aliabadi", "Amita Ajith Kamath", "Julien Gascon-Samson", "Karthik Pattabiraman"], "DOIs": ["https://doi.org/10.1145/3106237.3106282"], "tag": ["Research Papers"], "abstract": "ABSTRACT Cyber-Physical Systems (CPSes) are being widely deployed in security critical scenarios such as smart homes and medical devices. Unfortunately, the connectedness of these systems and their relative lack of security measures makes them ripe targets for attacks. Specification-based Intrusion Detection Systems (IDS) have been shown to be effective for securing CPSs. Unfortunately, deriving invariants for capturing the specifications of CPS systems is a tedious and error-prone process. Therefore, it is important to dynamically monitor the CPS system to learn its common behaviors and formulate invariants for detecting security attacks. Existing techniques for invariant mining only incorporate data and events, but not time. However, time is central to most CPS systems, and hence incorporating time in addition to data and events, is essential for achieving low false positives and false negatives. This paper proposes ARTINALI, which mines dynamic system properties by incorporating time as a first-class property of the system. We build ARTINALI-based Intrusion Detection Systems (IDSes) for two CPSes, namely smart meters and smart medical devices, and measure their efficacy. We find that the ARTINALI-based IDSes significantly reduce the ratio of false positives and false negatives by 16 to 48% (average 30.75%) and 89 to 95% (average 93.4%) respectively over other dynamic invariant detection tools."}, {"id": "conf/sigsoft/KuventMR17", "title": "A symbolic justice violations transition system for unrealizable GR(1) specifications.", "authors": ["Aviv Kuvent", "Shahar Maoz", "Jan Oliver Ringert"], "DOIs": ["https://doi.org/10.1145/3106237.3106240"], "tag": ["Research Papers"], "abstract": "ABSTRACT One of the main challenges of reactive synthesis, an automated procedure to obtain a correct-by-construction reactive system, is to deal with unrealizable specifications. Existing approaches to deal with unrealizability, in the context of GR(1), an expressive assume-guarantee fragment of LTL that enables efficient synthesis, include the generation of concrete counter-strategies and the computation of an unrealizable core. Although correct, such approaches produce large and complicated counter-strategies, often containing thousands of states. This hinders their use by engineers.  In this work we present the Justice Violations Transition System (JVTS), a novel symbolic representation of counter-strategies for GR(1). The JVTS is much smaller and simpler than its corresponding concrete counter-strategy. Moreover, it is annotated with invariants that explain how the counter-strategy forces the system to violate the specification. We compute the JVTS symbolically, and thus more efficiently, without the expensive enumeration of concrete states. Finally, we provide the JVTS with an on-demand interactive concrete and symbolic play.  We implemented our work, validated its correctness, and evaluated it on 14 unrealizable specifications of autonomous Lego robots as well as on benchmarks from the literature. The evaluation shows not only that the JVTS is in most cases much smaller than the corresponding concrete counter-strategy, but also that its computation is faster."}, {"id": "conf/sigsoft/MaggioPFH17", "title": "Automated control of multiple software goals using multiple actuators.", "authors": ["Martina Maggio", "Alessandro Vittorio Papadopoulos", "Antonio Filieri", "Henry Hoffmann"], "DOIs": ["https://doi.org/10.1145/3106237.3106247"], "tag": ["Research Papers"], "abstract": "ABSTRACT Modern software should satisfy multiple goals simultaneously: it should provide predictable performance, be robust to failures, handle peak loads and deal seamlessly with unexpected conditions and changes in the execution environment. For this to happen, software designs should account for the possibility of runtime changes and provide formal guarantees of the software's behavior. Control theory is one of the possible design drivers for runtime adaptation, but adopting control theoretic principles often requires additional, specialized knowledge. To overcome this limitation, automated methodologies have been proposed to extract the necessary information from experimental data and design a control system for runtime adaptation. These proposals, however, only process one goal at a time, creating a chain of controllers. In this paper, we propose and evaluate the first automated strategy that takes into account multiple goals without separating them into multiple control strategies. Avoiding the separation allows us to tackle a larger class of problems and provide stronger guarantees. We test our methodology's generality with three case studies that demonstrate its broad applicability in meeting performance, reliability, quality, security, and energy goals despite environmental or requirements changes."}, {"id": "conf/sigsoft/AbdalkareemNWMS17", "title": "Why do developers use trivial packages? an empirical case study on npm.", "authors": ["Rabe Abdalkareem", "Olivier Nourry", "Sultan Wehaibi", "Suhaib Mujahid", "Emad Shihab"], "DOIs": ["https://doi.org/10.1145/3106237.3106267"], "tag": ["Research Papers"], "abstract": "ABSTRACT Code reuse is traditionally seen as good practice. Recent trends have pushed the concept of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call `trivial packages'. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix made it imperative to question the growing use of trivial packages.  Therefore, in this paper, we mine more than 230,000 npm packages and 38,000 JavaScript applications in order to study the prevalence of trivial packages. We found that trivial packages are common and are increasing in popularity, making up 16.8% of the studied npm packages. We performed a survey with 88 Node.js developers who use trivial packages to understand the reasons and drawbacks of their use. Our survey revealed that trivial packages are used because they are perceived to be well implemented and tested pieces of code. However, developers are concerned about maintaining and the risks of breakages due to the extra dependencies trivial packages introduce. To objectively verify the survey results, we empirically validate the most cited reason and drawback and find that, contrary to developers' beliefs, only 45.2% of trivial packages even have tests. However, trivial packages appear to be `deployment tested' and to have similar test, usage and community interest as non-trivial packages. On the other hand, we found that 11.5% of the studied trivial packages have more than 20 dependencies. Hence, developers should be careful about which trivial packages they decide to use."}, {"id": "conf/sigsoft/ChaparroLZMPMBN17", "title": "Detecting missing information in bug descriptions.", "authors": ["Oscar Chaparro", "Jing Lu", "Fiorella Zampetti", "Laura Moreno", "Massimiliano Di Penta", "Andrian Marcus", "Gabriele Bavota", "Vincent Ng"], "DOIs": ["https://doi.org/10.1145/3106237.3106285"], "tag": ["Research Papers"], "abstract": "ABSTRACT Bug reports document unexpected software behaviors experienced by users. To be effective, they should allow bug triagers to easily understand and reproduce the potential reported bugs, by clearly describing the Observed Behavior (OB), the Steps to Reproduce (S2R), and the Expected Behavior (EB). Unfortunately, while considered extremely useful, reporters often miss such pieces of information in bug reports and, to date, there is no effective way to automatically check and enforce their presence. We manually analyzed nearly 3k bug reports to understand to what extent OB, EB, and S2R are reported in bug reports and what discourse patterns reporters use to describe such information. We found that (i) while most reports contain OB (i.e., 93.5%), only 35.2% and 51.4% explicitly describe EB and S2R, respectively; and (ii) reporters recurrently use 154 discourse patterns to describe such content. Based on these findings, we designed and evaluated an automated approach to detect the absence (or presence) of EB and S2R in bug descriptions. With its best setting, our approach is able to detect missing EB (S2R) with 85.9% (69.2%) average precision and 93.2% (83%) average recall. Our approach intends to improve bug descriptions quality by alerting reporters about missing EB and S2R at reporting time."}, {"id": "conf/sigsoft/ZibaeenejadZA17", "title": "Continuous variable-specific resolutions of feature interactions.", "authors": ["Mohammad Hadi Zibaeenejad", "Chi Zhang", "Joanne M. Atlee"], "DOIs": ["https://doi.org/10.1145/3106237.3106302"], "tag": ["Research Papers"], "abstract": "ABSTRACTSystems that are assembled from independently developed features suffer from feature interactions, in which features affect one another's behaviour in surprising ways. The Feature Interaction Problem results from trying to implement an appropriate resolution for each interaction within each possible context, because the number of possible contexts to consider increases exponentially with the number of features in the system. Resolution strategies aim to combat the Feature Interaction Problem by offering default strategies that resolve entire classes of interactions, thereby reducing the work needed to resolve lots of interactions. However most such approaches employ coarse-grained resolution strategies (e.g., feature priority) or a centralized arbitrator. Our work focuses on employing variable-specific default-resolution strategies that aim to resolve at runtime features- conflicting actions on a system's outputs. In this paper, we extend prior work to enable co-resolution of interactions on coupled output variables and to promote smooth continuous resolutions over execution paths. We implemented our approach within the PreScan simulator and performed a case study involving 15 automotive features; this entailed our devising and implementing three resolution strategies for three output variables. The results of the case study show that the approach produces smooth and continuous resolutions of interactions throughout interesting scenarios."}, {"id": "conf/sigsoft/BagherzadehHD17", "title": "Model-level, platform-independent debugging in the context of the model-driven development of real-time systems.", "authors": ["Mojtaba Bagherzadeh", "Nicolas Hili", "Juergen Dingel"], "DOIs": ["https://doi.org/10.1145/3106237.3106278"], "tag": ["Research Papers"], "abstract": "ABSTRACT Providing proper support for debugging models at model-level is one of the main barriers to a broader adoption of Model Driven Development (MDD). In this paper, we focus on the use of MDD for the development of real-time embedded systems (RTE). We introduce a new platform-independent approach to implement model-level debuggers. We describe how to realize support for model-level debugging entirely in terms of the modeling language and show how to implement this support in terms of a model-to-model transformation. Key advantages of the approach over existing work are that (1) it does not require a program debugger for the code generated from the model, and that (2) any changes to, e.g., the code generator, the target language, or the hardware platform leave the debugger completely unaffected. We also describe an implementation of the approach in the context of Papyrus-RT, an open source MDD tool based on the modeling language UML-RT. We summarize the results of the use of our model-based debugger on several use cases to determine its overhead in terms of size and performance. Despite being a prototype, the performance overhead is in the order of microseconds, while the size overhead is comparable with that of GDB, the GNU Debugger."}, {"id": "conf/sigsoft/SorensenED17", "title": "Cooperative kernels: GPU multitasking for blocking algorithms.", "authors": ["Tyler Sorensen", "Hugues Evrard", "Alastair F. Donaldson"], "DOIs": ["https://doi.org/10.1145/3106237.3106265"], "tag": ["Research Papers"], "abstract": "ABSTRACT There is growing interest in accelerating irregular data-parallel algorithms on GPUs. These algorithms are typically blocking, so they require fair scheduling. But GPU programming models (e.g. OpenCL) do not mandate fair scheduling, and GPU schedulers are unfair in practice. Current approaches avoid this issue by exploiting scheduling quirks of today's GPUs in a manner that does not allow the GPU to be shared with other workloads (such as graphics rendering tasks). We propose cooperative kernels, an extension to the traditional GPU programming model geared towards writing blocking algorithms. Workgroups of a cooperative kernel are fairly scheduled, and multitasking is supported via a small set of language extensions through which the kernel and scheduler cooperate. We describe a prototype implementation of a cooperative kernel framework implemented in OpenCL 2.0 and evaluate our approach by porting a set of blocking GPU applications to cooperative kernels and examining their performance under multitasking. Our prototype exploits no vendor-specific hardware, driver or compiler support, thus our results provide a lower-bound on the efficiency with which cooperative kernels can be implemented in practice."}, {"id": "conf/sigsoft/GarbervetskyZL17", "title": "Toward full elasticity in distributed static analysis: the case of callgraph analysis.", "authors": ["Diego Garbervetsky", "Edgardo Zoppi", "Benjamin Livshits"], "DOIs": ["https://doi.org/10.1145/3106237.3106261"], "tag": ["Research Papers"], "abstract": "ABSTRACT In this paper we present the design and implementation of a distributed, whole-program static analysis framework that is designed to scale with the size of the input. Our approach is based on the actor programming model and is deployed in the cloud. Our reliance on a cloud cluster provides a degree of elasticity for CPU, memory, and storage resources. To demonstrate the potential of our technique, we show how a typical call graph analysis can be implemented in a distributed setting. The vision that motivates this work is that every large-scale software repository such as GitHub, BitBucket, or Visual Studio Online will be able to perform static analysis on a large scale. We experimentally validate our implementation of the distributed call graph analysis using a combination of both synthetic and real benchmarks. To show scalability, we demonstrate how the analysis presented in this paper is able to handle inputs that are almost 10 million lines of code (LOC) in size, without running out of memory. Our results show that the analysis scales well in terms of memory pressure independently of the input size, as we add more virtual machines (VMs). As the number of worker VMs increases, we observe that the analysis time generally improves as well. Lastly, we demonstrate that querying the results can be performed with a median latency of 15 ms."}, {"id": "conf/sigsoft/LlerenaSR17", "title": "Probabilistic model checking of perturbed MDPs with applications to cloud computing.", "authors": ["Yamilet R. Serrano Llerena", "Guoxin Su", "David S. Rosenblum"], "DOIs": ["https://doi.org/10.1145/3106237.3106301"], "tag": ["Research Papers"], "abstract": "ABSTRACTProbabilistic model checking is a formal verification technique that has been applied successfully in a variety of domains, providing identification of system errors through quantitative verification of stochastic system models. One domain that can benefit from probabilistic model checking is cloud computing, which must provide highly reliable and secure computational and storage services to large numbers of mission-critical software systems. For real-world domains like cloud computing, external system factors and environmental changes must be estimated accurately in the form of probabilities in system models; inaccurate estimates for the model probabilities can lead to invalid verification results. To address the effects of uncertainty in probability estimates, in previous work we have developed a variety of techniques for perturbation analysis of discrete- and continuous-time Markov chains (DTMCs and CTMCs). These techniques determine the consequences of the uncertainty on verification of system properties. In this paper, we present the first approach for perturbation analysis of Markov decision processes (MDPs), a stochastic formalism that is especially popular due to the significant expressive power it provides through the combination of both probabilistic and nondeterministic choice. Our primary contribution is a novel technique for efficiently analyzing the effects of perturbations of model probabilities on verification of reachability properties of MDPs. The technique heuristically explores the space of adversaries of an MDP, which encode the different ways of resolving the MDP's nondeterministic choices. We demonstrate the practical effectiveness of our approach by applying it to two case studies of cloud systems."}, {"id": "conf/sigsoft/CedrimGMGSMFRC17", "title": "Understanding the impact of refactoring on smells: a longitudinal study of 23 software projects.", "authors": ["Diego Cedrim", "Alessandro Garcia", "Melina Mongiovi", "Rohit Gheyi", "Leonardo da Silva Sousa", "Rafael Maiani de Mello", "Baldoino Fonseca", "M\u00e1rcio Ribeiro", "Alexander Ch\u00e1vez"], "DOIs": ["https://doi.org/10.1145/3106237.3106259"], "tag": ["Research Papers"], "abstract": "ABSTRACTCode smells in a program represent indications of structural quality problems, which can be addressed by software refactoring. However, refactoring intends to achieve different goals in practice, and its application may not reduce smelly structures. Developers may neglect or end up creating new code smells through refactoring. Unfortunately, little has been reported about the beneficial and harmful effects of refactoring on code smells. This paper reports a longitudinal study intended to address this gap. We analyze how often commonly-used refactoring types affect the density of 13 types of code smells along the version histories of 23 projects. Our findings are based on the analysis of 16,566 refactorings distributed in 10 different types. Even though 79.4% of the refactorings touched smelly elements, 57% did not reduce their occurrences. Surprisingly, only 9.7% of refactorings removed smells, while 33.3% induced the introduction of new ones. More than 95% of such refactoring-induced smells were not removed in successive commits, which suggest refactorings tend to more frequently introduce long-living smells instead of eliminating existing ones. We also characterized and quantified typical refactoring-smell patterns, and observed that harmful patterns are frequent, including: (i) approximately 30% of the Move Method and Pull Up Method refactorings induced the emergence of God Class, and (ii) the Extract Superclass refactoring creates the smell Speculative Generality in 68% of the cases."}, {"id": "conf/sigsoft/RastogiDCJM17", "title": "Cimplifier: automatically debloating containers.", "authors": ["Vaibhav Rastogi", "Drew Davidson", "Lorenzo De Carli", "Somesh Jha", "Patrick D. McDaniel"], "DOIs": ["https://doi.org/10.1145/3106237.3106271"], "tag": ["Research Papers"], "abstract": "ABSTRACTApplication containers, such as those provided by Docker, have recently gained popularity as a solution for agile and seamless software deployment. These light-weight virtualization environments run applications that are packed together with their resources and configuration information, and thus can be deployed across various software platforms. Unfortunately, the ease with which containers can be created is oftentimes a double-edged sword, encouraging the packaging of logically distinct applications, and the inclusion of significant amount of unnecessary components, within a single container. These practices needlessly increase the container size-sometimes by orders of magnitude. They also decrease the overall security, as each included component-necessary or not-may bring in security issues of its own, and there is no isolation between multiple applications packaged within the same container image. We propose algorithms and a tool called Cimplifier, which address these concerns: given a container and simple user-defined constraints, our tool partitions it into simpler containers, which (i) are isolated from each other, only communicating as necessary, and (ii) only include enough resources to perform their functionality. Our evaluation on real-world containers demonstrates that Cimplifier preserves the original functionality, leads to reduction in image size of up to 95%, and processes even large containers in under thirty seconds."}, {"id": "conf/sigsoft/DietschHMNP17", "title": "Craig vs. Newton in software model checking.", "authors": ["Daniel Dietsch", "Matthias Heizmann", "Betim Musa", "Alexander Nutz", "Andreas Podelski"], "DOIs": ["https://doi.org/10.1145/3106237.3106307"], "tag": ["Research Papers"], "abstract": "ABSTRACTEver since the seminal work on SLAM and BLAST, software model checking with counterexample-guided abstraction refinement (CEGAR) has been an active topic of research. The crucial procedure here is to analyze a sequence of program statements (the counterexample) to find building blocks for the overall proof of the program. We can distinguish two approaches (which we name Craig and Newton) to implement the procedure. The historically first approach, Newton (named after the tool from the SLAM toolkit), is based on symbolic execution. The second approach, Craig, is based on Craig interpolation. It was widely believed that Craig is substantially more effective than Newton. In fact, 12 out of the 15 CEGAR-based tools in SV-COMP are based on Craig. Advances in software model checkers based on Craig, however, can go only lockstep with advances in SMT solvers with Craig interpolation. It may be time to revisit Newton and ask whether Newton can be as effective as Craig. We have implemented a total of 11 variants of Craig and Newton in two different state-of-the-art software model checking tools and present the outcome of our experimental comparison."}, {"id": "conf/sigsoft/GalhotraBM17", "title": "Fairness testing: testing software for discrimination.", "authors": ["Sainyam Galhotra", "Yuriy Brun", "Alexandra Meliou"], "DOIs": ["https://doi.org/10.1145/3106237.3106277"], "tag": ["Research Papers"], "abstract": "ABSTRACT This paper defines software fairness and discrimination and develops a testing-based method for measuring if and how much software discriminates, focusing on causality in discriminatory behavior. Evidence of software discrimination has been found in modern software systems that recommend criminal sentences, grant access to financial products, and determine who is allowed to participate in promotions. Our approach, Themis, generates efficient test suites to measure discrimination. Given a schema describing valid system inputs, Themis generates discrimination tests automatically and does not require an oracle. We evaluate Themis on 20 software systems, 12 of which come from prior work with explicit focus on avoiding discrimination. We find that (1) Themis is effective at discovering software discrimination, (2) state-of-the-art techniques for removing discrimination from algorithms fail in many situations, at times discriminating against as much as 98% of an input subdomain, (3) Themis optimizations are effective at producing efficient test suites for measuring discrimination, and (4) Themis is more efficient on systems that exhibit more discrimination. We thus demonstrate that fairness testing is a critical aspect of the software development cycle in domains with possible discrimination and provide initial tools for measuring software discrimination."}, {"id": "conf/sigsoft/BrownVLR17", "title": "The care and feeding of wild-caught mutants.", "authors": ["David Bingham Brown", "Michael Vaughn", "Ben Liblit", "Thomas W. Reps"], "DOIs": ["https://doi.org/10.1145/3106237.3106280"], "tag": ["Research Papers"], "abstract": "ABSTRACTMutation testing of a test suite and a program provides a way to measure the quality of the test suite. In essence, mutation testing is a form of sensitivity testing: by running mutated versions of the program against the test suite, mutation testing measures the suite's sensitivity for detecting bugs that a programmer might introduce into the program. This paper introduces a technique to improve mutation testing that we call wild-caught mutants; it provides a method for creating potential faults that are more closely coupled with changes made by actual programmers. This technique allows the mutation tester to have more certainty that the test suite is sensitive to the kind of changes that have been observed to have been made by programmers in real-world cases."}, {"id": "conf/sigsoft/WangNT17", "title": "QTEP: quality-aware test case prioritization.", "authors": ["Song Wang", "Jaechang Nam", "Lin Tan"], "DOIs": ["https://doi.org/10.1145/3106237.3106258"], "tag": ["Research Papers"], "abstract": "ABSTRACT Test case prioritization (TCP) is a practical activity in software testing for exposing faults earlier. Researchers have proposed many TCP techniques to reorder test cases. Among them, coverage-based TCPs have been widely investigated. Specifically, coverage-based TCP approaches leverage coverage information between source code and test cases, i.e., static code coverage and dynamic code coverage, to schedule test cases. Existing coverage-based TCP techniques mainly focus on maximizing coverage while often do not consider the likely distribution of faults in source code. However, software faults are not often equally distributed in source code, e.g., around 80% faults are located in about 20% source code. Intuitively, test cases that cover the faulty source code should have higher priorities, since they are more likely to find faults.  In this paper, we present a quality-aware test case prioritization technique, QTEP, to address the limitation of existing coverage-based TCP algorithms. In QTEP, we leverage code inspection techniques, i.e., a typical statistic defect prediction model and a typical static bug finder, to detect fault-prone source code and then adapt existing coverage-based TCP algorithms by considering the weighted source code in terms of fault-proneness. Our evaluation with 16 variant QTEP techniques on 33 different versions of 7 open source Java projects shows that QTEP could improve existing coverage-based TCP techniques for both regression and new test cases. Specifically, the improvement of the best variant of QTEP for regression test cases could be up to 15.0% and on average 7.6%, and for all test cases (both regression and new test cases), the improvement could be up to 10.0% and on average 5.0%."}, {"id": "conf/sigsoft/BrennanTRAB17", "title": "Constraint normalization and parameterized caching for quantitative program analysis.", "authors": ["Tegan Brennan", "Nestan Tsiskaridze", "Nicol\u00e1s Rosner", "Abdulbaki Aydin", "Tevfik Bultan"], "DOIs": ["https://doi.org/10.1145/3106237.3106303"], "tag": ["Research Papers"], "abstract": "ABSTRACT Symbolic program analysis techniques rely on satisfiability-checking constraint solvers, while quantitative program analysis techniques rely on model-counting constraint solvers. Hence, the efficiency of satisfiability checking and model counting is crucial for efficiency of modern program analysis techniques. In this paper, we present a constraint caching framework to expedite potentially expensive satisfiability and model-counting queries. Integral to this framework is our new constraint normalization procedure under which the cardinality of the solution set of a constraint, but not necessarily the solution set itself, is preserved. We extend these constraint normalization techniques to string constraints in order to support analysis of string-manipulating code. A group-theoretic framework which generalizes earlier results on constraint normalization is used to express our normalization techniques. We also present a parameterized caching approach where, in addition to storing the result of a model-counting query, we also store a model-counter object in the constraint store that allows us to efficiently recount the number of satisfying models for different maximum bounds. We implement our caching framework in our tool Cashew, which is built as an extension of the Green caching framework, and integrate it with the symbolic execution tool Symbolic PathFinder (SPF) and the model-counting constraint solver ABC. Our experiments show that constraint caching can significantly improve the performance of symbolic and quantitative program analyses. For instance, Cashew can normalize the 10,104 unique constraints in the SMC/Kaluza benchmark down to 394 normal forms, achieve a 10x speedup on the SMC/Kaluza-Big dataset, and an average 3x speedup in our SPF-based side-channel analysis experiments."}, {"id": "conf/sigsoft/GoldBHIKY17", "title": "Generalized observational slicing for tree-represented modelling languages.", "authors": ["Nicolas E. Gold", "David W. Binkley", "Mark Harman", "Syed S. Islam", "Jens Krinke", "Shin Yoo"], "DOIs": ["https://doi.org/10.1145/3106237.3106304"], "tag": ["Research Papers"], "abstract": "ABSTRACT Model-driven software engineering raises the abstraction level making complex systems easier to understand than if written in textual code. Nevertheless, large complicated software systems can have large models, motivating the need for slicing techniques that reduce the size of a model. We present a generalization of observation-based slicing that allows the criterion to be defined using a variety of kinds of observable behavior and does not require any complex dependence analysis. We apply our implementation of generalized observational slicing for tree-structured representations to Simulink models. The resulting slice might be the subset of the original model responsible for an observed failure or simply the sub-model semantically related to a classic slicing criterion. Unlike its predecessors, the algorithm is also capable of slicing embedded Stateflow state machines. A study of nine real-world models drawn from four different application domains demonstrates the effectiveness of our approach at dramatically reducing Simulink model sizes for realistic observation scenarios: for 9 out of 20 cases, the resulting model has fewer than 25% of the original model's elements."}, {"id": "conf/sigsoft/AlrajehPN17", "title": "On evidence preservation requirements for forensic-ready systems.", "authors": ["Dalal Alrajeh", "Liliana Pasquale", "Bashar Nuseibeh"], "DOIs": ["https://doi.org/10.1145/3106237.3106308"], "tag": ["Research Papers"], "abstract": "ABSTRACT Forensic readiness denotes the capability of a system to support digital forensic investigations of potential, known incidents by preserving in advance data that could serve as evidence explaining how an incident occurred. Given the increasing rate at which (potentially criminal) incidents occur, designing so\u2030ware systems that are forensic-ready can facilitate and reduce the costs of digital forensic investigations. However, to date, little or no attention has been given to how forensic-ready so\u2030ftware systems can be designed systematically. In this paper we propose to explicitly represent evidence preservation requirements prescribing preservation of the minimal amount of data that would be relevant to a future digital investigation. We formalise evidence preservation requirements and propose an approach for synthesising specifications for systems to meet these requirements. We present our prototype implementation\u2014based on a satisfiability solver and a logic-based learner\u2014which we use to evaluate our approach, applying it to two digital forensic corpora. Our evaluation suggests that our approach preserves relevant data that could support hypotheses of potential incidents. Moreover, it enables significant reduction in the volume of data that would need to be examined during an investigation."}, {"id": "conf/sigsoft/PastoreMM17", "title": "BDCI: behavioral driven conflict identification.", "authors": ["Fabrizio Pastore", "Leonardo Mariani", "Daniela Micucci"], "DOIs": ["https://doi.org/10.1145/3106237.3106296"], "tag": ["Research Papers"], "abstract": "ABSTRACT Source Code Management (SCM) systems support software evolution by providing features, such as version control, branching, and conflict detection. Despite the presence of these features, support to parallel software development is often limited. SCM systems can only address a subset of the conflicts that might be introduced by developers when concurrently working on multiple parallel branches. In fact, SCM systems can detect textual conflicts, which are generated by the concurrent modification of the same program locations, but they are unable to detect higher-order conflicts, which are generated by the concurrent modification of different program locations that generate program misbehaviors once merged. Higher-order conflicts are painful to detect and expensive to fix because they might be originated by the interference of apparently unrelated changes.  In this paper we present Behavioral Driven Conflict Identification (BDCI), a novel approach to conflict detection. BDCI moves the analysis of conflicts from the source code level to the level of program behavior by generating and comparing behavioral models. The analysis based on behavioral models can reveal interfering changes as soon as they are introduced in the SCM system, even if they do not introduce any textual conflict.  To evaluate the effectiveness and the cost of the proposed approach, we developed BDCIf, a specific instance of BDCI dedicated to the detection of higher-order conflicts related to the functional behavior of a program. The evidence collected by analyzing multiple versions of Git and Redis suggests that BDCIf can effectively detect higher-order conflicts and report how changes might interfere."}, {"id": "conf/sigsoft/DAntoniSV17", "title": "NoFAQ: synthesizing command repairs from examples.", "authors": ["Loris D'Antoni", "Rishabh Singh", "Michael Vaughn"], "DOIs": ["https://doi.org/10.1145/3106237.3106241"], "tag": ["Research Papers"], "abstract": "ABSTRACT Command-line tools are confusing and hard to use due to their cryptic error messages and lack of documentation. Novice users often resort to online help-forums for finding corrections to their buggy commands, but have a hard time in searching precisely for posts that are relevant to their problem and then applying the suggested solutions to their buggy command. We present NoFAQ, a tool that uses a set of rules to suggest possible fixes when users write buggy commands that trigger commonly occurring errors. The rules are expressed in a language called FIXIT and each rule pattern-matches against the user's buggy command and corresponding error message, and uses these inputs to produce a possible fixed command. NoFAQ automatically learns FIXIT rules from examples of buggy and repaired commands. We evaluate NoFAQ on two fronts. First, we use 92 benchmark problems drawn from an existing tool and show that NoFAQ is able to synthesize rules for 81 benchmark problems in real time using just 2 to 5 input-output examples for each rule. Second, we run our learning algorithm on the examples obtained through a crowd-sourcing interface and show that the learning algorithm scales to large sets of examples."}, {"id": "conf/sigsoft/LeCLGV17", "title": "S3: syntax- and semantic-guided repair synthesis via programming by examples.", "authors": ["Xuan-Bach D. Le", "Duc-Hiep Chu", "David Lo", "Claire Le Goues", "Willem Visser"], "DOIs": ["https://doi.org/10.1145/3106237.3106309"], "tag": ["Research Papers"], "abstract": "ABSTRACT A notable class of techniques for automatic program repair is known as semantics-based. Such techniques, e.g., Angelix, infer semantic specifications via symbolic execution, and then use program synthesis to construct new code that satisfies those inferred specifications. However, the obtained specifications are naturally incomplete, leaving the synthesis engine with a difficult task of synthesizing a general solution from a sparse space of many possible solutions that are consistent with the provided specifications but that do not necessarily generalize. We present S3, a new repair synthesis engine that leverages programming-by-examples methodology to synthesize high-quality bug repairs. The novelty in S3 that allows it to tackle the sparse search space to create more general repairs is three-fold: (1) A systematic way to customize and constrain the syntactic search space via a domain-specific language, (2) An efficient enumeration- based search strategy over the constrained search space, and (3) A number of ranking features based on measures of the syntactic and semantic distances between candidate solutions and the original buggy program. We compare S3\u2019s repair effectiveness with state-of-the-art synthesis engines Angelix, Enumerative, and CVC4. S3 can successfully and correctly fix at least three times more bugs than the best baseline on datasets of 52 bugs in small programs, and 100 bugs in real-world large programs."}, {"id": "conf/sigsoft/NguyenARH17", "title": "Counterexample-guided approach to finding numerical invariants.", "authors": ["ThanhVu Nguyen", "Timos Antonopoulos", "Andrew Ruef", "Michael Hicks"], "DOIs": ["https://doi.org/10.1145/3106237.3106281"], "tag": ["Research Papers"], "abstract": "ABSTRACT Numerical invariants, e.g., relationships among numerical variables in a program, represent a useful class of properties to analyze programs. General polynomial invariants represent more complex numerical relations, but they are often required in many scientific and engineering applications. We present NumInv, a tool that implements a counterexample-guided invariant generation (CEGIR) technique to automatically discover numerical invariants, which are polynomial equality and inequality relations among numerical variables. This CEGIR technique infers candidate invariants from program traces and then checks them against the program source code using the KLEE test-input generation tool. If the invariants are incorrect KLEE returns counterexample traces, which help the dynamic inference obtain better results. Existing CEGIR approaches often require sound invariants, however NumInv sacrifices soundness and produces results that KLEE cannot refute within certain time bounds. This design and the use of KLEE as a verifier allow NumInv to discover useful and important numerical invariants for many challenging programs.  Preliminary results show that NumInv generates required invariants for understanding and verifying correctness of programs involving complex arithmetic. We also show that NumInv discovers polynomial invariants that capture precise complexity bounds of programs used to benchmark existing static complexity analysis techniques. Finally, we show that NumInv performs competitively comparing to state of the art numerical invariant analysis tools."}, {"id": "conf/sigsoft/SmithFA17", "title": "Discovering relational specifications.", "authors": ["Calvin Smith", "Gabriel Ferns", "Aws Albarghouthi"], "DOIs": ["https://doi.org/10.1145/3106237.3106279"], "tag": ["Research Papers"], "abstract": "ABSTRACTFormal specifications of library functions play a critical role in a number of program analysis and development tasks. We present Bach, a technique for discovering likely relational specifications from data describing input-output behavior of a set of functions comprising a library or a program. Relational specifications correlate different executions of different functions; for instance, commutativity, transitivity, equivalence of two functions, etc. Bach combines novel insights from program synthesis and databases to discover a rich array of specifications. We apply Bach to learn specifications from data generated for a number of standard libraries. Our experimental evaluation demonstrates Bach's ability to learn useful and deep specifications in a small amount of time."}, {"id": "conf/sigsoft/LiCCLLT17", "title": "Steelix: program-state based binary fuzzing.", "authors": ["Yuekang Li", "Bihuan Chen", "Mahinthan Chandramohan", "Shang-Wei Lin", "Yang Liu", "Alwen Tiu"], "DOIs": ["https://doi.org/10.1145/3106237.3106295"], "tag": ["Research Papers"], "abstract": "ABSTRACT Coverage-based fuzzing is one of the most effective techniques to find vulnerabilities, bugs or crashes. However, existing techniques suffer from the difficulty in exercising the paths that are protected by magic bytes comparisons (e.g., string equality comparisons). Several approaches have been proposed to use heavy-weight program analysis to break through magic bytes comparisons, and hence are less scalable. In this paper, we propose a program-state based binary fuzzing approach, named Steelix, which improves the penetration power of a fuzzer at the cost of an acceptable slow down of the execution speed. In particular, we use light-weight static analysis and binary instrumentation to provide not only coverage information but also comparison progress information to a fuzzer. Such program state information informs a fuzzer about where the magic bytes are located in the test input and how to perform mutations to match the magic bytes efficiently. We have implemented Steelix and evaluated it on three datasets: LAVA-M dataset, DARPA CGC sample binaries and five real-life programs. The results show that Steelix has better code coverage and bug detection capability than the state-of-the-art fuzzers. Moreover, we found one CVE and nine new bugs."}, {"id": "conf/sigsoft/GlanzAERHLM17", "title": "CodeMatch: obfuscation won't conceal your repackaged app.", "authors": ["Leonid Glanz", "Sven Amann", "Michael Eichberg", "Michael Reif", "Ben Hermann", "Johannes Lerch", "Mira Mezini"], "DOIs": ["https://doi.org/10.1145/3106237.3106305"], "tag": ["Research Papers"], "abstract": "ABSTRACTAn established way to steal the income of app developers, or to trick users into installing malware, is the creation of repackaged apps. These are clones of - typically - successful apps. To conceal their nature, they are often obfuscated by their creators. But, given that it is a common best practice to obfuscate apps, a trivial identification of repackaged apps is not possible. The problem is further intensified by the prevalent usage of libraries. In many apps, the size of the overall code base is basically determined by the used libraries. Therefore, two apps, where the obfuscated code bases are very similar, do not have to be repackages of each other. To reliably detect repackaged apps, we propose a two step approach which first focuses on the identification and removal of the library code in obfuscated apps. This approach - LibDetect - relies on code representations which abstract over several parts of the underlying bytecode to be resilient against certain obfuscation techniques. Using this approach, we are able to identify on average 70% more used libraries per app than previous approaches. After the removal of an app's library code, we then fuzzy hash the most abstract representation of the remaining app code to ensure that we can identify repackaged apps even if very advanced obfuscation techniques are used. This makes it possible to identify repackaged apps. Using our approach, we found that \u2248 15% of all apps in Android app stores are repackages"}, {"id": "conf/sigsoft/0001RS17", "title": "A compiler and verifier for page access oblivious computation.", "authors": ["Rohit Sinha", "Sriram K. Rajamani", "Sanjit A. Seshia"], "DOIs": ["https://doi.org/10.1145/3106237.3106248"], "tag": ["Research Papers"], "abstract": "ABSTRACT Trusted hardware primitives such as Intel's SGX instructions provide applications with a protected address space, called an enclave, for trusted code and data. However, building enclaves that preserve confidentiality of sensitive data continues to be a challenge. The developer must not only avoid leaking secrets via the enclave's outputs but also prevent leaks via side channels induced by interactions with the untrusted platform. Recent attacks have demonstrated that simply observing the page faults incurred during an enclave's execution can reveal its secrets if the enclave makes data accesses or control flow decisions based on secret values. To address this problem, a developer needs compilers to automatically produce confidential programs, and verification tools to certify the absence of secret-dependent page access patterns (a property that we formalize as page-access obliviousness). To that end, we implement an efficient compiler for a type and memory-safe language, a compiler pass that enforces page-access obliviousness with low runtime overheads, and an automatic, modular verifier that certifies page-access obliviousness at the machine-code level, thus removing the compiler from our trusted computing base. We evaluate this toolchain on several machine learning algorithms and image processing routines that we run within SGX enclaves."}, {"id": "conf/sigsoft/GarciaHGM17", "title": "Automatic generation of inter-component communication exploits for Android applications.", "authors": ["Joshua Garcia", "Mahmoud Hammad", "Negar Ghorbani", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3106237.3106286"], "tag": ["Research Papers"], "abstract": "ABSTRACTAlthough a wide variety of approaches identify vulnerabilities in Android apps, none attempt to determine exploitability of those vulnerabilities. Exploitability can aid in reducing false positives of vulnerability analysis, and can help engineers triage bugs. Specifically, one of the main attack vectors of Android apps is their inter-component communication interface, where apps may receive messages called Intents. In this paper, we provide the first approach for automatically generating exploits for Android apps, called LetterBomb, relying on a combined path-sensitive symbolic execution-based static analysis, and the use of software instrumentation and test oracles. We run LetterBomb on 10,000 Android apps from Google Play, where we identify 181 exploits from 835 vulnerable apps. Compared to a state-of-the-art detection approach for three ICC-based vulnerabilities, LetterBomb obtains 33%-60% more vulnerabilities at a 6.66 to 7 times faster speed."}, {"id": "conf/sigsoft/WeiLC17", "title": "OASIS: prioritizing static analysis warnings for Android apps based on app user reviews.", "authors": ["Lili Wei", "Yepang Liu", "Shing-Chi Cheung"], "DOIs": ["https://doi.org/10.1145/3106237.3106294"], "tag": ["Research Papers"], "abstract": "ABSTRACT Lint is a widely-used static analyzer for detecting bugs/issues in Android apps. However, it can generate many false warnings. One existing solution to this problem is to leverage project history data (e.g., bug fixing statistics) for warning prioritization. Unfortunately, such techniques are biased toward a project\u2019s archived warnings and can easily miss newissues. Anotherweakness is that developers cannot readily relate the warnings to the impacts perceivable by users. To overcome these weaknesses, in this paper, we propose a semantics-aware approach, OASIS, to prioritizing Lint warnings by leveraging app user reviews. OASIS combines program analysis and NLP techniques to recover the intrinsic links between the Lint warnings for a given app and the user complaints on the app problems caused by the issues of concern. OASIS leverages the strength of such links to prioritize warnings. We evaluated OASIS on six popular and large-scale open-source Android apps. The results show that OASIS can effectively prioritize Lint warnings and help identify new issues that are previously-unknown to app developers."}, {"id": "conf/sigsoft/VasilescuCD17", "title": "Recovering clear, natural identifiers from obfuscated JS names.", "authors": ["Bogdan Vasilescu", "Casey Casalnuovo", "Premkumar T. Devanbu"], "DOIs": ["https://doi.org/10.1145/3106237.3106289"], "tag": ["Research Papers"], "abstract": "ABSTRACT Well-chosen variable names are critical to source code readability, reusability, and maintainability. Unfortunately, in deployed JavaScript code (which is ubiquitous on the web) the identifier names are frequently minified and overloaded. This is done both for efficiency and also to protect potentially proprietary intellectual property. In this paper, we describe an approach based on statistical machine translation (SMT) that recovers some of the original names from the JavaScript programs minified by the very popular UglifyJS. This simple tool, Autonym, performs comparably to the best currently available deobfuscator for JavaScript, JSNice, which uses sophisticated static analysis. In fact, Autonym is quite complementary to JSNice, performing well when it does not, and vice versa. We also introduce a new tool, JSNaughty, which blends Autonym and JSNice, and significantly outperforms both at identifier name recovery, while remaining just as easy to use as JSNice. JSNaughty is available online at http://jsnaughty.org."}, {"id": "conf/sigsoft/YuZW17", "title": "DESCRY: reproducing system-level concurrency failures.", "authors": ["Tingting Yu", "Tarannum S. Zaman", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3106237.3106266"], "tag": ["Research Papers"], "abstract": "ABSTRACT Concurrent systems may fail in the field due to various elusive faults such as race conditions. Reproducing such failures is hard because (1) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which cannot be handled by existing tools for reproducing intra-process (thread-level) failures; (2) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers; and (3) the debugging environment may differ from the deployed environment, which further complicates failure reproduction. To address these problems, we present DESCRY, the first fully automated tool for reproducing system-level concurrency failures based only on default log messages collected from the field. DESCRY uses a combination of static and dynamic analysis techniques, together with symbolic execution, to synthesize both the failure-inducing data input and the interleaving schedule, and leverages them to deterministically replay the failed execution using existing virtual platforms. We have evaluated DESCRY on 22 real-world multi-process Linux applications with a total of 236,875 lines of code to demonstrate both its effectiveness and its efficiency in reproducing failures that no other tool can reproduce."}, {"id": "conf/sigsoft/BianchiPT17", "title": "Reproducing concurrency failures from crash stacks.", "authors": ["Francesco A. Bianchi", "Mauro Pezz\u00e8", "Valerio Terragni"], "DOIs": ["https://doi.org/10.1145/3106237.3106292"], "tag": ["Research Papers"], "abstract": "ABSTRACT Reproducing field failures is the first essential step for understanding, localizing and removing faults. Reproducing concurrency field failures is hard due to the need of synthesizing a test code jointly with a thread interleaving that induce the failure in the presence of limited information from the field. Current techniques for reproducing concurrency failures focus on identifying failure-inducing interleavings, leaving largely open the problem of synthesizing the test code that manifests such interleavings. In this paper, we present ConCrash, a technique to automatically generate test codes that reproduce concurrency failures that violate thread-safety from crash stacks, which commonly summarize the conditions of field failures. ConCrash efficiently explores the huge space of possible test codes to identify a failure-inducing one by using a suitable set of search pruning strategies. Combined with existing techniques for exploring interleavings, ConCrash automatically reproduces a given concurrency failure that violates the thread-safety of a class by identifying both a failure-inducing test code and corresponding interleaving. In the paper, we define the ConCrash approach, present a prototype implementation of ConCrash, and discuss the experimental results that we obtained on a known set of ten field failures that witness the effectiveness of the approach."}, {"id": "conf/sigsoft/CastelluccioSVP17", "title": "Automatically analyzing groups of crashes for finding correlations.", "authors": ["Marco Castelluccio", "Carlo Sansone", "Luisa Verdoliva", "Giovanni Poggi"], "DOIs": ["https://doi.org/10.1145/3106237.3106306"], "tag": ["Research Papers"], "abstract": "ABSTRACT We devised an algorithm, inspired by contrast-set mining algorithms such as STUCCO, to automatically find statistically significant properties (correlations) in crash groups. Many earlier works focused on improving the clustering of crashes but, to the best of our knowledge, the problem of automatically describing properties of a cluster of crashes is so far unexplored. This means developers currently spend a fair amount of time analyzing the groups themselves, which in turn means that a) they are not spending their time actually developing a fix for the crash; and b) they might miss something in their exploration of the crash data (there is a large number of attributes in crash reports and it is hard and error-prone to manually analyze everything). Our algorithm helps developers and release managers understand crash reports more easily and in an automated way, helping in pinpointing the root cause of the crash. The tool implementing the algorithm has been deployed on Mozilla's crash reporting service."}, {"id": "conf/sigsoft/LongAR17", "title": "Automatic inference of code transforms for patch generation.", "authors": ["Fan Long", "Peter Amidon", "Martin Rinard"], "DOIs": ["https://doi.org/10.1145/3106237.3106253"], "tag": ["Research Papers"], "abstract": "ABSTRACT We present a new system, Genesis, that processes human patches to automatically infer code transforms for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the complete Genesis patch generation system working with real-world patches and defects collected from 372 Java projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from previous successful patches."}, {"id": "conf/sigsoft/YiAKTR17", "title": "A feasibility study of using automated program repair for introductory programming assignments.", "authors": ["Jooyong Yi", "Umair Z. Ahmed", "Amey Karkare", "Shin Hwei Tan", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1145/3106237.3106262"], "tag": ["Research Papers"], "abstract": "ABSTRACTDespite the fact an intelligent tutoring system for programming (ITSP) education has long attracted interest, its widespread use has been hindered by the difficulty of generating personalized feedback automatically. Meanwhile, automated program repair (APR) is an emerging new technology that automatically fixes software bugs, and it has been shown that APR can fix the bugs of large real-world software. In this paper, we study the feasibility of marrying intelligent programming tutoring and APR. We perform our feasibility study with four state-of-the-art APR tools (GenProg, AE, Angelix, and Prophet), and 661 programs written by the students taking an introductory programming course. We found that when APR tools are used out of the box, only about 30% of the programs in our dataset are repaired. This low repair rate is largely due to the student programs often being significantly incorrect - in contrast, professional software for which APR was successfully applied typically fails only a small portion of tests. To bridge this gap, we adopt in APR a new repair policy akin to the hint generation policy employed in the existing ITSP. This new repair policy admits partial repairs that address part of failing tests, which results in 84% improvement of repair rate. We also performed a user study with 263 novice students and 37 graders, and identified an understudied problem; while novice students do not seem to know how to effectively make use of generated repairs as hints, the graders do seem to gain benefits from repairs."}, {"id": "conf/sigsoft/TianR17", "title": "Automatically diagnosing and repairing error handling bugs in C.", "authors": ["Yuchi Tian", "Baishakhi Ray"], "DOIs": ["https://doi.org/10.1145/3106237.3106300"], "tag": ["Research Papers"], "abstract": "ABSTRACTCorrect error handling is essential for building reliable and secure systems. Unfortunately, low-level languages like C often do not support any error handling primitives and leave it up to the developers to create their own mechanisms for error propagation and handling. However, in practice, the developers often make mistakes while writing the repetitive and tedious error handling code and inadvertently introduce bugs. Such error handling bugs often have severe consequences undermining the security and reliability of the affected systems. Fixing these bugs is also tiring-they are repetitive and cumbersome to implement. Therefore, it is crucial to develop tool supports for automatically detecting and fixing error handling bugs. To understand the nature of error handling bugs that occur in widely used C programs, we conduct a comprehensive study of real world error handling bugs and their fixes. Leveraging the knowledge, we then design, implement, and evaluate ErrDoc, a tool that not only detects and characterizes different types of error handling bugs but also automatically fixes them. Our evaluation on five open-source projects shows that ErrDoc can detect error handling bugs with 100% to 84% precision and around 95% recall, and categorize them with 83% to 96% precision and above 90% recall. Thus, ErrDoc improves precision up to 5 percentage points, and recall up to 44 percentage points w.r.t. the state-of-the-art. We also demonstrate that ErrDoc can fix the bugs with high accuracy."}, {"id": "conf/sigsoft/HellendoornD17", "title": "Are deep neural networks the best choice for modeling source code?", "authors": ["Vincent J. Hellendoorn", "Premkumar T. Devanbu"], "DOIs": ["https://doi.org/10.1145/3106237.3106290"], "tag": ["Research Papers"], "abstract": "ABSTRACTCurrent statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add & remove text, and mix & swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N-gram, as well as RNN, and LSTM deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N-gram models for source code can yield performance that surpasses even RNN and LSTM based deep-learning models."}, {"id": "conf/sigsoft/MartieHK17", "title": "Understanding the impact of support for iteration on code search.", "authors": ["Lee Martie", "Andr\u00e9 van der Hoek", "Thomas Kwak"], "DOIs": ["https://doi.org/10.1145/3106237.3106293"], "tag": ["Research Papers"], "abstract": "ABSTRACT Sometimes, when programmers use a search engine they know more or less what they need. Other times, programmers use the search engine to look around and generate possible ideas for the programming problem they are working on. The key insight we explore in this paper is that the results found in the latter case tend to serve as inspiration or triggers for the next queries issued. We introduce two search engines, CodeExchange and CodeLikeThis, both of which are specifically designed to enable the user to directly leverage the results in formulating the next query. CodeExchange does this with a set of four features supporting the programmer to use characteristics of the results to find other code with or without those characteristics. CodeLikeThis supports simply selecting an entire result to find code that is analogous, to some degree, to that result. We evaluated how these approaches were used along with two approaches not explicitly supporting iteration, a baseline and Google, in a user study among 24 developers. We find that search engines that support using results to form the next query can improve the programmers\u2019 search experience and different approaches to iteration can provide better experiences depending on the task."}, {"id": "conf/sigsoft/MaAXLZLZ17", "title": "LAMP: data provenance for graph based machine learning algorithms through derivative computation.", "authors": ["Shiqing Ma", "Yousra Aafer", "Zhaogui Xu", "Wen-Chuan Lee", "Juan Zhai", "Yingqi Liu", "Xiangyu Zhang"], "DOIs": ["https://doi.org/10.1145/3106237.3106291"], "tag": ["Research Papers"], "abstract": "ABSTRACT Data provenance tracking determines the set of inputs related to a given output. It enables quality control and problem diagnosis in data engineering. Most existing techniques work by tracking program dependencies. They cannot quantitatively assess the importance of related inputs, which is critical to machine learning algorithms, in which an output tends to depend on a huge set of inputs while only some of them are of importance. In this paper, we propose LAMP, a provenance computation system for machine learning algorithms. Inspired by automatic differentiation (AD), LAMP quantifies the importance of an input for an output by computing the partial derivative. LAMP separates the original data processing and the more expensive derivative computation to different processes to achieve cost-effectiveness. In addition, it allows quantifying importance for inputs related to discrete behavior, such as control flow selection. The evaluation on a set of real world programs and data sets illustrates that LAMP produces more precise and succinct provenance than program dependence based techniques, with much less overhead. Our case studies demonstrate the potential of LAMP in problem diagnosis in data engineering."}, {"id": "conf/sigsoft/DotzlerKKP17", "title": "More accurate recommendations for method-level changes.", "authors": ["Georg Dotzler", "Marius Kamp", "Patrick Kreutzer", "Michael Philippsen"], "DOIs": ["https://doi.org/10.1145/3106237.3106276"], "tag": ["Research Papers"], "abstract": "ABSTRACT During the life span of large software projects, developers often apply the same code changes to different code locations in slight variations. Since the application of these changes to all locations is time-consuming and error-prone, tools exist that learn change patterns from input examples, search for possible pattern applications, and generate corresponding recommendations. In many cases, the generated recommendations are syntactically or semantically wrong due to code movements in the input examples. Thus, they are of low accuracy and developers cannot directly copy them into their projects without adjustments.  We present the Accurate REcommendation System (ARES) that achieves a higher accuracy than other tools because its algorithms take care of code movements when creating patterns and recommendations. On average, the recommendations by ARES have an accuracy of 96% with respect to code changes that developers have manually performed in commits of source code archives. At the same time ARES achieves precision and recall values that are on par with other tools."}, {"id": "conf/sigsoft/CelikVMG17", "title": "Regression test selection across JVM boundaries.", "authors": ["Ahmet \u00c7elik", "Marko Vasic", "Aleksandar Milicevic", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1145/3106237.3106297"], "tag": ["Research Papers"], "abstract": "ABSTRACT Modern software development processes recommend that changes be integrated into the main development line of a project multiple times a day. Before a new revision may be integrated, developers practice regression testing to ensure that the latest changes do not break any previously established functionality. The cost of regression testing is high, due to an increase in the number of revisions that are introduced per day, as well as the number of tests developers write per revision. Regression test selection (RTS) optimizes regression testing by skipping tests that are not affected by recent project changes. Existing dynamic RTS techniques support only projects written in a single programming language, which is unfortunate knowing that an open-source project is on average written in several programming languages.  We present the first dynamic RTS technique that does not stop at predefined language boundaries. Our technique dynamically detects, at the operating system level, all file artifacts a test depends on. Our technique is, hence, oblivious to the specific means the test uses to actually access the files: be it through spawning a new process, invoking a system call, invoking a library written in a different language, invoking a library that spawns a process which makes a system call, etc. We also provide a set of extension points which allow for a smooth integration with testing frameworks and build systems. We implemented our technique in a tool called RTSLinux as a loadable Linux kernel module and evaluated it on 21 Java projects that escape JVM by spawning new processes or invoking native code, totaling 2,050,791 lines of code. Our results show that RTSLinux, on average, skips 74.17% of tests and saves 52.83% of test execution time compared to executing all tests."}, {"id": "conf/sigsoft/LabuschagneIH17", "title": "Measuring the cost of regression testing in practice: a study of Java projects using continuous integration.", "authors": ["Adriaan Labuschagne", "Laura Inozemtseva", "Reid Holmes"], "DOIs": ["https://doi.org/10.1145/3106237.3106288"], "tag": ["Research Papers"], "abstract": "ABSTRACTSoftware defects cost time and money to diagnose and fix. Consequently, developers use a variety of techniques to avoid introducing defects into their systems. However, these techniques have costs of their own; the benefit of using a technique must outweigh the cost of applying it. In this paper we investigate the costs and benefits of automated regression testing in practice. Specifically, we studied 61 projects that use Travis CI, a cloud-based continuous integration tool, in order to examine real test failures that were encountered by the developers of those projects. We determined how the developers resolved the failures they encountered and used this information to classify the failures as being caused by a flaky test, by a bug in the system under test, or by a broken or obsolete test. We consider that test failures caused by bugs represent a benefit of the test suite, while failures caused by broken or obsolete tests represent a test suite maintenance cost. We found that 18% of test suite executions fail and that 13% of these failures are flaky. Of the non-flaky failures, only 74% were caused by a bug in the system under test; the remaining 26% were due to incorrect or obsolete tests. In addition, we found that, in the failed builds, only 0.38% of the test case executions failed and 64% of failed builds contained more than one failed test. Our findings contribute to a wider understanding of the unforeseen costs that can impact the overall cost effectiveness of regression testing in practice. They can also inform research into test case selection techniques, as we have provided an approximate empirical bound on the practical value that could be extracted from such techniques. This value appears to be large, as the 61 systems under study contained nearly 3 million lines of test code and yet over 99% of test case executions could have been eliminated with a perfect oracle."}, {"id": "conf/sigsoft/YangZLT17", "title": "Better test cases for better automated program repair.", "authors": ["Jinqiu Yang", "Alexey Zhikhartsev", "Yuefei Liu", "Lin Tan"], "DOIs": ["https://doi.org/10.1145/3106237.3106274"], "tag": ["Research Papers"], "abstract": "ABSTRACT Automated generate-and-validate program repair techniques (G&V techniques) suffer from generating many overfitted patches due to in-capabilities of test cases. Such overfitted patches are incor- rect patches, which only make all given test cases pass, but fail to fix the bugs. In this work, we propose an overfitted patch detec- tion framework named Opad (Overfitted PAtch Detection). Opad helps improve G&V techniques by enhancing existing test cases to filter out overfitted patches. To enhance test cases, Opad uses fuzz testing to generate new test cases, and employs two test or- acles (crash and memory-safety) to enhance validity checking of automatically-generated patches. Opad also uses a novel metric (named O-measure) for deciding whether automatically-generated patches overfit. Evaluated on 45 bugs from 7 large systems (the same benchmark used by GenProg and SPR), Opad filters out 75.2% (321/427) over- fitted patches generated by GenProg/AE, Kali, and SPR. In addition, Opad guides SPR to generate correct patches for one more bug (the original SPR generates correct patches for 11 bugs). Our analysis also shows that up to 40% of such automatically-generated test cases may further improve G&V techniques if empowered with better test oracles (in addition to crash and memory-safety oracles employed by Opad)."}, {"id": "conf/sigsoft/BuXXZTX17", "title": "When program analysis meets mobile security: an industrial study of misusing Android internet sockets.", "authors": ["Wenqi Bu", "Minhui Xue", "Lihua Xu", "Yajin Zhou", "Zhushou Tang", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3106237.3117764"], "tag": ["Testing and Security in the Real World"], "abstract": "ABSTRACTDespite recent progress in program analysis techniques to identify vulnerabilities in Android apps, significant challenges still remain for applying these techniques to large-scale industrial environments. Modern software-security providers, such as Qihoo 360 and Pwnzen (two leading companies in China), are often required to process more than 10 million mobile apps at each run. In this work, we focus on effectively and efficiently identifying vulnerable usage of Internet sockets in an industrial setting. To achieve this goal, we propose a practical hybrid approach that enables lightweight yet precise detection in the industrial setting. In particular, we integrate the process of categorizing potential vulnerable apps with analysis techniques, to reduce the inevitable human inspection effort. We categorize potential vulnerable apps based on characteristics of vulnerability signatures, to reduce the burden on static analysis. We flexibly integrate static and dynamic analyses for apps in each identified family, to refine the family signatures and hence target on precise detection. We implement our approach in a practical system and deploy the system on the Pwnzen platform. By using the system, we identify and report potential vulnerabilities of 24 vulnerable apps (falling into 3 vulnerability families) to their developers, and some of these reported vulnerabilities are previously unknown. The apps of each vulnerability family in total have over 50 million downloads. We also propose countermeasures and highlight promising directions for technology transfer."}, {"id": "conf/sigsoft/VasicPMG17", "title": "File-level vs. module-level regression test selection for .NET.", "authors": ["Marko Vasic", "Zuhair Parvez", "Aleksandar Milicevic", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1145/3106237.3117763"], "tag": ["Testing and Security in the Real World"], "abstract": "ABSTRACT Regression testing is used to check the correctness of evolving software. With the adoption of Agile development methodology, the number of tests and software revisions has dramatically increased, and hence has the cost of regression testing. Researchers proposed regression test selection (RTS) techniques that optimize regression testing by skipping tests that are not impacted by recent program changes. Ekstazi is one such state-of-the art technique; Ekstazi is implemented for the Java programming language and has been adopted by several companies and open-source projects.  We report on our experience implementing and evaluating Ekstazi#, an Ekstazi-like tool for .NET. We describe the key challenges of bringing the Ekstazi idea to the .NET platform. We evaluate Ekstazi# on 11 open-source projects, as well as an internal Microsoft project substantially larger than each of the open-source projects. Finally, we compare Ekstazi# to an incremental build system (also developed at Microsoft), which, out of the box, provides module-level dependency tracking and skipping tasks (including test execution) whenever dependencies of a task do not change between the current and the last successful build. Ekstazi# on average reduced regression testing time by 43.70% for the open-source projects and by 65.26% for the Microsoft project (the latter is in addition to the savings provided by incremental builds)."}, {"id": "conf/sigsoft/LamWLWZLYDX17", "title": "Record and replay for Android: are we there yet in industrial cases?", "authors": ["Wing Lam", "Zhengkai Wu", "Dengfeng Li", "Wenyu Wang", "Haibing Zheng", "Hui Luo", "Peng Yan", "Yuetang Deng", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3106237.3117769"], "tag": ["Testing and Security in the Real World"], "abstract": "ABSTRACT Mobile applications, or apps for short, are gaining popularity. The input sources (e.g., touchscreen, sensors, transmitters) of the smart devices that host these apps enable the apps to offer a rich experience to the users, but these input sources pose testing complications to the developers (e.g., writing tests to accurately utilize multiple input sources together and be able to replay such tests at a later time). To alleviate these complications, researchers and practitioners in recent years have developed a variety of record-and-replay tools to support the testing expressiveness of smart devices. These tools allow developers to easily record and automate the replay of complicated usage scenarios of their app. Due to Android's large share of the smart-device market, numerous record-and-replay tools have been developed using a variety of techniques to test Android apps. To better understand the strengths and weaknesses of these tools, we present a comparison of popular record-and-replay tools from researchers and practitioners, by applying these tools to test three popular industrial apps downloaded from the Google Play store. Our comparison is based on three main metrics: (1) ability to reproduce common usage scenarios, (2) space overhead of traces created by the tools, and (3) robustness of traces created by the tools (when being replayed on devices with different resolutions). The results from our comparison show which record-and-replay tools may be the best for developers and identify future directions for improving these tools to better address testing complications of smart devices."}, {"id": "conf/sigsoft/DijkCHB17", "title": "Model-driven software engineering in practice: privacy-enhanced filtering of network traffic.", "authors": ["Roel van Dijk", "Christophe Creeten", "Jeroen van der Ham", "Jeroen van den Bos"], "DOIs": ["https://doi.org/10.1145/3106237.3117777"], "tag": ["Testing and Security in the Real World"], "abstract": "ABSTRACT Network traffic data contains a wealth of information for use in security analysis and application development. Unfortunately, it also usually contains confidential or otherwise sensitive information, prohibiting sharing and analysis. Existing automated anonymization solutions are hard to maintain and tend to be outdated.  We present Privacy-Enhanced Filtering (PEF), a model-driven prototype framework that relies on declarative descriptions of protocols and a set of filter rules, which are used to automatically transform network traffic data to remove sensitive information. This paper discusses the design, implementation and application of PEF, which is available as open-source software and configured for use in a typical malware detection scenario."}, {"id": "conf/sigsoft/HuijgensLSRGR17", "title": "Strong agile metrics: mining log data to determine predictive power of software metrics for continuous delivery teams.", "authors": ["Hennie Huijgens", "Robert Lamping", "Dick Stevens", "Hartger Rothengatter", "Georgios Gousios", "Daniele Romano"], "DOIs": ["https://doi.org/10.1145/3106237.3117779"], "tag": ["The State of the Practice"], "abstract": "ABSTRACT ING Bank, a large Netherlands-based internationally operating bank, implemented a fully automated continuous delivery pipe-line for its software engineering activities in more than 300 teams, that perform more than 2500 deployments to production each month on more than 750 different applications. Our objective is to examine how strong metrics for agile (Scrum) DevOps teams can be set in an iterative fashion. We perform an exploratory case study that focuses on the classification based on predictive power of software metrics, in which we analyze log data derived from two initial sources within this pipeline. We analyzed a subset of 16 metrics from 59 squads. We identified two lagging metrics and assessed four leading metrics to be strong."}, {"id": "conf/sigsoft/VolfS17", "title": "Screening heuristics for project gating systems.", "authors": ["Zahy Volf", "Edi Shmueli"], "DOIs": ["https://doi.org/10.1145/3106237.3117766"], "tag": ["The State of the Practice"], "abstract": "ABSTRACT Continuous Integration (CI) is a hot topic. Yet, little attention is payed to how CI systems work and what impacts their behavior. In parallel, bug prediction in software is gaining high attention. But this is done mostly in the context of software engineering, and the relation to the realm of CI and CI systems engineering has not been established yet. In this paper we describe how Project Gating systems operate, which are a specific type of CI systems used to keep the mainline of development always clean. We propose and evaluate three heuristics for improving Gating performance and demonstrate their trade-offs. The third heuristic, which leverages state-of-the-art bug prediction achieves the best performance across the entire spectrum of workload conditions."}, {"id": "conf/sigsoft/SahaGMM17", "title": "Natural language querying in SAP-ERP platform.", "authors": ["Diptikalyan Saha", "Neelamadhav Gantayat", "Senthil Mani", "Barry Mitchell"], "DOIs": ["https://doi.org/10.1145/3106237.3117765"], "tag": ["The State of the Practice"], "abstract": "ABSTRACT With the omnipresence of mobile devices coupled with recent advances in automatic speech recognition capabilities, there has been a growing demand for natural language query (NLQ) interface to retrieve information from the knowledge bases. Business users particularly find this useful as NLQ interface enables them to ask questions without the knowledge of the query language or the data schema. In this paper, we apply an existing research technology called ``ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores'' in the industry domain of SAP-ERP systems. The goal is to enable users to query SAP-ERP data using natural language. We present the challenges and their solutions of such a technology transfer. We present the effectiveness of the natural language query interface on a set of questions given by a set of SAP practitioners."}, {"id": "conf/sigsoft/AdzicC17", "title": "Serverless computing: economic and architectural impact.", "authors": ["Gojko Adzic", "Robert Chatley"], "DOIs": ["https://doi.org/10.1145/3106237.3117767"], "tag": ["The State of the Practice"], "abstract": "ABSTRACTAmazon Web Services unveiled their \"Lambda\" platform in late 2014. Since then, each of the major cloud computing infrastructure providers has released services supporting a similar style of deployment and operation, where rather than deploying and running monolithic services, or dedicated virtual machines, users are able to deploy individual functions, and pay only for the time that their code is actually executing. These technologies are gathered together under the marketing term \"serverless\" and the providers suggest that they have the potential to significantly change how client/server applications are designed, developed and operated. This paper presents two case industrial studies of early adopters, showing how migrating an application to the Lambda deployment architecture reduced hosting costs - by between 66% and 95% - and discusses how further adoption of this trend might influence common software architecture design practices."}, {"id": "conf/sigsoft/IvanovRSYZ17", "title": "What do software engineers care about? gaps between research and practice.", "authors": ["Vladimir Ivanov", "Alan Rogers", "Giancarlo Succi", "Jooyong Yi", "Vasilii Zorin"], "DOIs": ["https://doi.org/10.1145/3106237.3117778"], "tag": ["Understanding Software Developers"], "abstract": "ABSTRACT It is a cliche to say that there is a gap between research and practice. As the interest and importance in the practical impact of research has been growing, the gap between research and practice is expected to be narrowing. However, our study reveals that there still seems to be a wide gap. We survey so ware engineers about what they care about when developing so ware. We then compare our survey results with the research topics of the papers published in ICSE/FSE recently. We found the following discrepancy: while so ware engineers care more about so ware development productivity than the quality of so ware, papers on research areas closely related to so ware productivity--such as so ware development process management and so ware development techniques--are significantly less published than papers on so ware verification and validation that account for more than half of publications. We also found that so ware engineers are in great need for techniques for accurate effort estimation, and they are not necessarily knowledgable about techniques they can use to meet their needs."}, {"id": "conf/sigsoft/GalsterAMT17", "title": "Reference architectures and Scrum: friends or foes?", "authors": ["Matthias Galster", "Samuil Angelov", "Silverio Mart\u00ednez-Fern\u00e1ndez", "Dan Tofan"], "DOIs": ["https://doi.org/10.1145/3106237.3117773"], "tag": ["Understanding Software Developers"], "abstract": "ABSTRACT Software reference architectures provide templates and guidelines for designing systems in a particular domain. Companies use them to achieve interoperability of (parts of) their software, standardization, and faster development. In contrast to system-specific software architectures that \"emerge\" during development, reference architectures dictate significant parts of the software design early on. Agile software development frameworks (such as Scrum) acknowledge changing software requirements and the need to adapt the software design accordingly. In this paper, we present lessons learned about how reference architectures interact with Scrum (the most frequently used agile process framework). These lessons are based on observing software development projects in five companies. We found that reference architectures can support good practice in Scrum: They provide enough design upfront without too much effort, reduce documentation activities, facilitate knowledge sharing, and contribute to \"architectural thinking\" of developers. However, reference architectures can impose risks or even threats to the success of Scrum (e.g., to self-organizing and motivated teams)."}, {"id": "conf/sigsoft/HarmsRI17", "title": "Guidelines for adopting frontend architectures and patterns in microservices-based systems.", "authors": ["Holger Harms", "Collin Rogowski", "Luigi Lo Iacono"], "DOIs": ["https://doi.org/10.1145/3106237.3117775"], "tag": ["Understanding Software Developers"], "abstract": "ABSTRACT Microservice-based systems enable the independent development, deployment, and scalability for separate system components of enterprise applications. A significant aspect during development is the microservice integration in frontends of web, mobile, and desktop applications. One challenge here is the selection of an adequate frontend architecture as well as suitable patterns that satisfy the application requirements. This paper analyses available strategies for organizing and implementing microservices frontends. These approaches are then evaluated based on a quality model and various prototypes of the same application implemented using the distinct approaches. The results of this analysis are generalized to a guideline that supports the selection of a suitable architecture."}, {"id": "conf/sigsoft/GarciaG17", "title": "Improving understanding of dynamically typed software developed by agile practitioners.", "authors": ["Jair Garc\u00eda", "Kelly Garc\u00e9s"], "DOIs": ["https://doi.org/10.1145/3106237.3117772"], "tag": ["Understanding Software Developers"], "abstract": "ABSTRACT Agile Development values working software over documentation. Therefore, in maintenance stages of existing software, the source code is the sole software artifact that developers have for analyzing the viability and impact of a new user story. Since functionality is often spread in hundreds of lines of code, it is hard for the developer to understand the system, which may lead to under-/overestimation of the new feature cost and rework/delays in the subsequent phases of development. In a previous work, we proposed a Model-Driven Reverse Engineering approach for obtaining software visualizations from source code. Two case studies of comprehension of applications written in statically typed languages have shown the applicability of this approach. A recent experience with an industrial partner, where the systems are developed on dynamically typed languages, has motivated us to adapt the previous proposal to take as input not only the source code but also the application data schema to complete the information that is missing in the code, and then automatically generate more meaningful diagrams that help developers in maintenance tasks. In this article, we present the adaptation of the general approach to support data schema as an additional input and its instrumentation in an industrial case study where the technology is Ruby on Rails. The paper ends by explaining the precision and performance of the instrumentation when used in a Colombian company as well as lessons learned."}, {"id": "conf/sigsoft/ZhouS17", "title": "Automated identification of security issues from commit messages and bug reports.", "authors": ["Yaqin Zhou", "Asankhaya Sharma"], "DOIs": ["https://doi.org/10.1145/3106237.3117771"], "tag": ["Data-Driven Improvement"], "abstract": "ABSTRACT The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classifier achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classifier in prior work on vulnerability identification in commit messages, we improve precision by 54.55% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations from running the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the effectiveness and generality of the proposed approach."}, {"id": "conf/sigsoft/WuY17", "title": "LaChouTi: kernel vulnerability responding framework for the fragmented Android devices.", "authors": ["JingZheng Wu", "Mutian Yang"], "DOIs": ["https://doi.org/10.1145/3106237.3117768"], "tag": ["Data-Driven Improvement"], "abstract": "ABSTRACTThe most criticized problem in the Android ecosystem is fragmentation, i.e., 24,093 Android devices in the wild are made by 1,294 manufacturers and installed with extremely customized operating systems. The existence of so many different active versions of Android makes security updates and vulnerability responses across the whole range of Android devices difficult. In this paper, we seek to respond to the unpatched kernel vulnerabilities for the fragmented Android devices. Specifically, we propose and implement LaChouTi, which is an automated kernel security update framework consisting of cloud service and end application update. LaChouTi first tracks and identifies the exposed vulnerabilities according to the CVE-Patch map for the target Android kernels. Then, it generates differential binary patches for the identified results. Finally, it pushes and applies the patches to the kernels. We evaluate LaChouTi using 12 Nexus Android devices that have different Android versions, different kernel versions, different series and different manufacturers, and find 1922 unpatched kernel vulnerabilities in these devices. The results show that: (1) the security risk of unpatched vulnerabilities caused by fragmentation is serious; and (2) the proposed LaChouTi is effective in responding to such security risk. Finally, we implement LaChouTi on new commercial devices by collaborating with four internationally renowned manufacturers. The results demonstrate that LaChouTi is effective for the manufacturers'security updates."}, {"id": "conf/sigsoft/LeeHLKJ17", "title": "Applying deep learning based automatic bug triager to industrial projects.", "authors": ["Sun-Ro Lee", "Min-Jae Heo", "Chan-Gun Lee", "Milhan Kim", "Gaeul Jeong"], "DOIs": ["https://doi.org/10.1145/3106237.3117776"], "tag": ["Data-Driven Improvement"], "abstract": "ABSTRACT Finding the appropriate developer for a bug report, so called `Bug Triage', is one of the bottlenecks in the bug resolution process. To address this problem, many approaches have proposed various automatic bug triage techniques in recent studies. We argue that most previous studies focused on open source projects only and did not consider deep learning techniques. In this paper, we propose to use Convolutional Neural Network and word embedding to build an automatic bug triager. The results of the experiments applied to both industrial and open source projects reveal benefits of the automatic approach and suggest co-operation of human and automatic triagers. Our experience in integrating and operating the proposed system in an industrial development environment is also reported."}, {"id": "conf/sigsoft/GarbervetskyP0M17", "title": "Static analysis for optimizing big data queries.", "authors": ["Diego Garbervetsky", "Zvonimir Pavlinovic", "Michael Barnett", "Madanlal Musuvathi", "Todd Mytkowicz", "Edgardo Zoppi"], "DOIs": ["https://doi.org/10.1145/3106237.3117774"], "tag": ["Data-Driven Improvement"], "abstract": "ABSTRACT Query languages for big data analysis provide user extensibility through a mechanism of user-defined operators (UDOs). These operators allow programmers to write proprietary functionalities on top of a relational query skeleton. However, achieving effective query optimization for such languages is extremely challenging since the optimizer needs to understand data dependencies induced by UDOs. SCOPE, the query language from Microsoft, allows for hand coded declarations of UDO data dependencies. Unfortunately, most programmers avoid using this facility since writing and maintaining the declarations is tedious and error-prone. In this work, we designed and implemented two sound and robust static analyses for computing UDO data dependencies. The analyses can detect what columns of an input table are never used or pass-through a UDO unchanged. This information can be used to significantly improve execution of SCOPE scripts. We evaluate our analyses on thousands of real-world queries and show we can catch many unused and pass-through columns automatically without relying on any manually provided declarations."}, {"id": "conf/sigsoft/MatinnejadNB17", "title": "Automated testing of hybrid Simulink/Stateflow controllers: industrial case studies.", "authors": ["Reza Matinnejad", "Shiva Nejati", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1145/3106237.3117770"], "tag": ["Data-Driven Improvement"], "abstract": "ABSTRACT We present the results of applying our approach for testing Simulink controllers to one public and one proprietary model, both industrial. Our approach combines explorative and exploitative search algorithms to visualize the controller behavior over its input space and to identify test scenarios in the controller input space that violate or are likely to violate the controller requirements. The engineers' feedback shows that our approach is easy to use in practice and gives them confidence about the behavior of their models."}, {"id": "conf/sigsoft/DovgalyukFVM17", "title": "QEMU-based framework for non-intrusive virtual machine instrumentation and introspection.", "authors": ["Pavel Dovgalyuk", "Natalia Fursova", "Ivan Vasiliev", "Vladimir Makarov"], "DOIs": ["https://doi.org/10.1145/3106237.3122817"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACT This paper presents the framework based on the emulator QEMU. Our framework provides set of multi-platform analysis tools for the virtual machines and mechanism for creating instrumentation and analysis tools. Our framework is based on a lightweight approach to dynamic analysis of binary code executed in virtual machines. This approach is non-intrusive and provides system-wide analysis capabilities. It does not require loading any guest agents and source code of the OS. Therefore it may be applied to ROM-based guest systems and enables using of record/replay of the system execution. We use application binary interface (ABI) of the platform to be analyzed for creating introspection tools. These tools recover the part of kernel-level information related to the system calls executed on the guest machine."}, {"id": "conf/sigsoft/YuanXXPZ17", "title": "RunDroid: recovering execution call graphs for Android applications.", "authors": ["Yujie Yuan", "Lihua Xu", "Xusheng Xiao", "Andy Podgurski", "Huibiao Zhu"], "DOIs": ["https://doi.org/10.1145/3106237.3122821"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACT Fault localization is a well-received technique for helping developers to identify faulty statements of a program. Research has shown that the coverages of faulty statements and its predecessors in program dependence graph are important for effective fault localization. However, app executions in Android split into segments in different components, i.e., methods, threads, and processes, posing challenges for traditional program dependence computation, and in turn rendering fault localization less effective. We present RunDroid, a tool for recovering the dynamic call graphs of app executions in Android, assisting existing tools for more precise program dependence computation. For each exectuion, RunDroid captures and recovers method calls from not only the application layer, but also between applications and the Android framework. Moreover, to deal with the widely adopted multi-threaded communications in Android applications, RunDroid also captures methods calls that are split among threads. Demo : https://github.com/MiJack/RunDroid Video : https://youtu.be/EM7TJbE-Oaw"}, {"id": "conf/sigsoft/YuCZWD17", "title": "RGSE: a regular property guided symbolic executor for Java.", "authors": ["Hengbiao Yu", "Zhenbang Chen", "Yufeng Zhang", "Ji Wang", "Wei Dong"], "DOIs": ["https://doi.org/10.1145/3106237.3122830"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACTIt is challenging to effectively check a regular property of a program. This paper presents RGSE, a regular property guided dynamic symbolic execution (DSE) engine, for finding a program path satisfying a regular property as soon as possible. The key idea is to evaluate the candidate branches based on the history and future information, and explore the branches along which the paths are more likely to satisfy the property in priority. We have applied RGSE to 16 real-world open source Java programs, totaling 270K lines of code. Compared with the state-of-the-art, RGSE achieves two orders of magnitude speedups for finding the first target path. RGSE can benefit many research topics of software testing and analysis, such as path-oriented test case generation, typestate bug finding, and performance tuning. The demo video is at: https://youtu.be/7zAhvRIdaUU, and RGSE can be accessed at: http://jrgse.github.io."}, {"id": "conf/sigsoft/ErataGTK17", "title": "A tool for automated reasoning about traces based on configurable formal semantics.", "authors": ["Ferhat Erata", "Arda Goknil", "Bedir Tekinerdogan", "Geylani Kardas"], "DOIs": ["https://doi.org/10.1145/3106237.3122825"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACT We present Tarski, a tool for specifying configurable trace semantics to facilitate automated reasoning about traces. Software development projects require that various types of traces be modeled between and within development artifacts. For any given artifact (e.g., requirements, architecture models and source code), Tarski allows the user to specify new trace types and their configurable semantics, while, using the semantics, it automatically infers new traces based on existing traces provided by the user, and checks the consistency of traces. It has been evaluated on three industrial case studies in the automotive domain (https://modelwriter.github.io/Tarski/)."}, {"id": "conf/sigsoft/PastoreM17", "title": "VART: a tool for the automatic detection of regression faults.", "authors": ["Fabrizio Pastore", "Leonardo Mariani"], "DOIs": ["https://doi.org/10.1145/3106237.3122819"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACT In this paper we present VART, a tool for automatically revealing regression faults missed by regression test suites. Interestingly, VART is not limited to faults causing crashing or exceptions, but can reveal faults that cause the violation of application-specific correctness properties. VART achieves this goal by combining static and dynamic program analysis."}, {"id": "conf/sigsoft/RegisCBPRPAGF17", "title": "DynAlloy analyzer: a tool for the specification and analysis of alloy models with dynamic behaviour.", "authors": ["Germ\u00e1n Regis", "C\u00e9sar Cornejo", "Sim\u00f3n Guti\u00e9rrez Brida", "Mariano Politano", "Fernando Raverta", "Pablo Ponzio", "Nazareno Aguirre", "Juan Pablo Galeotti", "Marcelo F. Frias"], "DOIs": ["https://doi.org/10.1145/3106237.3122826"], "tag": ["Models"], "abstract": "ABSTRACT We describe DynAlloy Analyzer, a tool that extends Alloy Analyzer with support for dynamic elements in Alloy models. The tool builds upon Alloy Analyzer in a way that makes it fully compatible with Alloy models, and extends their syntax with a particular idiom, inspired in dynamic logic, for the description of dynamic behaviours, understood as sequences of states over standard Alloy models, in terms of programs. The syntax is broad enough to accommodate abstract dynamic behaviours, e.g., using nondeterministic choice and finite unbounded iteration, as well as more concrete ones, using standard sequential programming constructions. The analysis of DynAlloy models resorts to the analysis of Alloy models, through an optimized translation that often makes the analysis more efficient than that of typical ad-hoc constructions to capture dynamism in Alloy.  Tool screencast, binaries and further details available in: http://dc.exa.unrc.edu.ar/tools/dynalloy"}, {"id": "conf/sigsoft/GreenyerGKDSW17", "title": "From scenario modeling to scenario programming for reactive systems with dynamic topology.", "authors": ["Joel Greenyer", "Daniel Gritzner", "Florian K\u00f6nig", "Jannik Dahlke", "Jianwei Shi", "Eric Wete"], "DOIs": ["https://doi.org/10.1145/3106237.3122827"], "tag": ["Models"], "abstract": "ABSTRACT Software-intensive systems often consist of cooperating reactive components. In mobile and reconfigurable systems, their topology changes at run-time, which influences how the components must cooperate. The Scenario Modeling Language (SML) offers a formal approach for specifying the reactive behavior such systems that aligns with how humans conceive and communicate behavioral requirements. Simulation and formal checks can find specification flaws early. We present a framework for the Scenario-based Programming (SBP) that reflects the concepts of SML in Java and makes the scenario modeling approach available for programming. SBP code can also be generated from SML and extended with platform-specific code, thus streamlining the transition from design to implementation. As an example serves a car-to-x communication system. Demo video and artifact: http://scenariotools.org/esecfse-2017-tool-demo/"}, {"id": "conf/sigsoft/RegisDDA17", "title": "CLTSA: labelled transition system analyser with counting fluent support.", "authors": ["Germ\u00e1n Regis", "Renzo Degiovanni", "Nicol\u00e1s D'Ippolito", "Nazareno Aguirre"], "DOIs": ["https://doi.org/10.1145/3106237.3122828"], "tag": ["Models"], "abstract": "ABSTRACTIn this paper we present CLTSA (Counting Fluents Labelled Transition System Analyser), an extension of LTSA (Labelled Transition System Analyser) that incorporates counting fluents, a useful mechanism to capture properties related to counting events. Counting fluent temporal logic is a formalism for specifying properties of event-based systems, which complements the notion of fluent by the related concept of counting fluent. While fluents allow us to capture boolean properties of the behaviour of a reactive system, counting fluents are numerical values, that enumerate event occurrences. The tool supports a superset of FSP (Finite State Processes), that allows one to define LTL properties involving counting fluents, which can be model checked on FSP processes. Detailed information can be found at http://dc.exa.unrc.edu.ar/tools/cltsa."}, {"id": "conf/sigsoft/DebreceniBBRV17", "title": "The MONDO collaboration framework: secure collaborative modeling over existing version control systems.", "authors": ["Csaba Debreceni", "G\u00e1bor Bergmann", "M\u00e1rton B\u00far", "Istv\u00e1n R\u00e1th", "D\u00e1niel Varr\u00f3"], "DOIs": ["https://doi.org/10.1145/3106237.3122829"], "tag": ["Models"], "abstract": "ABSTRACT Model-based systems engineering of critical cyber-physical systems necessitates effective collaboration between different stakeholders while still providing secure protection of intellectual properties of all involved parties. While engineering artifacts are frequently stored in version control repositories, secure access control is limited to file-level strategies in most existing frameworks where models are split into multiple fragments with all-or-nothing permissions, which becomes a scalability and usability bottleneck in case of complex industrial models.  In this paper, we introduce the MONDO Collaboration Framework, which provides rule-based fine-grained model-level secure access control, property-based locking and automated model merge integrated over existing version control systems such as Subversion (SVN) for storage and version control. Our framework simultaneously supports offline collaboration (asynchronous checkout-modify-commit) on top of off-the-shelf modeling tools and online scenarios (GoogleDocs-style short transactions) scenarios by offering a web-based modeling frontend.  Screencast Demo: https://youtu.be/Ix3CgmsYIU0"}, {"id": "conf/sigsoft/AhmadianPRJ17", "title": "Model-based privacy and security analysis with CARiSMA.", "authors": ["Amir Shayan Ahmadian", "Sven Peldszus", "Qusai Ramadan", "Jan J\u00fcrjens"], "DOIs": ["https://doi.org/10.1145/3106237.3122823"], "tag": ["Models"], "abstract": "ABSTRACT We present CARiSMA, a tool that is originally designed to support model-based security analysis of IT systems. In our recent work, we added several new functionalities to CARiSMA to support the privacy of personal data. Moreover, we introduced a mechanism to assist the system designers to perform a CARiSMA analysis by automatically initializing an appropriate CARiSMA analysis concerning security and privacy requirements. The motivation for our work is Article 25 of Regulation (EU) 2016/679, which requires appropriate technical and organizational controls must be implemented for ensuring that, by default, the processing of personal data complies with the principles on processing of personal data. This implies that initially IT systems must be analyzed to verify if such principles are respected. System models allow the system developers to handle the complexity of systems and to focus on key aspects such as privacy and security. CARiSMA is available at http://carisma.umlsec.de and our screen cast at https://youtu.be/b5zeHig3ARw."}, {"id": "conf/sigsoft/BunyakiatiP17", "title": "Cherry-picking of code commits in long-running, multi-release software.", "authors": ["Panuchart Bunyakiati", "Chadarat Phipathananunth"], "DOIs": ["https://doi.org/10.1145/3106237.3122818"], "tag": ["Misc"], "abstract": "ABSTRACT This paper presents Tartarian, a tool that supports maintenance of software with long-running, multi-release branches in distributed version control systems. When new maintenance code, such as bug fixes and code improvement, is committed into a branch, it is likely that such code can be applied or reused with some other branches. To do so, a developer may manually identify a commit and cherry pick it. Tartarian can support this activity by providing commit hashtags, which the developer uses as metadata to specify their intentions when committing the code. With these tags, Tartarian uses dependency graph, that represents the dependency constraints of the branches, and Branch Identifier, which matches the commit hashtags with the dependency graph, to identify the applicable branches for the commits. Using Tartarian, developers may be able to maintain software with multiple releases more efficiently. A video demo of Tartarian is available at www.github.com/tartarian."}, {"id": "conf/sigsoft/NunezMR17", "title": "ARCC: assistant for repetitive code comprehension.", "authors": ["Wilberto Z. Nunez", "Victor J. Marin", "Carlos R. Rivero"], "DOIs": ["https://doi.org/10.1145/3106237.3122824"], "tag": ["Misc"], "abstract": "ABSTRACT As software projects evolve, carefully understanding the behavior of a program is mandatory before making any change. Repetitive code snippets also tend to appear throughout the codebase, and developers have to understand similar semantics multiple times. Building on this observation, we present Arcc: an Assistant for Repetitive Code Comprehension. The tool, implemented as an Eclipse plugin, assists developers in leveraging knowledge of a program to understand other programs containing a subset of the semantics in the former. Arcc differs from existing approaches in that it uses an extensible knowledge base of recurrent semantic code snippets, instead of heuristics or salient features, to summarize the behavior of a program. Given a program, we detect the occurrences of such snippets. Developers can create strategies as combinations of the snippets found and search for strategy occurrences in their workspace. Arcc highlights the source code related to every snippet and their interleaving, assisting in getting an intuition of similar programs. Finally, Arcc underlines potential common errors associated with the snippets, assisting in detecting overlooked problems. https://youtube.com/playlist?list=PLmizZtBESdPHDyKXKHMXj13r2pBCKzIoA"}, {"id": "conf/sigsoft/ThomeSBB17", "title": "JoanAudit: a tool for auditing common injection vulnerabilities.", "authors": ["Julian Thom\u00e9", "Lwin Khin Shar", "Domenico Bianculli", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1145/3106237.3122822"], "tag": ["Misc"], "abstract": "ABSTRACT JoanAudit is a static analysis tool to assist security auditors in auditing Web applications and Web services for common injection vulnerabilities during software development. It automatically identifies parts of the program code that are relevant for security and generates an HTML report to guide security auditors audit the source code in a scalable way. JoanAudit is configured with various security-sensitive input sources and sinks relevant to injection vulnerabilities and standard sanitization procedures that prevent these vulnerabilities. It can also automatically fix some cases of vulnerabilities in source code \u2014 cases where inputs are directly used in sinks without any form of sanitization \u2014 by using standard sanitization procedures. Our evaluation shows that by using JoanAudit, security auditors are required to inspect only 1% of the total code for auditing common injection vulnerabilities. The screen-cast demo is available at https://github.com/julianthome/joanaudit."}, {"id": "conf/sigsoft/XuXXLL17", "title": "XSearch: a domain-specific cross-language relevant question retrieval tool.", "authors": ["Bowen Xu", "Zhenchang Xing", "Xin Xia", "David Lo", "Xuan-Bach D. Le"], "DOIs": ["https://doi.org/10.1145/3106237.3122820"], "tag": ["Misc"], "abstract": "ABSTRACT During software development process, Chinese developers often seek solutions to the technical problems they encounter by searching relevant questions on Q&A sites. When developers fail to find solutions on Q&A sites in Chinese, they could translate their query and search on the English Q&A sites. However, Chinese developers who are non-native English speakers often are not comfortable to ask or search questions in English, as they do not know the proper translation of the Chinese technical words into the English technical words. Furthermore, the process of manually formulating cross-language queries and determining the importance of query words is a tedious and time-consuming process. For the purpose of helping Chinese developers take advantages of the rich knowledge base of the English version of Stack Overflow and simplify the retrieval process, we propose an automated cross-language relevant question retrieval tool (XSearch) to retrieve relevant English questions on Stack Overflow for a given Chinese question. This tool can address the increasing need for developer to solve technical problems by retrieving cross-language relevant Q&A resources. Demo Tool Website: http://172.93.36.10:8080/XSearch Demo Video: https://goo.gl/h57sed"}, {"id": "conf/sigsoft/Wang17", "title": "Using search-based software engineering to handle the changes with uncertainties for self-adaptive systems.", "authors": ["Lu Wang"], "DOIs": ["https://doi.org/10.1145/3106237.3119871"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT The changes confronting contemporary Self-Adaptive Systems (SASs) are characterized by uncertainties in their relationships, priorities, and contexts. To generate adaptation strategies for handling these changes, existing adaptation planning methods, which ignore these uncertainties, must be improved. This thesis explores the possibilities of using Search-Based Software Engineering (SBSE) to establish a search-based planning method capable of handling multiple changes in an uncertain context without defining their priorities. Meanwhile, both the assurance approach to improving the efficiency of adaptation planning and the selection approach to choosing a unique strategy are proposed to solve emerging research questions that arise when such planning method is applied in actual SASs. From this experience, we are able to derive innovative methods for the designers of SASs as a reference, which may observably improve the ability of SASs and promote the widespread use of SBSE in SASs."}, {"id": "conf/sigsoft/Oliveira17", "title": "DRACO: discovering refactorings that improve architecture using fine-grained co-change dependencies.", "authors": ["Marcos C\u00e9sar de Oliveira"], "DOIs": ["https://doi.org/10.1145/3106237.3119872"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTCo-change dependencies arise whenever two source code entities, such as classes, methods, or fields, change frequently together. Similar to other kinds of software dependencies, it is possible to build software clusters from co-change relationships and, as such, previous studies explored the use of this kind of dependency in several software engineering tasks, such as predicting software faults, recommending related source code changes, and assessing software modularity. In this ongoing work, our goal is to provide tools to discover refactoring opportunities-such as move method, move field, split class, or merge classes-that are revealed when comparing the co-change clusters of fine-grained source code entities (methods, fields, constructors) to the original class decomposition; specifically when a source code entity is in the same class but in different clusters (or vice-versa). Our approach, named Draco, aims to produce minimal refactoring sequences that improve architecture decomposition."}, {"id": "conf/sigsoft/Abusair17", "title": "User- and analysis-driven context aware software development in mobile computing.", "authors": ["Mai Abusair"], "DOIs": ["https://doi.org/10.1145/3106237.3119873"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT Mobile applications may benefit from context awareness since they incur to context changes during their execution and their success depends on the user perceived quality. Context awareness requires context monitoring and system adaptation, these two tasks are very expensive especially in mobile applications. Our research aims at developing a methodology that enables effective context awareness techniques for mobile applications that allows adaptations of the mobile app to context changes so that the desired system quality properties and user satisfaction is maximized. Here effective means selecting a minimum set of context variables to monitor and a minimum set of adaptive tactics to inject into mobile applications that allows to guarantee the required software quality and to maximize the user satisfaction. In this paper, we show the devised methodology on a motivating example, detailing the ongoing work."}, {"id": "conf/sigsoft/Kogel17", "title": "Recommender system for model driven software development.", "authors": ["Stefan K\u00f6gel"], "DOIs": ["https://doi.org/10.1145/3106237.3119874"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT Models are key artifacts in model driven software engineering, similar to source code in traditional software engineering. Integrated development environments help users while writing source code, e.g. with typed auto completions, quick fixes, or automatic refactorings. Similar integrated features are rare for modeling IDEs. The above source code IDE features can be seen as a recommender system.  A recommender system for model driven software engineering can combine data from different sources in order to infer a list of relevant and actionable model changes in real time. These recommendations can speed up working on models by automating repetitive tasks and preventing errors when the changes are atypical for the changed models.  Recommendations can be based on common model transformations that are taken from the literature or learned from models in version control systems. Further information can be taken from instance- to meta-model relationships, modeling related artifacts (e.g. correctness constraints), and versions histories of models under version control.  We created a prototype recommender that analyses the change history of a single model. We computed its accuracy via cross-validation and found that it was between 0.43 and 0.82 for models from an open source project.  In order to have a bigger data set for the evaluation and the learning of model transformation, we also mined repositories from Eclipse projects for Ecore meta models and their versions. We found 4374 meta models with 17249 versions. 244 of these meta models were changed at least ten times and are candidates for learning common model transformations.  We plan to evaluate our recommender system in two ways: (1) In off-line evaluations with data sets of models from the literature, created by us, or taken from industry partners. (2) In on-line user studies with participants from academia and industry, performed as case studies and controlled experiments."}, {"id": "conf/sigsoft/Ellmann17", "title": "On the similarity of software development documentation.", "authors": ["Mathias Ellmann"], "DOIs": ["https://doi.org/10.1145/3106237.3119875"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT Software developers spent 20% of their time on information seeking on Stack Overflow, YouTube or an API reference documentation. Software developers can search within Stack Overflow for duplicates or similar posts. They can also take a look on software development documentations that have similar and additional information included as a Stack Overflow post or a development screencast in order to get new inspirations on how to solve their current development problem. The linkage of same and different types of software development documentation might safe time to evolve new software solutions and might increase the productivity of the developer\u2019s work day. In this paper we will discuss our approach to get a broader understanding of different similarity types (exact, similar and maybe) within and between software documentation as well as an understanding of how different software documentations can be extended."}, {"id": "conf/sigsoft/Schuler17", "title": "Application of search-based software engineering methodologies for test suite optimization and evolution in mission critical mobile application development.", "authors": ["Andreas Schuler"], "DOIs": ["https://doi.org/10.1145/3106237.3119876"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT The demand for high quality mobile applications is constantly rising, especially in mission critical settings. Thus, new software engineering methodologies are needed in order to ensure the desired quality of an application. The research presented proposes a quality assurance methodology for mobile applications through test automation by optimizing test suites. The desired goal is to find a minimal test suite while maintaining efficiency and reducing execution cost. Furthermore to avoid invalidating an optimized test suite as the system under test evolves, the approach further proposes to extract patterns from the applied changes to an application. The evaluation plan comprises a combination of an empirical and an industrial case study based on open source projects and an industrial project in the healthcare domain. It is expected that the presented approach supports the testing process on mobile application platforms."}, {"id": "conf/sigsoft/Kafer17", "title": "Summarizing software engineering communication artifacts from different sources.", "authors": ["Verena K\u00e4fer"], "DOIs": ["https://doi.org/10.1145/3106237.3119877"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT During software development, developers communicate a lot and with many different people. Communication is an important factor, to the point that communication failures are seen as the causes of productivity losses or even project failures. To communicate with each other, software developers use many different tools like mailing lists, forums, issue trackers or chats. Even in a short time span, a lot of information artifacts can arise through these channels, which can be very time consuming to get through after a long vacation or for new members of the team. This paper describes a research plan for an approach which can summarize different communication sources into one big summary using and improving existing text summarization approaches. The resulting tool would have the potential to decrease the effort needed for sense-making and comprehension of communication, as well as the time needed for locating and using information from the communication sources. This reduction in effort will result in a significant increase in the productivity of software development companies."}, {"id": "conf/sigsoft/Nigar17", "title": "Model-based dynamic software project scheduling.", "authors": ["Natasha Nigar"], "DOIs": ["https://doi.org/10.1145/3106237.3119879"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT Software project scheduling, under uncertain and dynamic environments, is one of the most important challenges in software engineering. Recent studies addressed this challenge in both static and dynamic scenarios for small and medium size software projects. The increasing trend of cloud based software solutions (large scale software projects) needs agility not only for sustainable maintenance but also for in time and within budget completion. Therefore, this paper formulates software project scheduling problem (SPSP) as an optimization problem under uncertainties and dynamics for hybrid scRUmP software model. In this regard, a mathematical model is constructed with five objectives as project duration, task fragmentation, robustness, cost, and stability."}, {"id": "conf/sigsoft/Tang17", "title": "System performance optimization via design and configuration space exploration.", "authors": ["Chong Tang"], "DOIs": ["https://doi.org/10.1145/3106237.3119880"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACT The runtime performance of a software system often depends on a large number of static parameters, which usually interact in complex ways to carry out system functionality and influence system performance. It's hard to understand such configuration spaces and find good combinations of parameter values to gain available levels of performance. Engineers in practice often just accept the default settings, leading such systems to significantly underperform relative to their potential. This problem, in turn, has impacts on cost, revenue, customer satisfaction, business reputation, and mission effectiveness. To improve the overall performance of the end-to-end systems, we propose to systematically explore (i) how to design new systems towards good performance through design space synthesis and evaluation, and (ii) how to auto-configure an existing system to obtain better performance through heuristic configuration space search. In addition, this research further studies execution traces of a system to predict runtime performance under new configurations."}, {"id": "conf/sigsoft/Jaffe17", "title": "Suggesting meaningful variable names for decompiled code: a machine translation approach.", "authors": ["Alan Jaffe"], "DOIs": ["https://doi.org/10.1145/3106237.3121274"], "tag": ["Student Research Competition"], "abstract": "ABSTRACT Decompiled code lacks meaningful variable names. We used statistical machine translation to suggest variable names that are natural given the context. This technique has previously been successfully applied to obfuscated JavaScript code, but decompiled C code poses unique challenges in constructing an aligned corpus and selecting the best translation from among several candidates."}, {"id": "conf/sigsoft/Yu17", "title": "Practical symbolic verification of regular properties.", "authors": ["Hengbiao Yu"], "DOIs": ["https://doi.org/10.1145/3106237.3121275"], "tag": ["Student Research Competition"], "abstract": "ABSTRACT It is challenging to verify regular properties of programs. This paper presents symbolic regular verification (SRV), a dynamic symbolic execution based technique for verifying regular properties. The key technique of SRV is a novel synergistic combination of property-oriented path slicing and guiding to mitigate the path explosion problem. Indeed, slicing can prune redundant paths, while guiding can boost the finding of counterexamples. We have implemented SRV for Java and evaluated it on 16 real-world open-source Java programs (totaling 270K lines of code). The experimental results demonstrate the effectiveness and efficiency of SRV."}, {"id": "conf/sigsoft/Pashchenko17", "title": "FOSS version differentiation as a benchmark for static analysis security testing tools.", "authors": ["Ivan Pashchenko"], "DOIs": ["https://doi.org/10.1145/3106237.3121276"], "tag": ["Student Research Competition"], "abstract": "ABSTRACT We propose a novel methodology that allows automatic construction of benchmarks for Static Analysis Security Testing (SAST) tools based on real-world software projects by differencing vulnerable and fixed versions in FOSS repositories. The methodology allows us to evaluate ``actual'' performance of SAST tools (without unrelated alarms). To test our approach, we benchmarked 7 SAST tools (although we report only results for the two best tools), against 70 revisions of four major versions of Apache Tomcat with 62 distinct CVEs as the source of ground truth vulnerabilities."}, {"id": "conf/sigsoft/Kohli17", "title": "DecisionDroid: a supervised learning-based system to identify cloned Android applications.", "authors": ["Ayush Kohli"], "DOIs": ["https://doi.org/10.1145/3106237.3121277"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTThis study presents DecisionDroid, a supervised learning based system to identify cloned Android app pairs. DecisionDroid is trained using a manually verified diverse dataset of 12,000 Android app pairs. On a hundred ten-fold cross validations, DecisionDroid achieved 97.9% precision, 98.3% recall, and 98.4% accuracy."}, {"id": "conf/sigsoft/Abdalkareem17", "title": "Reasons and drawbacks of using trivial npm packages: the developers' perspective.", "authors": ["Rabe Abdalkareem"], "DOIs": ["https://doi.org/10.1145/3106237.3121278"], "tag": ["Student Research Competition"], "abstract": "ABSTRACT Code reuse is traditionally seen as good practice. Recent trends have pushed the idea of code reuse to an extreme, by using packages that implement simple and trivial tasks, which we call \u2018trivial packages\u2019. A recent incident where a trivial package led to the breakdown of some of the most popular web applications such as Facebook and Netflix, put the spotlight on whether using trivial packages should be encouraged. Therefore, in this research, we mine more than 230,000 npm packages and 38,000 JavaScript projects in order to study the prevalence of trivial packages. We found that trivial packages are common, making up 16.8% of the studied npm packages. We performed a survey with 88 Node.js developers who use trivial packages to understand the reasons for and drawbacks of their use. We found that trivial packages are used because they are perceived to be well-implemented and tested pieces of code. However, developers are concerned about maintaining and the risks of breakages due to the extra dependencies trivial packages introduce."}, {"id": "conf/sigsoft/Mujahid17", "title": "Detecting wearable app permission mismatches: a case study on Android wear.", "authors": ["Suhaib Mujahid"], "DOIs": ["https://doi.org/10.1145/3106237.3121279"], "tag": ["Student Research Competition"], "abstract": "ABSTRACT Wearable devices are becoming increasingly popular. These wearable devices run what is known as wearable apps. Wearable apps are packaged with handheld apps, that must be installed on the accompanying handheld device (e.g., phone).  Given that wearable apps are tightly coupled with the handheld apps, any wearable permission must also be requested in the handheld version of the app on the Android Wear platform. However, in some cases, the wearable apps may request permissions that do not exist in the handheld app, resulting in a permission mismatch, and causing the wearable app to error or crash. In this paper, we propose a technique to detect wear app permission mismatches. We perform a case study on 2,409 free Android Wear apps and find that 73 released wearable apps suffer from the permission mismatch problem."}, {"id": "conf/sigsoft/Mills17", "title": "Automating traceability link recovery through classification.", "authors": ["Chris Mills"], "DOIs": ["https://doi.org/10.1145/3106237.3121280"], "tag": ["Student Research Competition"], "abstract": "ABSTRACT Traceability Link Recovery (TLR) is an important software engineering task in which a stakeholder establishes links between related items in two sets of software artifacts. Most existing approaches leverage Information Retrieval (IR) techniques, and formulate the TLR task as a retrieval problem, where pairs of similar artifacts are retrieved and presented to a user. These approaches still require significant human effort, as a stakeholder needs to manually inspect the list of recommendations and decide which ones are true links and which ones are false. In this work, we aim to automate TLR by re-imagining it as a binary classification problem. More specifically, our machine learning classification approach is able to automatically classify each link in the set of all potential links as either valid or invalid, therefore circumventing the substantial human effort required by existing techniques."}, {"id": "conf/sigsoft/Schramm17", "title": "Improving performance of automatic program repair using learned heuristics.", "authors": ["Liam Schramm"], "DOIs": ["https://doi.org/10.1145/3106237.3121281"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTAutomatic program repair offers the promise of significant reduction in debugging time, but still faces challenges in making the process efficient, accurate, and generalizable enough for practical application. Recent efforts such as Prophet demonstrate that machine learning can be used to develop heuristics about which patches are likely to be correct, reducing overfitting problems and improving speed of repair. SearchRepair takes a different approach to accuracy, using blocks of human-written code as patches to better constrain repairs and avoid overfitting. This project combines Prophet's learning techniques with SearchRepair's larger block size to create a method that is both fast and accurate, leading to higher-quality repairs. We propose a novel first-pass filter to substantially reduce the number of candidate patches in SearchRepair and demonstrate 85% reduction in runtime over standard SearchRepair on the IntroClass dataset."}], "2018": [{"id": "conf/sigsoft/Meijer18", "title": "Behind every great deep learning framework is an even greater programming languages concept (keynote).", "authors": ["Erik Meijer"], "DOIs": ["https://doi.org/10.1145/3236024.3280855"], "tag": ["Invited Papers"], "abstract": "ABSTRACTIn many areas, such as image recognition, natural language processing, search, recommendation, autonomous cars, systems software and infrastructure, and even Software Engineering tools themselves, Software 2.0 (= programming using learned models) is quickly swallowing Software 1.0 (= programming using handcrafted algorithms). Where the Software 1.0 Engineer formally specifies their problem, carefully designs algorithms, composes systems out of subsystems or decomposes complex systems into smaller components, the Software 2.0 Engineer amasses training data and simply feeds it into an ML algorithm that will synthesize an approximation of the function whose partial extensional definition is that training data. Instead of code as the artifact of interest, in Software 2.0 it is all about the data where compilation of source code is replaced by training models with data. This new style of programming has far-reaching consequences for traditional software engineering practices. Everything we have learned about life cycle models, project planning and estimation, requirements analysis, program design, construction, debugging, testing, maintenance and implementation, \u2026 runs the danger of becoming obsolete.  One way to try to prepare for the new realities of software engineering is not to zero in on the differences between Software 1.0 and Software 2.0 but instead focus on their similarities. If you carefully look at what a neural net actually represents, you realize that in essence it is a pure function, from multi-dimensional arrays of floating point numbers to multi-dimensional arrays of floating point numbers (tensors). What is special about these functions is that they are differentiable (yes, exactly as you remember from middle school calculus), which allows them to be trained using back propagation. The programming language community has also discovered that there is a deep connection between back propagation and continuations. Moreover, when you look closely at how Software 2.0 Engineers construct complex neural nets like CNNs, RNNs, LSTMs, \u2026 you recognize they are (implicitly) using high-order combinators like map, fold, zip, scan, recursion, conditionals, function composition, \u2026 to compose complex neural network architectures out of simple building blocks. Constructing neural networks using pure and higher-order differentiable functions and training them using reverse-mode automatic differentiation is unsurprisingly called Differentiable Programming. This talk will illustrate the deep programming language principles behind Differentiable Programming, which will hopefully inspire the working Software 1.0 engineer to pay serious attention to the threats and opportunities of Software 2.0."}, {"id": "conf/sigsoft/GlonduJS18", "title": "Ten years of hunting for similar code for fun and profit (keynote).", "authors": ["St\u00e9phane Glondu", "Lingxiao Jiang", "Zhendong Su"], "DOIs": ["https://doi.org/10.1145/3236024.3280856"], "tag": ["Invited Papers"], "abstract": "ABSTRACTIn 2007, the Deckard paper was published at ICSE. Since its publication, it has led to much follow-up research and applications. The paper made two core contributions: a novel vector embedding of structured code for fast similarity detection, and an application of the embedding for clone detection, resulting in the Deckard tool. The vector embedding is simple and easy to adapt. Similar code detection is also fundamental for a range of classical and emerging problems in software engineering, security, and computer science education (e.g., code reuse, refactoring, porting, translation, synthesis, program repair, malware detection, and feedback generation). Both have buttressed the paper\u2019s influence.  In 2018, the Deckard paper received the ACM SIGSOFT Impact Paper award. In this keynote, we take the opportunity to review the work\u2019s inception, evolution and impact on its subsequent work and applications, and to share our thoughts on exciting ongoing and future developments."}, {"id": "conf/sigsoft/LuLL018", "title": "CloudRaid: hunting concurrency bugs in the cloud via log-mining.", "authors": ["Jie Lu", "Feng Li", "Lian Li", "Xiaobing Feng"], "DOIs": ["https://doi.org/10.1145/3236024.3236071"], "tag": ["Concurrency and Races"], "abstract": "ABSTRACTCloud systems suffer from distributed concurrency bugs, which are notoriously difficult to detect and often lead to data loss and service outage. This paper presents CloudRaid, a new effective tool to battle distributed concurrency bugs. CloudRaid automatically detects concurrency bugs in cloud systems, by analyzing and testing those message orderings that are likely to expose errors. We observe that large-scale online cloud applications process millions of user requests per second, exercising many permutations of message orderings extensively. Those already sufficiently-tested message orderings are unlikely to expose errors. Hence, CloudRaid mines logs from previous executions to uncover those message orderings which are feasible, but not sufficiently tested. Specifically, CloudRaid tries to flip the order of a pair of messages <S,P> if they may happen in parallel, but S always arrives before P from existing logs, i.e., excercising the order P \u21a3 S. The log-based approach makes it suitable to live systems. We have applied CloudRaid to automatically test four representative distributed systems: Apache Hadoop2/Yarn, HBase, HDFS and Cassandra. CloudRaid can automatically test 40 different versions of the 4 systems (10 versions per system) in 35 hours, and can successfully trigger 28 concurrency bugs, including 8 new bugs that have never been found before. The 8 new bugs have all been confirmed by their original developers, and 3 of them are considered as critical bugs that have already been fixed."}, {"id": "conf/sigsoft/Chen00M018", "title": "Testing multithreaded programs via thread speed control.", "authors": ["Dongjie Chen", "Yanyan Jiang", "Chang Xu", "Xiaoxing Ma", "Jian Lu"], "DOIs": ["https://doi.org/10.1145/3236024.3236077"], "tag": ["Concurrency and Races"], "abstract": "ABSTRACTA multithreaded program's interleaving space is discrete and astronomically large, making effectively sampling thread schedules for manifesting concurrency bugs a challenging task. Observing that concurrency bugs can be manifested by adjusting thread relative speeds, this paper presents the new concept of speed space in which each vector denotes a family of thread schedules. A multithreaded program's speed space is approximately continuous, easy-to-sample, and preserves certain categories of concurrency bugs. We discuss the design, implementation, and evaluation of our speed-controlled scheduler for exploring adversarial/abnormal schedules. The experimental results confirm that our technique is effective in sampling diverse schedules. Our implementation also found previously unknown concurrency bugs in real-world multithreaded programs."}, {"id": "conf/sigsoft/KiniM018", "title": "Data race detection on compressed traces.", "authors": ["Dileep Kini", "Umang Mathur", "Mahesh Viswanathan"], "DOIs": ["https://doi.org/10.1145/3236024.3236025"], "tag": ["Concurrency and Races"], "abstract": "ABSTRACTWe consider the problem of detecting data races in program traces that have been compressed using straight line programs (SLP), which are special context-free grammars that generate exactly one string, namely the trace that they represent. We consider two classical approaches to race detection --- using the happens-before relation and the lockset discipline. We present algorithms for both these methods that run in time that is linear in the size of the compressed, SLP representation. Typical program executions almost always exhibit patterns that lead to significant compression. Thus, our algorithms are expected to result in large speedups when compared with analyzing the uncompressed trace. Our experimental evaluation of these new algorithms on standard benchmarks confirms this observation."}, {"id": "conf/sigsoft/AdamsenMAT18", "title": "Practical AJAX race detection for JavaScript web applications.", "authors": ["Christoffer Quist Adamsen", "Anders M\u00f8ller", "Saba Alimadadi", "Frank Tip"], "DOIs": ["https://doi.org/10.1145/3236024.3236038"], "tag": ["Concurrency and Races"], "abstract": "ABSTRACTAsynchronous client-server communication is a common source of errors in JavaScript web applications. Such errors are difficult to detect using ordinary testing because of the nondeterministic scheduling of AJAX events. Existing automated event race detectors are generally too imprecise or too inefficient to be practically useful. To address this problem, we present a new approach based on a lightweight combination of dynamic analysis and controlled execution that directly targets identification of harmful AJAX event races.  We experimentally demonstrate using our implementation, AjaxRacer, that this approach is capable of automatically detecting harmful AJAX event races in many websites, and producing informative error messages that support diagnosis and debugging. Among 20 widely used web pages that use AJAX, AjaxRacer discovers harmful AJAX races in 12 of them, with a total of 72 error reports, and with very few false positives."}, {"id": "conf/sigsoft/AmarBB0M18", "title": "Using finite-state models for log differencing.", "authors": ["Hen Amar", "Lingfeng Bao", "Nimrod Busany", "David Lo", "Shahar Maoz"], "DOIs": ["https://doi.org/10.1145/3236024.3236069"], "tag": ["Log Mining"], "abstract": "ABSTRACTMuch work has been published on extracting various kinds of models from logs that document the execution of running systems. In many cases, however, for example in the context of evolution, testing, or malware analysis, engineers are interested not only in a single log but in a set of several logs, each of which originated from a different set of runs of the system at hand. Then, the difference between the logs is the main target of interest.  In this work we investigate the use of finite-state models for log differencing. Rather than comparing the logs directly, we generate concise models to describe and highlight their differences. Specifically, we present two algorithms based on the classic k-Tails algorithm: 2KDiff, which computes and highlights simple traces containing sequences of k events that belong to one log but not the other, and nKDiff, which extends k-Tails from one to many logs, and distinguishes the sequences of length k that are common to all logs from the ones found in only some of them, all on top of a single, rich model. Both algorithms are sound and complete modulo the abstraction defined by the use of k-Tails.  We implemented both algorithms and evaluated their performance on mutated logs that we generated based on models from the literature. We conducted a user study including 60 participants demonstrating the effectiveness of the approach in log differencing tasks. We have further performed a case study to examine the use of our approach in malware analysis. Finally, we have made our work available in a prototype web-application, for experiments."}, {"id": "conf/sigsoft/HeLLZLZ18", "title": "Identifying impactful service system problems via log analysis.", "authors": ["Shilin He", "Qingwei Lin", "Jian-Guang Lou", "Hongyu Zhang", "Michael R. Lyu", "Dongmei Zhang"], "DOIs": ["https://doi.org/10.1145/3236024.3236083"], "tag": ["Log Mining"], "abstract": "ABSTRACTLogs are often used for troubleshooting in large-scale software systems. For a cloud-based online system that provides 24/7 service, a huge number of logs could be generated every day. However, these logs are highly imbalanced in general, because most logs indicate normal system operations, and only a small percentage of logs reveal impactful problems. Problems that lead to the decline of system KPIs (Key Performance Indicators) are impactful and should be fixed by engineers with a high priority. Furthermore, there are various types of system problems, which are hard to be distinguished manually. In this paper, we propose Log3C, a novel clustering-based approach to promptly and precisely identify impactful system problems, by utilizing both log sequences (a sequence of log events) and system KPIs. More specifically, we design a novel cascading clustering algorithm, which can greatly save the clustering time while keeping high accuracy by iteratively sampling, clustering, and matching log sequences. We then identify the impactful problems by correlating the clusters of log sequences with system KPIs. Log3C is evaluated on real-world log data collected from an online service system at Microsoft, and the results confirm its effectiveness and efficiency. Furthermore, our approach has been successfully applied in industrial practice."}, {"id": "conf/sigsoft/JamshidiVKS18", "title": "Learning to sample: exploiting similarities across environments to learn performance models for configurable systems.", "authors": ["Pooyan Jamshidi", "Miguel Velez", "Christian K\u00e4stner", "Norbert Siegmund"], "DOIs": ["https://doi.org/10.1145/3236024.3236074"], "tag": ["Performance"], "abstract": "ABSTRACTMost software systems provide options that allow users to tailor the system in terms of functionality and qualities. The increased flexibility raises challenges for understanding the configuration space and the effects of options and their interactions on performance and other non-functional properties. To identify how options and interactions affect the performance of a system, several sampling and learning strategies have been recently proposed. However, existing approaches usually assume a fixed environment (hardware, workload, software release) such that learning has to be repeated once the environment changes. Repeating learning and measurement for each environment is expensive and often practically infeasible. Instead, we pursue a strategy that transfers knowledge across environments but sidesteps heavyweight and expensive transfer-learning strategies. Based on empirical insights about common relationships regarding (i) influential options, (ii) their interactions, and (iii) their performance distributions, our approach, L2S (Learning to Sample), selects better samples in the target environment based on information from the source environment. It progressively shrinks and adaptively concentrates on interesting regions of the configuration space. With both synthetic benchmarks and several real systems, we demonstrate that L2S outperforms state of the art performance learning and transfer-learning approaches in terms of measurement effort and learning accuracy."}, {"id": "conf/sigsoft/FedorovaMBRWMY18", "title": "Performance comprehension at WiredTiger.", "authors": ["Alexandra Fedorova", "Craig Mustard", "Ivan Beschastnikh", "Julia Rubin", "Augustine Wong", "Svetozar Miucin", "Louis Ye"], "DOIs": ["https://doi.org/10.1145/3236024.3236081"], "tag": ["Performance"], "abstract": "ABSTRACTSoftware debugging is a time-consuming and challenging process. Supporting debugging has been a focus of the software engineering field since its inception with numerous empirical studies, theories, and tools to support developers in this task. Performance bugs and performance debugging is a sub-genre of debugging that has received less attention. In this paper we contribute an empirical case study of performance bug diagnosis in the WiredTiger project, the default database engine behind MongoDB. We perform an in-depth analysis of 44 Jira tickets documenting WiredTiger performance-related issues. We investigate how developers diagnose performance bugs: what information they collect, what tools they use, and what processes they follow. Our findings show that developers spend the majority of their performance debugging time chasing outlier events, such as latency spikes and throughput drops. Yet, they are not properly supported by existing performance debugging tools in this task. We also observe that developers often use tools without knowing in advance whether the obtained information will be relevant to debugging the problem. Therefore, we believe developers can benefit from tools that can be used for unstructured exploration of performance data, rather than for answering specific questions."}, {"id": "conf/sigsoft/LeeHO18", "title": "MemFix: static analysis-based repair of memory deallocation errors for C.", "authors": ["Junhee Lee", "Seongjoon Hong", "Hakjoo Oh"], "DOIs": ["https://doi.org/10.1145/3236024.3236079"], "tag": ["Performance"], "abstract": "ABSTRACTWe present MemFix, an automated technique for fixing memory deallocation errors in C programs. MemFix aims to fix memory-leak, double-free, and use-after-free errors, which occur when developers fail to properly deallocate memory objects. MemFix attempts to fix these errors by finding a set of free-statements that correctly deallocate all allocated objects without causing double-frees and use-after-frees. The key insight behind MemFix is that finding such a set of deallocation statements corresponds to solving an exact cover problem derived from a variant of typestate static analysis. We formally present the technique and experimentally show that MemFix is able to fix real errors found in open-source programs. Because MemFix is based on a sound static analysis, the generated patches guarantee to fix the original errors without introducing new errors."}, {"id": "conf/sigsoft/DashAB18", "title": "RefiNym: using names to refine types.", "authors": ["Santanu Kumar Dash", "Miltiadis Allamanis", "Earl T. Barr"], "DOIs": ["https://doi.org/10.1145/3236024.3236042"], "tag": ["Software Analysis I"], "abstract": "ABSTRACTSource code is bimodal: it combines a formal, algorithmic channel and a natural language channel of identifiers and comments. In this work, we model the bimodality of code with name flows, an assignment flow graph augmented to track identifier names. Conceptual types are logically distinct types that do not always coincide with program types. Passwords and URLs are example conceptual types that can share the program type string. Our tool, RefiNym, is an unsupervised method that mines a lattice of conceptual types from name flows and reifies them into distinct nominal types. For string, RefiNym finds and splits conceptual types originally merged into a single type, reducing the number of same-type variables per scope from 8.7 to 2.2 while eliminating 21.9% of scopes that have more than one same-type variable in scope. This makes the code more self-documenting and frees the type system to prevent a developer from inadvertently assigning data across conceptual types."}, {"id": "conf/sigsoft/BasiosLWKB18", "title": "Darwinian data structure selection.", "authors": ["Michail Basios", "Lingbo Li", "Fan Wu", "Leslie Kanthan", "Earl T. Barr"], "DOIs": ["https://doi.org/10.1145/3236024.3236043"], "tag": ["Software Analysis I"], "abstract": "ABSTRACTData structure selection and tuning is laborious but can vastly improve an application\u2019s performance and memory footprint. Some data structures share a common interface and enjoy multiple implementations. We call them Darwinian Data Structures (DDS), since we can subject their implementations to survival of the fittest. We introduce ARTEMIS a multi-objective, cloud-based search-based optimisation framework that automatically finds optimal, tuned DDS modulo a test suite, then changes an application to use that DDS. ARTEMIS achieves substantial performance improvements for every project in 5 Java projects from DaCapo benchmark, 8 popular projects and 30 uniformly sampled projects from GitHub. For execution time, CPU usage, and memory consumption, ARTEMIS finds at least one solution that improves all measures for 86% (37/43) of the projects. The median improvement across the best solutions is 4.8%, 10.1%, 5.1% for runtime, memory and CPU usage. These aggregate results understate ARTEMIS\u2019s potential impact. Some of the benchmarks it improves are libraries or utility functions. Two examples are gson, a ubiquitous Java serialization framework, and xalan, Apache\u2019s XML transformation tool. ARTEMIS improves gson by 16.5%, 1% and 2.2% for memory, runtime, and CPU; ARTEMIS improves xalan\u2019s memory consumption by 23.5%. Every client of these projects will benefit from these performance improvements."}, {"id": "conf/sigsoft/LiTMS18", "title": "Scalability-first pointer analysis with self-tuning context-sensitivity.", "authors": ["Yue Li", "Tian Tan", "Anders M\u00f8ller", "Yannis Smaragdakis"], "DOIs": ["https://doi.org/10.1145/3236024.3236041"], "tag": ["Software Analysis I"], "abstract": "ABSTRACTContext-sensitivity is important in pointer analysis to ensure high precision, but existing techniques suffer from unpredictable scalability. Many variants of context-sensitivity exist, and it is difficult to choose one that leads to reasonable analysis time and obtains high precision, without running the analysis multiple times.  We present the Scaler framework that addresses this problem. Scaler efficiently estimates the amount of points-to information that would be needed to analyze each method with different variants of context-sensitivity. It then selects an appropriate variant for each method so that the total amount of points-to information is bounded, while utilizing the available space to maximize precision.  Our experimental results demonstrate that Scaler achieves predictable scalability for all the evaluated programs (e.g., speedups can reach 10x for 2-object-sensitivity), while providing a precision that matches or even exceeds that of the best alternative techniques."}, {"id": "conf/sigsoft/ZhaoH18", "title": "DeepSim: deep learning code functional similarity.", "authors": ["Gang Zhao", "Jeff Huang"], "DOIs": ["https://doi.org/10.1145/3236024.3236068"], "tag": ["Deep Learning"], "abstract": "ABSTRACTMeasuring code similarity is fundamental for many software engineering tasks, e.g., code search, refactoring and reuse. However, most existing techniques focus on code syntactical similarity only, while measuring code functional similarity remains a challenging problem. In this paper, we propose a novel approach that encodes code control flow and data flow into a semantic matrix in which each element is a high dimensional sparse binary feature vector, and we design a new deep learning model that measures code functional similarity based on this representation. By concatenating hidden representations learned from a code pair, this new model transforms the problem of detecting functionally similar code to binary classification, which can effectively learn patterns between functionally similar code with very different syntactics.  We have implemented our approach, DeepSim, for Java programs and evaluated its recall, precision and time performance on two large datasets of functionally similar code. The experimental results show that DeepSim significantly outperforms existing state-of-the-art techniques, such as DECKARD, RtvNN, CDLH, and two baseline deep neural networks models."}, {"id": "conf/sigsoft/HellendoornBBA18", "title": "Deep learning type inference.", "authors": ["Vincent J. Hellendoorn", "Christian Bird", "Earl T. Barr", "Miltiadis Allamanis"], "DOIs": ["https://doi.org/10.1145/3236024.3236051"], "tag": ["Deep Learning"], "abstract": "ABSTRACTDynamically typed languages such as JavaScript and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like TypeScript offer a middle-ground for JavaScript: a strict superset of JavaScript, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like JavaScript as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose DeepTyper, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. DeepTyper, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95% precision that could not be inferred without the aid of DeepTyper."}, {"id": "conf/sigsoft/HenkelLLR18", "title": "Code vectors: understanding programs through embedded abstracted symbolic traces.", "authors": ["Jordan Henkel", "Shuvendu K. Lahiri", "Ben Liblit", "Thomas W. Reps"], "DOIs": ["https://doi.org/10.1145/3236024.3236085"], "tag": ["Deep Learning"], "abstract": "ABSTRACTWith the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied.  In this paper, we use abstractions of traces obtained from symbolic execution of a program as a representation for learning word embeddings. We trained a variety of word embeddings under hundreds of parameterizations, and evaluated each learned embedding on a suite of different tasks. In our evaluation, we obtain 93% top-1 accuracy on a benchmark consisting of over 19,000 API-usage analogies extracted from the Linux kernel. In addition, we show that embeddings learned from (mainly) semantic abstractions provide nearly triple the accuracy of those learned from (mainly) syntactic abstractions."}, {"id": "conf/sigsoft/MaLLZG18", "title": "MODE: automated neural network model debugging via state differential analysis and input selection.", "authors": ["Shiqing Ma", "Yingqi Liu", "Wen-Chuan Lee", "Xiangyu Zhang", "Ananth Grama"], "DOIs": ["https://doi.org/10.1145/3236024.3236082"], "tag": ["Deep Learning"], "abstract": "ABSTRACTArtificial intelligence models are becoming an integral part of modern computing systems. Just like software inevitably has bugs, models have bugs too, leading to poor classification/prediction accuracy. Unlike software bugs, model bugs cannot be easily fixed by directly modifying models. Existing solutions work by providing additional training inputs. However, they have limited effectiveness due to the lack of understanding of model misbehaviors and hence the incapability of selecting proper inputs. Inspired by software debugging, we propose a novel model debugging technique that works by first conducting model state differential analysis to identify the internal features of the model that are responsible for model bugs and then performing training input selection that is similar to program input selection in regression testing. Our evaluation results on 29 different models for 6 different applications show that our technique can fix model bugs effectively and efficiently without introducing new bugs. For simple applications (e.g., digit recognition), MODE improves the test accuracy from 75% to 93% on average whereas the state-of-the-art can only improve to 85% with 11 times more training time. For complex applications and models (e.g., object recognition), MODE is able to improve the accuracy from 75% to over 91% in minutes to a few hours, whereas state-of-the-art fails to fix the bug or even degrades the test accuracy."}, {"id": "conf/sigsoft/Baltes018", "title": "Towards a theory of software development expertise.", "authors": ["Sebastian Baltes", "Stephan Diehl"], "DOIs": ["https://doi.org/10.1145/3236024.3236061"], "tag": ["Developer Studies"], "abstract": "ABSTRACTSoftware development includes diverse tasks such as implementing new features, analyzing requirements, and fixing bugs. Being an expert in those tasks requires a certain set of skills, knowledge, and experience. Several studies investigated individual aspects of software development expertise, but what is missing is a comprehensive theory. We present a first conceptual theory of software development expertise that is grounded in data from a mixed-methods survey with 335 software developers and in literature on expertise and expert performance. Our theory currently focuses on programming, but already provides valuable insights for researchers, developers, and employers. The theory describes important properties of software development expertise and which factors foster or hinder its formation, including how developers' performance may decline over time. Moreover, our quantitative results show that developers' expertise self-assessments are context-dependent and that experience is not necessarily related to expertise."}, {"id": "conf/sigsoft/RamSCB18", "title": "What makes a code change easier to review: an empirical investigation on code change reviewability.", "authors": ["Achyudh Ram", "Anand Ashok Sawant", "Marco Castelluccio", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1145/3236024.3236080"], "tag": ["Developer Studies"], "abstract": "ABSTRACTPeer code review is a practice widely adopted in software projects to improve the quality of code. In current code review practices, code changes are manually inspected by developers other than the author before these changes are integrated into a project or put into production. We conducted a study to obtain an empirical understanding of what makes a code change easier to review. To this end, we surveyed published academic literature and sources from gray literature (blogs and white papers), we interviewed ten professional developers, and we designed and deployed a reviewability evaluation tool that professional developers used to rate the reviewability of 98 changes. We find that reviewability is defined through several factors, such as the change description, size, and coherent commit history. We provide recommendations for practitioners and researchers. Public preprint [https://doi.org/10.5281/zenodo.1323659]; data and materials [https://doi.org/10.5281/zenodo.1323659]."}, {"id": "conf/sigsoft/WeiCFFD18", "title": "Singularity: pattern fuzzing for worst case complexity.", "authors": ["Jiayi Wei", "Jia Chen", "Yu Feng", "Kostas Ferles", "Isil Dillig"], "DOIs": ["https://doi.org/10.1145/3236024.3236039"], "tag": ["Testing I"], "abstract": "ABSTRACTWe describe a new blackbox complexity testing technique for determining the worst-case asymptotic complexity of a given application. The key idea is to look for an input pattern \u2014rather than a concrete input\u2014 that maximizes the asymptotic resource usage of the target program. Because input patterns can be described concisely as programs in a restricted language, our method transforms the complexity testing problem to optimal program synthesis. In particular, we express these input patterns using a new model of computation called Recurrent Computation Graph (RCG) and solve the optimal synthesis problem by developing a genetic programming algorithm that operates on RCGs. We have implemented the proposed ideas in a tool called Singularityand evaluate it on a diverse set of benchmarks. Our evaluation shows that Singularitycan effectively discover the worst-case complexity of various algorithms and that it is more scalable compared to existing state-of-the-art techniques. Furthermore, our experiments also corroborate that Singularitycan discover previously unknown performance bugs and availability vulnerabilities in real-world applications such as Google Guava and JGraphT."}, {"id": "conf/sigsoft/RoyPDH18", "title": "Bug synthesis: challenging bug-finding tools with deep faults.", "authors": ["Subhajit Roy", "Awanish Pandey", "Brendan Dolan-Gavitt", "Yu Hu"], "DOIs": ["https://doi.org/10.1145/3236024.3236084"], "tag": ["Testing I"], "abstract": "ABSTRACTIn spite of decades of research in bug detection tools, there is a surprising dearth of ground-truth corpora that can be used to evaluate the efficacy of such tools. Recently, systems such as LAVA and EvilCoder have been proposed to automatically inject bugs into software to quickly generate large bug corpora, but the bugs created so far differ from naturally occurring bugs in a number of ways. In this work, we propose a new automated bug injection system, Apocalypse, that uses formal techniques\u2014symbolic execution, constraint-based program synthesis and model counting\u2014to automatically inject fair (can potentially be discovered by current bug-detection tools), deep (requiring a long sequence of dependencies to be satisfied to fire), uncorrelated (each bug behaving independent of others), reproducible (a trigger input being available) and rare (can be triggered by only a few program inputs) bugs in large software code bases. In our evaluation, we inject bugs into thirty Coreutils programs as well as the TCAS test suite. We find that bugs synthesized by Apocalypse are highly realistic under a variety of metrics, that they do not favor a particular bug-finding strategy (unlike bugs produced by LAVA), and that they are more difficult to find than manually injected bugs, requiring up around 240\u00d7 more tests to discover with a state-of-the-art symbolic execution tool."}, {"id": "conf/sigsoft/Tzoref-BrillM18", "title": "Modify, enhance, select: co-evolution of combinatorial models and test plans.", "authors": ["Rachel Tzoref-Brill", "Shahar Maoz"], "DOIs": ["https://doi.org/10.1145/3236024.3236067"], "tag": ["Testing I"], "abstract": "ABSTRACTThe evolution of software introduces many challenges to its testing. Considerable test maintenance efforts are dedicated to the adaptation of the tests to the changing software. As a result, over time, the test repository may inflate and drift away from an optimal test plan for the software version at hand. Combinatorial Testing (CT) is a well-known test design technique to achieve a small and effective test plan. It requires a manual definition of the test space in the form of a combinatorial model, and then automatically generates a test plan design, which maximizes the added value of each of the tests. CT is considered a best practice, however its applicability to evolving software is hardly explored.  In this work, we introduce a first co-evolution approach for combinatorial models and test plans. By combining three building blocks, to minimally modify existing tests, to enhance them, and to select from them, we provide five alternatives for co-evolving the test plan with the combinatorial model, considering tradeoffs between maximizing fine-grained reuse and minimizing total test plan size, all while meeting the required combinatorial coverage.  We use our solution to co-evolve test plans of 48 real-world industrial models with 68 version commits. The results demonstrate the need for co-evolution as well as the efficiency and effectiveness of our approach and its implementation. We further report on an industrial project that found our co-evolution solution necessary to enable adoption of CT with an agile development process."}, {"id": "conf/sigsoft/DavisCSL18", "title": "The impact of regular expression denial of service (ReDoS) in practice: an empirical study at the ecosystem scale.", "authors": ["James C. Davis", "Christy A. Coghlan", "Francisco Servant", "Dongyoon Lee"], "DOIs": ["https://doi.org/10.1145/3236024.3236027"], "tag": ["Security"], "abstract": "ABSTRACTRegular expressions (regexes) are a popular and powerful means of automatically manipulating text. Regexes are also an understudied denial of service vector (ReDoS). If a regex has super-linear worst-case complexity, an attacker may be able to trigger this complexity, exhausting the victim\u2019s CPU resources and causing denial of service. Existing research has shown how to detect these superlinear regexes, and practitioners have identified super-linear regex anti-pattern heuristics that may lead to such complexity.  In this paper, we empirically study three major aspects of ReDoS that have hitherto been unexplored: the incidence of super-linear regexes in practice, how they can be prevented, and how they can be repaired. In the ecosystems of two of the most popular programming languages \u2014 JavaScript and Python \u2013 we detected thousands of super-linear regexes affecting over 10,000 modules across diverse application domains. We also found that the conventional wisdom for super-linear regex anti-patterns has few false negatives but many false positives; these anti-patterns appear to be necessary, but not sufficient, signals of super-linear behavior. Finally, we found that when faced with a super-linear regex, developers favor revising it over truncating input or developing a custom parser, regardless of whether they had been shown examples of all three fix strategies. These findings motivate further research into ReDoS, since many modules are vulnerable to it and existing mechanisms to avoid it are insufficient. We believe that ReDoS vulnerabilities are a larger threat in practice than might have been guessed."}, {"id": "conf/sigsoft/DongWLGBLXK18", "title": "FraudDroid: automated ad fraud detection for Android apps.", "authors": ["Feng Dong", "Haoyu Wang", "Li Li", "Yao Guo", "Tegawend\u00e9 F. Bissyand\u00e9", "Tianming Liu", "Guoai Xu", "Jacques Klein"], "DOIs": ["https://doi.org/10.1145/3236024.3236045"], "tag": ["Security"], "abstract": "ABSTRACTAlthough mobile ad frauds have been widespread, state-of-the-art approaches in the literature have mainly focused on detecting the so-called static placement frauds, where only a single UI state is involved and can be identified based on static information such as the size or location of ad views. Other types of fraud exist that involve multiple UI states and are performed dynamically while users interact with the app. Such dynamic interaction frauds, although now widely spread in apps, have not yet been explored nor addressed in the literature. In this work, we investigate a wide range of mobile ad frauds to provide a comprehensive taxonomy to the research community. We then propose, FraudDroid, a novel hybrid approach to detect ad frauds in mobile Android apps. FraudDroid analyses apps dynamically to build UI state transition graphs and collects their associated runtime network traffics, which are then leveraged to check against a set of heuristic-based rules for identifying ad fraudulent behaviours. We show empirically that FraudDroid detects ad frauds with a high precision (\u223c 93%) and recall (\u223c 92%). Experimental results further show that FraudDroid is capable of detecting ad frauds across the spectrum of fraud types. By analysing 12,000 ad-supported Android apps, FraudDroid identified 335 cases of fraud associated with 20 ad networks that are further confirmed to be true positive results and are shared with our fellow researchers to promote advanced ad fraud detection."}, {"id": "conf/sigsoft/HuZY18", "title": "AppFlow: using machine learning to synthesize robust, reusable UI tests.", "authors": ["Gang Hu", "Linjie Zhu", "Junfeng Yang"], "DOIs": ["https://doi.org/10.1145/3236024.3236055"], "tag": ["Mobile Apps"], "abstract": "ABSTRACTUI testing is known to be difficult, especially as today\u2019s development cycles become faster. Manual UI testing is tedious, costly and error- prone. Automated UI tests are costly to write and maintain. This paper presents AppFlow, a system for synthesizing highly robust, highly reusable UI tests. It leverages machine learning to automatically recognize common screens and widgets, relieving developers from writing ad hoc, fragile logic to use them in tests. It enables developers to write a library of modular tests for the main functionality of an app category (e.g., an \u201cadd to cart\u201d test for shopping apps). It can then quickly test a new app in the same category by synthesizing full tests from the modular ones in the library. By focusing on the main functionality, AppFlow provides \u201csmoke testing\u201d requiring little manual work. Optionally, developers can customize AppFlow by adding app-specific tests for completeness. We evaluated AppFlow on 60 popular apps in the shopping and the news category, two case studies on the BBC news app and the JackThreads shopping app, and a user-study of 15 subjects on the Wish shopping app. Results show that AppFlow accurately recognizes screens and widgets, synthesizes highly robust and reusable tests, covers 46.6% of all automatable tests for Jackthreads with the tests it synthesizes, and reduces the effort to test a new app by up to 90%. Interestingly, it found eight bugs in the evaluated apps, including seven functionality bugs, despite that they were publicly released and supposedly went through thorough testing."}, {"id": "conf/sigsoft/NoeiCZ18", "title": "Winning the app production rally.", "authors": ["Ehsan Noei", "Daniel Alencar da Costa", "Ying Zou"], "DOIs": ["https://doi.org/10.1145/3236024.3236044"], "tag": ["Mobile Apps"], "abstract": "ABSTRACTWhen a user looks for an Android app in Google Play Store, a number of apps appear in a specific rank. Mobile apps with higher ranks are more likely to be noticed and downloaded by users. The goal of this work is to understand the evolution of ranks and identify the variables that share a strong relationship with ranks. We explore 900 apps with a total of 4,878,011 user-reviews in 30 app development areas. We discover 13 clusters of rank trends. We observe that the majority of the subject apps (i.e., 61%) dropped in the rankings over the two years of our study. By applying a regression model, we find the variables that statistically significantly explain the rank trends, such as the number of releases. Moreover, we build a mixed effects model to study the changes in ranks across apps and various versions of each app. We find that not all the variables that common-wisdom would deem important have a significant relationship with ranks. Furthermore, app developers should not be afraid of a late entry into the market as new apps can achieve higher ranks than existing apps. Finally, we present the findings to 51 developers. According to the feedback, the findings can help app developers to achieve better ranks in Google Play Store."}, {"id": "conf/sigsoft/ZhangVWF18", "title": "One size does not fit all: an empirical study of containerized continuous deployment workflows.", "authors": ["Yang Zhang", "Bogdan Vasilescu", "Huaimin Wang", "Vladimir Filkov"], "DOIs": ["https://doi.org/10.1145/3236024.3236033"], "tag": ["Software Maintenance I"], "abstract": "ABSTRACTContinuous deployment (CD) is a software development practice aimed at automating delivery and deployment of a software product, following any changes to its code. If properly implemented, CD together with other automation in the development process can bring numerous benefits, including higher control and flexibility over release schedules, lower risks, fewer defects, and easier on-boarding of new developers. Here we focus on the (r)evolution in CD workflows caused by containerization, the virtualization technology that enables packaging an application together with all its dependencies and execution environment in a light-weight, self-contained unit, of which Docker has become the de-facto industry standard. There are many available choices for containerized CD workflows, some more appropriate than others for a given project. Owing to cross-listing of GitHub projects on Docker Hub, in this paper we report on a mixed-methods study to shed light on developers' experiences and expectations with containerized CD workflows. Starting from a survey, we explore the motivations, specific workflows, needs, and barriers with containerized CD. We find two prominent workflows, based on the automated builds feature on Docker Hub or continuous integration services, with different trade-offs. We then propose hypotheses and test them in a large-scale quantitative study."}, {"id": "conf/sigsoft/TuZZZ18", "title": "Be careful of when: an empirical study on time-related misuse of issue tracking data.", "authors": ["Feifei Tu", "Jiaxin Zhu", "Qimu Zheng", "Minghui Zhou"], "DOIs": ["https://doi.org/10.1145/3236024.3236054"], "tag": ["Software Maintenance I"], "abstract": "ABSTRACTIssue tracking data have been used extensively to aid in predicting or recommending software development practices. Issue attributes typically change over time, but users may use data from a separate time of data collection rather than the time of their application scenarios. We, therefore, investigate data leakage, which results from ignoring the chronological order in which the data were produced. Information leaked from the \"future\" makes prediction models misleadingly optimistic. We examine existing literature to confirm the existence of data leakage and reproduce three typical studies (detecting duplicate issues, localizing issues, and predicting issue-fix time) adjusted for appropriate data to quantify the impact of the data leakage. We confirm that 11 out of 58 studies have leakage problem, while 44 are suspected. We observe biased results caused by data leakage while the extent is not striking. Attributes of summary, component, and assignee have the largest impact on the results. Our findings suggest that data users are often unaware of the context of the data being produced. We recommend researchers and practitioners who attempt to utilize issue tracking data to address software development problems to have a full understanding of their application scenarios, the origin and change of the data, and the influential issue attributes to manage data leakage risks."}, {"id": "conf/sigsoft/WangWLWWYYZC18", "title": "Do the dependency conflicts in my project matter?", "authors": ["Ying Wang", "Ming Wen", "Zhenwei Liu", "Rongxin Wu", "Rui Wang", "Bo Yang", "Hai Yu", "Zhiliang Zhu", "Shing-Chi Cheung"], "DOIs": ["https://doi.org/10.1145/3236024.3236056"], "tag": ["Software Maintenance I"], "abstract": "ABSTRACTIntensive dependencies of a Java project on third-party libraries can easily lead to the presence of multiple library or class versions on its classpath. When this happens, JVM will load one version and shadows the others. Dependency conflict (DC) issues occur when the loaded version fails to cover a required feature (e.g., method) referenced by the project, thus causing runtime exceptions. However, the warnings of duplicate classes or libraries detected by existing build tools such as Maven can be benign since not all instances of duplication will induce runtime exceptions, and hence are often ignored by developers. In this paper, we conducted an empirical study on real-world DC issues collected from large open source projects. We studied the manifestation and fixing patterns of DC issues. Based on our findings, we designed Decca, an automated detection tool that assesses DC issues' severity and filters out the benign ones. Our evaluation results on 30 projects show that Decca achieves a precision of 0.923 and recall of 0.766 in detecting high-severity DC issues. Decca also detected new DC issues in these projects. Subsequently, 20 DC bug reports were filed, and 11 of them were confirmed by developers. Issues in 6 reports were fixed with our suggested patches."}, {"id": "conf/sigsoft/PauckBW18", "title": "Do Android taint analysis tools keep their promises?", "authors": ["Felix Pauck", "Eric Bodden", "Heike Wehrheim"], "DOIs": ["https://doi.org/10.1145/3236024.3236029"], "tag": ["Software Analysis II"], "abstract": "ABSTRACTIn recent years, researchers have developed a number of tools to conduct taint analysis of Android applications. While all the respective papers aim at providing a thorough empirical evaluation, comparability is hindered by varying or unclear evaluation targets. Sometimes, the apps used for evaluation are not precisely described. In other cases, authors use an established benchmark but cover it only partially. In yet other cases, the evaluations differ in terms of the data leaks searched for, or lack a ground truth to compare against. All those limitations make it impossible to truly compare the tools based on those published evaluations.  We thus present ReproDroid, a framework allowing the accurate comparison of Android taint analysis tools. ReproDroid supports researchers in inferring the ground truth for data leaks in apps, in automatically applying tools to benchmarks, and in evaluating the obtained results. We use ReproDroid to comparatively evaluate on equal grounds the six prominent taint analysis tools Amandroid, DIALDroid, DidFail, DroidSafe, FlowDroid and IccTA. The results are largely positive although four tools violate some promises concerning features and accuracy. Finally, we contribute to the area of unbiased benchmarking with a new and improved version of the open test suite DroidBench."}, {"id": "conf/sigsoft/ZhaoARJO18", "title": "Neural-augmented static analysis of Android communication.", "authors": ["Jinman Zhao", "Aws Albarghouthi", "Vaibhav Rastogi", "Somesh Jha", "Damien Octeau"], "DOIs": ["https://doi.org/10.1145/3236024.3236066"], "tag": ["Software Analysis II"], "abstract": "ABSTRACTWe address the problem of discovering communication links between applications in the popular Android mobile operating system, an important problem for security and privacy in Android. Any scalable static analysis in this complex setting is bound to produce an excessive amount of false-positives, rendering it impractical. To improve precision, we propose to augment static analysis with a trained neural-network model that estimates the probability that a communication link truly exists. We describe a neural-network architecture that encodes abstractions of communicating objects in two applications and estimates the probability with which a link indeed exists. At the heart of our architecture are type-directed encoders (TDE), a general framework for elegantly constructing encoders of a compound data type by recursively composing encoders for its constituent types. We evaluate our approach on a large corpus of Android applications, and demonstrate that it achieves very high accuracy. Further, we conduct thorough interpretability studies to understand the internals of the learned neural networks."}, {"id": "conf/sigsoft/SainiFLBL18", "title": "Oreo: detection of clones in the twilight zone.", "authors": ["Vaibhav Saini", "Farima Farmahinifarahani", "Yadong Lu", "Pierre Baldi", "Cristina V. Lopes"], "DOIs": ["https://doi.org/10.1145/3236024.3236026"], "tag": ["Software Analysis II"], "abstract": "ABSTRACTSource code clones are categorized into four types of increasing difficulty of detection, ranging from purely textual (Type-1) to purely semantic (Type-4). Most clone detectors reported in the literature work well up to Type-3, which accounts for syntactic differences. In between Type-3 and Type-4, however, there lies a spectrum of clones that, although still exhibiting some syntactic similarities, are extremely hard to detect \u2013 the Twilight Zone. Most clone detectors reported in the literature fail to operate in this zone. We present Oreo, a novel approach to source code clone detection that not only detects Type-1 to Type-3 clones accurately, but is also capable of detecting harder-to-detect clones in the Twilight Zone. Oreo is built using a combination of machine learning, information retrieval, and software metrics. We evaluate the recall of Oreo on BigCloneBench, and perform manual evaluation for precision. Oreo has both high recall and precision. More importantly, it pushes the boundary in detection of clones with moderate to weak syntactic similarity in a scalable manner"}, {"id": "conf/sigsoft/YiH18", "title": "Concurrency verification with maximal path causality.", "authors": ["Qiuping Yi", "Jeff Huang"], "DOIs": ["https://doi.org/10.1145/3236024.3236048"], "tag": ["Symbolic Execution and Constraint Solving"], "abstract": "ABSTRACTWe present a technique that systematically explores the state spaces of concurrent programs across both the schedule space and the input space. The cornerstone is a new model called Maximal Path Causality (MPC), which captures all combinations of thread schedules and program inputs that reach the same path as one equivalency class, and generates a unique schedule+input combination to explore each path. Moreover, the exploration for different paths can be easily parallelized. Our extensive evaluation on both popular concurrency benchmarks and real-world C/C++ applications shows that MPC significantly improves the performance of existing techniques."}, {"id": "conf/sigsoft/GuoWW18", "title": "Adversarial symbolic execution for detecting concurrency-related cache timing leaks.", "authors": ["Shengjian Guo", "Meng Wu", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3236024.3236028"], "tag": ["Symbolic Execution and Constraint Solving"], "abstract": "ABSTRACTThe timing characteristics of cache, a high-speed storage between the fast CPU and the slow memory, may reveal sensitive information of a program, thus allowing an adversary to conduct side-channel attacks. Existing methods for detecting timing leaks either ignore cache all together or focus only on passive leaks generated by the program itself, without considering leaks that are made possible by concurrently running some other threads. In this work, we show that timing-leak-freedom is not a compositional property: a program that is not leaky when running alone may become leaky when interleaved with other threads. Thus, we develop a new method, named adversarial symbolic execution, to detect such leaks. It systematically explores both the feasible program paths and their interleavings while modeling the cache, and leverages an SMT solver to decide if there are timing leaks. We have implemented our method in LLVM and evaluated it on a set of real-world ciphers with 14,455 lines of C code in total. Our experiments demonstrate both the efficiency of our method and its effectiveness in detecting side-channel leaks."}, {"id": "conf/sigsoft/MechtaevGCR18", "title": "Symbolic execution with existential second-order constraints.", "authors": ["Sergey Mechtaev", "Alberto Griggio", "Alessandro Cimatti", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1145/3236024.3236049"], "tag": ["Symbolic Execution and Constraint Solving"], "abstract": "ABSTRACTSymbolic execution systematically explores program paths by solving path conditions --- formulas over symbolic variables. Typically, the symbolic variables range over numbers, arrays and strings. We introduce symbolic execution with existential second-order constraints --- an extension of traditional symbolic execution that allows symbolic variables to range over functions whose interpretations are restricted by a user-defined language. The aims of this new technique are twofold. First, it offers a general analysis framework that can be applied in multiple domains such as program repair and library modelling. Secondly, it addresses the path explosion problem of traditional first-order symbolic execution in certain applications. To realize this technique, we integrate symbolic execution with program synthesis. Specifically, we propose a method of second-order constraint solving that provides efficient proofs of unsatisfiability, which is critical for the performance of symbolic execution. Our evaluation shows that the proposed technique (1) helps to repair programs with loops by mitigating the path explosion, (2) can enable analysis of applications written against unavailable libraries by modelling these libraries from the usage context."}, {"id": "conf/sigsoft/AydinEBBGBY18", "title": "Parameterized model counting for string and numeric constraints.", "authors": ["Abdulbaki Aydin", "William Eiers", "Lucas Bang", "Tegan Brennan", "Miroslav Gavrilov", "Tevfik Bultan", "Fang Yu"], "DOIs": ["https://doi.org/10.1145/3236024.3236064"], "tag": ["Symbolic Execution and Constraint Solving"], "abstract": "ABSTRACTRecently, symbolic program analysis techniques have been extended to quantitative analyses using model counting constraint solvers. Given a constraint and a bound, a model counting constraint solver computes the number of solutions for the constraint within the bound. We present a parameterized model counting constraint solver for string and numeric constraints. We first construct a multi-track deterministic finite state automaton that accepts all solutions to the given constraint. We limit the numeric constraints to linear integer arithmetic, and for non-regular string constraints we over-approximate the solution set. Counting the number of accepting paths in the generated automaton solves the model counting problem. Our approach is parameterized in the sense that, we do not assume a finite domain size during automata construction, resulting in a potentially infinite set of solutions, and our model counting approach works for arbitrarily large bounds. We experimentally demonstrate the effectiveness of our approach on a large set of string and numeric constraints extracted from software applications. We experimentally compare our tool to five existing model counting constraint solvers for string and numeric constraints and demonstrate that our tool is as efficient and as or more precise than other solvers. Moreover, our tool can handle mixed constraints with string and integer variables that no other tool can."}, {"id": "conf/sigsoft/BianLSH018", "title": "NAR-miner: discovering negative association rules from code for bug detection.", "authors": ["Pan Bian", "Bin Liang", "Wenchang Shi", "Jianjun Huang", "Yan Cai"], "DOIs": ["https://doi.org/10.1145/3236024.3236032"], "tag": ["Mining"], "abstract": "ABSTRACTInferring programming rules from source code based on data mining techniques has been proven to be effective to detect software bugs. Existing studies focus on discovering positive rules in the form of A \u21d2 B, indicating that when operation A appears, operation B should also be here. Unfortunately, the negative rules (A \u21d2 \u00ac B), indicating the mutual suppression or conflict relationships among program elements, have not gotten the attention they deserve. In fact, violating such negative rules can also result in serious bugs. In this paper, we propose a novel method called NAR-Miner to automatically extract negative association programming rules from large-scale systems, and detect their violations to find bugs. However, mining negative rules faces a more serious rule explosion problem than mining positive ones. Most of the obtained negative rules are uninteresting and can lead to unacceptable false alarms. To address the issue, we design a semantics-constrained mining algorithm to focus rule mining on the elements with strong semantic relationships. Furthermore, we introduce information entropy to rank candidate negative rules and highlight the interesting ones. Consequently, we effectively mitigate the rule explosion problem. We implement NAR-Miner and apply it to a Linux kernel (v4.12-rc6). The experiments show that the uninteresting rules are dramatically reduced and 17 detected violations have been confirmed as real bugs and patched by kernel community. We also apply NAR-Miner to PostgreSQL, OpenSSL and FFmpeg and discover six real bugs."}, {"id": "conf/sigsoft/DeFreezTR18", "title": "Path-based function embedding and its application to error-handling specification mining.", "authors": ["Daniel DeFreez", "Aditya V. Thakur", "Cindy Rubio-Gonz\u00e1lez"], "DOIs": ["https://doi.org/10.1145/3236024.3236059"], "tag": ["Mining"], "abstract": "ABSTRACTIdentifying relationships among program elements is useful for program understanding, debugging, and analysis. One such kind of relationship is synonymy. Function synonyms are functions that play a similar role in code; examples include functions that perform initialization for different device drivers, and functions that implement different symmetric-key encryption schemes. Function synonyms are not necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or functional equivalence cannot be used to identify them. This paper presents Func2<pre>vec</pre>, a technique that learns an embedding mapping each function to a vector in a continuous vector space such that vectors for function synonyms are in close proximity. We compute the function embedding by training a neural network on sentences generated using random walks over the interprocedural control-flow graph. We show the effectiveness of Func2<pre>vec</pre> at identifying function synonyms in the Linux kernel. Finally, we apply Func2<pre>vec</pre> to the problem of mining error-handling specifications in Linux file systems and drivers. We show that the function synonyms identified by Func2<pre>vec</pre> result in error-handling specifications with high support."}, {"id": "conf/sigsoft/HeH18", "title": "Putback-based bidirectional model transformations.", "authors": ["Xiao He", "Zhenjiang Hu"], "DOIs": ["https://doi.org/10.1145/3236024.3236070"], "tag": ["Models"], "abstract": "ABSTRACTBidirectional model transformation (BX) plays a vital role in Model-Driven Engineering. A major challenge in conventional relational and bidirectionalization-based BX approaches is the ambiguity issue, i.e., the backward transformation may not be uniquely determined by the consistency relation or the forward transformation. A promising solution to the ambiguity issue is to adopt putback-based bidirectional programming, which realizes a BX by specifying the backward transformation. However, existing putback-based approaches do not support multiple conversions of the same node (namely a shared node). Since a model is a graph, shared nodes are very common and inevitable. Consequently, existing putback-based approaches cannot be directly applied to bidirectional model transformation. This paper proposes a novel approach to BX. We define a new model-merging-based BX combinator, which can combine two BXs owning shared nodes into a well behaved composite BX. Afterwards, we propose a putback-based BX language XMU to address the ambiguity issue, which is built on the model-merging-based BX combinator. We present the formal semantics of XMU which can be proven well behaved. Finally, a tool support is also introduced to illustrate the usefulness of our approach."}, {"id": "conf/sigsoft/HebigSBPW18", "title": "Model transformation languages under a magnifying glass: a controlled experiment with Xtend, ATL, and QVT.", "authors": ["Regina Hebig", "Christoph Seidl", "Thorsten Berger", "John Kook Pedersen", "Andrzej Wasowski"], "DOIs": ["https://doi.org/10.1145/3236024.3236046"], "tag": ["Models"], "abstract": "ABSTRACTIn Model-Driven Software Development, models are automatically processed to support the creation, build, and execution of systems. A large variety of dedicated model-transformation languages exists, promising to efficiently realize the automated processing of models. To investigate the actual benefit of using such specialized languages, we performed a large-scale controlled experiment in which over 78 subjects solve 231 individual tasks using three languages. The experiment sheds light on commonalities and differences between model transformation languages (ATL, QVT-O) and on benefits of using them in common development tasks (comprehension, change, and creation) against a modern general-purpose language (Xtend). Our results show no statistically significant benefit of using a dedicated transformation language over a modern general-purpose language. However, we were able to identify several aspects of transformation programming where domain-specific transformation languages do appear to help, including copying objects, context identification, and conditioning the computation on types."}, {"id": "conf/sigsoft/ChenFKM18", "title": "Applications of psychological science for actionable analytics.", "authors": ["Di Chen", "Wei Fu", "Rahul Krishna", "Tim Menzies"], "DOIs": ["https://doi.org/10.1145/3236024.3236050"], "tag": ["Models"], "abstract": "ABSTRACTAccording to psychological scientists, humans understand models that most match their own internal models, which they characterize as lists of \"heuristic\"s (i.e. lists of very succinct rules). One such heuristic rule generator is the Fast-and-Frugal Trees (FFT) preferred by psychological scientists. Despite their successful use in many applied domains, FFTs have not been applied in software analytics. Accordingly, this paper assesses FFTs for software analytics.  We find that FFTs are remarkably effective in that their models are very succinct (5 lines or less describing a binary decision tree) while also outperforming result from very recent, top-level, conference papers. Also, when we restrict training data to operational attributes (i.e., those attributes that are frequently changed by developers), the performance of FFTs are not effected (while the performance of other learners can vary wildly).  Our conclusions are two-fold. Firstly, there is much that software analytics community could learn from psychological science. Secondly, proponents of complex methods should always baseline those methods against simpler alternatives. For example, FFTs could be used as a standard baseline learner against which other software analytics tools are compared."}, {"id": "conf/sigsoft/SongM018", "title": "A novel automated approach for software effort estimation based on data augmentation.", "authors": ["Liyan Song", "Leandro L. Minku", "Xin Yao"], "DOIs": ["https://doi.org/10.1145/3236024.3236052"], "tag": ["Estimation and Prediction"], "abstract": "ABSTRACTSoftware effort estimation (SEE) usually suffers from data scarcity problem due to the expensive or long process of data collection. As a result, companies usually have limited projects for effort estimation, causing unsatisfactory prediction performance. Few studies have investigated strategies to generate additional SEE data to aid such learning. We aim to propose a synthetic data generator to address the data scarcity problem of SEE. Our synthetic generator enlarges the SEE data set size by slightly displacing some randomly chosen training examples. It can be used with any SEE method as a data preprocessor. Its effectiveness is justified with 6 state-of-the-art SEE models across 14 SEE data sets. We also compare our data generator against the only existing approach in the SEE literature. Experimental results show that our synthetic projects can significantly improve the performance of some SEE methods especially when the training data is insufficient. When they cannot significantly improve the prediction performance, they are not detrimental either. Besides, our synthetic data generator is significantly superior or perform similarly to its competitor in the SEE literature. Therefore, our data generator plays a non-harmful if not significantly beneficial effect on the SEE methods investigated in this paper. Therefore, it is helpful in addressing the data scarcity problem of SEE."}, {"id": "conf/sigsoft/LinHDZSXLLWYCZ18", "title": "Predicting Node failure in cloud service systems.", "authors": ["Qingwei Lin", "Ken Hsieh", "Yingnong Dang", "Hongyu Zhang", "Kaixin Sui", "Yong Xu", "Jian-Guang Lou", "Chenggang Li", "Youjiang Wu", "Randolph Yao", "Murali Chintalapati", "Dongmei Zhang"], "DOIs": ["https://doi.org/10.1145/3236024.3236060"], "tag": ["Estimation and Prediction"], "abstract": "ABSTRACTIn recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice."}, {"id": "conf/sigsoft/WoodRAM18", "title": "Detecting speech act types in developer question/answer conversations during bug repair.", "authors": ["Andrew Wood", "Paige Rodeghero", "Ameer Armaly", "Collin McMillan"], "DOIs": ["https://doi.org/10.1145/3236024.3236031"], "tag": ["Repair and Synthesis"], "abstract": "ABSTRACTThis paper targets the problem of speech act detection in conversations about bug repair. We conduct a ``Wizard of Oz'' experiment with 30 professional programmers, in which the programmers fix bugs for two hours, and use a simulated virtual assistant for help. Then, we use an open coding manual annotation procedure to identify the speech act types in the conversations. Finally, we train and evaluate a supervised learning algorithm to automatically detect the speech act types in the conversations. In 30 two-hour conversations, we made 2459 annotations and uncovered 26 speech act types. Our automated detection achieved 69% precision and 50% recall. The key application of this work is to advance the state of the art for virtual assistants in software engineering. Virtual assistant technology is growing rapidly, though applications in software engineering are behind those in other areas, largely due to a lack of relevant data and experiments. This paper targets this problem in the area of developer Q/A conversations about bug repair."}, {"id": "conf/sigsoft/StoccoYM18", "title": "Visual web test repair.", "authors": ["Andrea Stocco", "Rahulkrishna Yandrapally", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1145/3236024.3236063"], "tag": ["Repair and Synthesis"], "abstract": "ABSTRACTWeb tests are prone to break frequently as the application under test evolves, causing much maintenance effort in practice. To detect the root causes of a test breakage, developers typically inspect the test's interactions with the application through the GUI. Existing automated test repair techniques focus instead on the code and entirely ignore visual aspects of the application. We propose a test repair technique that is informed by a visual analysis of the application. Our approach captures relevant visual information from tests execution and analyzes them through a fast image processing pipeline to visually validate test cases as they re-executed for regression purposes. Then, it reports the occurrences of breakages and potential fixes to the testers. Our approach is also equipped with a local crawling mechanism to handle non-trivial breakage scenarios such as the ones that require to repair the test's workflow. We implemented our approach in a tool called Vista. Our empirical evaluation on 2,672 test cases spanning 86 releases of four web applications shows that Vista is able to repair, on average, 81% of the breakages, a 41% increment with respect to existing techniques."}, {"id": "conf/sigsoft/SiLZAKN18", "title": "Syntax-guided synthesis of Datalog programs.", "authors": ["Xujie Si", "Woosuk Lee", "Richard Zhang", "Aws Albarghouthi", "Paraschos Koutris", "Mayur Naik"], "DOIs": ["https://doi.org/10.1145/3236024.3236034"], "tag": ["Repair and Synthesis"], "abstract": "ABSTRACTDatalog has witnessed promising applications in a variety of domains. We propose a programming-by-example system, ALPS, to synthesize Datalog programs from input-output examples. Scaling synthesis to realistic programs in this manner is challenging due to the rich expressivity of Datalog. We present a syntax-guided synthesis approach that prunes the search space by exploiting the observation that in practice Datalog programs comprise rules that have similar latent syntactic structure. We evaluate ALPS on a suite of 34 benchmarks from three domains\u2014knowledge discovery, program analysis, and database queries. The evaluation shows that ALPS can synthesize 33 of these benchmarks, and outperforms the state-of-the-art tools Metagol and Zaatar, which can synthesize only up to 10 of the benchmarks."}, {"id": "conf/sigsoft/MaddoxLR18", "title": "Large-scale study of substitutability in the presence of effects.", "authors": ["Jackson Maddox", "Yuheng Long", "Hridesh Rajan"], "DOIs": ["https://doi.org/10.1145/3236024.3236075"], "tag": ["Software Maintenance II"], "abstract": "ABSTRACTA majority of modern software is constructed using languages that compute by producing side-effects such as reading/writing from/to files, throwing exceptions, acquiring locks, etc. To understand a piece of software, e.g. a class, it is important for a developer to understand its side-effects. Similarly, to replace a class with another, it is important to understand whether the replacement is a safe substitution for the former in terms of its behavior, a property known as substitutability, because mismatch may lead to bugs. The problem is especially severe for superclass-subclass pairs since at runtime an instance of the subclass may be used in the client code where a superclass is mentioned. Despite the importance of this property, we do not yet know whether substitutability w.r.t. effects between subclass and superclass is preserved in the wild, and if not what sorts of substitutability violations are common and what is the impact of such violations. This paper conducts a large scale study on over 20 million Java classes, in order to compare the effects of the methods of subclasses and superclasses in practice. Our comprehensive study considers the exception, synchronization, I/O, and method call effects. It reveals that in pairs with effects, only 8-24% have the same effects, and 31-56% of submethods have more effects, and the effects of a large percentage of submethods cannot be inferred from the supermethod."}, {"id": "conf/sigsoft/GaoDQGW0HZW18", "title": "An empirical study on crash recovery bugs in large-scale distributed systems.", "authors": ["Yu Gao", "Wensheng Dou", "Feng Qin", "Chushu Gao", "Dong Wang", "Jun Wei", "Ruirui Huang", "Li Zhou", "Yongming Wu"], "DOIs": ["https://doi.org/10.1145/3236024.3236030"], "tag": ["Software Maintenance II"], "abstract": "ABSTRACTIn large-scale distributed systems, node crashes are inevitable, and can happen at any time. As such, distributed systems are usually designed to be resilient to these node crashes via various crash recovery mechanisms, such as write-ahead logging in HBase and hinted handoffs in Cassandra. However, faults in crash recovery mechanisms and their implementations can introduce intricate crash recovery bugs, and lead to severe consequences.  In this paper, we present CREB, the most comprehensive study on 103 Crash REcovery Bugs from four popular open-source distributed systems, including ZooKeeper, Hadoop MapReduce, Cassandra and HBase. For all the studied bugs, we analyze their root causes, triggering conditions, bug impacts and fixing. Through this study, we obtain many interesting findings that can open up new research directions for combating crash recovery bugs."}, {"id": "conf/sigsoft/NguyenTPNT0NN18", "title": "Complementing global and local contexts in representing API descriptions to improve API retrieval tasks.", "authors": ["Thanh Van Nguyen", "Ngoc M. Tran", "Hung Phan", "Trong Duc Nguyen", "Linh H. Truong", "Anh Tuan Nguyen", "Hoan Anh Nguyen", "Tien N. Nguyen"], "DOIs": ["https://doi.org/10.1145/3236024.3236036"], "tag": ["Software Maintenance II"], "abstract": "ABSTRACTWhen being trained on API documentation and tutorials, Word2vec produces vector representations to estimate the relevance between texts and API elements. However, existing Word2vec-based approaches to measure document similarities aggregate Word2vec vectors of individual words or APIs to build the representation of a document as if the words are independent. Thus, the semantics of API descriptions or code fragments are not well represented.  In this work, we introduce D2Vec, a new model that fits with API documentation better than Word2vec. D2Vec is a neural network model that considers two complementary contexts to better capture the semantics of API documentation. We first connect the global context of the current API topic under description to all the text phrases within the description of that API. Second, the local orders of words and API elements in the text phrases are maintained in computing the vector representations for the APIs. We conducted an experiment to verify two intrinsic properties of D2Vec's vectors: 1) similar words and relevant API elements are projected into nearby locations; and 2) some vector operations carry semantics. We demonstrate the usefulness and good performance of D2Vec in three applications: API code search (text-to-code retrieval), API tutorial fragment search (code-to-text retrieval), and mining API mappings between software libraries (code-to-code retrieval). Finally, we provide actionable insights and implications for researchers in using our model in other applications with other types of documents."}, {"id": "conf/sigsoft/KateOZEX18", "title": "Phys: probabilistic physical unit assignment and inconsistency detection.", "authors": ["Sayali Kate", "John-Paul Ore", "Xiangyu Zhang", "Sebastian G. Elbaum", "Zhaogui Xu"], "DOIs": ["https://doi.org/10.1145/3236024.3236035"], "tag": ["Probabilistic Reasoning"], "abstract": "ABSTRACTProgram variables used in robotic and cyber-physical systems often have implicit physical units that cannot be determined from their variable types. Inferring an abstract physical unit type for variables and checking their physical unit type consistency is of particular importance for validating the correctness of such systems. For instance, a variable with the unit of \u2018meter\u2019 should not be assigned to another variable with the unit of \u2018degree-per-second\u2019. Existing solutions have various limitations such as requiring developers to annotate variables with physical units and only handling variables that are directly or transitively used in popular robotic libraries with known physical unit information. We observe that there are a lot of physical unit hints in these softwares such as variable names and specific forms of expressions. These hints have uncertainty as developers may not respect conventions. We propose to model them with probability distributions and conduct probabilistic inference. At the end, our technique produces a unit distribution for each variable. Unit inconsistencies can then be detected using the highly probable unit assignments. Experimental results on 30 programs show that our technique can infer units for 159.3% more variables compared to the state-of-the-art with more than 88.7% true positives, and inconsistencies detection on 90 programs shows that our technique reports 103.3% more inconsistencies with 85.3% true positives."}, {"id": "conf/sigsoft/DuttaLHM18", "title": "Testing probabilistic programming systems.", "authors": ["Saikat Dutta", "Owolabi Legunsen", "Zixin Huang", "Sasa Misailovic"], "DOIs": ["https://doi.org/10.1145/3236024.3236057"], "tag": ["Probabilistic Reasoning"], "abstract": "ABSTRACTProbabilistic programming systems (PP systems) allow developers to model stochastic phenomena and perform efficient inference on the models. The number and adoption of probabilistic programming systems is growing significantly. However, there is no prior study of bugs in these systems and no methodology for systematically testing PP systems. Yet, testing PP systems is highly non-trivial, especially when they perform approximate inference. In this paper, we characterize 118 previously reported bugs in three open-source PP systems\u2014Edward, Pyro and Stan\u2014and pro- pose ProbFuzz, an extensible system for testing PP systems. Prob- Fuzz allows a developer to specify templates of probabilistic models, from which it generates concrete probabilistic programs and data for testing. ProbFuzz uses language-specific translators to generate these concrete programs, which use the APIs of each PP system. ProbFuzz finds potential bugs by checking the output from running the generated programs against several oracles, including an accu- racy checker. Using ProbFuzz, we found 67 previously unknown bugs in recent versions of these PP systems. Developers already accepted 51 bug fixes that we submitted to the three PP systems, and their underlying systems, PyTorch and TensorFlow."}, {"id": "conf/sigsoft/LlerenaBBSR18", "title": "Verifying the long-run behavior of probabilistic system models in the presence of uncertainty.", "authors": ["Yamilet R. Serrano Llerena", "Marcel B\u00f6hme", "Marc Br\u00fcnink", "Guoxin Su", "David S. Rosenblum"], "DOIs": ["https://doi.org/10.1145/3236024.3236078"], "tag": ["Probabilistic Reasoning"], "abstract": "ABSTRACTVerifying that a stochastic system is in a certain state when it has reached equilibrium has important applications. For instance, the probabilistic verification of the long-run behavior of a safety-critical system enables assessors to check whether it accepts a human abort-command at any time with a probability that is sufficiently high. The stochastic system is represented as probabilistic model, a long-run property is asserted and a probabilistic verifier checks the model against the property. However, existing probabilistic verifiers do not account for the imprecision of the probabilistic parameters in the model. Due to uncertainty, the probability of any state transition may be subject to small perturbations which can have direct consequences for the veracity of the verification result. In reality, the safety-critical system may accept the abort-command with an insufficient probability. In this paper, we introduce the first probabilistic verification technique that accounts for uncertainty on the verification of long-run properties of a stochastic system. We present a mathematical framework for the asymptotic analysis of the stationary distribution of a discrete-time Markov chain, making no assumptions about the distribution of the perturbations. Concretely, our novel technique computes upper and lower bounds on the long-run probability, given a certain degree of uncertainty about the stochastic system."}, {"id": "conf/sigsoft/HashimotoMI18", "title": "Automated patch extraction via syntax- and semantics-aware Delta debugging on source code changes.", "authors": ["Masatomo Hashimoto", "Akira Mori", "Tomonori Izumida"], "DOIs": ["https://doi.org/10.1145/3236024.3236047"], "tag": ["Debugging and Bug Localization"], "abstract": "ABSTRACTDelta debugging (DD) is an approach to automating the debugging activities based on systematic testing. DD algorithms find the cause of a regression of a program by minimizing the changes applied between a working version and a faulty version of the program. However, it is still an open problem to minimize a huge set of changes while avoiding any invalid subsets that do not result in testable programs, especially in case that no software configuration management system is available. In this paper, we propose a rule-based approach to syntactic and semantic decomposition of changes into independent components to facilitate DD on source code changes, and hence to extract patches automatically. For analyzing changes, we make use of tree differencing on abstract syntax trees instead of common differencing on plain texts. We have developed an experimental implementation for Java programs and applied it to 194 bug fixes from Defects4J and 8 real-life regression bugs from 6 open source Java projects. Compared to a DD tool based on plain text differencing, it extracted patches whose size is reduced by 50% at the cost of 5% more test executions for the former dataset and by 73% at the cost of 40% more test executions for the latter, both on average."}, {"id": "conf/sigsoft/LehmannP18", "title": "Feedback-directed differential testing of interactive debuggers.", "authors": ["Daniel Lehmann", "Michael Pradel"], "DOIs": ["https://doi.org/10.1145/3236024.3236037"], "tag": ["Debugging and Bug Localization"], "abstract": "ABSTRACTTo understand, localize, and fix programming errors, developers often rely on interactive debuggers. However, as debuggers are software, they may themselves have bugs, which can make debugging unnecessarily hard or even cause developers to reason about bugs that do not actually exist in their code. This paper presents the first automated testing technique for interactive debuggers. The problem of testing debuggers is fundamentally different from the well-studied problem of testing compilers because debuggers are interactive and because they lack a specification of expected behavior. Our approach, called DBDB, generates debugger actions to exercise the debugger and records traces that summarize the debugger's behavior. By comparing traces of multiple debuggers with each other, we find diverging behavior that points to bugs and other noteworthy differences. We evaluate DBDB on the JavaScript debuggers of Firefox and Chromium, finding 19 previously unreported bugs, eight of which are already fixed by the developers."}, {"id": "conf/sigsoft/0001R18", "title": "Improving IR-based bug localization with context-aware query reformulation.", "authors": ["Mohammad Masudur Rahman", "Chanchal K. Roy"], "DOIs": ["https://doi.org/10.1145/3236024.3236065"], "tag": ["Debugging and Bug Localization"], "abstract": "ABSTRACTRecent findings suggest that Information Retrieval (IR)-based bug localization techniques do not perform well if the bug report lacks rich structured information (e.g., relevant program entity names). Conversely, excessive structured information (e.g., stack traces) in the bug report might not always help the automated localization either. In this paper, we propose a novel technique--BLIZZARD-- that automatically localizes buggy entities from project source using appropriate query reformulation and effective information retrieval. In particular, our technique determines whether there are excessive program entities or not in a bug report (query), and then applies appropriate reformulations to the query for bug localization. Experiments using 5,139 bug reports show that our technique can localize the buggy source documents with 7%--56% higher Hit@10, 6%--62% higher MAP@10 and 6%--62% higher MRR@10 than the baseline technique. Comparison with the state-of-the-art techniques and their variants report that our technique can improve 19% in MAP@10 and 20% in MRR@10 over the state-of-the-art, and can improve 59% of the noisy queries and 39% of the poor queries."}, {"id": "conf/sigsoft/BarikFMP18", "title": "How should compilers explain problems to developers?", "authors": ["Titus Barik", "Denae Ford", "Emerson R. Murphy-Hill", "Chris Parnin"], "DOIs": ["https://doi.org/10.1145/3236024.3236040"], "tag": ["Debugging and Bug Localization"], "abstract": "ABSTRACTCompilers primarily give feedback about problems to developers through the use of error messages. Unfortunately, developers routinely find these messages to be confusing and unhelpful. In this paper, we postulate that because error messages present poor explanations, theories of explanation---such as Toulmin's model of argument---can be applied to improve their quality. To understand how compilers should present explanations to developers, we conducted a comparative evaluation with 68 professional software developers and an empirical study of compiler error messages found in Stack Overflow questions across seven different programming languages.  Our findings suggest that, given a pair of error messages, developers significantly prefer the error message that employs proper argument structure over a deficient argument structure when neither offers a resolution---but will accept a deficient argument structure if it provides a resolution to the problem. Human-authored explanations on Stack Overflow converge to one of the three argument structures: those that provide a resolution to the error, simple arguments, and extended arguments that provide additional evidence for the problem. Finally, we contribute three practical design principles to inform the design and evaluation of compiler error messages."}, {"id": "conf/sigsoft/ValievVH18", "title": "Ecosystem-level determinants of sustained activity in open-source projects: a case study of the PyPI ecosystem.", "authors": ["Marat Valiev", "Bogdan Vasilescu", "James D. Herbsleb"], "DOIs": ["https://doi.org/10.1145/3236024.3236062"], "tag": ["Ecosystems and Crowdsourcing"], "abstract": "ABSTRACTOpen-source projects do not exist in a vacuum. They benefit from reusing other projects and themselves are being reused by others, creating complex networks of interdependencies, i.e., software ecosystems. Therefore, the sustainability of projects comprising ecosystems may no longer by determined solely by factors internal to the project, but rather by the ecosystem context as well.  In this paper we report on a mixed-methods study of ecosystem-level factors affecting the sustainability of open-source Python projects. Quantitatively, using historical data from 46,547 projects in the PyPI ecosystem, we modeled the chances of project development entering a period of dormancy (limited activity) as a function of the projects' position in their dependency networks, organizational support, and other factors. Qualitatively, we triangulated the revealed effects and further expanded on our models through interviews with project maintainers. Results show that the number of project ties and the relative position in the dependency network have significant impact on sustained project activity, with nuanced effects early in a project's life cycle and later on."}, {"id": "conf/sigsoft/ChenLZZWH018", "title": "Optimizing test prioritization via test distribution analysis.", "authors": ["Junjie Chen", "Yiling Lou", "Lingming Zhang", "Jianyi Zhou", "Xiaoleng Wang", "Dan Hao", "Lu Zhang"], "DOIs": ["https://doi.org/10.1145/3236024.3236053"], "tag": ["Testing II"], "abstract": "ABSTRACTTest prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice.  To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming state-of-the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks from the testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads."}, {"id": "conf/sigsoft/WangS18", "title": "How well are regular expressions tested in the wild?", "authors": ["Peipei Wang", "Kathryn T. Stolee"], "DOIs": ["https://doi.org/10.1145/3236024.3236072"], "tag": ["Testing II"], "abstract": "ABSTRACTDevelopers report testing their regular expressions less than the rest of their code. In this work, we explore how thoroughly tested regular expressions are by examining open source projects.  Using standard metrics of coverage, such as line and branch coverage, gives an incomplete picture of the test coverage of regular expressions. We adopt graph-based coverage metrics for the DFA representation of regular expressions, providing fine-grained test coverage metrics. Using over 15,000 tested regular expressions in 1,225 Java projects on GitHub, we measure node, edge, and edge-pair coverage. Our results show that only 17% of the regular expressions in the repositories are tested at all. For those that are tested, the median number of test inputs is two. For nearly 42% of the tested regular expressions, only one test input is used. Average node and edge coverage levels on the DFAs for tested regular expressions are 59% and 29%, respectively. Due to the lack of testing of regular expressions, we explore whether a string generation tool for regular expressions, Rex, achieves high coverage levels. With some exceptions, we found that tools such as Rex can be used to write test inputs with similar coverage to the developer tests."}, {"id": "conf/sigsoft/KimC018", "title": "Which generated test failures are fault revealing? prioritizing failures based on inferred precondition violations using PAF.", "authors": ["Mijung Kim", "Shing-Chi Cheung", "Sunghun Kim"], "DOIs": ["https://doi.org/10.1145/3236024.3236058"], "tag": ["Testing II"], "abstract": "ABSTRACTAutomated unit testing tools, such as Randoop, have been developed to produce failing tests as means of finding faults. However, these tools often produce false alarms, so are not widely used in practice. The main reason for a false alarm is that the generated failing test violates an implicit precondition of the method under test, such as a field should not be null at the entry of the method. This condition is not explicitly programmed or documented but implicitly assumed by developers. To address this limitation, we propose a technique called PAF to cluster generated test failures due to the same cause and reorder them based on their likelihood of violating an implicit precondition of the method under test. From various test executions, PAF observes their dataflows to the variables whose values are used when the program fails. Based on the dataflow similarity and where these values are originated, PAF clusters failures and determines their likelihood of being fault revealing. We integrated PAF into Randoop. Our empirical results on open-source projects show that PAF effectively clusters fault revealing tests arising from the same fault and successfully prioritizes the fault-revealing ones."}, {"id": "conf/sigsoft/0006WR18", "title": "Detection of energy inefficiencies in Android wear watch faces.", "authors": ["Hailong Zhang", "Haowei Wu", "Atanas Rountev"], "DOIs": ["https://doi.org/10.1145/3236024.3236073"], "tag": ["Energy"], "abstract": "ABSTRACTThis work considers watch faces for Android Wear devices such as smartwatches. Watch faces are a popular category of apps that display current time and relevant contextual information. Our study of watch faces in an app market indicates that energy efficiency is a key concern for users and developers.  The first contribution of this work is the definition of several energy-inefficiency patterns of watch face behavior, focusing on two energy-intensive resources: sensors and displays. Based on these patterns, we propose a control-flow model and static analysis algorithms to identify instances of these patterns. The algorithms use interprocedural control-flow analysis of callback methods and the invocation sequences of these methods. Potential energy inefficiencies are then used for automated test generation and execution, where the static analysis reports are validated via run-time execution. Our experimental results and case studies demonstrate that the analysis achieves high precision and low cost, and provide insights into potential pitfalls faced by developers of watch faces."}, {"id": "conf/sigsoft/CaninoLM18", "title": "Stochastic energy optimization for mobile GPS applications.", "authors": ["Anthony Canino", "Yu David Liu", "Hidehiko Masuhara"], "DOIs": ["https://doi.org/10.1145/3236024.3236076"], "tag": ["Energy"], "abstract": "ABSTRACTMobile applications regularly interact with their noisy and ever-changing physical environment. The fundamentally uncertain nature of such interactions leads to significant challenges in energy optimization, a crucial goal of software engineering on mobile devices. This paper presents Aeneas, a novel energy optimization framework for Android in the presence of uncertainty. Aeneas provides a minimalistic programming model where acceptable program behavioral settings are abstracted as knobs and application-specific optimization goals \u2014 such as meeting an energy budget \u2014 are crystallized as rewards, both of which are directly programmable. At its heart, Aeneas is endowed with a stochastic optimizer to adaptively and intelligently select the reward-optimal knob setting through a form of reinforcement learning. We evaluate Aeneas on mobile GPS applications built over Google LocationService API. Through an in-field case study that covers approximately 6500 miles and 150 hours of driving as well as 20 hours of biking and hiking, we find that Aeneas can effectively and resiliently meet programmer-specified energy budgets in uncertain physical environments where individual GPS readings undergo significant fluctuation. Compared with non-stochastic approaches such as profile-guided optimization, Aeneas produces significantly more stable results across runs."}, {"id": "conf/sigsoft/DoB18", "title": "Gamifying static analysis.", "authors": ["Lisa Nguyen Quang Do", "Eric Bodden"], "DOIs": ["https://doi.org/10.1145/3236024.3264830"], "tag": ["NIER I"], "abstract": "ABSTRACTIn the past decades, static code analysis has become a prevalent means to detect bugs and security vulnerabilities in software systems. As software becomes more complex, analysis tools also report lists of increasingly complex warnings that developers need to address on a daily basis. The novel insight we present in this work is that static analysis tools and video games both require users to take on repetitive and challenging tasks. Importantly, though, while good video games manage to keep players engaged, static analysis tools are notorious for their lacking user experience, which prevents developers from using them to their full potential, frequently resulting in dissatisfaction and even tool abandonment. We show parallels between gaming and using static analysis tools, and advocate that the user-experience issues of analysis tools can be addressed by looking at the analysis tooling system as a whole, and by integrating gaming elements that keep users engaged, such as providing immediate and clear feedback, collaborative problem solving, or motivators such as points and badges."}, {"id": "conf/sigsoft/Salem18", "title": "The case for experiment-oriented computing.", "authors": ["Paulo Salem"], "DOIs": ["https://doi.org/10.1145/3236024.3264831"], "tag": ["NIER I"], "abstract": "ABSTRACTExperimentation aspects (e.g., systematic observation, exploration of alternatives, formulation of hypotheses and empirical testing) can be found dispersed and intertwined in many software systems. Numerous examples are provided in this article. This suggests that experimental activity is a class of computation in its own right. By abstracting the relevant experimentation features, a general Experiment-Oriented Computing (EOC) approach, orthogonal to other Software Engineering issues, is formulated in this article. Through this separation of concerns, it is possible to clearly pursue both theoretical and applied research with respect to experimental aspects. Concrete directions for such research and development are also given to illustrate the value of the approach."}, {"id": "conf/sigsoft/HellendoornDA18", "title": "On the naturalness of proofs.", "authors": ["Vincent J. Hellendoorn", "Premkumar T. Devanbu", "Mohammad Amin Alipour"], "DOIs": ["https://doi.org/10.1145/3236024.3264832"], "tag": ["NIER I"], "abstract": "ABSTRACTProofs play a key role in reasoning about programs and verification of properties of systems. Mechanized proof assistants help users in developing and checking the consistency of proofs using the proof language developed by the systems; but even then writing proofs is tedious and could benefit from automated insight. In this paper, we analyze proofs in two different proof assistant systems (Coq and HOL Light) to investigate if there is evidence of \"naturalness\" in these proofs: viz., recurring linguistic patterns that are amenable to language models, in the way that programming languages are known to be. Such models could be used to find errors, rewrite proofs, help suggest dependencies, and perhaps even synthesize (steps of) proofs. We apply state-of-the-art language models to large corpora of proofs to show that this is indeed the case: proofs are remarkably predictable, much like other programming languages. Code completion tools for Coq proofs could save over 60% of typing effort. As proofs have become increasingly central to writing provably correct, large programs (such as the CompCert C compiler), our demonstration that they are amenable to general statistical models unlocks a range of linguistics-inspired tool support."}, {"id": "conf/sigsoft/McNamaraSM18", "title": "Does ACM's code of ethics change ethical decision making in software development?", "authors": ["Andrew McNamara", "Justin Smith", "Emerson R. Murphy-Hill"], "DOIs": ["https://doi.org/10.1145/3236024.3264833"], "tag": ["NIER I"], "abstract": "ABSTRACTEthical decisions in software development can substantially impact end-users, organizations, and our environment, as is evidenced by recent ethics scandals in the news. Organizations, like the ACM, publish codes of ethics to guide software-related ethical decisions. In fact, the ACM has recently demonstrated renewed interest in its code of ethics and made updates for the first time since 1992. To better understand how the ACM code of ethics changes software-related decisions, we replicated a prior behavioral ethics study with 63 software engineering students and 105 professional software developers, measuring their responses to 11 ethical vignettes. We found that explicitly instructing participants to consider the ACM code of ethics in their decision making had no observed effect when compared with a control group. Our findings suggest a challenge to the research community: if not a code of ethics, what techniques can improve ethical decision making in software engineering?"}, {"id": "conf/sigsoft/XuDZG0018", "title": "How are spreadsheet templates used in practice: a case study on Enron.", "authors": ["Liang Xu", "Wensheng Dou", "Jiaxin Zhu", "Chushu Gao", "Jun Wei", "Tao Huang"], "DOIs": ["https://doi.org/10.1145/3236024.3264834"], "tag": ["NIER I"], "abstract": "ABSTRACTTo reduce the effort of creating similar spreadsheets, end users may create expected spreadsheets from some predesigned templates, which contain necessary table layouts (e.g., headers and styles) and formulas, other than from scratch. When there are no explicitly predesigned spreadsheet templates, end users often take an existing spreadsheet as the instance template to create a new spreadsheet. However, improper template design and usage can introduce various issues. For example, a formula error in the template can be easily propagated to all its instances without users\u2019 noticing. Since template design and usage are rarely documented in literature and practice, practitioners and researchers lack understanding of them to achieve effective improvement. In this paper, we conduct the first empirical study on the design and the usage of spreadsheet templates based on 47 predesigned templates (490 instances in total), and 21 instance template groups (168 template and instance pairs in total), extracted from the Enron corpus. Our study reveals a number of spreadsheet template design and usage issues in practice, and also sheds lights on several interesting research directions."}, {"id": "conf/sigsoft/GuoJZCS18", "title": "DLFuzz: differential fuzzing testing of deep learning systems.", "authors": ["Jianmin Guo", "Yu Jiang", "Yue Zhao", "Quan Chen", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3236024.3264835"], "tag": ["NIER I"], "abstract": "ABSTRACTDeep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the first differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59% more adversarial inputs with 89.82% smaller perturbations, averagely obtain 2.86% higher neuron coverage, and save 20.11% time consumption."}, {"id": "conf/sigsoft/ImtiazB18", "title": "Towards data-driven vulnerability prediction for requirements.", "authors": ["Sayem Mohammad Imtiaz", "Tanmay Bhowmik"], "DOIs": ["https://doi.org/10.1145/3236024.3264836"], "tag": ["NIER I"], "abstract": "ABSTRACTDue to the abundance of security breaches we continue to see, the software development community is recently paying attention to a more proactive approach towards security. This includes predicting vulnerability before exploitation employing static code analysis and machine learning techniques. Such mechanisms, however, are designed to detect post-implementation vulnerabilities. As the root of a vulnerability can often be traced back to the requirement specification, and vulnerability discovered later in the development life cycle is more expensive to fix, we need additional preventive mechanisms capable of predicting vulnerability at a much earlier stage. In this paper, we propose a novel framework providing an automated support to predict vulnerabilities for a requirement as early as during requirement engineering. We further present a preliminary demonstration of our framework and the promising results we observe clearly indicate the value of this new research idea."}, {"id": "conf/sigsoft/WongMK18", "title": "Beyond testing configurable systems: applying variational execution to automatic program repair and higher order mutation testing.", "authors": ["Chu-Pan Wong", "Jens Meinicke", "Christian K\u00e4stner"], "DOIs": ["https://doi.org/10.1145/3236024.3264837"], "tag": ["NIER II"], "abstract": "ABSTRACTGenerate-and-validate automatic program repair and higher order mutation testing often use search-based techniques to find optimal or good enough solutions in huge search spaces. As search spaces continue to grow, finding solutions that require interactions of multiple changes can become challenging. To tackle the huge search space, we propose to use variational execution. Variational execution has been shown to be effective in exhaustively exploring variations and identifying interactions in a huge but often finite configuration space. The key idea is to encode alternatives in the search space as variations and use variational execution as a black-box technique to generate useful insights so that existing search heuristics can be informed. We show that this idea is promising and identify criteria for problems in which variational execution is a promising tool, which may be useful to identify further applications."}, {"id": "conf/sigsoft/BrunM18", "title": "Software fairness.", "authors": ["Yuriy Brun", "Alexandra Meliou"], "DOIs": ["https://doi.org/10.1145/3236024.3264838"], "tag": ["NIER II"], "abstract": "ABSTRACTA goal of software engineering research is advancing software quality and the success of the software engineering process. However, while recent studies have demonstrated a new kind of defect in software related to its ability to operate in fair and unbiased manner, software engineering has not yet wholeheartedly tackled these new kinds of defects, thus leaving software vulnerable. This paper outlines a vision for how software engineering research can help reduce fairness defects and represents a call to action by the software engineering research community to reify that vision. Modern software is riddled with examples of biased behavior, from automated translation injecting gender stereotypes, to vision systems failing to see faces of certain races, to the US criminal justice sytem relying on biased computational assessments of crime recidivism. While systems may learn bias from biased data, bias can also emerge from ambiguous or incomplete requirement specification, poor design, implementation bugs, and unintended component interactions. We argue that software fairness is analogous to software quality, and that numerous software engineering challenges in the areas of requirements, specification, design, testing, and verification need to be tackled to solve this problem."}, {"id": "conf/sigsoft/ShermanD18", "title": "Software engineering collaboratories (SEClabs) and collaboratories as a service (CaaS).", "authors": ["Elena Sherman", "Robert Dyer"], "DOIs": ["https://doi.org/10.1145/3236024.3264839"], "tag": ["NIER II"], "abstract": "ABSTRACTNovel research ideas require strong evaluations. Modern software engineering research evaluation typically requires a set of benchmark programs. Open source software repositories have provided a great opportunity for researchers to find such programs for use in their evaluations. Many tools/techniques have been developed to help automate the curation of open source software. There has also been encouragement for researchers to provide their research artifacts so that other researchers can easily reproduce the results. We argue that these two trends (i.e., curating open source software for research evaluation and the providing of research artifacts) drive the need for Software Engineer Collaboratories (SEClabs). We envision research communities coming together to create SEClab instances, where research artifacts can be made publicly available to other researchers. The community can then vet such artifacts and make them available as a service, thus turning the collaboratory into a Collaboratory as a Service (CaaS). If our vision is realized, the speed and transparency of research will drastically increase."}, {"id": "conf/sigsoft/GadelhaMCN18", "title": "Towards counterexample-guided k-induction for fast bug detection.", "authors": ["Mikhail Y. R. Gadelha", "Felipe R. Monteiro", "Lucas C. Cordeiro", "Denis A. Nicole"], "DOIs": ["https://doi.org/10.1145/3236024.3264840"], "tag": ["NIER II"], "abstract": "ABSTRACTRecently, the k-induction algorithm has proven to be a successful approach for both finding bugs and proving correctness. However, since the algorithm is an incremental approach, it might waste resources trying to prove incorrect programs. In this paper, we extend the k-induction algorithm to shorten the number of steps required to find a property violation. We convert the algorithm into a meet-in-the-middle bidirectional search algorithm, using the counterexample produced from over-approximating the program. The main advantage is in the reduction of the state explosion by reducing the maximum required steps from k to \u230ak/2 + 1\u230b."}, {"id": "conf/sigsoft/0002JCHZ18", "title": "Salient-class location: help developers understand code change in code review.", "authors": ["Yuan Huang", "Nan Jia", "Xiangping Chen", "Kai Hong", "Zibin Zheng"], "DOIs": ["https://doi.org/10.1145/3236024.3264841"], "tag": ["NIER II"], "abstract": "ABSTRACTCode review involves a significant amount of human effort to understand the code change, because the information required to inspect code changes may distribute across multiple files that reviewers are not familiar with. Code changes are often organized as commits for review. In this paper, we found that most of the commits contain a salient class, which is saliently modified and causes the modification of the rest classes in a commit. Our user studies confirmed that identifying the salient class in a commit can facilitate reviewers in understanding code change. We model the salient class identification as a binary classification problem and extract a number of discriminative features from commit to characterize the salience of a class. The initial experiment result shows that the proposed approach can improve the efficiency of reviewers understanding code changes in code review."}, {"id": "conf/sigsoft/RenYHFK18", "title": "Towards quantifying the development value of code contributions.", "authors": ["Jinglei Ren", "Hezheng Yin", "Qingda Hu", "Armando Fox", "Wojciech Koszek"], "DOIs": ["https://doi.org/10.1145/3236024.3264842"], "tag": ["NIER II"], "abstract": "ABSTRACTQuantifying the value of developers\u2019 code contributions to a software project requires more than simply counting lines of code or commits. We define the development value of code as a combination of its structural value (the effect of code reuse) and its non-structural value (the impact on development). We propose techniques to automatically calculate both components of development value and combine them using Learning to Rank. Our preliminary empirical study shows that our analysis yields richer results than those obtained by human assessment or simple counting methods and demonstrates the potential of our approach."}, {"id": "conf/sigsoft/MougoueiPHSW18", "title": "Operationalizing human values in software: a research roadmap.", "authors": ["Davoud Mougouei", "Harsha Perera", "Waqar Hussain", "Rifat Ara Shams", "Jon Whittle"], "DOIs": ["https://doi.org/10.1145/3236024.3264843"], "tag": ["NIER II"], "abstract": "ABSTRACTSoftware influences several aspects of people's lives and therefore needs to reflect their values. However, existing software engineering methods fail to account for human values, which may result in breaching those values in software and, therefore, dissatisfaction of users and loss of profit and reputation. To avoid such negative consequences, human values need to be integrated -- in a verifiable way -- into software. We refer to this as Operationalizing Human Values in Software. But this is not easy to achieve due to three main obstacles: first, human values are hard to define in a way that can be put into practice; second, existing software design decisions are mainly ignorant of values; finally, values are hard to determine and quantify in software. This paper aims to establish a research roadmap for overcoming these obstacles. The proposed roadmap focuses on (i) establishing practical definitions for human values, (ii) integrating values into software design, and (iii) measuring values in the software development life cycle."}, {"id": "conf/sigsoft/MansoorSSBCF18", "title": "Modeling and testing a family of surgical robots: an experience report.", "authors": ["Niloofar Mansoor", "Jonathan A. Saddler", "Bruno Silva", "Hamid Bagheri", "Myra B. Cohen", "Shane Farritor"], "DOIs": ["https://doi.org/10.1145/3236024.3275534"], "tag": ["Formal Analysis"], "abstract": "ABSTRACTSafety-critical applications often use dependability cases to validate that specified properties are invariant, or to demonstrate a counter example showing how that property might be violated. However, most dependability cases are written with a single product in mind. At the same time, software product lines (families of related software products) have been studied with the goal of modeling variability and commonality, and building family based techniques for both analysis and testing. However, there has been little work on building an end to end dependability case for a software product line (where a property is modeled, a counter example is found and then validated as a true positive via testing), and none that we know of in an emerging safety-critical domain, that of robotic surgery. In this paper, we study a family of surgical robots, that combine hardware and software, and are highly configurable, representing over 1300 unique robots. At the same time, they are considered safety-critical and should have associated dependability cases. We perform a case study to understand how we can bring together lightweight formal analysis, feature modeling, and testing to provide an end to end pipeline to find potential violations of important safety properties. In the process, we learned that there are some interesting and open challenges for the research community, which if solved will lead towards more dependable safety-critical cyber-physical systems."}, {"id": "conf/sigsoft/FooCYAS18", "title": "Efficient static checking of library updates.", "authors": ["Darius Foo", "Hendy Chua", "Jason Yeo", "Ming Yi Ang", "Asankhaya Sharma"], "DOIs": ["https://doi.org/10.1145/3236024.3275535"], "tag": ["Formal Analysis"], "abstract": "ABSTRACTSoftware engineering practices have evolved to the point where a developer writing a new application today doesn\u2019t start from scratch, but reuses a number of open source libraries and components. These third-party libraries evolve independently of the applications in which they are used, and may not maintain stable interfaces as bugs and vulnerabilities in them are fixed. This in turn causes API incompatibilities in downstream applications which must be manually resolved. Oversight here may manifest in many ways, from test failures to crashes at runtime. To address this problem, we present a static analysis for automatically and efficiently checking if a library upgrade introduces an API incompatibility. Our analysis does not rely on reported version information from library developers, and instead computes the actual differences between methods in libraries across different versions. The analysis is scalable, enabling real-time diff queries involving arbitrary pairs of library versions. It supports a vulnerability remediation product which suggests library upgrades automatically and is lightweight enough to be part of a continuous integration/delivery (CI/CD) pipeline. To evaluate the effectiveness of our approach, we determine semantic versioning adherence of a corpus of open source libraries taken from Maven Central, PyPI, and RubyGems. We find that on average, 26% of library versions are in violation of semantic versioning. We also analyze a collection of popular open source projects from GitHub to determine if we can automatically update libraries in them without causing API incompatibilities. Our results indicate that we can suggest upgrades automatically for 10% of the libraries."}, {"id": "conf/sigsoft/ChenSFMXLX18", "title": "Are mobile banking apps secure? what can be improved?", "authors": ["Sen Chen", "Ting Su", "Lingling Fan", "Guozhu Meng", "Minhui Xue", "Yang Liu", "Lihua Xu"], "DOIs": ["https://doi.org/10.1145/3236024.3275523"], "tag": ["Security"], "abstract": "ABSTRACTMobile banking apps, as one of the most contemporary FinTechs, have been widely adopted by banking entities to provide instant financial services. However, our recent work discovered thousands of vulnerabilities in 693 banking apps, which indicates these apps are not as secure as we expected. This motivates us to conduct this study for understanding the current security status of them. First, we take 6 months to track the reporting and patching procedure of these vulnerabilities. Second, we audit 4 state-of the-art vulnerability detection tools on those patched vulnerabilities. Third, we discuss with 7 banking entities via in-person or online meetings and conduct an online survey to gain more feedback from financial app developers. Through this study, we reveal that (1) people may have inconsistent understandings of the vulnerabilities and different criteria for rating severity; (2) state-of-the-art tools are not effective in detecting vulnerabilities that the banking entities most concern; and (3) more efforts should be endeavored in different aspects to secure banking apps. We believe our study can help bridge the existing gaps, and further motivate different parties, including banking entities, researchers and policy makers, to better tackle security issues altogether."}, {"id": "conf/sigsoft/GaoYFJSS18", "title": "VulSeeker-pro: enhanced semantic learning based binary vulnerability seeker with emulation.", "authors": ["Jian Gao", "Xin Yang", "Ying Fu", "Yu Jiang", "Heyuan Shi", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3236024.3275524"], "tag": ["Security"], "abstract": "ABSTRACTLearning-based clone detection is widely exploited for binary vulnerability search. Although they solve the problem of high time overhead of traditional dynamic and static search approaches to some extent, their accuracy is limited, and need to manually identify the true positive cases among the top-M search results during the industrial practice. This paper presents VulSeeker-Pro, an enhanced binary vulnerability seeker that integrates function semantic emulation at the back end of semantic learning, to release the engineers from the manual identification work. It first uses the semantic learning based predictor to quickly predict the top-M candidate functions which are the most similar to the vulnerability from the target binary. Then the top-M candidates are fed to the emulation engine to resort, and more accurate top-N candidate functions are obtained. With fast filtering of semantic learning and dynamic trace generation of function semantic emulation, VulSeeker-Pro can achieve higher search accuracy with little time overhead. The experimental results on 15 known CVE vulnerabilities involving 6 industry widely used programs show that VulSeeker-Pro significantly outperforms the state-of-the-art approaches in terms of accuracy. In a total of 45 searches, VulSeeker-Pro finds 40 and 43 real vulnerabilities in the top-1 and top-5 candidate functions, which are 12.33\u00d7 and 2.58\u00d7 more than the most recent and related work Gemini. In terms of efficiency, it takes 0.22 seconds on average to determine whether the target binary function contains a known vulnerability or not."}, {"id": "conf/sigsoft/LiangJCWZS18", "title": "PAFL: extend fuzzing optimizations of single mode to industrial parallel mode.", "authors": ["Jie Liang", "Yu Jiang", "Yuanliang Chen", "Mingzhe Wang", "Chijin Zhou", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3236024.3275525"], "tag": ["Security"], "abstract": "ABSTRACTResearchers have proposed many optimizations to improve the efficiency of fuzzing, and most optimized strategies work very well on their targets when running in single mode with instantiating one fuzzer instance. However, in real industrial practice, most fuzzers run in parallel mode with instantiating multiple fuzzer instances, and those optimizations unfortunately fail to maintain the efficiency improvements.  In this paper, we present PAFL, a framework that utilizes efficient guiding information synchronization and task division to extend those existing fuzzing optimizations of single mode to industrial parallel mode. With an additional data structure to store the guiding information, the synchronization ensures the information is shared and updated among different fuzzer instances timely. Then, the task division promotes the diversity of fuzzer instances by splitting the fuzzing task into several sub-tasks based on branch bitmap. We first evaluate PAFL using 12 different real-world programs from Google fuzzer-test-suite. Results show that in parallel mode, two AFL improvers\u2013AFLFast and FairFuzz do not outperform AFL, which is different from the case in single mode. However, when augmented with PAFL, the performance of AFLFast and FairFuzz in parallel mode improves. They cover 8% and 17% more branches, trigger 79% and 52% more unique crashes. For further evaluation on more widely-used software systems from GitHub, optimized fuzzers augmented with PAFL find more real bugs, and 25 of which are security-critical vulnerabilities registered as CVEs in the US National Vulnerability Database."}, {"id": "conf/sigsoft/WongW18", "title": "Software development challenges with air-gap isolation.", "authors": ["Sunny Wong", "Anne Woepse"], "DOIs": ["https://doi.org/10.1145/3236024.3275526"], "tag": ["Security"], "abstract": "ABSTRACTWhile existing research has explored the trade-off between security and performance, these efforts primarily focus on software consumers and often overlook the effectiveness and productivity of software producers. In this paper, we highlight an established security practice, air-gap isolation, and some challenges it uniquely instigates. To better understand and start quantifying the impacts of air-gap isolation on software development productivity, we conducted a survey at a commercial software company: Analytical Graphics, Inc. Based on our insights of dealing with air-gap isolation daily, we suggest some possible directions for future research. Our goal is to bring attention to this neglected area of research and to start a discussion in the SE community about the struggles faced by many commercial and governmental organizations."}, {"id": "conf/sigsoft/IvanovPRSYZ18", "title": "Design and validation of precooked developer dashboards.", "authors": ["Vladimir Ivanov", "Vladislav Pischulin", "Alan Rogers", "Giancarlo Succi", "Jooyong Yi", "Vasilii Zorin"], "DOIs": ["https://doi.org/10.1145/3236024.3275530"], "tag": ["End User Programming and Financial Applications"], "abstract": "ABSTRACTDespite increasing popularity of developer dashboards, the effectiveness of dashboards is still in question. In order to design a dashboard that is effective and useful for developers, it is important to know (a) what information developers need to see in a dashboard, and (b) how developers want to use a dashboard with that necessary information. To answer these questions, we conducted two series of face-to-face individual interviews with developers. In the first step we analyzed answers, build a Goal-Question-Metric model and designed a precooked developer dashboard. Then, during the second separate series of interviews, we validated the GQM and derived feedback on the designed dashboard. Given that the cost of dashboard customization prevents developers from utilizing dashboards, we believe that our findings can provide a solid starting point to build precooked developer dashboards that can be readily utilized by software companies."}, {"id": "conf/sigsoft/BrabermanGGUCPP18", "title": "Testing and validating end user programmed calculated fields.", "authors": ["V\u00edctor A. Braberman", "Diego Garbervetsky", "Javier Godoy", "Sebasti\u00e1n Uchitel", "Guido de Caso", "Ignacio Perez", "Santiago Perez"], "DOIs": ["https://doi.org/10.1145/3236024.3275531"], "tag": ["End User Programming and Financial Applications"], "abstract": "ABSTRACTThis paper reports on an approach for systematically generating test data from production databases for end user calculated field program via a novel combination of symbolic execution and database queries. We also discuss the opportunities and challenges that this specific domain poses for symbolic execution and shows how database queries can help complement some of symbolic execution's weaknesses, namely in the treatment of loops and also of path conditions that exceed SMT solver capabilities."}, {"id": "conf/sigsoft/ZhangHHZZ18", "title": "Automated refactoring of nested-IF formulae in spreadsheets.", "authors": ["Jie Zhang", "Shi Han", "Dan Hao", "Lu Zhang", "Dongmei Zhang"], "DOIs": ["https://doi.org/10.1145/3236024.3275532"], "tag": ["End User Programming and Financial Applications"], "abstract": "ABSTRACTSpreadsheets are the most popular end-user programming software, where formulae act like programs and also have smells. One well recognized smell is the use of nested-IF expressions, which have low readability and high cognitive cost for users, and are error-prone during reuse or maintenance. End users usually lack essential programming language knowledge to tackle or even realize this problem, yet no automatic approaches are currently available. This paper proposes the first exploration of the nest-if usage status against two large-scale spreadsheet corpora containing over 80,000 industry-level spreadsheets. It turns out the use of nested-IF expressions are surprisingly common among end users. We then present an approach to tackling this problem through automatic formula refactoring. The general idea of the automatic approach is two-fold. First, we detect and remove logic redundancy based on the AST of a formula. Second, we identify higher-level semantics that have been represented with fragmented and scattered syntax, and reassemble the syntax using concise built-in functions. A comprehensive evaluation with over 28 million nested-IF formulae reveals that the approach is able to relieve the smell of over 90% of nested-IF formulae."}, {"id": "conf/sigsoft/WangGXXNDHX18", "title": "FACTS: automated black-box testing of FinTech systems.", "authors": ["Qingshun Wang", "Lintao Gu", "Minhui Xue", "Lihua Xu", "Wenyu Niu", "Liang Dou", "Liang He", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3236024.3275533"], "tag": ["End User Programming and Financial Applications"], "abstract": "ABSTRACTFinTech, short for ``financial technology,'' has advanced the process of transforming financial business from a traditional manual-process-driven to an automation-driven model by providing various software platforms. However, the current FinTech-industry still heavily depends on manual testing, which becomes the bottleneck of FinTech industry development. To automate the testing process, we propose an approach of black-box testing for a FinTech system with effective tool support for both test generation and test oracles. For test generation, we first extract input categories from business-logic specifications, and then mutate real data collected from system logs with values randomly picked from each extracted input category. For test oracles, we propose a new technique of priority differential testing where we evaluate execution results of system-test inputs on the system's head (i.e., latest) version in the version repository (1) against the last legacy version in the version repository (only when the executed test inputs are on new, not-yet-deployed services) and (2) against both the currently-deployed version and the last legacy version (only when the test inputs are on existing, deployed services). When we rank the behavior-inconsistency results for developers to inspect, for the latter case, we give the currently-deployed version as a higher-priority source of behavior to check. We apply our approach to the CSTP subsystem, one of the largest data processing and forwarding modules of the China Foreign Exchange Trade System (CFETS) platform, whose annual total transaction volume reaches 150 trillion US dollars. Extensive experimental results show that our approach can substantially boost the branch coverage by approximately 40%, and is also efficient to identify common faults in the FinTech system."}, {"id": "conf/sigsoft/CelikLG18", "title": "Regression test selection for TizenRT.", "authors": ["Ahmet \u00c7elik", "Young-Chul Lee", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1145/3236024.3275527"], "tag": ["Test Automation"], "abstract": "ABSTRACTRegression testing - running tests after code modifications - is widely practiced in industry, including at Samsung. Regression Test Selection (RTS) optimizes regression testing by skipping tests that are not affected by recent code changes. Recent work has developed robust RTS tools, which mostly target managed languages, e.g., Java and C#, and thus are not applicable to large C projects, e.g., TizenRT, a lightweight RTOS-based platform.  We present Selfection, an RTS tool for projects written in C; we discuss the key challenges to develop Selfection and our design decisions. Selfection uses the objdump and readelf tools to statically build a dependency graph of functions from binaries and detect modified code elements. We integrated Selfection in TizenRT and evaluated its benefits if tests are run in an emulator and on a supported hardware platform (ARTIK 053). We used the latest 150 revisions of TizenRT available on GitHub. We measured the benefits of Selfection as the reduction in the number of tests and reduction in test execution time over running all tests at each revision (i.e., RetestAll). Our results show that Selfection can reduce, on average, the number of tests to 4.95% and end-to-end execution time to 7.04% when tests are executed in the emulator, and to 5.74% and 26.82% when tests are executed on the actual hardware. Our results also show that the time taken to maintain the dependency graph and detect modified functions is negligible."}, {"id": "conf/sigsoft/DebroyMB18", "title": "Building lean continuous integration and delivery pipelines by applying DevOps principles: a case study at Varidesk.", "authors": ["Vidroha Debroy", "Senecca Miller", "Lance Brimble"], "DOIs": ["https://doi.org/10.1145/3236024.3275528"], "tag": ["Test Automation"], "abstract": "ABSTRACTContinuous Integration (CI) and Continuous Delivery (CD) are widely considered to be best practices in software development. Studies have shown however, that adopting these practices can be challenging and there are many barriers that engineers may face, such as \u2013 overly long build times, lack of support for desired workflows, issues with configuration, etc. At Varidesk, we recently began shifting our primary web application (from a monolithic) to a micro-services-based architecture and also adapted our software development practices to aim for more effective CI/CD. In doing so, we also ran into some of the same afore-mentioned barriers. In this paper we focus on two specific challenges that we faced \u2013 long wait times for builds/releases to be queued and completed, and the lack of support for tooling, especially from a cross-cloud perspective. We then present the solutions that we came up with, which involved re-thinking DevOps as it applied to us, and re-building our own CI/CD pipelines based on DevOps-supporting approaches such as containerization, infrastructure-as-code, and orchestration. Our re-designed pipelines have led us to see speed increases, in terms of total build/release time, in the range of 330x-1110x and have enabled us to seamlessly move from a single-cloud to a multi- cloud environment, with no architectural changes to any apps."}, {"id": "conf/sigsoft/RahmanR18", "title": "The impact of failing, flaky, and high failure tests on the number of crash reports associated with Firefox builds.", "authors": ["Md Tajmilur Rahman", "Peter C. Rigby"], "DOIs": ["https://doi.org/10.1145/3236024.3275529"], "tag": ["Test Automation"], "abstract": "ABSTRACTTesting is an integral part of release engineering and continuous integration. In theory, a failed test on a build indicates a problem that should be fixed and the build should not be released. In practice, tests decay and developers often release builds, ignoring failing tests. In this paper, we studying the link between builds with failing tests and the number of crash reports on the Firefox webbrowser. Builds with all tests passing have a median of only two crash reports. In contrast, builds with one or more failing tests are associated with a median of 508 and 291 crash reports for Beta and Production builds, respectively. We further investigate the impact of ``flaky'' tests, which can both pass and fail on the same build, and find that they have a median of 514 and 234 crash reports for Beta and Production builds. Finally, building on previous research that has shown that tests that have failed frequently in the past will fail frequently in the future, we find that Builds with HighFailureTests have a median of 585 and 780 crash reports for Beta and Production builds. Unlike other types of test failures, HighFailureTests have a larger impact on Production releases than on Beta builds, and they have a median of 2.7 times more crashes than builds with normal test failures. We conclude that ignoring test failures is related to a dramatic increase in the number of crashes reported by users."}, {"id": "conf/sigsoft/GulzarWK18", "title": "BigSift: automated debugging of big data analytics in data-intensive scalable computing.", "authors": ["Muhammad Ali Gulzar", "Siman Wang", "Miryung Kim"], "DOIs": ["https://doi.org/10.1145/3236024.3264586"], "tag": ["Testing"], "abstract": "ABSTRACTDeveloping Big Data Analytics often involves trial and error debugging, due to the unclean nature of datasets or wrong assumptions made about data. When errors (e.g. program crash, outlier results, etc.) arise, developers are often interested in pinpointing the root cause of errors. To address this problem, BigSift takes an Apache Spark program, a user-defined test oracle function, and a dataset as input and outputs a minimum set of input records that reproduces the same test failure by combining the insights from delta debugging with data provenance. The technical contribution of BigSift is the design of systems optimizations that bring automated debugging closer to a reality for data intensive scalable computing.  BigSift exposes an interactive web interface where a user can monitor a big data analytics job running remotely on the cloud, write a user-defined test oracle function, and then trigger the automated debugging process. BigSift also provides a set of predefined test oracle functions, which can be used for explaining common types of anomalies in big data analytics--for example, finding the origin of the output value that is more than k standard deviations away from the median. The demonstration video is available at https://youtu.be/jdBsCd61a1Q."}, {"id": "conf/sigsoft/ChenLCXL18", "title": "FOT: a versatile, configurable, extensible fuzzing framework.", "authors": ["Hongxu Chen", "Yuekang Li", "Bihuan Chen", "Yinxing Xue", "Yang Liu"], "DOIs": ["https://doi.org/10.1145/3236024.3264593"], "tag": ["Testing"], "abstract": "ABSTRACTGreybox fuzzing is one of the most effective approaches for detecting software vulnerabilities. Various new techniques have been continuously emerging to enhance the effectiveness and/or efficiency by incorporating novel ideas into different components of a greybox fuzzer. However, there lacks a modularized fuzzing framework that can easily plugin new techniques and hence facilitate the reuse, integration and comparison of different techniques. To address this problem, we propose a fuzzing framework, namely Fuzzing Orchestration Toolkit (FOT). FOT is designed to be versatile, configurable and extensible. With FOT and its extensions, we have found 111 new bugs from 11 projects. Among these bugs, 18 CVEs have been assigned. Video link: https://youtu.be/O6Qu7BJ8RP0."}, {"id": "conf/sigsoft/AngellJBM18", "title": "Themis: automatically testing software for discrimination.", "authors": ["Rico Angell", "Brittany Johnson", "Yuriy Brun", "Alexandra Meliou"], "DOIs": ["https://doi.org/10.1145/3236024.3264590"], "tag": ["Testing"], "abstract": "ABSTRACTBias in decisions made by modern software is becoming a common and serious problem. We present Themis, an automated test suite generator to measure two types of discrimination, including causal relationships between sensitive inputs and program behavior. We explain how Themis can measure discrimination and aid its debugging, describe a set of optimizations Themis uses to reduce test suite size, and demonstrate Themis' effectiveness on open-source software. Themis is open-source and all our evaluation data are available at http://fairness.cs.umass.edu/. See a video of Themis in action: https://youtu.be/brB8wkaUesY"}, {"id": "conf/sigsoft/StoccoYM18a", "title": "Vista: web test repair using computer vision.", "authors": ["Andrea Stocco", "Rahulkrishna Yandrapally", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1145/3236024.3264592"], "tag": ["Testing"], "abstract": "ABSTRACTRepairing broken web element locators represents the major main- tenance cost of web test cases. To detect possible repairs, testers typically inspect the tests\u2019 interactions with the application under test through the GUI. Existing automated test repair techniques focus instead on the code and ignore visual aspects of the applica- tion. In this demo paper, we give an overview of Vista, a novel test repair technique that leverages computer vision and local crawling to automatically suggest and apply repairs to broken web tests. URL: https://github.com/saltlab/Vista"}, {"id": "conf/sigsoft/Reinhardt0MK18", "title": "Augmenting stack overflow with API usage patterns mined from GitHub.", "authors": ["Anastasia Reinhardt", "Tianyi Zhang", "Mihir Mathur", "Miryung Kim"], "DOIs": ["https://doi.org/10.1145/3236024.3264585"], "tag": ["Bugs"], "abstract": "ABSTRACTProgrammers often consult Q&A websites such as Stack Overflow (SO) to learn new APIs. However, online code snippets are not always complete or reliable in terms of API usage. To assess online code snippets, we build a Chrome extension, ExampleCheck that detects API usage violations in SO posts using API usage patterns mined from 380K GitHub projects. It quantifies how many GitHub examples follow common API usage and illustrates how to remedy the detected violation in a given SO snippet. With ExampleCheck, programmers can easily identify the pitfalls of a given SO snippet and learn how much it deviates from common API usage patterns in GitHub. The demo video is at https://youtu.be/WOnN-wQZsH0."}, {"id": "conf/sigsoft/YangYSLC18", "title": "PowerStation: automatically detecting and fixing inefficiencies of database-backed web applications in IDE.", "authors": ["Junwen Yang", "Cong Yan", "Pranav Subramaniam", "Shan Lu", "Alvin Cheung"], "DOIs": ["https://doi.org/10.1145/3236024.3264589"], "tag": ["Bugs"], "abstract": "ABSTRACTModern web applications are built using a myriad of software components, and each of them exposes different programming models (e.g., application logic expressed in an imperative language, database queries expressed using declarative SQL). To improve programmer productivity, Object Relational Mapping (ORM) frameworks have been developed to allow developers build web applications in an object-oriented manner. Despite such frameworks, prior work has found that developers still struggle in developing performant ORM-based web applications. This paper presents PowerStation, a RubyMine IDE plugin for optimizing web applications developed using the Ruby on Rails ORM. Using automated static analysis, PowerStation detects ORM-related inefficiency problems and suggests fixes to developers. Our evaluation using 12 real-world applications shows that PowerStation can automatically detects 1221 performance issues across them. A tutorial on using PowerStation can be found at https://youtu.be/rAV8CGuSj6k."}, {"id": "conf/sigsoft/HuaZWK18", "title": "SketchFix: a tool for automated program repair approach using lazy candidate generation.", "authors": ["Jinru Hua", "Mengshi Zhang", "Kaiyuan Wang", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1145/3236024.3264600"], "tag": ["Bugs"], "abstract": "ABSTRACTManually locating and removing bugs in faulty program is often tedious and error-prone. A common automated program repair approach called generate-and-validate (G&V) iteratively creates candidate fixes, compiles them, and runs these candidates against the given tests. This approach can be costly due to a large number of re-compilations and re-executions of the program. To tackle this limitation, recent work introduced the SketchFix approach that tightly integrates the generation and validation phases, and utilizes runtime behaviors to substantially prune a large amount of repair candidates. This tool paper describes our Java implementation of SketchFix, which is an open-source library that we released on Github. Our experimental evaluation using Defects4J benchmark shows that SketchFix can significantly reduce the number of re-compilations and re-executions compared to other approaches and work particularly well in repairing expression manipulation at the AST node-level granularity.The demo video is at: https://youtu.be/AO-YCH8vGzQ."}, {"id": "conf/sigsoft/QuerelR18", "title": "WarningsGuru: integrating statistical bug models with static analysis to provide timely and specific bug warnings.", "authors": ["Louis-Philippe Querel", "Peter C. Rigby"], "DOIs": ["https://doi.org/10.1145/3236024.3264599"], "tag": ["Bugs"], "abstract": "ABSTRACTThe detection of bugs in software systems has been divided into two research areas: static code analysis and statistical modeling of historical data. Static analysis indicates precise problems on line numbers but has the disadvantage of suggesting many warning which are often false positives. In contrast, statistical models use the history of the system to suggest which files or commits are likely to contain bugs. These course-grained predictions do not indicate to the developer the precise reasons for the bug prediction. We combine static analysis with statistical bug models to limit the number of warnings and provide specific warnings information at the line level. Previous research was able to process only a limited number of releases, our tool, WarningsGuru, can analyze all commits in a source code repository and we currently have processed thousands of commits and warnings. Since we process every commit, we present developers with more precise information about when a warning is introduced allowing us to show recent warnings that are introduced in statistically risky commits. Results from two OSS projects show that CommitGuru's statistical model flags 25% and 29% of all commits as risky. When we combine this with static analysis in WarningsGuru the number of risky commits with warnings is 20% for both projects and the number commits with new warnings is only 3% and 6%. We can drastically reduce the number of commits and warnings developers have to examine. The tool, source code, and demo is available at https://github.com/louisq/warningsguru."}, {"id": "conf/sigsoft/LeB018", "title": "DSM: a specification mining tool using recurrent neural network based language model.", "authors": ["Tien-Duy B. Le", "Lingfeng Bao", "David Lo"], "DOIs": ["https://doi.org/10.1145/3236024.3264597"], "tag": ["Mining"], "abstract": "ABSTRACTFormal specifications are important but often unavailable. Furthermore, writing these specifications is time-consuming and requires skills from developers. In this work, we present Deep Specification Miner (DSM), an automated tool that applies deep learning to mine finite-state automaton (FSA) based specifications. DSM accepts as input a set of execution traces to train a Recurrent Neural Network Language Model (RNNLM). From the input traces, DSM creates a Prefix Tree Acceptor (PTA) and leverages the inferred RNNLM to extract many features. These features are then forwarded to clustering algorithms for merging similar automata states in the PTA for assembling a number of FSAs. Next, our tool performs a model selection heuristic to approximate F-measure of FSAs, and outputs the one with the highest estimated F-measure. Noticeably, our implementation of DSM provides several options that allows users to optimize quality of resultant FSAs.  Our video demonstration on the performance of DSM is publicly available at https://goo.gl/Ju4yFS."}, {"id": "conf/sigsoft/LiuYLJZS18", "title": "EClone: detect semantic clones in Ethereum via symbolic transaction sketch.", "authors": ["Han Liu", "Zhiqiang Yang", "Chao Liu", "Yu Jiang", "Wenqi Zhao", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3236024.3264596"], "tag": ["Mining"], "abstract": "ABSTRACTThe Ethereum ecosystem has created a prosperity of smart contract applications in public blockchains, with transparent, traceable and programmable transactions. However, the flexibility that everybody can write and deploy smart contracts on Ethereum causes a large collection of similar contracts, i.e., clones. In practice, smart contract clones may amplify severe threats like security attacks, resource waste etc. In this paper, we have developed EClone, a semantic clone detector for Ethereum. The key insight of our clone detection is Symbolic Transaction Sketch, i.e., a set of critical semantic properties generated from symbolic transaction. Sketches of two smart contracts will be normalized into numeric vectors with a same length. Then, the clone detection problem is modeled as a similarity computation process where sketches and other syntactic information are combined. We have applied EClone in identifying semantic clones of deployed Ethereum smart contracts and achieved an accuracy of 93.27%. A demo video of EClone is at https://youtu.be/IRasOVv6vyc."}, {"id": "conf/sigsoft/GaoZ0LLK18", "title": "INFAR: insight extraction from app reviews.", "authors": ["Cuiyun Gao", "Jichuan Zeng", "David Lo", "Chin-Yew Lin", "Michael R. Lyu", "Irwin King"], "DOIs": ["https://doi.org/10.1145/3236024.3264595"], "tag": ["Mining"], "abstract": "ABSTRACTApp reviews play an essential role for users to convey their feedback about using the app. The critical information contained in app reviews can assist app developers for maintaining and updating mobile apps. However, the noisy nature and large-quantity of daily generated app reviews make it difficult to understand essential information carried in app reviews. Several prior studies have proposed methods that can automatically classify or cluster user reviews into a few app topics (e.g., security). These methods usually act on a static collection of user reviews. However, due to the dynamic nature of user feedback (i.e., reviews keep coming as new users register or new app versions being released) and multiple analysis dimensions (e.g., review quantity and user rating), developers still need to spend substantial effort in extracting contrastive information that can only be teased out by comparing data from multiple time periods or analysis dimensions. This is needed to answer questions such as: what kind of issues users are experiencing most? is there an unexpected rise in a particular kind of issue? etc. To address this need, in this paper, we introduce INFAR, a tool that automatically extracts INsights From App Reviews across time periods and analysis dimensions, and presents them in natural language supported by an interactive chart. The insights INFAR extracts include several perspectives: (1) salient topics (i.e., issue topics with significantly lower ratings), (2) abnormal topics (i.e., issue topics that experience a rapid rise in volume during a time period), (3) correlations between two topics, and (4) causal factors to rating or review quantity changes. To evaluate our tool, we conduct an empirical evaluation by involving six popular apps and 12 industrial practitioners, and 92% (11/12) of them approve the practical usefulness of the insights summarized by INFAR. Demo Tool Website: https://remine-lab.github.io/paper/infar.html Demo Video: https://youtu.be/MjcoiyjA5TE"}, {"id": "conf/sigsoft/SpadiniAB18", "title": "PyDriller: Python framework for mining software repositories.", "authors": ["Davide Spadini", "Maur\u00edcio Finavaro Aniche", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1145/3236024.3264598"], "tag": ["Mining"], "abstract": "ABSTRACTSoftware repositories contain historical and valuable information about the overall development of software systems. Mining software repositories (MSR) is nowadays considered one of the most interesting growing fields within software engineering. MSR focuses on extracting and analyzing data available in software repositories to uncover interesting, useful, and actionable information about the system. Even though MSR plays an important role in software engineering research, few tools have been created and made public to support developers in extracting information from Git repository. In this paper, we present PyDriller, a Python Framework that eases the process of mining Git. We compare our tool against the state-of-the-art Python Framework GitPython, demonstrating that PyDriller can achieve the same results with, on average, 50% less LOC and significantly lower complexity.  URL: https://github.com/ishepard/pydriller  Materials: https://doi.org/10.5281/zenodo.1327363  Pre-print: https://doi.org/10.5281/zenodo.1327411"}, {"id": "conf/sigsoft/0001ZSDR18", "title": "A formal verification tool for Ethereum VM bytecode.", "authors": ["Daejun Park", "Yi Zhang", "Manasvi Saxena", "Philip Daian", "Grigore Rosu"], "DOIs": ["https://doi.org/10.1145/3236024.3264591"], "tag": ["Models"], "abstract": "ABSTRACTIn this paper, we present a formal verification tool for the Ethereum Virtual Machine (EVM) bytecode. To precisely reason about all possible behaviors of the EVM bytecode, we adopted KEVM, a complete formal semantics of the EVM, and instantiated the K-framework's reachability logic theorem prover to generate a correct-by-construction deductive verifier for the EVM. We further optimized the verifier by introducing EVM-specific abstractions and lemmas to improve its scalability. Our EVM verifier has been used to verify various high-profile smart contracts including the ERC20 token, Ethereum Casper, and DappHub MakerDAO contracts."}, {"id": "conf/sigsoft/WangSMK18", "title": "ASketch: a sketching framework for Alloy.", "authors": ["Kaiyuan Wang", "Allison Sullivan", "Darko Marinov", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1145/3236024.3264594"], "tag": ["Models"], "abstract": "ABSTRACTAlloy is a declarative modeling language that supports first-order logic with transitive closure. Alloy has been used in a variety of domains to model software systems and find design deficiencies. However, it is often challenging to make an Alloy model correct or to debug a faulty Alloy model. ASketch is a sketching/synthesis technique that can help users write correct Alloy models. ASketch allows users to provide a partial Alloy model with holes, a generator that specifies candidate fragments to be considered for each hole, and a set of tests that capture the desired model properties. Then, the tool completes the holes such that all tests for the completed model pass. ASketch uses tests written for the recently introduced AUnit framework, which provides a foundation of testing (unit tests, test execution, and model coverage) for Alloy models in the spirit of traditional unit testing. This paper describes our Java implementation of ASketch, which is a command-line tool, released as an open-source project on GitHub. Our experimental results show that ASketch can handle partial Alloy models with multiple holes and a large search space. The demo video for ASketch can be found at https://youtu.be/T5NIVsV329E."}, {"id": "conf/sigsoft/ErataGKT18", "title": "AlloyInEcore: embedding of first-order relational logic into meta-object facility for automated model reasoning.", "authors": ["Ferhat Erata", "Arda Goknil", "Ivan Kurtev", "Bedir Tekinerdogan"], "DOIs": ["https://doi.org/10.1145/3236024.3264588"], "tag": ["Models"], "abstract": "ABSTRACTWe present AlloyInEcore, a tool for specifying metamodels with their static semantics to facilitate automated, formal reasoning on models. Software development projects require that software systems be specified in various models (e.g., requirements models, architecture models, test models, and source code). It is crucial to reason about those models to ensure the correct and complete system specifications. AlloyInEcore~allows the user to specify metamodels with their static semantics, while, using the semantics, it automatically detects inconsistent models, and completes partial models. It has been evaluated on three industrial case studies in the automotive domain (https://modelwriter.github.io/AlloyInEcore/)."}, {"id": "conf/sigsoft/BaoXX0L18", "title": "VT-revolution: interactive programming tutorials made possible.", "authors": ["Lingfeng Bao", "Zhenchang Xing", "Xin Xia", "David Lo", "Shanping Li"], "DOIs": ["https://doi.org/10.1145/3236024.3264587"], "tag": ["Models"], "abstract": "ABSTRACTProgramming video tutorials showcase programming tasks and associated workflows. Although video tutorials are easy to create, it is often difficult to explore the captured workflows and interact with the programs in the videos. In this work, we propose a tool named VTRevolution -- an interactive programming video tutorial authoring system. VTRevolution has two components: 1) a tutorial authoring system leverages operating system level instrumentation to log workflow history while tutorial authors are creating programming video tutorials; 2) a tutorial watching system enhances the learning experience of video tutorials by providing operation history and timeline-based browsing interactions. Our tutorial authoring system does not require any special recording tools or instrumentation of target applications. Neither does it incur any additional burden on tutorial authors to add interactions to video tutorials. Given a video tutorial enriched with synchronously-logged workflow history, our tutorial watching system allows tutorial watchers to explore the captured workflows and interact with files and code in a way that is impossible for video data alone. We conduct a user study of 90 developers to evaluate the design and effectiveness of our system in helping developers learn programming knowledge in video tutorials."}, {"id": "conf/sigsoft/Spadini18", "title": "Practices and tools for better software testing.", "authors": ["Davide Spadini"], "DOIs": ["https://doi.org/10.1145/3236024.3275424"], "tag": ["Doctorial Symposium"], "abstract": "ABSTRACTAutomated testing (hereafter referred to as just `testing') has become an essential process for improving the quality of software systems. In fact, testing can help to point out defects and to ensure that production code is robust under many usage conditions. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. Managers, as well as developers, do not treat test code as equally important as production code, and this behaviour could lead to poor test code quality, and in the future to defect-prone production code. The goal of my research is to bring awareness to developers on the effect of poor testing, as well as helping them in writing better test code. To this aim, I am working on 2 different perspectives: (1) studying best practices on software testing, identifying problems and challenges of current approaches, and (2) building new tools that better support the writing of test code, that tackle the issues we discovered with previous studies. Pre-print: https://doi.org/10.5281/zenodo.1411241"}, {"id": "conf/sigsoft/Fazzini18", "title": "Automated support for mobile application testing and maintenance.", "authors": ["Mattia Fazzini"], "DOIs": ["https://doi.org/10.1145/3236024.3275425"], "tag": ["Doctorial Symposium"], "abstract": "ABSTRACTMobile applications are an essential part of our daily life. In fact, they can be used for tasks that range from reading the news to performing bank transactions. Considering the impact that mobile applications have in our lives, it is important for developers to test them and gain confidence that they behave as expected. However, testing mobile applications proves to be challenging. In fact, mobile companies report that they do not have enough time and the right methods to test. In addition, in the case of Android applications, the situation is further complicated by the \"fragmentation\" of the ecosystem. Developers not only need to ensure that an application behaves as expected but also need to make sure that the application does so on a multitude of different devices. Finally, because it is virtually impossible to release a bug free application, developers also need to quickly react to bug reports and release a fixed version of the application before customer loss. The research plan proposed in this paper, aims to provide novel techniques to automate the support for mobile application testing and maintenance. Specifically, it proposes techniques to: test apps more effectively and efficiently, tackle the problems caused by the \"fragmentation\" of the Android ecosystem, and help developers in quickly handling bug reports."}, {"id": "conf/sigsoft/Fakhoury18", "title": "Moving towards objective measures of program comprehension.", "authors": ["Sarah Fakhoury"], "DOIs": ["https://doi.org/10.1145/3236024.3275426"], "tag": ["Doctorial Symposium"], "abstract": "ABSTRACTTraditionally, program comprehension research relies heavily on indirect measures of comprehension, where subjects report on their own comprehension levels or summarize part of an artifact so that researchers can instead deduce the level of comprehension. However, there are several potential issues that can result from using these indirect measures because they are prone to participant biases and implicitly deduce comprehension based on various factors.  The proposed research presents a framework to move towards more objective measures of program comprehension through the use of brain imaging and eye tracking technology. We aim to shed light on how the human brain processes comprehension tasks, specifically what aspects of the source code cause measurable increases in the cognitive load of developers in both bug localization tasks, as well as code reviews. We discuss the proposed methodology, preliminary results, and overall contributions of the work"}, {"id": "conf/sigsoft/Hosseini18", "title": "Semantic inference from natural language privacy policies and Android code.", "authors": ["Mitra Bokaei Hosseini"], "DOIs": ["https://doi.org/10.1145/3236024.3275427"], "tag": ["Doctorial Symposium"], "abstract": "ABSTRACTMobile apps collect dierent categories of personal information to provide users with various services. Companies use privacy policies containing critical requirements to inform users about their data practices. With the growing access to personal information and the scale of mobile app deployment, traceability of links between privacy policy requirements and app code is increasingly important. Automated traceability can be achieved using natural language processing and code analysis techniques. However, such techniques must address two main challenges: ambiguity in privacy policy terminology and unbounded information types provided by users through input elds in GUI. In this work, we propose approaches to interpret abstract terms in privacy policies, identify information types in Android layout code, and create a mapping between them using natural language processing techniques."}, {"id": "conf/sigsoft/Zhou18", "title": "Intelligent bug fixing with software bug knowledge graph.", "authors": ["Cheng Zhou"], "DOIs": ["https://doi.org/10.1145/3236024.3275428"], "tag": ["Doctorial Symposium"], "abstract": "ABSTRACTSoftware bugs continuously emerge during the process of software evolution. With the increasing size and complexity of software, bug fixing becomes increasingly more difficult. Bug and commit data of open source projects, Q&A documents and other software resources contain a sea of bug knowledge which can be utilized to help developers understand and fix bugs. Existing work focuses on data mining from a certain software resource in isolation to assist in bug fixing, which may reduce the efficiency of bug fixing. How to obtain, organize and understand bug knowledge from multi-source software data is an urgent problem to be solved. In order to solve this problem, we utilize knowledge graph (KG) technology to explore the deep semantic and structural relationships in the multi-source software data, propose effective search and recommendation techniques based on the knowledge graph, and design a bug-fix knowledge question & answering system to assist developers in intelligent software bug fixing. At present, we have designed a bug knowledge graph construction framework, proposed the identification principles and methods for bug knowledge entities and relationships, constructed a preliminary knowledge graph based on the bug repository. In the following work, we will further improve the knowledge graph, complete the knowledge graph fusion of multi-source database, comprehend bug knowledge through knowledge reasoning, utilize the collaborative search and recommendation technology for bug-fixing knowledge question and answering."}, {"id": "conf/sigsoft/Afzal18", "title": "Quality assurance automation in autonomous systems.", "authors": ["Afsoon Afzal"], "DOIs": ["https://doi.org/10.1145/3236024.3275429"], "tag": ["Doctorial Symposium"], "abstract": "ABSTRACTRobots and autonomous systems are finding their way to interact with the public and failures in these systems could be extremely expensive, even deadly. However, low-cost software-based simulation could be a promising approach to systematically test robotics systems and prevent failures as early as possible. In our early work, we showed that the majority of bugs could actually be reproduced and discovered using low-fidelity simulation environment. We created a high-level framework for automated testing of popular ArduPilot systems. In this work, I propose novel approaches to automatically infer powerful representation of system models, and generate test suites with the purpose of enhancing automated fault localization performance and describing the root cause of failures. Finally, I propose to use those novel approaches to inform the construction of automated program repair techniques for autonomous systems."}, {"id": "conf/sigsoft/Brindescu18", "title": "How do developers resolve merge conflicts? an investigation into the processes, tools, and improvements.", "authors": ["Caius Brindescu"], "DOIs": ["https://doi.org/10.1145/3236024.3275430"], "tag": ["Doctorial Symposium"], "abstract": "ABSTRACTMost software development is done in teams. When more than one developer is modifying the source code, there is a change that their changes will conflict. When this happens, developers have to interrupt their workflow in order to resolve the merge conflict. This interruption can lead to frustration and lost productivity. This makes collaboration, and the problems associated with it, an important aspect of software development. Merge conflicts are some of the more difficult issues that arise when working in a team.  We plan to bring in more information about the strategies developers use when resolving merge conflicts. We will gather information through in-situ observations and interviews of developers resolving conflicts when working on real development tasks, combined with analytical methods. The information obtained can then be used to improve the existing tools and make it easier for developers when working in a collaborative environment."}, {"id": "conf/sigsoft/Lima18", "title": "Automated scenario-based integration testing of distributed systems.", "authors": ["Bruno Lima"], "DOIs": ["https://doi.org/10.1145/3236024.3275431"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTIn a growing number of domains, the provisioning of end-to-end services to the users depends on the proper interoperation of multiple systems, forming a new distributed system, often subject to timing constraints. To ensure interoperability and integrity, it is important to conduct integration tests that verify the interactions with the environment and between the system components in key scenarios. To tackle test automation challenges, we propose algorithms for decentralized conformance checking and test input generation, and for checking and enforcing the conditions (local observability and controllability) that allow decentralized test execution."}, {"id": "conf/sigsoft/Sun18", "title": "Towards learning-augmented languages.", "authors": ["Xinyuan Sun"], "DOIs": ["https://doi.org/10.1145/3236024.3275432"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTReinforcement learning (RL) has seen tremendous success at solving a variety of problems ranging from industrial automation to games. This paper describes how existing programming languages can be augmented with new features so as to allow developers to exploit the power of modern RL algorithms and implementations."}, {"id": "conf/sigsoft/Gusmanov18", "title": "On the adoption of neural networks in modeling software reliability.", "authors": ["Kamill Gusmanov"], "DOIs": ["https://doi.org/10.1145/3236024.3275433"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTThis work models the reliability of software systems using recurrent neural networks with long short-term memory (LSTM) units and truncated backpropagation algorithm, and encoder-decoder LSTM architecture and proposes LSTM with software reliability functions as activation functions and LSTM with input features as the output of software reliability functions. An initial evaluation on data coming from 4 industrial projects is also provided."}, {"id": "conf/sigsoft/Ketkar18", "title": "Type migration in large-scale code bases.", "authors": ["Ameya Ketkar"], "DOIs": ["https://doi.org/10.1145/3236024.3275434"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTType migration is a frequent refactoring activity in which an existing type is replaced with another one throughout the source code. Recent studies have shown that type migration is more frequent in larger codebases. The state-of-the-art type migration tools cannot scale to large projects. Moreover, these tools do not fit into modern software development workflows, e.g., in Continuous Integration. This paper presents an IDE-independent type migration technique that scales to ultra-large-scale codebases through a MapReduce parallel and distributed process. We have implemented our approach in a tool called T2R. We evaluated it on codebases as large as 790 KLOC for specializing functional interfaces. Our results show that T2R is safe, scalable and useful. Open source developers accepted 70 migration patches spanning over 202 files."}, {"id": "conf/sigsoft/Nocera18", "title": "Reshaping distributed agile and adaptive development environment.", "authors": ["Francesco Nocera"], "DOIs": ["https://doi.org/10.1145/3236024.3275435"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTTowards the interest of (Collaborative Networked) Organizations in the adoption of emerging technologies to support communication, collaboration and monitoring needs of their Distributed Agile and Adaptive Development Environment (DADE), a tool based on the emerging Liquid Multi-Device Software paradigm is presented."}, {"id": "conf/sigsoft/Erofeeva18", "title": "How Dance(Sport) can help to produce better software.", "authors": ["Irina Erofeeva"], "DOIs": ["https://doi.org/10.1145/3236024.3275436"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTDancing and dancesport have a long tradition of instructions and development. Furthermore, there are many aspects of them that resemble the software development process. This work analyses their features with the intent to explore what of them is already applied in software development and what could be borrowed and applied in the future. Additionally, an investigation of the associated brain activities could be performed to gather a deeper understanding of analogies and differences."}, {"id": "conf/sigsoft/Nguyen18", "title": "Feature-interaction aware configuration prioritization.", "authors": ["Son Nguyen"], "DOIs": ["https://doi.org/10.1145/3236024.3275437"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTUnexpected interactions among features induce most bugs in a configurable software system. Exhaustively analyzing all exponential number of possible configurations is prohibitively costly. Thus, various sampling methods have been proposed to systematically narrow down the exponential number of configurations to be tested. Since testing all selected configurations can require a huge amount of effort, fault-based configuration prioritization, that helps detect bugs earlier, can yield practical benefits in quality assurance. In this paper, we propose CoPo, a novel formulation of feature-interaction bugs via common program entities enabled/disabled by the features. Leveraging from that, we develop an efficient feature-interaction-aware configuration prioritization technique for configurable systems by ranking configurations according to their total number of potential bugs. We conducted several experiments to evaluate CoPo on a public benchmark. We found that CoPo outperforms the state-of-the-art configuration prioritization methods. Interestingly, it is able to detect 17 not-yet-discovered feature-interaction bugs."}, {"id": "conf/sigsoft/Anand18", "title": "Dara: hybrid model checking of distributed systems.", "authors": ["Vaastav Anand"], "DOIs": ["https://doi.org/10.1145/3236024.3275438"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTBuilding correct implementations of distributed systems continues to elude us. Solutions consist of abstract modeling languages such as TLA+, PLusCal, which specify models of systems and tools like Coq, and SPIN which verify correctness of models but require considerable amount of effort, or transparent model checkers like MODIST, CMC and CHESS which suffer from state space explosion, rendering them impractical to use as they are too slow.  We propose Dara, a novel hybrid technique that combines the speed of abstract model checkers with the correctness and ease-of-use of transparent model checkers. Dara utilizes tests as well as a transparent model checker to generate logs from real executions of the system. The generated logs are analyzed to infer a model of the system which is model-checked by SPIN to verify user-provided invariants. Invariant violations are reported as likely bug traces. These traces are then passed to a replay engine which tries to replay the traces as real executions of the system to remove false positives. We are currently evaluating Dara's efficiency and usability."}, {"id": "conf/sigsoft/Tomassi18", "title": "Bugs in the wild: examining the effectiveness of static analyzers at finding real-world bugs.", "authors": ["David A. Tomassi"], "DOIs": ["https://doi.org/10.1145/3236024.3275439"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTStatic analysis is a powerful technique to find software bugs. In past years, a few static analysis tools have become available for developers to find certain kinds of bugs in their programs. However, there is no evidence on how effective the tools are in finding bugs in real-world software. In this paper, we present a preliminary study on the popular static analyzers ErrorProne and SpotBugs. Specifically, we consider 320 real Java bugs from the BugSwarm dataset, and determine which of these bugs can potentially be found by the analyzers, and how many are indeed detected. We find that 30.3% and 40.3% of the bugs are candidates for detection by ErrorProne and SpotBugs, respectively. Our evaluation shows that the analyzers are relatively easy to incorporate into the tool chain of diverse projects that use the Maven build system. However, the analyzers are not as effective detecting the bugs under study, with only one bug successfully detected by SpotBugs."}, {"id": "conf/sigsoft/DeFreez18", "title": "Mining error-handling specifications for systems software.", "authors": ["Daniel DeFreez"], "DOIs": ["https://doi.org/10.1145/3236024.3275440"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTThis paper presents a technique for mining error-handling specifications from systems software. It presents a static analysis for detecting error handlers in low-level code, and it shows how function synonyms can be used to mine for error-handling specifications with only a few supporting examples."}, {"id": "conf/sigsoft/Robson18", "title": "Diversity and decorum in open source communities.", "authors": ["Neill Robson"], "DOIs": ["https://doi.org/10.1145/3236024.3275441"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTOpen source software communities are increasingly aware of how social biases and discrimination negatively affect their culture. Many choose to establish policies regulating their contributors' social interactions without considering the efficacy of such measures. If these communities lack an empirical awareness of their policies' impact, they may find themselves adopting dogmatic practices that serve only to increase overhead maintenance costs. Conducting a gender diversity analysis of popular open source projects, I discovered no significant change in the proportion of women contributing to projects with or without a code of conduct. In light of this discovery, the open source community should consider supplemental strategies in order to foster diverse participation in their projects."}], "2019": [{"id": "conf/sigsoft/Atlee19", "title": "Living with feature interactions (keynote).", "authors": ["Joanne M. Atlee"], "DOIs": ["https://doi.org/10.1145/3338906.3342811"], "tag": ["Keynotes"], "abstract": "ABSTRACTFeature-oriented software development enables rapid software creation and evolution, through incremental and parallel feature development or through product line engineering. However, in practice, features are often not separate concerns. They behave differently in the presence of other features, and they sometimes interfere with each other in surprising ways.  This talk will explore challenges in feature interactions and their resolutions. Resolution strategies can tackle large classes of interactions, but are imperfect and incomplete, leading to research opportunities in software architecture, composition semantics, and verification."}, {"id": "conf/sigsoft/Kwiatkowska19", "title": "Safety and robustness for deep learning with provable guarantees (keynote).", "authors": ["Marta Kwiatkowska"], "DOIs": ["https://doi.org/10.1145/3338906.3342812"], "tag": ["Keynotes"], "abstract": "ABSTRACTComputing systems are becoming ever more complex, with decisions increasingly often based on deep learning components. A wide variety of applications are being developed, many of them safety-critical, such as self-driving cars and medical diagnosis. Since deep learning is unstable with respect to adversarial perturbations, there is a need for rigorous software development methodologies that encompass machine learning components. This lecture will describe progress with developing automated verification and testing techniques for deep neural networks to ensure safety and robustness of their decisions with respect to input perturbations. The techniques exploit Lipschitz continuity of the networks and aim to approximate, for a given set of inputs, the reachable set of network outputs in terms of lower and upper bounds, in anytime manner, with provable guarantees. We develop novel algorithms based on feature-guided search, games, global optimisation and Bayesian methods, and evaluate them on state-of-the-art networks. The lecture will conclude with an overview of the challenges in this field."}, {"id": "conf/sigsoft/Mockus19", "title": "Insights from open source software supply chains (keynote).", "authors": ["Audris Mockus"], "DOIs": ["https://doi.org/10.1145/3338906.3342813"], "tag": ["Keynotes"], "abstract": "ABSTRACTOpen Source Software (OSS) forms an infrastructure on which numerous (often critical) software applications are based. Substantial research was done to investigate central projects such as Linux kernel but we have only a limited understanding of how the periphery of the larger OSS ecosystem is interconnected through technical dependencies, code sharing, and knowledge flows. We aim to close this gap by a) creating a nearly complete and rapidly updateable collection of version control data for FLOSS projects; b) by cleaning, correcting, and augmenting the data to measure several types of dependencies among code, developers, and projects; c) by creating models that rely on the resulting supply chains to investigate structural and dynamic properties of the entire OSS. The current implementation is capable of being updated each month, occupies over 300Tb of disk space with 1.5B commits and 12B git objects. Highly accurate algorithms to correct identity data and extract dependencies from the source code are used to characterize the current structure of OSS and the way it has evolved. In particular, models of technology spread demonstrate the implicit factors developers use when choosing software components. We expect the resulting research platform will both spur investigations on how the huge periphery in OSS both sustains and is sustained by the central OSS projects and, as a result, will increase resiliency and effectiveness of the OSS."}, {"id": "conf/sigsoft/AhmadiD19", "title": "Concolic testing for models of state-based systems.", "authors": ["Reza Ahmadi", "Juergen Dingel"], "DOIs": ["https://doi.org/10.1145/3338906.3338908"], "tag": ["Main Research"], "abstract": "ABSTRACTTesting models of modern cyber-physical systems is not straightforward due to timing constraints, numerous if not infinite possible behaviors, and complex communications between components. Software testing tools and approaches that can generate test cases to test these systems are therefore important. Many of the existing automatic approaches support testing at the implementation level only. The existing model-level testing tools either treat the model as a black box (e.g., random testing approaches) or have limitations when it comes to generating complex test sequences (e.g., symbolic execution). This paper presents a novel approach and tool support for automatic unit testing of models of real-time embedded systems by conducting concolic testing, a hybrid testing technique based on concrete and symbolic execution. Our technique conducts automatic concolic testing in two phases. In the first phase, model is isolated from its environment, is transformed to a testable model and is integrated with a test harness. In the second phase, the harness tests the model concolically and reports the test execution results. We describe an implementation of our approach in the context of Papyrus-RT, an open source Model Driven Engineering (MDE) tool based on the modeling language UML-RT, and report the results of applying our concolic testing approach to a set of standard benchmark models to validate our approach."}, {"id": "conf/sigsoft/KimHK19", "title": "Target-driven compositional concolic testing with function summary refinement for effective bug detection.", "authors": ["Yunho Kim", "Shin Hong", "Moonzoo Kim"], "DOIs": ["https://doi.org/10.1145/3338906.3338934"], "tag": ["Main Research"], "abstract": "ABSTRACTConcolic testing is popular in unit testing because it can detect bugs quickly in a relatively small search space. But, in system-level testing, it suffers from the symbolic path explosion and often misses bugs. To resolve this problem, we have developed a focusedcompositional concolic testing technique, FOCAL, for effective bug detection. Focusing on a target unit failure v (a crash or an assert violation) detected by concolic unit testing, FOCAL generates a system-level test input that validates v. This test input is obtained by building and solving symbolic path formulas that represent system-level executions raising v. FOCAL builds such formulas by combining function summaries one by one backward from a function that raised v to main. If a function summary \u03c6a of function a conflicts with the summaries of the other functions, FOCAL refines \u03c6a to \u03c6a\u2032 by applying a refining constraint learned from the conflict. FOCAL showed high system-level bug detection ability by detecting 71 out of the 100 real-world target bugs in the SIR benchmark, while other relevant cutting edge techniques (i.e., AFL-fast, KATCH, Mix-CCBSE) detected at most 40 bugs. Also, FOCAL detected 13 new crash bugs in popular file parsing programs."}, {"id": "conf/sigsoft/MenghiNGB19", "title": "Generating automated and online test oracles for Simulink models with continuous and uncertain behaviors.", "authors": ["Claudio Menghi", "Shiva Nejati", "Khouloud Gaaloul", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1145/3338906.3338920"], "tag": ["Main Research"], "abstract": "ABSTRACTTest automation requires automated oracles to assess test outputs. For cyber physical systems (CPS), oracles, in addition to be automated, should ensure some key objectives: (i) they should check test outputs in an online manner to stop expensive test executions as soon as a failure is detected; (ii) they should handle time- and magnitude-continuous CPS behaviors; (iii) they should provide a quantitative degree of satisfaction or failure measure instead of binary pass/fail outputs; and (iv) they should be able to handle uncertainties due to CPS interactions with the environment. We propose an automated approach to translate CPS requirements specified in a logic-based language into test oracles specified in Simulink - a widely-used development and simulation language for CPS. Our approach achieves the objectives noted above through the identification of a fragment of Signal First Order logic (SFOL) to specify requirements, the definition of a quantitative semantics for this fragment and a sound translation of the fragment into Simulink. The results from applying our approach on 11 industrial case studies show that: (i) our requirements language can express all the 98 requirements of our case studies; (ii) the time and effort required by our approach are acceptable, showing potentials for the adoption of our work in practice, and (iii) for large models, our approach can dramatically reduce the test execution time compared to when test outputs are checked in an offline manner."}, {"id": "conf/sigsoft/ShahinCS19", "title": "Lifting Datalog-based analyses to software product lines.", "authors": ["Ramy Shahin", "Marsha Chechik", "Rick Salay"], "DOIs": ["https://doi.org/10.1145/3338906.3338928"], "tag": ["Main Research"], "abstract": "ABSTRACTApplying program analyses to Software Product Lines (SPLs) has been a fundamental research problem at the intersection of Product Line Engineering and software analysis. Different attempts have been made to \u201dlift\u201d particular product-level analyses to run on the entire product line. In this paper, we tackle the class of Datalog-based analyses (e.g., pointer and taint analyses), study the theoretical aspects of lifting Datalog inference, and implement a lifted inference algorithm inside the Souffl\u00e9 Datalog engine. We evaluate our implementation on a set of benchmark product lines. We show significant savings in processing time and fact database size (billions of times faster on one of the benchmarks) compared to brute-force analysis of each product individually."}, {"id": "conf/sigsoft/MordahlOKWG19", "title": "An empirical study of real-world variability bugs detected by variability-oblivious tools.", "authors": ["Austin Mordahl", "Jeho Oh", "Ugur Koc", "Shiyi Wei", "Paul Gazzillo"], "DOIs": ["https://doi.org/10.1145/3338906.3338967"], "tag": ["Main Research"], "abstract": "ABSTRACTMany critical software systems developed in C utilize compile-time configurability. The many possible configurations of this software make bug detection through static analysis difficult. While variability-aware static analyses have been developed, there remains a gap between those and state-of-the-art static bug detection tools. In order to collect data on how such tools may perform and to develop real-world benchmarks, we present a way to leverage configuration sampling, off-the-shelf \u201cvariability-oblivious\u201d bug detectors, and automatic feature identification techniques to simulate a variability-aware analysis. We instantiate our approach using four popular static analysis tools on three highly configurable, real-world C projects, obtaining 36,061 warnings, 80% of which are variability warnings. We analyze the warnings we collect from these experiments, finding that most results are variability warnings of a variety of kinds such as NULL dereference. We then manually investigate these warnings to produce a benchmark of 77 confirmed true bugs (52 of which are variability bugs) useful for future development of variability-aware analyses."}, {"id": "conf/sigsoft/NesicKSB19", "title": "Principles of feature modeling.", "authors": ["Damir Nesic", "Jacob Kr\u00fcger", "Stefan Stanciulescu", "Thorsten Berger"], "DOIs": ["https://doi.org/10.1145/3338906.3338974"], "tag": ["Main Research"], "abstract": "ABSTRACTFeature models are arguably one of the most intuitive and successful notations for modeling the features of a variant-rich software system. Feature models help developers to keep an overall understanding of the system, and also support scoping, planning, development, variant derivation, configuration, and maintenance activities that sustain the system's long-term success. Unfortunately, feature models are difficult to build and evolve. Features need to be identified, grouped, organized in a hierarchy, and mapped to software assets. Also, dependencies between features need to be declared. While feature models have been the subject of three decades of research, resulting in many feature-modeling notations together with automated analysis and configuration techniques, a generic set of principles for engineering feature models is still missing. It is not even clear whether feature models could be engineered using recurrent principles. Our work shows that such principles in fact exist. We analyzed feature-modeling practices elicited from ten interviews conducted with industrial practitioners and from 31 relevant papers. We synthesized a set of 34 principles covering eight different phases of feature modeling, from planning over model construction, to model maintenance and evolution. Grounded in empirical evidence, these principles provide practical, context-specific advice on how to perform feature modeling, describe what information sources to consider, and highlight common characteristics of feature models. We believe that our principles can support researchers and practitioners enhancing feature-modeling tooling, synthesis, and analyses techniques, as well as scope future research."}, {"id": "conf/sigsoft/RiggerMAM19", "title": "Understanding GCC builtins to develop better tools.", "authors": ["Manuel Rigger", "Stefan Marr", "Bram Adams", "Hanspeter M\u00f6ssenb\u00f6ck"], "DOIs": ["https://doi.org/10.1145/3338906.3338907"], "tag": ["Main Research"], "abstract": "ABSTRACTC programs can use compiler builtins to provide functionality that the C language lacks. On Linux, GCC provides several thousands of builtins that are also supported by other mature compilers, such as Clang and ICC. Maintainers of other tools lack guidance on whether and which builtins should be implemented to support popular projects. To assist tool developers who want to support GCC builtins, we analyzed builtin use in 4,913 C projects from GitHub. We found that 37% of these projects relied on at least one builtin. Supporting an increasing proportion of projects requires support of an exponentially increasing number of builtins; however, implementing only 10 builtins already covers over 30% of the projects. Since we found that many builtins in our corpus remained unused, the effort needed to support 90% of the projects is moderate, requiring about 110 builtins to be implemented. For each project, we analyzed the evolution of builtin use over time and found that the majority of projects mostly added builtins. This suggests that builtins are not a legacy feature and must be supported in future tools. Systematic testing of builtin support in existing tools revealed that many lacked support for builtins either partially or completely; we also discovered incorrect implementations in various tools, including the formally verified CompCert compiler."}, {"id": "conf/sigsoft/ChaparroBLMMPPN19", "title": "Assessing the quality of the steps to reproduce in bug reports.", "authors": ["Oscar Chaparro", "Carlos Bernal-C\u00e1rdenas", "Jing Lu", "Kevin Moran", "Andrian Marcus", "Massimiliano Di Penta", "Denys Poshyvanyk", "Vincent Ng"], "DOIs": ["https://doi.org/10.1145/3338906.3338947"], "tag": ["Main Research"], "abstract": "ABSTRACTA major problem with user-written bug reports, indicated by developers and documented by researchers, is the (lack of high) quality of the reported steps to reproduce the bugs. Low-quality steps to reproduce lead to excessive manual effort spent on bug triage and resolution. This paper proposes Euler, an approach that automatically identifies and assesses the quality of the steps to reproduce in a bug report, providing feedback to the reporters, which they can use to improve the bug report. The feedback provided by Euler was assessed by external evaluators and the results indicate that Euler correctly identified 98% of the existing steps to reproduce and 58% of the missing ones, while 73% of its quality annotations are correct."}, {"id": "conf/sigsoft/Wang0LXBXW19", "title": "A learning-based approach for automatic construction of domain glossary from source code and documentation.", "authors": ["Chong Wang", "Xin Peng", "Mingwei Liu", "Zhenchang Xing", "Xuefang Bai", "Bing Xie", "Tuo Wang"], "DOIs": ["https://doi.org/10.1145/3338906.3338963"], "tag": ["Main Research"], "abstract": "ABSTRACTA domain glossary that organizes domain-specific concepts and their aliases and relations is essential for knowledge acquisition and software development. Existing approaches use linguistic heuristics or term-frequency-based statistics to identify domain specific terms from software documentation, and thus the accuracy is often low. In this paper, we propose a learning-based approach for automatic construction of domain glossary from source code and software documentation. The approach uses a set of high-quality seed terms identified from code identifiers and natural language concept definitions to train a domain-specific prediction model to recognize glossary terms based on the lexical and semantic context of the sentences mentioning domain-specific concepts. It then merges the aliases of the same concepts to their canonical names, selects a set of explanation sentences for each concept, and identifies \"is a\", \"has a\", and \"related to\" relations between the concepts. We apply our approach to deep learning domain and Hadoop domain and harvest 5,382 and 2,069 concepts together with 16,962 and 6,815 relations respectively. Our evaluation validates the accuracy of the extracted domain glossary and its usefulness for the fusion and acquisition of knowledge from different documents of different projects."}, {"id": "conf/sigsoft/FucciMM19", "title": "On using machine learning to identify knowledge in API reference documentation.", "authors": ["Davide Fucci", "Alireza Mollaalizadehbahnemiri", "Walid Maalej"], "DOIs": ["https://doi.org/10.1145/3338906.3338943"], "tag": ["Main Research"], "abstract": "ABSTRACTUsing API reference documentation like JavaDoc is an integral part of software development. Previous research introduced a grounded taxonomy that organizes API documentation knowledge in 12 types, including knowledge about the Functionality, Structure, and Quality of an API. We study how well modern text classification approaches can automatically identify documentation containing specific knowledge types. We compared conventional machine learning (k-NN and SVM) with deep learning approaches trained on manually-annotated Java and .NET API documentation (n = 5,574). When classifying the knowledge types individually (i.e., multiple binary classifiers) the best AUPRC was up to 87"}, {"id": "conf/sigsoft/Liu0MXXXL19", "title": "Generating query-specific class API summaries.", "authors": ["Mingwei Liu", "Xin Peng", "Andrian Marcus", "Zhenchang Xing", "Wenkai Xie", "Shuangshuang Xing", "Yang Liu"], "DOIs": ["https://doi.org/10.1145/3338906.3338971"], "tag": ["Main Research"], "abstract": "ABSTRACTSource code summaries are concise representations, in form of text and/or code, of complex code elements and are meant to help developers gain a quick understanding that in turns help them perform specific tasks. Generation of summaries that are task-specific is still a challenge in the automatic code summarization field. We propose an approach for generating on-demand, extrinsic hybrid summaries for API classes, relevant to a programming task, formulated as a natural language query. The summaries include the most relevant sentences extracted from the API reference documentation and the most relevant methods.  External evaluators assessed the summaries generated for classes retrieved from JDK and Android libraries for several programming tasks. The majority found that the summaries are complete, concise, and readable. A comparison with summaries produce by three baseline approaches revealed that the information present only in our summaries is more relevant than the one present only in the baselines summaries. Finally, an extrinsic evaluation study showed that the summaries help the users evaluating the correctness of API retrieval results, faster and more accurately."}, {"id": "conf/sigsoft/JiangLZ19", "title": "Semantic relation based expansion of abbreviations.", "authors": ["Yanjie Jiang", "Hui Liu", "Lu Zhang"], "DOIs": ["https://doi.org/10.1145/3338906.3338929"], "tag": ["Main Research"], "abstract": "ABSTRACTIdentifiers account for 70% of source code in terms of characters, and thus the quality of such identifiers is critical for program comprehension and software maintenance. For various reasons, however, many identifiers contain abbreviations, which reduces the readability and maintainability of source code. To this end, a number of approaches have been proposed to expand abbreviations in identifiers. However, such approaches are either inaccurate or confined to specific identifiers. To this end, in this paper we propose a generic and accurate approach to expand identifier abbreviations. The key insight of the approach is that abbreviations in the name of software entity e have great chance to find their full terms in names of software entities that are semantically related to e. Consequently, the proposed approach builds a knowledge graph to represent such entities and their relationships with e, and searches the graph for full terms. The optimal searching strategy for the graph could be learned automatically from a corpus of manually expanded abbreviations. We evaluate the proposed approach on nine well known open-source projects. Results of our k-fold evaluation suggest that the proposed approach improves the state of the art. It improves precision significantly from 29% to 85%, and recall from 29% to 77%. Evaluation results also suggest that the proposed generic approach is even better than the state-of-the-art parameter-specific approach in expanding parameter abbreviations, improving F1 score significantly from 75% to 87%."}, {"id": "conf/sigsoft/Biagiola0RT19", "title": "Diversity-based web test generation.", "authors": ["Matteo Biagiola", "Andrea Stocco", "Filippo Ricca", "Paolo Tonella"], "DOIs": ["https://doi.org/10.1145/3338906.3338970"], "tag": ["Main Research"], "abstract": "ABSTRACTExisting web test generators derive test paths from a navigational model of the web application, completed with either manually or randomly generated input values. However, manual test data selection is costly, while random generation often results in infeasible input sequences, which are rejected by the application under test. Random and search-based generation can achieve the desired level of model coverage only after a large number of test execution at- tempts, each slowed down by the need to interact with the browser during test execution. In this work, we present a novel web test generation algorithm that pre-selects the most promising candidate test cases based on their diversity from previously generated tests. As such, only the test cases that explore diverse behaviours of the application are considered for in-browser execution. We have implemented our approach in a tool called DIG. Our empirical evaluation on six real-world web applications shows that DIG achieves higher coverage and fault detection rates significantly earlier than crawling-based and search-based web test generators."}, {"id": "conf/sigsoft/Biagiola00RT19", "title": "Web test dependency detection.", "authors": ["Matteo Biagiola", "Andrea Stocco", "Ali Mesbah", "Filippo Ricca", "Paolo Tonella"], "DOIs": ["https://doi.org/10.1145/3338906.3338948"], "tag": ["Main Research"], "abstract": "ABSTRACTE2E web test suites are prone to test dependencies due to the heterogeneous multi-tiered nature of modern web apps, which makes it difficult for developers to create isolated program states for each test case. In this paper, we present the first approach for detecting and validating test dependencies present in E2E web test suites. Our approach employs string analysis to extract an approximated set of dependencies from the test code. It then filters potential false dependencies through natural language processing of test names. Finally, it validates all dependencies, and uses a novel recovery algorithm to ensure no true dependencies are missed in the final test dependency graph. Our approach is implemented in a tool called TEDD and evaluated on the test suites of six open-source web apps. Our results show that TEDD can correctly detect and validate test dependencies up to 72% faster than the baseline with the original test ordering in which the graph contains all possible dependencies. The test dependency graphs produced by TEDD enable test execution parallelization, with a speed-up factor of up to 7\u00d7."}, {"id": "conf/sigsoft/StahlbauerKF19", "title": "Testing scratch programs automatically.", "authors": ["Andreas Stahlbauer", "Marvin Kreis", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1145/3338906.3338910"], "tag": ["Main Research"], "abstract": "ABSTRACTBlock-based programming environments like Scratch foster engagement with computer programming and are used by millions of young learners. Scratch allows learners to quickly create entertaining programs and games, while eliminating syntactical program errors that could interfere with progress. However, functional programming errors may still lead to incorrect programs, and learners and their teachers need to identify and understand these errors. This is currently an entirely manual process. In this paper, we introduce a formal testing framework that describes the problem of Scratch testing in detail. We instantiate this formal framework with the Whisker tool, which provides automated and property-based testing functionality for Scratch programs. Empirical evaluation on real student and teacher programs demonstrates that Whisker can successfully test Scratch programs, and automatically achieves an average of 95.25 % code coverage. Although well-known testing problems such as test flakiness also exist in the scenario of Scratch testing, we show that automated and property-based testing can accurately reproduce and replace the manually and laboriously produced grading efforts of a teacher, and opens up new possibilities to support learners of programming in their struggles."}, {"id": "conf/sigsoft/Zhang0C0Z19", "title": "A large-scale empirical study of compiler errors in continuous integration.", "authors": ["Chen Zhang", "Bihuan Chen", "Linlin Chen", "Xin Peng", "Wenyun Zhao"], "DOIs": ["https://doi.org/10.1145/3338906.3338917"], "tag": ["Main Research"], "abstract": "ABSTRACTContinuous Integration (CI) is a widely-used software development practice to reduce risks. CI builds often break, and a large amount of efforts are put into troubleshooting broken builds. Despite that compiler errors have been recognized as one of the most frequent types of build failures, little is known about the common types, fix efforts and fix patterns of compiler errors that occur in CI builds of open-source projects. To fill such a gap, we present a large-scale empirical study on 6,854,271 CI builds from 3,799 open-source Java projects hosted on GitHub. Using the build data, we measured the frequency of broken builds caused by compiler errors, investigated the ten most common compiler error types, and reported their fix time. We manually analyzed 325 broken builds to summarize fix patterns of the ten most common compiler error types. Our findings help to characterize and understand compiler errors during CI and provide practical implications to developers, tool builders and researchers."}, {"id": "conf/sigsoft/HeMS0PS19", "title": "A statistics-based performance testing methodology for cloud applications.", "authors": ["Sen He", "Glenna Manns", "John Saunders", "Wei Wang", "Lori L. Pollock", "Mary Lou Soffa"], "DOIs": ["https://doi.org/10.1145/3338906.3338912"], "tag": ["Main Research"], "abstract": "ABSTRACTThe low cost of resource ownership and flexibility have led users to increasingly port their applications to the clouds. To fully realize the cost benefits of cloud services, users usually need to reliably know the execution performance of their applications. However, due to the random performance fluctuations experienced by cloud applications, the black box nature of public clouds and the cloud usage costs, testing on clouds to acquire accurate performance results is extremely difficult. In this paper, we present a novel cloud performance testing methodology called PT4Cloud. By employing non-parametric statistical approaches of likelihood theory and the bootstrap method, PT4Cloud provides reliable stop conditions to obtain highly accurate performance distributions with confidence bands. These statistical approaches also allow users to specify intuitive accuracy goals and easily trade between accuracy and testing cost. We evaluated PT4Cloud with 33 benchmark configurations on Amazon Web Service and Chameleon clouds. When compared with performance data obtained from extensive performance tests, PT4Cloud provides testing results with 95.4% accuracy on average while reducing the number of test runs by 62%. We also propose two test execution reduction techniques for PT4Cloud, which can reduce the number of test runs by 90.1% while retaining an average accuracy of 91%. We compared our technique to three other techniques and found that our results are much more accurate."}, {"id": "conf/sigsoft/CotroneoSLNB19", "title": "How bad can a bug get? an empirical analysis of software failures in the OpenStack cloud computing platform.", "authors": ["Domenico Cotroneo", "Luigi De Simone", "Pietro Liguori", "Roberto Natella", "Nematollah Bidokhti"], "DOIs": ["https://doi.org/10.1145/3338906.3338916"], "tag": ["Main Research"], "abstract": "ABSTRACTCloud management systems provide abstractions and APIs for programmatically configuring cloud infrastructures. Unfortunately, residual software bugs in these systems can potentially lead to high-severity failures, such as prolonged outages and data losses. In this paper, we investigate the impact of failures in the context widespread OpenStack cloud management system, by performing fault injection and by analyzing the impact of the resulting failures in terms of fail-stop behavior, failure detection through logging, and failure propagation across components. The analysis points out that most of the failures are not timely detected and notified; moreover, many of these failures can silently propagate over time and through components of the cloud management system, which call for more thorough run-time checks and fault containment."}, {"id": "conf/sigsoft/LinCLLZ19", "title": "Towards more efficient meta-heuristic algorithms for combinatorial test generation.", "authors": ["Jinkun Lin", "Shaowei Cai", "Chuan Luo", "Qingwei Lin", "Hongyu Zhang"], "DOIs": ["https://doi.org/10.1145/3338906.3338914"], "tag": ["Main Research"], "abstract": "ABSTRACTCombinatorial interaction testing (CIT) is a popular approach to detecting faults in highly configurable software systems. The core task of CIT is to generate a small test suite called a t-way covering array (CA), where t is the covering strength. Many meta-heuristic algorithms have been proposed to solve the constrained covering array generating (CCAG) problem. A major drawback of existing algorithms is that they usually need considerable time to obtain a good-quality solution, which hinders the wider applications of such algorithms. We observe that the high time consumption of existing meta-heuristic algorithms for CCAG is mainly due to the procedure of score computation. In this work, we propose a much more efficient method for score computation. The score computation method is applied to a state-of-the-art algorithm TCA, showing significant improvements. The new score computation method opens a way to utilize algorithmic ideas relying on scores which were not affordable previously. We integrate a gradient descent search step to further improve the algorithm, leading to a new algorithm called FastCA. Experiments on a broad range of real-world benchmarks and synthetic benchmarks show that, FastCA significantly outperforms state-of-the-art algorithms for CCAG algorithms, in terms of both the size of obtained covering array and the run time."}, {"id": "conf/sigsoft/0003HSZHZ19", "title": "Compiler bug isolation via effective witness test program generation.", "authors": ["Junjie Chen", "Jiaqi Han", "Peiyi Sun", "Lingming Zhang", "Dan Hao", "Lu Zhang"], "DOIs": ["https://doi.org/10.1145/3338906.3338957"], "tag": ["Main Research"], "abstract": "ABSTRACTCompiler bugs are extremely harmful, but are notoriously difficult to debug because compiler bugs usually produce few debugging information. Given a bug-triggering test program for a compiler, hundreds of compiler files are usually involved during compilation, and thus are suspect buggy files. Although there are lots of automated bug isolation techniques, they are not applicable to compilers due to the scalability or effectiveness problem. To solve this problem, in this paper, we transform the compiler bug isolation problem into a search problem, i.e., searching for a set of effective witness test programs that are able to eliminate innocent compiler files from suspects. Based on this intuition, we propose an automated compiler bug isolation technique, DiWi, which (1) proposes a heuristic-based search strategy to generate such a set of effective witness test programs via applying our designed witnessing mutation rules to the given failing test program, and (2) compares their coverage to isolate bugs following the practice of spectrum-based bug isolation. The experimental results on 90 real bugs from popular GCC and LLVM compilers show that DiWi effectively isolates 66.67%/78.89% bugs within Top-10/Top-20 compiler files, significantly outperforming state-of-the-art bug isolation techniques."}, {"id": "conf/sigsoft/ChaO19", "title": "Concolic testing with adaptively changing search heuristics.", "authors": ["Sooyoung Cha", "Hakjoo Oh"], "DOIs": ["https://doi.org/10.1145/3338906.3338964"], "tag": ["Main Research"], "abstract": "ABSTRACTWe present Chameleon, a new approach for adaptively changing search heuristics during concolic testing. Search heuristics play a central role in concolic testing as they mitigate the path-explosion problem by focusing on particular program paths that are likely to increase code coverage as quickly as possible. A variety of techniques for search heuristics have been proposed over the past decade. However, existing approaches are limited in that they use the same search heuristics throughout the entire testing process, which is inherently insufficient to exercise various execution paths. Chameleon overcomes this limitation by adapting search heuristics on the fly via an algorithm that learns new search heuristics based on the knowledge accumulated during concolic testing. Experimental results show that the transition from the traditional non-adaptive approaches to ours greatly improves the practicality of concolic testing in terms of both code coverage and bug-finding."}, {"id": "conf/sigsoft/BaresiDQ19", "title": "Symbolic execution-driven extraction of the parallel execution plans of Spark applications.", "authors": ["Luciano Baresi", "Giovanni Denaro", "Giovanni Quattrocchi"], "DOIs": ["https://doi.org/10.1145/3338906.3338973"], "tag": ["Main Research"], "abstract": "ABSTRACTThe execution of Spark applications is based on the execution order and parallelism of the different jobs, given data and available resources. Spark reifies these dependencies in a graph that we refer to as the (parallel) execution plan of the application. All the approaches that have studied the estimation of the execution times and the dynamic provisioning of resources for this kind of applications have always assumed that the execution plan is unique, given the computing resources at hand. This assumption is at least simplistic for applications that include conditional branches or loops and limits the precision of the prediction techniques.  This paper introduces SEEPEP, a novel technique based on symbolic execution and search-based test generation, that: i) automatically extracts the possible execution plans of a Spark application, along with dedicated launchers with properly synthesized data that can be used for profiling, and ii) tunes the allocation of resources at runtime based on the knowledge of the execution plans for which the path conditions hold. The assessment we carried out shows that SEEPEP can effectively complement dynaSpark, an extension of Spark with dynamic resource provisioning capabilities, to help predict the execution duration and the allocation of resources."}, {"id": "conf/sigsoft/GambiHF19", "title": "Generating effective test cases for self-driving cars from police reports.", "authors": ["Alessio Gambi", "Tri Huynh", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1145/3338906.3338942"], "tag": ["Main Research"], "abstract": "ABSTRACTAutonomous driving carries the promise to drastically reduce the number of car accidents; however, recently reported fatal crashes involving self-driving cars show that such an important goal is not yet achieved. This calls for better testing of the software controlling self-driving cars, which is difficult because it requires producing challenging driving scenarios. To better test self-driving car soft- ware, we propose to specifically test car crash scenarios, which are critical par excellence. Since real car crashes are difficult to test in field operation, we recreate them as physically accurate simulations in an environment that can be used for testing self-driving car software. To cope with the scarcity of sensory data collected during real car crashes which does not enable a full reproduction, we extract the information to recreate real car crashes from the police reports which document them. Our extensive evaluation, consisting of a user study involving 34 participants and a quantitative analysis of the quality of the generated tests, shows that we can generate accurate simulations of car crashes in a matter of minutes. Compared to tests which implement non critical driving scenarios, our tests effectively stressed the test subject in different ways and exposed several shortcomings in its implementation."}, {"id": "conf/sigsoft/LuPZ0L19", "title": "Preference-wise testing for Android applications.", "authors": ["Yifei Lu", "Minxue Pan", "Juan Zhai", "Tian Zhang", "Xuandong Li"], "DOIs": ["https://doi.org/10.1145/3338906.3338980"], "tag": ["Main Research"], "abstract": "ABSTRACTPreferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore, need to be thoroughly tested. Unfortunately, the specific preferences used in test cases are typically not explicitly specified, forcing testers to manually set options or blindly try different option combinations. To effectively test the impacts of different preference options, this paper presents PREFEST, as a preference-wise enhanced automatic testing approach, for Android apps. Given a set of test cases, PREFEST can locate the preferences that may affect the test cases with a static and dynamic combined analysis on the app under test, and execute these test cases only under necessary option combinations. The evaluation shows that PREFEST can improve 6.8% code coverage and 12.3% branch coverage and find five more real bugs compared to testing with the original test cases. The test cost is reduced by 99% for both the number of test cases and the testing time, compared to testing under pairwise combination of options."}, {"id": "conf/sigsoft/NajafiRS19", "title": "Bisecting commits and modeling commit risk during testing.", "authors": ["Armin Najafi", "Peter C. Rigby", "Weiyi Shang"], "DOIs": ["https://doi.org/10.1145/3338906.3338944"], "tag": ["Main Research"], "abstract": "ABSTRACTSoftware testing is one of the costliest stages in the software development life cycle. One approach to reducing the test execution cost is to group changes and test them as a batch (i.e. batch testing). However, when tests fail in a batch, commits in the batch need to be re-tested to identify the cause of the failure, i.e. the culprit commit. The re-testing is typically done through bisection (i.e. a binary search through the commits in a batch). Intuitively, the effectiveness of batch testing highly depends on the size of the batch. Larger batches require fewer initial test runs, but have a higher chance of a test failure that can lead to expensive test re-runs to find the culprit. We are unaware of research that investigates and simulates the impact of batch sizes on the cost of testing in industry. In this work, we first conduct empirical studies on the effectiveness of batch testing in three large-scale industrial software systems at Ericsson. Using 9 months of testing data, we simulate batch sizes from 1 to 20 and find the most cost-effective BatchSize for each project. Our results show that batch testing saves 72% of test executions compared to testing each commit individually. In a second simulation, we incorporate flaky tests that pass and fail on the same commit as they are a significant source of additional test executions on large projects. We model the degree of flakiness for each project and find that test flakiness reduces the cost savings to 42%. In a third simulation, we guide bisection to reduce the likelihood of batch-testing failures. We model the riskiness of each commit in a batch using a bug model and a test execution history model. The risky commits are tested individually, while the less risky commits are tested in a single larger batch. Culprit predictions with our approach reduce test executions up to 9% compared to Ericsson\u2019s current bisection approach. The results have been adopted by developers at Ericsson and a tool to guide bisection is in the process of being added to Ericsson\u2019s continuous integration pipeline."}, {"id": "conf/sigsoft/GulzarMMK19", "title": "White-box testing of big data analytics with complex user-defined functions.", "authors": ["Muhammad Ali Gulzar", "Shaghayegh Mardani", "Madanlal Musuvathi", "Miryung Kim"], "DOIs": ["https://doi.org/10.1145/3338906.3338953"], "tag": ["Main Research"], "abstract": "ABSTRACTData-intensive scalable computing (DISC) systems such as Google\u2019s MapReduce, Apache Hadoop, and Apache Spark are being leveraged to process massive quantities of data in the cloud. Modern DISC applications pose new challenges in exhaustive, automatic testing because they consist of dataflow operators, and complex user-defined functions (UDF) are prevalent unlike SQL queries. We design a new white-box testing approach, called BigTest to reason about the internal semantics of UDFs in tandem with the equivalence classes created by each dataflow and relational operator. Our evaluation shows that, despite ultra-large scale input data size, real world DISC applications are often significantly skewed and inadequate in terms of test coverage, leaving 34% of Joint Dataflow and UDF (JDU) paths untested. BigTest shows the potential to minimize data size for local testing by 10^5 to 10^8 orders of magnitude while revealing 2X more manually-injected faults than the previous approach. Our experiment shows that only few of the data records (order of tens) are actually required to achieve the same JDU coverage as the entire production data. The reduction in test data also provides CPU time saving of 194X on average, demonstrating that interactive and fast local testing is feasible for big data analytics, obviating the need to test applications on huge production data."}, {"id": "conf/sigsoft/DurieuxDMA19", "title": "Empirical review of Java program repair tools: a large-scale experiment on 2, 141 bugs and 23, 551 repair attempts.", "authors": ["Thomas Durieux", "Fernanda Madeiral", "Matias Martinez", "Rui Abreu"], "DOIs": ["https://doi.org/10.1145/3338906.3338911"], "tag": ["Main Research"], "abstract": "ABSTRACTIn the past decade, research on test-suite-based automatic program repair has grown significantly. Each year, new approaches and implementations are featured in major software engineering venues. However, most of those approaches are evaluated on a single benchmark of bugs, which are also rarely reproduced by other researchers. In this paper, we present a large-scale experiment using 11 Java test-suite-based repair tools and 2,141 bugs from 5 benchmarks. Our goal is to have a better understanding of the current state of automatic program repair tools on a large diversity of benchmarks. Our investigation is guided by the hypothesis that the repairability of repair tools might not be generalized across different benchmarks. We found that the 11 tools 1) are able to generate patches for 21% of the bugs from the 5 benchmarks, and 2) have better performance on Defects4J compared to other benchmarks, by generating patches for 47% of the bugs from Defects4J compared to 10-30% of bugs from the other benchmarks. Our experiment comprises 23,551 repair attempts, which we used to find causes of non-patch generation. These causes are reported in this paper, which can help repair tool designers to improve their approaches and tools."}, {"id": "conf/sigsoft/KoyuncuLB0MKT19", "title": "iFixR: bug report driven program repair.", "authors": ["Anil Koyuncu", "Kui Liu", "Tegawend\u00e9 F. Bissyand\u00e9", "Dongsun Kim", "Martin Monperrus", "Jacques Klein", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3338906.3338935"], "tag": ["Main Research"], "abstract": "ABSTRACTIssue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and- validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation)."}, {"id": "conf/sigsoft/WenWLTXCS19", "title": "Exploring and exploiting the correlations between bug-inducing and bug-fixing commits.", "authors": ["Ming Wen", "Rongxin Wu", "Yepang Liu", "Yongqiang Tian", "Xuan Xie", "Shing-Chi Cheung", "Zhendong Su"], "DOIs": ["https://doi.org/10.1145/3338906.3338962"], "tag": ["Main Research"], "abstract": "ABSTRACTBug-inducing commits provide important information to understand when and how bugs were introduced. Therefore, they have been extensively investigated by existing studies and frequently leveraged to facilitate bug fixings in industrial practices.  Due to the importance of bug-inducing commits in software debugging, we are motivated to conduct the first systematic empirical study to explore the correlations between bug-inducing and bug-fixing commits in terms of code elements and modifications. To facilitate the study, we collected the inducing and fixing commits for 333 bugs from seven large open-source projects. The empirical findings reveal important and significant correlations between a bug's inducing and fixing commits. We further exploit the usefulness of such correlation findings from two aspects. First, they explain why the SZZ algorithm, the most widely-adopted approach to collecting bug-inducing commits, is imprecise. In view of SZZ's imprecision, we revisited the findings of previous studies based on SZZ, and found that 8 out of 10 previous findings are significantly affected by SZZ's imprecision. Second, they shed lights on the design of automated debugging techniques. For demonstration, we designed approaches that exploit the correlations with respect to statements and change actions. Our experiments on Defects4J show that our approaches can boost the performance of fault localization significantly and also advance existing APR techniques."}, {"id": "conf/sigsoft/KrugerCBLS19", "title": "Effects of explicit feature traceability on program comprehension.", "authors": ["Jacob Kr\u00fcger", "G\u00fcl \u00c7alikli", "Thorsten Berger", "Thomas Leich", "Gunter Saake"], "DOIs": ["https://doi.org/10.1145/3338906.3338968"], "tag": ["Main Research"], "abstract": "ABSTRACTDevelopers spend a substantial amount of their time with program comprehension. To improve their comprehension and refresh their memory, developers need to communicate with other developers, read the documentation, and analyze the source code. Many studies show that developers focus primarily on the source code and that small improvements can have a strong impact. As such, it is crucial to bring the code itself into a more comprehensible form. A particular technique for this purpose are explicit feature traces to easily identify a program\u2019s functionalities. To improve our empirical understanding about the effects of feature traces, we report an online experiment with 49 professional software developers. We studied the impact of explicit feature traces, namely annotations and decomposition, on program comprehension and compared them to the same code without traces. Besides this experiment, we also asked our participants about their opinions in order to combine quantitative and qualitative data. Our results indicate that, as opposed to purely object-oriented code: (1) annotations can have positive effects on program comprehension; (2) decomposition can have a negative impact on bug localization; and (3) our participants perceive both techniques as beneficial. Moreover, none of the three code versions yields significant improvements on task completion time. Overall, our results indicate that lightweight traceability, such as using annotations, provides immediate benefits to developers during software development and maintenance without extensive training or tooling; and can improve current industrial practices that rely on heavyweight traceability tools (e.g., DOORS) and retroactive fulfillment of standards (e.g., ISO-26262, DO-178B)."}, {"id": "conf/sigsoft/ZhouVK19", "title": "What the fork: a study of inefficient and efficient forking practices in social coding.", "authors": ["Shurui Zhou", "Bogdan Vasilescu", "Christian K\u00e4stner"], "DOIs": ["https://doi.org/10.1145/3338906.3338918"], "tag": ["Main Research"], "abstract": "ABSTRACTForking and pull requests have been widely used in open-source communities as a uniform development and contribution mechanism, giving developers the flexibility to modify their own fork without affecting others before attempting to contribute back. However, not all projects use forks efficiently; many experience lost and duplicate contributions and fragmented communities. In this paper, we explore how open-source projects on GitHub differ with regard to forking inefficiencies. First, we observed that different communities experience these inefficiencies to widely different degrees and interviewed practitioners to understand why. Then, using multiple regression modeling, we analyzed which context factors correlate with fewer inefficiencies.We found that better modularity and centralized management are associated with more contributions and a higher fraction of accepted pull requests, suggesting specific best practices that project maintainers can adopt to reduce forking-related inefficiencies in their communities."}, {"id": "conf/sigsoft/SongZH19", "title": "ServDroid: detecting service usage inefficiencies in Android applications.", "authors": ["Wei Song", "Jing Zhang", "Jeff Huang"], "DOIs": ["https://doi.org/10.1145/3338906.3338950"], "tag": ["Main Research"], "abstract": "ABSTRACTServices in Android applications are frequently-used components for performing time-consuming operations in the background. While services play a crucial role in the app performance, our study shows that service uses in practice are not as efficient as expected, e.g., they tend to cause unnecessary resource occupation and/or energy consumption. Moreover, as service usage inefficiencies do not manifest with immediate failures, e.g., app crashes, existing testing-based approaches fall short in finding them. In this paper, we identify four anti-patterns of such service usage inefficiency bugs, including premature create, late destroy, premature destroy, and service leak, and present a static analysis technique, ServDroid, to automatically and effectively detect them based on the anti-patterns. We have applied ServDroid to a large collection of popular real-world Android apps. Our results show that, surprisingly, service usage inefficiencies are prevalent and can severely impact the app performance."}, {"id": "conf/sigsoft/PauckW19", "title": "Together strong: cooperative Android app analysis.", "authors": ["Felix Pauck", "Heike Wehrheim"], "DOIs": ["https://doi.org/10.1145/3338906.3338915"], "tag": ["Main Research"], "abstract": "ABSTRACTRecent years have seen the development of numerous tools for the analysis of taint flows in Android apps. Taint analyses aim at detecting data leaks, accidentally or by purpose programmed into apps. Often, such tools specialize in the treatment of specific features impeding precise taint analysis (like reflection or inter-app communication). This multitude of tools, their specific applicability and their various combination options complicate the selection of a tool (or multiple tools) when faced with an analysis instance, even for knowledgeable users, and hence hinders the successful adoption of taint analyses.  In this work, we thus present CoDiDroid, a framework for cooperative Android app analysis. CoDiDroid (1) allows users to ask questions about flows in apps in varying degrees of detail, (2) automatically generates subtasks for answering such questions, (3) distributes tasks onto analysis tools (currently DroidRA, FlowDroid, HornDroid, IC3 and two novel tools) and (4) at the end merges tool answers on subtasks into an overall answer. Thereby, users are freed from having to learn about the use and functionality of all these tools while still being able to leverage their capabilities. Moreover, we experimentally show that cooperation among tools pays off with respect to effectiveness, precision and scalability."}, {"id": "conf/sigsoft/NieRLKMG19", "title": "A framework for writing trigger-action todo comments in executable format.", "authors": ["Pengyu Nie", "Rishabh Rai", "Junyi Jessy Li", "Sarfraz Khurshid", "Raymond J. Mooney", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1145/3338906.3338965"], "tag": ["Main Research"], "abstract": "ABSTRACTNatural language elements, e.g., todo comments, are frequently used to communicate among developers and to describe tasks that need to be performed (actions) when specific conditions hold on artifacts related to the code repository (triggers), e.g., from the Apache Struts project: \u201cremove expectedJDK15 and if() after switching to Java 1.6\u201d. As projects evolve, development processes change, and development teams reorganize, these comments, because of their informal nature, frequently become irrelevant or forgotten. We present the first framework, dubbed TrigIt, to specify trigger-action todo comments in executable format. Thus, actions are executed automatically when triggers evaluate to true. TrigIt specifications are written in the host language (e.g., Java) and are evaluated as part of the build process. The triggers are specified as query statements over abstract syntax trees, abstract representation of build configuration scripts, issue tracking systems, and system clock time. The actions are either notifications to developers or code transformation steps. We implemented TrigIt for the Java programming language and migrated 44 existing trigger-action comments from several popular open-source projects. Evaluation of TrigIt, via a user study, showed that users find TrigIt easy to learn and use. TrigIt has the potential to enforce more discipline in writing and maintaining comments in large code repositories."}, {"id": "conf/sigsoft/SafwanS19", "title": "Decomposing the rationale of code commits: the software developer's perspective.", "authors": ["Khadijah Al Safwan", "Francisco Servant"], "DOIs": ["https://doi.org/10.1145/3338906.3338979"], "tag": ["Main Research"], "abstract": "ABSTRACTCommunicating the rationale behind decisions is essential for the success of software engineering projects. In particular, understanding the rationale of code commits is an important and often difficult task. We posit that part of such difficulty lies in rationale often being treated as a single piece of information. In this paper, we set to discover the breakdown of components in which developers decompose the rationale of code commits in the context of software maintenance, and to understand their experience with it and with its individual components. For this goal, we apply a mixed-methods approach, interviewing 20 software developers to ask them how they decompose rationale, and surveying an additional 24 developers to understand their experiences needing, finding, and recording those components. We found that developers decompose the rationale of code commits into 15 components, each of which is differently needed, found, and recorded. These components are: goal, need, benefits, constraints, alternatives, selected alternative, dependencies, committer, time, location, modifications, explanation of modifications, validation, maturity stage, and side effects. Our findings provide multiple implications. Educators can now disseminate the multiple dimensions and importance of the rationale of code commits. For practitioners, our decomposition of rationale defines a \"common vocabulary\" to use when discussing rationale of code commits, which we expect to strengthen the quality of their rationale sharing and documentation process. For researchers, our findings enable techniques for automatically assessing, improving, and generating rationale of code commits to specifically target the components that developers need."}, {"id": "conf/sigsoft/MollerT19", "title": "Model-based testing of breaking changes in Node.js libraries.", "authors": ["Anders M\u00f8ller", "Martin Toldam Torp"], "DOIs": ["https://doi.org/10.1145/3338906.3338940"], "tag": ["Main Research"], "abstract": "ABSTRACTSemantic versioning is widely used by library developers to indicate whether updates contain changes that may break existing clients. Especially for dynamic languages like JavaScript, using semantic versioning correctly is known to be difficult, which often causes program failures and makes client developers reluctant to switch to new library versions.  The concept of type regression testing has recently been introduced as an automated mechanism to assist the JavaScript library developers. That mechanism is effective for detecting breaking changes in widely used libraries, but it suffers from scalability limitations that make it slow and also less useful for libraries that do not have many available clients.  This paper presents a model-based variant of type regression testing. Instead of comparing API models of a library before and after an update, it finds breaking changes by automatically generating tests from a reusable API model. Experiments show that this new approach significantly improves scalability: it runs faster, and it can find breaking changes in more libraries."}, {"id": "conf/sigsoft/WinterACD19", "title": "Monitoring-aware IDEs.", "authors": ["Jos Winter", "Maur\u00edcio Finavaro Aniche", "J\u00fcrgen Cito", "Arie van Deursen"], "DOIs": ["https://doi.org/10.1145/3338906.3338926"], "tag": ["Main Research"], "abstract": "ABSTRACTEngineering modern large-scale software requires software developers to not solely focus on writing code, but also to continuously examine monitoring data to reason about the dynamic behavior of their systems. These additional monitoring responsibilities for developers have only emerged recently, in the light of DevOps culture. Interestingly, software development activities happen mainly in the IDE, while reasoning about production monitoring happens in separate monitoring tools. We propose an approach that integrates monitoring signals into the development environment and workflow. We conjecture that an IDE with such capability improves the performance of developers as time spent continuously context switching from development to monitoring would be eliminated. This paper takes a first step towards understanding the benefits of a possible monitoring-aware IDE. We implemented a prototype of a Monitoring-Aware IDE, connected to the monitoring systems of Adyen, a large-scale payment company that performs intense monitoring in their software systems. Given our results, we firmly believe that monitoring-aware IDEs can play an essential role in improving how developers perform monitoring."}, {"id": "conf/sigsoft/BagherzadehK19", "title": "Going big: a large-scale study on what big data developers ask.", "authors": ["Mehdi Bagherzadeh", "Raffi Khatchadourian"], "DOIs": ["https://doi.org/10.1145/3338906.3338939"], "tag": ["Main Research"], "abstract": "ABSTRACTSoftware developers are increasingly required to write big data code. However, they find big data software development challenging. To help these developers it is necessary to understand big data topics that they are interested in and the difficulty of finding answers for questions in these topics. In this work, we conduct a large-scale study on Stackoverflow to understand the interest and difficulties of big data developers. To conduct the study, we develop a set of big data tags to extract big data posts from Stackoverflow; use topic modeling to group these posts into big data topics; group similar topics into categories to construct a topic hierarchy; analyze popularity and difficulty of topics and their correlations; and discuss implications of our findings for practice, research and education of big data software development and investigate their coincidence with the findings of previous work."}, {"id": "conf/sigsoft/DavisMCSL19", "title": "Why aren't regular expressions a lingua franca? an empirical study on the re-use and portability of regular expressions.", "authors": ["James C. Davis", "Louis G. Michael IV", "Christy A. Coghlan", "Francisco Servant", "Dongyoon Lee"], "DOIs": ["https://doi.org/10.1145/3338906.3338909"], "tag": ["Main Research"], "abstract": "ABSTRACTThis paper explores the extent to which regular expressions (regexes) are portable across programming languages. Many languages offer similar regex syntaxes, and it would be natural to assume that regexes can be ported across language boundaries. But can regexes be copy/pasted across language boundaries while retaining their semantic and performance characteristics?  In our survey of 158 professional software developers, most indicated that they re-use regexes across language boundaries and about half reported that they believe regexes are a universal language.We experimentally evaluated the riskiness of this practice using a novel regex corpus \u2014 537,806 regexes from 193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and Rust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex portability problems: logic errors due to semantic differences, and security vulnerabilities due to performance differences.  We report that developers\u2019 belief in a regex lingua franca is understandable but unfounded. Though most regexes compile across language boundaries, 15% exhibit semantic differences across languages and 10% exhibit performance differences across languages. We explained these differences using regex documentation, and further illuminate our findings by investigating regex engine implementations. Along the way we found bugs in the regex engines of JavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance regex bugs in thousands of modules."}, {"id": "conf/sigsoft/NielsenHG19", "title": "Nodest: feedback-driven static analysis of Node.js applications.", "authors": ["Benjamin Barslev Nielsen", "Behnaz Hassanshahi", "Fran\u00e7ois Gauthier"], "DOIs": ["https://doi.org/10.1145/3338906.3338933"], "tag": ["Main Research"], "abstract": "ABSTRACTNode.js provides the ability to write JavaScript programs for the server-side and has become a popular language for developing web applications. Node.js allows direct access to the underlying filesystem, operating system resources, and databases, but does not provide any security mechanism such as sandboxing of untrusted code, and injection vulnerabilities are now commonly reported in Node.js modules. Existing static dataflow analysis techniques do not scale to Node.js applications to find injection vulnerabilities because small Node.js web applications typically depend on many third-party modules. We present a new feedback-driven static analysis that scales well to detect injection vulnerabilities in Node.js applications. The key idea behind our new technique is that not all third-party modules need to be analyzed to detect an injection vulnerability. Results of running our analysis, Nodest, on real-world Node.js applications show that the technique scales to large applications and finds previously known as well as new vulnerabilities. In particular, Nodest finds 63 true positive taint flows in a set of our benchmarks, whereas a state-of-the-art static analysis reports 3 only. Moreover, our analysis scales to Express, the most popular Node.js web framework, and reports non-trivial injection vulnerabilities."}, {"id": "conf/sigsoft/DeFreezBRT19", "title": "Effective error-specification inference via domain-knowledge expansion.", "authors": ["Daniel DeFreez", "Haaken Martinson Baldwin", "Cindy Rubio-Gonz\u00e1lez", "Aditya V. Thakur"], "DOIs": ["https://doi.org/10.1145/3338906.3338960"], "tag": ["Main Research"], "abstract": "ABSTRACTError-handling code responds to the occurrence of runtime errors. Failure to correctly handle errors can lead to security vulnerabilities and data loss. This paper deals with error handling in software written in C that uses the return-code idiom: the presence and type of error is encoded in the return value of a function. This paper describes EESI, a static analysis that infers the set of values that a function can return on error. Such a function error-specification can then be used to identify bugs related to incorrect error handling. The key insight of EESI is to bootstrap the analysis with domain knowledge related to error handling provided by a developer. EESI uses a combination of intraprocedural, flow-sensitive analysis and interprocedural, context-insensitive analysis to ensure precision and scalability. We built a tool ECC to demonstrate how the function error-specifications inferred by EESI can be used to automatically find bugs related to incorrect error handling. ECC detected 246 bugs across 9 programs, of which 110 have been confirmed. ECC detected 220 previously unknown bugs, of which 99 are confirmed. Two patches have already been merged into OpenSSL."}, {"id": "conf/sigsoft/DuXLM0Z19", "title": "DeepStellar: model-based quantitative analysis of stateful deep learning systems.", "authors": ["Xiaoning Du", "Xiaofei Xie", "Yi Li", "Lei Ma", "Yang Liu", "Jianjun Zhao"], "DOIs": ["https://doi.org/10.1145/3338906.3338954"], "tag": ["Main Research"], "abstract": "ABSTRACTDeep Learning (DL) has achieved tremendous success in many cutting-edge applications. However, the state-of-the-art DL systems still suffer from quality issues. While some recent progress has been made on the analysis of feed-forward DL systems, little study has been done on the Recurrent Neural Network (RNN)-based stateful DL systems, which are widely used in audio, natural languages and video processing, etc. In this paper, we initiate the very first step towards the quantitative analysis of RNN-based DL systems. We model RNN as an abstract state transition system to characterize its internal behaviors. Based on the abstract model, we design two trace similarity metrics and five coverage criteria which enable the quantitative analysis of RNNs. We further propose two algorithms powered by the quantitative measures for adversarial sample detection and coverage-guided test generation. We evaluate DeepStellar on four RNN-based systems covering image classification and automated speech recognition. The results demonstrate that the abstract model is useful in capturing the internal behaviors of RNNs, and confirm that (1) the similarity metrics could effectively capture the differences between samples even with very small perturbations (achieving 97% accuracy for detecting adversarial samples) and (2) the coverage criteria are useful in revealing erroneous behaviors (generating three times more adversarial samples than random testing and hundreds times more than the unrolling approach)."}, {"id": "conf/sigsoft/WuJYBSPX19", "title": "REINAM: reinforcement learning for input-grammar inference.", "authors": ["Zhengkai Wu", "Evan Johnson", "Wei Yang", "Osbert Bastani", "Dawn Song", "Jian Peng", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3338906.3338958"], "tag": ["Main Research"], "abstract": "ABSTRACTProgram input grammars (i.e., grammars encoding the language of valid program inputs) facilitate a wide range of applications in software engineering such as symbolic execution and delta debugging. Grammars synthesized by existing approaches can cover only a small part of the valid input space mainly due to unanalyzable code (e.g., native code) in programs and lacking high-quality and high-variety seed inputs. To address these challenges, we present REINAM, a reinforcement-learning approach for synthesizing probabilistic context-free program input grammars without any seed inputs. REINAM uses an industrial symbolic execution engine to generate an initial set of inputs for the given target program, and then uses an iterative process of grammar generalization to proactively generate additional inputs to infer grammars generalized from these initial seed inputs. To efficiently search for target generalizations in a huge search space of candidate generalization operators, REINAM includes a novel formulation of the search problem as a reinforcement learning problem. Our evaluation on eleven real-world benchmarks shows that REINAM outperforms an existing state-of-the-art approach on precision and recall of synthesized grammars, and fuzz testing based on REINAM substantially increases the coverage of the space of valid inputs. REINAM is able to synthesize a grammar covering the entire valid input space for some benchmarks without decreasing the accuracy of the grammar."}, {"id": "conf/sigsoft/LiM0CX019", "title": "Boosting operational DNN testing efficiency through conditioning.", "authors": ["Zenan Li", "Xiaoxing Ma", "Chang Xu", "Chun Cao", "Jingwei Xu", "Jian L\u00fc"], "DOIs": ["https://doi.org/10.1145/3338906.3338930"], "tag": ["Main Research"], "abstract": "ABSTRACTWith the increasing adoption of Deep Neural Network (DNN) models as integral parts of software systems, efficient operational testing of DNNs is much in demand to ensure these models' actual performance in field conditions. A challenge is that the testing often needs to produce precise results with a very limited budget for labeling data collected in field.  Viewing software testing as a practice of reliability estimation through statistical sampling, we re-interpret the idea behind conventional structural coverages as conditioning for variance reduction. With this insight we propose an efficient DNN testing method based on the conditioning on the representation learned by the DNN model under testing. The representation is defined by the probability distribution of the output of neurons in the last hidden layer of the model. To sample from this high dimensional distribution in which the operational data are sparsely distributed, we design an algorithm leveraging cross entropy minimization.  Experiments with various DNN models and datasets were conducted to evaluate the general efficiency of the approach. The results show that, compared with simple random sampling, this approach requires only about a half of labeled inputs to achieve the same level of precision."}, {"id": "conf/sigsoft/IslamNPR19", "title": "A comprehensive study on deep learning bug characteristics.", "authors": ["Md Johirul Islam", "Giang Nguyen", "Rangeet Pan", "Hridesh Rajan"], "DOIs": ["https://doi.org/10.1145/3338906.3338955"], "tag": ["Main Research"], "abstract": "ABSTRACTDeep learning has gained substantial popularity in recent years. Developers mainly rely on libraries and tools to add deep learning capabilities to their software. What kinds of bugs are frequently found in such software? What are the root causes of such bugs? What impacts do such bugs have? Which stages of deep learning pipeline are more bug prone? Are there any antipatterns? Understanding such characteristics of bugs in deep learning software has the potential to foster the development of better deep learning platforms, debugging mechanisms, development practices, and encourage the development of analysis and verification frameworks. Therefore, we study 2716 high-quality posts from Stack Overflow and 500 bug fix commits from Github about five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand the types of bugs, root causes of bugs, impacts of bugs, bug-prone stage of deep learning pipeline as well as whether there are some common antipatterns found in this buggy software. The key findings of our study include: data bug and logic bug are the most severe bug types in deep learning software appearing more than 48% of the times, major root causes of these bugs are Incorrect Model Parameter (IPS) and Structural Inefficiency (SI) showing up more than 43% of the times.We have also found that the bugs in the usage of deep learning libraries have some common antipatterns."}, {"id": "conf/sigsoft/LiewCDS19", "title": "Just fuzz it: solving floating-point constraints using coverage-guided fuzzing.", "authors": ["Daniel Liew", "Cristian Cadar", "Alastair F. Donaldson", "J. Ryan Stinnett"], "DOIs": ["https://doi.org/10.1145/3338906.3338921"], "tag": ["Main Research"], "abstract": "ABSTRACTWe investigate the use of coverage-guided fuzzing as a means of proving satisfiability of SMT formulas over finite variable domains, with specific application to floating-point constraints. We show how an SMT formula can be encoded as a program containing a location that is reachable if and only if the program\u2019s input corresponds to a satisfying assignment to the formula. A coverage-guided fuzzer can then be used to search for an input that reaches the location, yielding a satisfying assignment. We have implemented this idea in a tool, Just Fuzz-it Solver (JFS), and we present a large experimental evaluation showing that JFS is both competitive with and complementary to state-of-the-art SMT solvers with respect to solving floating-point constraints, and that the coverage-guided approach of JFS provides significant benefit over naive fuzzing in the floating-point domain. Applied in a portfolio manner, the JFS approach thus has the potential to complement traditional SMT solvers for program analysis tasks that involve reasoning about floating-point constraints."}, {"id": "conf/sigsoft/LiXCWZXWL19", "title": "Cerebro: context-aware adaptive fuzzing for effective vulnerability detection.", "authors": ["Yuekang Li", "Yinxing Xue", "Hongxu Chen", "Xiuheng Wu", "Cen Zhang", "Xiaofei Xie", "Haijun Wang", "Yang Liu"], "DOIs": ["https://doi.org/10.1145/3338906.3338975"], "tag": ["Main Research"], "abstract": "ABSTRACTExisting greybox fuzzers mainly utilize program coverage as the goal to guide the fuzzing process. To maximize their outputs, coverage-based greybox fuzzers need to evaluate the quality of seeds properly, which involves making two decisions: 1) which is the most promising seed to fuzz next (seed prioritization), and 2) how many efforts should be made to the current seed (power scheduling). In this paper, we present our fuzzer, Cerebro, to address the above challenges. For the seed prioritization problem, we propose an online multi-objective based algorithm to balance various metrics such as code complexity, coverage, execution time, etc. To address the power scheduling problem, we introduce the concept of input potential to measure the complexity of uncovered code and propose a cost-effective algorithm to update it dynamically. Unlike previous approaches where the fuzzer evaluates an input solely based on the execution traces that it has covered, Cerebro is able to foresee the benefits of fuzzing the input by adaptively evaluating its input potential. We perform a thorough evaluation for Cerebro on 8 different real-world programs. The experiments show that Cerebro can find more vulnerabilities and achieve better coverage than state-of-the-art fuzzers such as AFL and AFLFast."}, {"id": "conf/sigsoft/ShiLO0M19", "title": "iFixFlakies: a framework for automatically fixing order-dependent flaky tests.", "authors": ["August Shi", "Wing Lam", "Reed Oei", "Tao Xie", "Darko Marinov"], "DOIs": ["https://doi.org/10.1145/3338906.3338925"], "tag": ["Main Research"], "abstract": "ABSTRACTRegression testing provides important pass or fail signals that developers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming.  We propose iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. Our evaluation on 110 truly orderdependent tests from a public dataset shows that 58 of them have helpers, and iFixFlakies can fix all 58. We opened pull requests for 56 order-dependent tests (2 of 58 were already fixed), and developers have already accepted pull requests for 21 of them, with all the remaining ones still pending."}, {"id": "conf/sigsoft/KalhaugeP19", "title": "Binary reduction of dependency graphs.", "authors": ["Christian Gram Kalhauge", "Jens Palsberg"], "DOIs": ["https://doi.org/10.1145/3338906.3338956"], "tag": ["Main Research"], "abstract": "ABSTRACTDelta debugging is a technique for reducing a failure-inducing input to a small input that reveals the cause of the failure. This has been successful for a wide variety of inputs including C programs, XML data, and thread schedules. However, for input that has many internal dependencies, delta debugging scales poorly. Such input includes C#, Java, and Java bytecode and they have presented a major challenge for input reduction until now. In this paper, we show that the core challenge is a reduction problem for dependency graphs, and we present a general strategy for reducing such graphs. We combine this with a novel algorithm for reduction called Binary Reduction in a tool called J-Reduce for Java bytecode. Our experiments show that our tool is 12x faster and achieves more reduction than delta debugging on average. This enabled us to create and submit short bug reports for three Java bytecode decompilers."}, {"id": "conf/sigsoft/PobeeC19", "title": "AggrePlay: efficient record and replay of multi-threaded programs.", "authors": ["Ernest Bota Pobee", "Wing Kwong Chan"], "DOIs": ["https://doi.org/10.1145/3338906.3338959"], "tag": ["Main Research"], "abstract": "ABSTRACTDeterministic replay presents challenges and often results in high memory and runtime overheads. Previous studies deterministically reproduce program outputs often only after several replay iterations or may produce a non-deterministic sequence of output to external sources. In this paper, we propose AggrePlay, a deterministic replay technique which is based on recording read-write interleavings leveraging thread-local determinism and summarized read values. During the record phase, AggrePlay records a read count vector clock for each thread on each memory location. Each thread checks the logged vector clock against the current read count in the replay phase before a write event. We present an experiment and analyze the results using the Splash2x benchmark suite as well as two real-world applications. The experimental results show that on average, AggrePlay experiences a better reduction in compressed log size, and 56% better runtime slowdown during the record phase, as well as a 41.58% higher probability in the replay phase than existing work."}, {"id": "conf/sigsoft/HiraoMIM19", "title": "The review linkage graph for code review analytics: a recovery approach and empirical study.", "authors": ["Toshiki Hirao", "Shane McIntosh", "Akinori Ihara", "Kenichi Matsumoto"], "DOIs": ["https://doi.org/10.1145/3338906.3338949"], "tag": ["Main Research"], "abstract": "ABSTRACTModern Code Review (MCR) is a pillar of contemporary quality assurance approaches, where developers discuss and improve code changes prior to integration. Since review interactions (e.g., comments, revisions) are archived, analytics approaches like reviewer recommendation and review outcome prediction have been proposed to support the MCR process. These approaches assume that reviews evolve and are adjudicated independently; yet in practice, reviews can be interdependent.  In this paper, we set out to better understand the impact of review linkage on code review analytics. To do so, we extract review linkage graphs where nodes represent reviews, while edges represent recovered links between reviews. Through a quantitative analysis of six software communities, we observe that (a) linked reviews occur regularly, with linked review rates of 25% in OpenStack, 17% in Chromium, and 3%\u20138% in Android, Qt, Eclipse, and Libreoffice; and (b) linkage has become more prevalent over time. Through qualitative analysis, we discover that links span 16 types that belong to five categories. To automate link category recovery, we train classifiers to label links according to the surrounding document content. Those classifiers achieve F1-scores of 0.71\u20130.79, at least doubling the F1-scores of a ZeroR baseline. Finally, we show that the F1-scores of reviewer recommenders can be improved by 37%\u201388% (5\u201314 percentage points) by incorporating information from linked reviews that is available at prediction time. Indeed, review linkage should be exploited by future code review analytics."}, {"id": "conf/sigsoft/WangSW19", "title": "Mitigating power side channels during compilation.", "authors": ["Jingbo Wang", "Chungha Sung", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3338906.3338913"], "tag": ["Main Research"], "abstract": "ABSTRACTThe code generation modules inside modern compilers, which use a limited number of CPU registers to store a large number of program variables, may introduce side-channel leaks even in software equipped with state-of-the-art countermeasures. We propose a program analysis and transformation based method to eliminate such leaks. Our method has a type-based technique for detecting leaks, which leverages Datalog-based declarative analysis and domain-specific optimizations to achieve high efficiency and accuracy. It also has a mitigation technique for the compiler's backend, more specifically the register allocation modules, to ensure that leaky intermediate computation results are stored in different CPU registers or memory locations. We have implemented and evaluated our method in LLVM for the x86 instruction set architecture. Our experiments on cryptographic software show that the method is effective in removing the side channel while being efficient, i.e., our mitigated code is more compact and runs faster than code mitigated using state-of-the-art techniques."}, {"id": "conf/sigsoft/ChenMF19", "title": "Maximal multi-layer specification synthesis.", "authors": ["Yanju Chen", "Ruben Martins", "Yu Feng"], "DOIs": ["https://doi.org/10.1145/3338906.3338951"], "tag": ["Main Research"], "abstract": "ABSTRACTThere has been a significant interest in applying programming-by-example to automate repetitive and tedious tasks. However, due to the incomplete nature of input-output examples, a synthesizer may generate programs that pass the examples but do not match the user intent. In this paper, we propose MARS, a novel synthesis framework that takes as input a multi-layer specification composed by input-output examples, textual description, and partial code snippets that capture the user intent. To accurately capture the user intent from the noisy and ambiguous description, we propose a hybrid model that combines the power of an LSTM-based sequence-to-sequence model with the apriori algorithm for mining association rules through unsupervised learning. We reduce the problem of solving a multi-layer specification synthesis to a Max-SMT problem, where hard constraints encode well-typed concrete programs and soft constraints encode the user intent learned by the hybrid model. We instantiate our hybrid model to the data wrangling domain and compare its performance against Morpheus, a state-of-the-art synthesizer for data wrangling tasks. Our experiments demonstrate that our approach outperforms MORPHEUS in terms of running time and solved benchmarks. For challenging benchmarks, our approach can suggest candidates with rankings that are an order of magnitude better than MORPHEUS which leads to running times that are 15x faster than MORPHEUS."}, {"id": "conf/sigsoft/BavishiYP19", "title": "Phoenix: automated data-driven synthesis of repairs for static analysis violations.", "authors": ["Rohan Bavishi", "Hiroaki Yoshida", "Mukul R. Prasad"], "DOIs": ["https://doi.org/10.1145/3338906.3338952"], "tag": ["Main Research"], "abstract": "ABSTRACTTraditional automatic program repair (APR) tools rely on a test-suite as a repair specification. But test suites even when available are not of specification quality, limiting the performance and hence viability of test-suite based repair. On the other hand, static analysis-based bug finding tools are seeing increasing adoption in industry but still face challenges since the reported violations are viewed as not easily actionable. We propose a novel solution that solves both these challenges through a technique for automatically generating high-quality patches for static analysis violations by learning from examples. Our approach uses the static analyzer as an oracle and does not require a test suite. We realize our solution in a system, Phoenix, that implements a fully-automated pipeline that mines and cleans patches for static analysis violations from the wild, learns generalized executable repair strategies as programs in a novel Domain Specific Language (DSL), and then instantiates concrete repairs from them on new unseen violations. Using Phoenix we mine a corpus of 5,389 unique violations and patches from 517 Github projects. In a cross-validation study on this corpus Phoenix successfully produced 4,596 bug-fixes, with a recall of 85% and a precision of 54%. When applied to the latest revisions of a further5 Github projects, Phoenix produced 94 correct patches to previously unknown bugs, 19 of which have already been accepted and merged by the development teams. To the best of our knowledge this constitutes, by far the largest application of any automatic patch generation technology to large-scale real-world systems"}, {"id": "conf/sigsoft/AggarwalLNDS19", "title": "Black box fairness testing of machine learning models.", "authors": ["Aniya Aggarwal", "Pranay Lohia", "Seema Nagar", "Kuntal Dey", "Diptikalyan Saha"], "DOIs": ["https://doi.org/10.1145/3338906.3338937"], "tag": ["Main Research"], "abstract": "ABSTRACTAny given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine."}, {"id": "conf/sigsoft/PontesGSGR19", "title": "Java reflection API: revealing the dark side of the mirror.", "authors": ["Felipe Pontes", "Rohit Gheyi", "Sabrina Souto", "Alessandro Garcia", "M\u00e1rcio Ribeiro"], "DOIs": ["https://doi.org/10.1145/3338906.3338946"], "tag": ["Main Research"], "abstract": "ABSTRACTDevelopers of widely used Java Virtual Machines (JVMs) implement and test the Java Reflection API based on a Javadoc, which is specified using a natural language. However, there is limited knowledge on whether Java Reflection API developers are able to systematically reveal i) underdetermined specifications; and ii) non-conformances between their implementation and the Javadoc. Moreover, current automatic test suite generators cannot be used to detect them. To better understand the problem, we analyze test suites of two widely used JVMs, and we conduct a survey with 130 developers who use the Java Reflection API to see whether the Javadoc impacts on their understanding. We also propose a technique to detect underdetermined specifications and non-conformances between the Javadoc and the implementations of the Java Reflection API. It automatically creates test cases, and executes them using different JVMs. Then, we manually execute some steps to identify underdetermined specifications and to confirm whether a non-conformance candidate is indeed a bug. We evaluate our technique in 439 input programs. Our technique identifies underdetermined specification and non-conformance candidates in 32 Java Reflection API public methods of 7 classes. We report underdetermined specification candidates in 12 Java Reflection API methods. Java Reflection API specifiers accept 3 underdetermined specification candidates (25%). We also report 24 non-conformance candidates to Eclipse OpenJ9 JVM, and 7 to Oracle JVM. Eclipse OpenJ9 JVM developers accept and fix 21 candidates (87.5%), and Oracle JVM developers accept 5 and fix 4 non-conformance candidates."}, {"id": "conf/sigsoft/WidderHKV19", "title": "A conceptual replication of continuous integration pain points in the context of Travis CI.", "authors": ["David Gray Widder", "Michael Hilton", "Christian K\u00e4stner", "Bogdan Vasilescu"], "DOIs": ["https://doi.org/10.1145/3338906.3338922"], "tag": ["Main Research"], "abstract": "ABSTRACTContinuous integration (CI) is an established software quality assurance practice, and the focus of much prior research with a diverse range of methods and populations. In this paper, we first conduct a literature review of 37 papers on CI pain points. We then conduct a conceptual replication study on results from these papers using a triangulation design consisting of a survey with 132 responses, 12 interviews, and two logistic regressions predicting Travis CI abandonment and switching on a dataset of 6,239 GitHub projects. We report and discuss which past results we were able to replicate, those for which we found conflicting evidence, those for which we did not find evidence, and the implications of these findings."}, {"id": "conf/sigsoft/ZhangHZHB19", "title": "Ethnographic research in software engineering: a critical review and checklist.", "authors": ["He Zhang", "Xin Huang", "Xin Zhou", "Huang Huang", "Muhammad Ali Babar"], "DOIs": ["https://doi.org/10.1145/3338906.3338976"], "tag": ["Main Research"], "abstract": "ABSTRACTSoftware Engineering (SE) community has recently been investing significant amount of effort in qualitative research to study the human and social aspects of SE processes, practices, and technologies. Ethnography is one of the major qualitative research methods, which is based on constructivist paradigm that is different from the hypothetic-deductive research model usually used in SE. Hence, the adoption of ethnographic research method in SE can present significant challenges in terms of sufficient understanding of the methodological requirements and the logistics of its applications. It is important to systematically identify and understand various aspects of adopting ethnography in SE and provide effective guidance. We carried out an empirical inquiry by integrating a systematic literature review and a confirmatory survey. By reviewing the ethnographic studies reported in 111 identified papers and 26 doctoral theses and analyzing the authors' responses of 29 of those papers, we revealed several unique insights. These identified insights were then transformed into a preliminary checklist that helps improve the state-of-the-practice of using ethnography in SE. This study also identifies the areas where methodological improvements of ethnography are needed in SE."}, {"id": "conf/sigsoft/SantosSCGM19", "title": "Achilles' heel of plug-and-Play software architectures: a grounded theory based approach.", "authors": ["Joanna C. S. Santos", "Adriana Sejfia", "Taylor Corrello", "Smruthi Gadenkanahalli", "Mehdi Mirakhorli"], "DOIs": ["https://doi.org/10.1145/3338906.3338969"], "tag": ["Main Research"], "abstract": "ABSTRACTThrough a set of well-defined interfaces, plug-and-play architectures enable additional functionalities to be added or removed from a system at its runtime. However, plug-ins can also increase the application\u2019s attack surface or introduce untrusted behavior into the system. In this paper, we (1) use a grounded theory-based approach to conduct an empirical study of common vulnerabilities in plug-and-play architectures; (2) conduct a systematic literature survey and evaluate the extent that the results of the empirical study are novel or supported by the literature; (3) evaluate the practicality of the findings by interviewing practitioners with several years of experience in plug-and-play systems. By analyzing Chromium, Thunderbird, Firefox, Pidgin, WordPress, Apache OfBiz, and OpenMRS, we found a total of 303 vulnerabilities rooted in extensibility design decisions and observed that these plugin-related vulnerabilities were caused by 16 different types of vulnerabilities. Out of these 16 vulnerability types we identified 19 mitigation procedures for fixing them. The literature review supported 12 vulnerability types and 8 mitigation techniques discovered in our empirical study, and indicated that 5 mitigation techniques were not covered in our empirical study. Furthermore, it indicated that 4 vulnerability types and 11 mitigation techniques discovered in our empirical study were not covered in the literature. The interviews with practitioners confirmed the relevance of the findings and highlighted ways that the results of this empirical study can have an impact in practice."}, {"id": "conf/sigsoft/Zhou0X0JLXH19", "title": "Latent error prediction and fault localization for microservice applications by learning from system trace logs.", "authors": ["Xiang Zhou", "Xin Peng", "Tao Xie", "Jun Sun", "Chao Ji", "Dewei Liu", "Qilin Xiang", "Chuan He"], "DOIs": ["https://doi.org/10.1145/3338906.3338961"], "tag": ["Main Research"], "abstract": "ABSTRACTIn the production environment, a large part of microservice failures are related to the complex and dynamic interactions and runtime environments, such as those related to multiple instances, environmental configurations, and asynchronous interactions of microservices. Due to the complexity and dynamism of these failures, it is often hard to reproduce and diagnose them in testing environments. It is desirable yet still challenging that these failures can be detected and the faults can be located at runtime of the production environment to allow developers to resolve them efficiently. To address this challenge, in this paper, we propose MEPFL, an approach of latent error prediction and fault localization for microservice applications by learning from system trace logs. Based on a set of features defined on the system trace logs, MEPFL trains prediction models at both the trace level and the microservice level using the system trace logs collected from automatic executions of the target application and its faulty versions produced by fault injection. The prediction models thus can be used in the production environment to predict latent errors, faulty microservices, and fault types for trace instances captured at runtime. We implement MEPFL based on the infrastructure systems of container orchestrator and service mesh, and conduct a series of experimental studies with two opensource microservice applications (one of them being the largest open-source microservice application to our best knowledge). The results indicate that MEPFL can achieve high accuracy in intraapplication prediction of latent errors, faulty microservices, and fault types, and outperforms a state-of-the-art approach of failure diagnosis for distributed systems. The results also show that MEPFL can effectively predict latent errors caused by real-world fault cases."}, {"id": "conf/sigsoft/JimenezRPSTH19", "title": "The importance of accounting for real-world labelling when predicting software vulnerabilities.", "authors": ["Matthieu Jimenez", "Renaud Rwemalika", "Mike Papadakis", "Federica Sarro", "Yves Le Traon", "Mark Harman"], "DOIs": ["https://doi.org/10.1145/3338906.3338941"], "tag": ["Main Research"], "abstract": "ABSTRACTPrevious work on vulnerability prediction assume that predictive models are trained with respect to perfect labelling information (includes labels from future, as yet undiscovered vulnerabilities). In this paper we present results from a comprehensive empirical study of 1,898 real-world vulnerabilities reported in 74 releases of three security-critical open source systems (Linux Kernel, OpenSSL and Wiresark). Our study investigates the effectiveness of three previously proposed vulnerability prediction approaches, in two settings: with and without the unrealistic labelling assumption. The results reveal that the unrealistic labelling assumption can profoundly mis- lead the scientific conclusions drawn; suggesting highly effective and deployable prediction results vanish when we fully account for realistically available labelling in the experimental methodology. More precisely, MCC mean values of predictive effectiveness drop from 0.77, 0.65 and 0.43 to 0.08, 0.22, 0.10 for Linux Kernel, OpenSSL and Wiresark, respectively. Similar results are also obtained for precision, recall and other assessments of predictive efficacy. The community therefore needs to upgrade experimental and empirical methodology for vulnerability prediction evaluation and development to ensure robust and actionable scientific findings."}, {"id": "conf/sigsoft/0001ZMYHSL19", "title": "Detecting concurrency memory corruption vulnerabilities.", "authors": ["Yan Cai", "Biyun Zhu", "Ruijie Meng", "Hao Yun", "Liang He", "Purui Su", "Bin Liang"], "DOIs": ["https://doi.org/10.1145/3338906.3338927"], "tag": ["Main Research"], "abstract": "ABSTRACTMemory corruption vulnerabilities can occur in multithreaded executions, known as concurrency vulnerabilities in this paper. Due to non-deterministic multithreaded executions, they are extremely difficult to detect. Recently, researchers tried to apply data race detectors to detect concurrency vulnerabilities. Unfortunately, these detectors are ineffective on detecting concurrency vulnerabilities. For example, most (90%) of data races are benign. However, concurrency vulnerabilities are harmful and can usually be exploited to launch attacks. Techniques based on maximal causal model rely on constraints solvers to predict scheduling; they can miss concurrency vulnerabilities in practice. Our insight is, a concurrency vulnerability is more related to the orders of events that can be reversed in different executions, no matter whether the corresponding accesses can form data races. We then define exchangeable events to identify pairs of events such that their execution orders can be probably reversed in different executions. We further propose algorithms to detect three major kinds of concurrency vulnerabilities. To overcome potential imprecision of exchangeable events, we also adopt a validation to isolate real vulnerabilities. We implemented our algorithms as a tool ConVul and applied it on 10 known concurrency vulnerabilities and the MySQL database server. Compared with three widely-used race detectors and one detector based on maximal causal model, ConVul was significantly more effective by detecting 9 of 10 known vulnerabilities and 6 zero-day vulnerabilities on MySQL (four have been confirmed). However, other detectors only detected at most 3 out of the 16 known and zero-day vulnerabilities."}, {"id": "conf/sigsoft/WangXLLLQLL19", "title": "Locating vulnerabilities in binaries via memory layout recovering.", "authors": ["Haijun Wang", "Xiaofei Xie", "Shang-Wei Lin", "Yun Lin", "Yuekang Li", "Shengchao Qin", "Yang Liu", "Ting Liu"], "DOIs": ["https://doi.org/10.1145/3338906.3338966"], "tag": ["Main Research"], "abstract": "ABSTRACTLocating vulnerabilities is an important task for security auditing, exploit writing, and code hardening. However, it is challenging to locate vulnerabilities in binary code, because most program semantics (e.g., boundaries of an array) is missing after compilation. Without program semantics, it is difficult to determine whether a memory access exceeds its valid boundaries in binary code. In this work, we propose an approach to locate vulnerabilities based on memory layout recovery. First, we collect a set of passed executions and one failed execution. Then, for passed and failed executions, we restore their program semantics by recovering fine-grained memory layouts based on the memory addressing model. With the memory layouts recovered in passed executions as reference, we can locate vulnerabilities in failed execution by memory layout identification and comparison. Our experiments show that the proposed approach is effective to locate vulnerabilities on 24 out of 25 DARPA\u2019s CGC programs (96%), and can effectively classifies 453 program crashes (in 5 Linux programs) into 19 groups based on their root causes."}, {"id": "conf/sigsoft/0001ZHM19", "title": "Storm: program reduction for testing and debugging probabilistic programming systems.", "authors": ["Saikat Dutta", "Wenxian Zhang", "Zixin Huang", "Sasa Misailovic"], "DOIs": ["https://doi.org/10.1145/3338906.3338972"], "tag": ["Main Research"], "abstract": "ABSTRACTProbabilistic programming languages offer an intuitive way to model uncertainty by representing complex probability models as simple probabilistic programs. Probabilistic programming systems (PP systems) hide the complexity of inference algorithms away from the program developer. Unfortunately, if a failure occurs during the run of a PP system, a developer typically has very little support in finding the part of the probabilistic program that causes the failure in the system.  This paper presents Storm, a novel general framework for reducing probabilistic programs. Given a probabilistic program (with associated data and inference arguments) that causes a failure in a PP system, Storm finds a smaller version of the program, data, and arguments that cause the same failure. Storm leverages both generic code and data transformations from compiler testing and domain-specific, probabilistic transformations. The paper presents new transformations that reduce the complexity of statements and expressions, reduce data size, and simplify inference arguments (e.g., the number of iterations of the inference algorithm).  We evaluated Storm on 47 programs that caused failures in two popular probabilistic programming systems, Stan and Pyro. Our experimental results show Storm\u2019s effectiveness. For Stan, our minimized programs have 49% less code, 67% less data, and 96% fewer iterations. For Pyro, our minimized programs have 58% less code, 96% less data, and 99% fewer iterations. We also show the benefits of Storm when debugging probabilistic programs."}, {"id": "conf/sigsoft/BanerjeeCS19", "title": "NullAway: practical type-based null safety for Java.", "authors": ["Subarno Banerjee", "Lazaro Clapp", "Manu Sridharan"], "DOIs": ["https://doi.org/10.1145/3338906.3338919"], "tag": ["Main Research"], "abstract": "ABSTRACTNullPointerExceptions (NPEs) are a key source of crashes in modern Java programs. Previous work has shown how such errors can be prevented at compile time via code annotations and pluggable type checking. However, such systems have been difficult to deploy on large-scale software projects, due to significant build-time overhead and / or a high annotation burden. This paper presents NullAway, a new type-based null safety checker for Java that overcomes these issues. NullAway has been carefully engineered for low overhead, so it can run as part of every build. Further, NullAway reduces annotation burden through targeted unsound assumptions, aiming for no false negatives in practice on checked code. Our evaluation shows that NullAway has significantly lower build-time overhead (1.15\u00d7) than comparable tools (2.8-5.1\u00d7). Further, on a corpus of production crash data for widely-used Android apps built with NullAway, remaining NPEs were due to unchecked third-party libraries (64%), deliberate error suppressions (17%), or reflection and other forms of post-checking code modification (17%), never due to NullAway\u2019s unsound assumptions for checked code."}, {"id": "conf/sigsoft/JiaLYLW19", "title": "Automatically detecting missing cleanup for ungraceful exits.", "authors": ["Zhouyang Jia", "Shanshan Li", "Tingting Yu", "Xiangke Liao", "Ji Wang"], "DOIs": ["https://doi.org/10.1145/3338906.3338938"], "tag": ["Main Research"], "abstract": "ABSTRACTSoftware encounters ungraceful exits due to either bugs in the interrupt/signal handler code or the intention of developers to debug the software. Users may suffer from \u201dweird\u201d problems caused by leftovers of the ungraceful exits. A common practice to fix these problems is rebooting, which wipes away the stale state of the software. This solution, however, is heavyweight and often leads to poor user experience because it requires restarting other normal processes. In this paper, we design SafeExit, a tool that can automatically detect and pinpoint the root causes of the problems caused by ungraceful exits, which can help users fix the problems using lightweight solutions. Specifically, SafeExit checks the program exit behaviors in the case of an interrupted execution against its expected exit behaviors to detect the missing cleanup behaviors required for avoiding the ungraceful exit. The expected behaviors are obtained by monitoring the program exit under a normal execution. We apply SafeExit to 38 programs across 10 domains. SafeExit finds 133 types of cleanup behaviors from 36 programs and detects 2861 missing behaviors from 292 interrupted executions. To predict missing behaviors for unseen input scenarios, SafeExit trains prediction models using a set of sampled input scenarios. The results show that SafeExit is accurate with an average F-measure of 92.5%."}, {"id": "conf/sigsoft/ZhangSYZPS19", "title": "Finding and understanding bugs in software model checkers.", "authors": ["Chengyu Zhang", "Ting Su", "Yichen Yan", "Fuyuan Zhang", "Geguang Pu", "Zhendong Su"], "DOIs": ["https://doi.org/10.1145/3338906.3338932"], "tag": ["Main Research"], "abstract": "ABSTRACTSoftware Model Checking (SMC) is a well-known automatic program verification technique and frequently adopted for checking safety-critical software. Thus, the reliability of SMC tools themselves (i.e., software model checkers) is critical. However, little work exists on validating software model checkers, an important problem that this paper tackles by introducing a practical, automated fuzzing technique. For its simplicity and generality, we focus on control-flow reachability (e.g., whether or how many times a branch is reached) and address two specific challenges for effective fuzzing: oracle and scalability. Given a deterministic program, we (1) leverage its concrete executions to synthesize valid branch reachability properties (thus solving the oracle problem) and (2) fuse such individual properties into a single safety property (thus improving the scalability of fuzzing and reducing manual inspection). We have realized our approach as the MCFuzz tool and applied it to extensively test three state-of-the-art C software model checkers, CPAchecker, CBMC, and SeaHorn. MCFuzz has found 62 unique bugs in all three model checkers -- 58 have been confirmed, and 20 have been fixed. We have further analyzed and categorized these bugs (which are diverse), and summarized several lessons for building reliable and robust model checkers. Our testing effort has been well-appreciated by the model checker developers, and also led to improved tool usability and documentation."}, {"id": "conf/sigsoft/KapusC19", "title": "A segmented memory model for symbolic execution.", "authors": ["Timotej Kapus", "Cristian Cadar"], "DOIs": ["https://doi.org/10.1145/3338906.3338936"], "tag": ["Main Research"], "abstract": "ABSTRACTSymbolic execution is an effective technique for exploring paths in a program and reasoning about all possible values on those paths. However, the technique still struggles with code that uses complex heap data structures, in which a pointer is allowed to refer to more than one memory object. In such cases, symbolic execution typically forks execution into multiple states, one for each object to which the pointer could refer.  In this paper, we propose a technique that avoids this expensive forking by using a segmented memory model. In this model, memory is split into segments, so that each symbolic pointer refers to objects in a single segment. The size of the segments are bound by a threshold, in order to avoid expensive constraints. This results in a memory model where forking due to symbolic pointer dereferences is significantly reduced, often completely.  We evaluate our segmented memory model on a mix of whole program benchmarks (such as m4 and make) and library benchmarks (such as SQLite), and observe significant decreases in execution time and memory usage."}, {"id": "conf/sigsoft/KulaRHDG19", "title": "Releasing fast and slow: an exploratory case study at ING.", "authors": ["Elvan Kula", "Ayushi Rastogi", "Hennie Huijgens", "Arie van Deursen", "Georgios Gousios"], "DOIs": ["https://doi.org/10.1145/3338906.3338978"], "tag": ["Main Research"], "abstract": "ABSTRACTThe appeal of delivering new features faster has led many software projects to adopt rapid releases. However, it is not well understood what the effects of this practice are. This paper presents an exploratory case study of rapid releases at ING, a large banking company that develops software solutions in-house, to characterize rapid releases. Since 2011, ING has shifted to a rapid release model. This switch has resulted in a mixed environment of 611 teams releasing relatively fast and slow. We followed a mixed-methods approach in which we conducted a survey with 461 participants and corroborated their perceptions with 2 years of code quality data and 1 year of release delay data. Our research shows that: rapid releases are more commonly delayed than their non-rapid counterparts, however, rapid releases have shorter delays; rapid releases can be beneficial in terms of reviewing and user-perceived quality; rapidly released software tends to have a higher code churn, a higher test coverage and a lower average complexity; challenges in rapid releases are related to managing dependencies and certain code aspects, e.g., design debt."}, {"id": "conf/sigsoft/BuiYJ19", "title": "SAR: learning cross-language API mappings with little knowledge.", "authors": ["Nghi D. Q. Bui", "Yijun Yu", "Lingxiao Jiang"], "DOIs": ["https://doi.org/10.1145/3338906.3338924"], "tag": ["Main Research"], "abstract": "ABSTRACTTo save effort, developers often translate programs from one programming language to another, instead of implementing it from scratch. Translating application program interfaces (APIs) used in one language to functionally equivalent ones available in another language is an important aspect of program translation. Existing approaches facilitate the translation by automatically identifying the API mappings across programming languages. However, these approaches still require large amount of parallel corpora, ranging from pairs of APIs or code fragments that are functionally equivalent, to similar code comments.  To minimize the need of parallel corpora, this paper aims at an automated approach that can map APIs across languages with much less a priori knowledge than other approaches. The approach is based on an realization of the notion of domain adaption, combined with code embedding, to better align two vector spaces. Taking as input large sets of programs, our approach first generates numeric vector representations of the programs (including the APIs used in each language), and it adapts generative adversarial networks (GAN) to align the vectors in different spaces of two languages. For a better alignment, we initialize the GAN with parameters derived from API mapping seeds that can be identified accurately with a simple automatic signature-based matching heuristic. Then the cross language API mappings can be identified via nearest-neighbors queries in the aligned vector spaces. We have implemented the approach (SAR, named after three main technical components in the approach) in a prototype for mapping APIs across Java and C# programs. Our evaluation on about 2 million Java files and 1 million C# files shows that the approach can achieve 54% and 82% mapping accuracy in its top-1 and top-10 API mapping results with only 174 automatically identified seeds, more accurate than other approaches using the same or much more mapping seeds."}, {"id": "conf/sigsoft/ZhangXLQZDXYCLC19", "title": "Robust log-based anomaly detection on unstable log data.", "authors": ["Xu Zhang", "Yong Xu", "Qingwei Lin", "Bo Qiao", "Hongyu Zhang", "Yingnong Dang", "Chunyu Xie", "Xinsheng Yang", "Qian Cheng", "Ze Li", "Junjie Chen", "Xiaoting He", "Randolph Yao", "Jian-Guang Lou", "Murali Chintalapati", "Furao Shen", "Dongmei Zhang"], "DOIs": ["https://doi.org/10.1145/3338906.3338931"], "tag": ["Main Research"], "abstract": "ABSTRACTLogs are widely used by large and complex software-intensive systems for troubleshooting. There have been a lot of studies on log-based anomaly detection. To detect the anomalies, the existing methods mainly construct a detection model using log event data extracted from historical logs. However, we find that the existing methods do not work well in practice. These methods have the close-world assumption, which assumes that the log data is stable over time and the set of distinct log events is known. However, our empirical study shows that in practice, log data often contains previously unseen log events or log sequences. The instability of log data comes from two sources: 1) the evolution of logging statements, and 2) the processing noise in log data. In this paper, we propose a new log-based anomaly detection approach, called LogRobust. LogRobust extracts semantic information of log events and represents them as semantic vectors. It then detects anomalies by utilizing an attention-based Bi-LSTM model, which has the ability to capture the contextual information in the log sequences and automatically learn the importance of different log events. In this way, LogRobust is able to identify and handle unstable log events and sequences. We have evaluated LogRobust using logs collected from the Hadoop system and an actual online service system of Microsoft. The experimental results show that the proposed approach can well address the problem of log instability and achieve accurate and robust results on real-world, ever-changing log data."}, {"id": "conf/sigsoft/SuWC019", "title": "Pinpointing performance inefficiencies in Java.", "authors": ["Pengfei Su", "Qingsen Wang", "Milind Chabbi", "Xu Liu"], "DOIs": ["https://doi.org/10.1145/3338906.3338923"], "tag": ["Main Research"], "abstract": "ABSTRACTMany performance inefficiencies such as inappropriate choice of algorithms or data structures, developers' inattention to performance, and missed compiler optimizations show up as wasteful memory operations. Wasteful memory operations are those that produce/consume data to/from memory that may have been avoided. We present, JXPerf, a lightweight performance analysis tool for pinpointing wasteful memory operations in Java programs. Traditional byte code instrumentation for such analysis (1) introduces prohibitive overheads and (2) misses inefficiencies in machine code generation. JXPerf overcomes both of these problems. JXPerf uses hardware performance monitoring units to sample memory locations accessed by a program and uses hardware debug registers to monitor subsequent accesses to the same memory. The result is a lightweight measurement at the machine code level with attribution of inefficiencies to their provenance --- machine and source code within full calling contexts. JXPerf introduces only 7% runtime overhead and 7% memory overhead making it useful in production. Guided by JXPerf, we optimize several Java applications by improving code generation and choosing superior data structures and algorithms, which yield significant speedups."}, {"id": "conf/sigsoft/EckPCB19", "title": "Understanding flaky tests: the developer's perspective.", "authors": ["Moritz Eck", "Fabio Palomba", "Marco Castelluccio", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1145/3338906.3338945"], "tag": ["Main Research"], "abstract": "ABSTRACTFlaky tests are software tests that exhibit a seemingly random outcome (pass or fail) despite exercising unchanged code. In this work, we examine the perceptions of software developers about the nature, relevance, and challenges of flaky tests.  We asked 21 professional developers to classify 200 flaky tests they previously fixed, in terms of the nature and the origin of the flakiness, as well as of the fixing effort. We also examined developers' fixing strategies. Subsequently, we conducted an online survey with 121 developers with a median industrial programming experience of five years. Our research shows that: The flakiness is due to several different causes, four of which have never been reported before, despite being the most costly to fix; flakiness is perceived as significant by the vast majority of developers, regardless of their team's size and project's domain, and it can have effects on resource allocation, scheduling, and the perceived reliability of the test suite; and the challenges developers report to face regard mostly the reproduction of the flaky behavior and the identification of the cause for the flakiness. Public preprint [http://arxiv.org/abs/1907.01466], data and materials [https://doi.org/10.5281/zenodo.3265785]."}, {"id": "conf/sigsoft/ChenCLML19", "title": "SEntiMoji: an emoji-powered learning approach for sentiment analysis in software engineering.", "authors": ["Zhenpeng Chen", "Yanbin Cao", "Xuan Lu", "Qiaozhu Mei", "Xuanzhe Liu"], "DOIs": ["https://doi.org/10.1145/3338906.3338977"], "tag": ["Main Research"], "abstract": "ABSTRACTSentiment analysis has various application scenarios in software engineering (SE), such as detecting developers' emotions in commit messages and identifying their opinions on Q&A forums. However, commonly used out-of-the-box sentiment analysis tools cannot obtain reliable results on SE tasks and the misunderstanding of technical jargon is demonstrated to be the main reason. Then, researchers have to utilize labeled SE-related texts to customize sentiment analysis for SE tasks via a variety of algorithms. However, the scarce labeled data can cover only very limited expressions and thus cannot guarantee the analysis quality. To address such a problem, we turn to the easily available emoji usage data for help. More specifically, we employ emotional emojis as noisy labels of sentiments and propose a representation learning approach that uses both Tweets and GitHub posts containing emojis to learn sentiment-aware representations for SE-related texts. These emoji-labeled posts can not only supply the technical jargon, but also incorporate more general sentiment patterns shared across domains. They as well as labeled data are used to learn the final sentiment classifier. Compared to the existing sentiment analysis methods used in SE, the proposed approach can achieve significant improvement on representative benchmark datasets. By further contrast experiments, we find that the Tweets make a key contribution to the power of our approach. This finding informs future research not to unilaterally pursue the domain-specific resource, but try to transform knowledge from the open domain through ubiquitous signals such as emojis."}, {"id": "conf/sigsoft/JinWXPDQ0X19", "title": "FinExpert: domain-specific test generation for FinTech systems.", "authors": ["Tiancheng Jin", "Qingshun Wang", "Lihua Xu", "Chunmei Pan", "Liang Dou", "Haifeng Qian", "Liang He", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3338906.3340441"], "tag": ["Industry Papers"], "abstract": "ABSTRACTTo assure high quality of software systems, the comprehensiveness of the created test suite and efficiency of the adopted testing process are highly crucial, especially in the FinTech industry, due to a FinTech system\u2019s complicated system logic, mission-critical nature, and large test suite. However, the state of the testing practice in the FinTech industry still heavily relies on manual efforts. Our recent research efforts contributed our previous approach as the first attempt to automate the testing process in China Foreign Exchange Trade System (CFETS) Information Technology Co. Ltd., a subsidiary of China\u2019s Central Bank that provides China\u2019s foreign exchange transactions, and revealed that automating test generation for such complex trading platform could help alleviate some of these manual efforts. In this paper, we investigate further the dilemmas faced in testing the CFETS trading platform, identify the importance of domain knowledge in its testing process, and propose a new approach of domain-specific test generation to further improve the effectiveness and efficiency of our previous approach in industrial settings. We also present findings of our empirical studies of conducting domain-specific testing on subsystems of the CFETS Trading Platform."}, {"id": "conf/sigsoft/LohiaKSM19", "title": "Design diagrams as ontological source.", "authors": ["Pranay Lohia", "Kalapriya Kannan", "Biplav Srivastava", "Sameep Mehta"], "DOIs": ["https://doi.org/10.1145/3338906.3340446"], "tag": ["Industry Papers"], "abstract": "ABSTRACTbeginabstractIncustomsoftwaredevelopmentprojects, itisfrequentlythecasethatthesametypeofsoftwareisbeingbuiltfordifferentcustomers. Thedeliverablesaresimilarbecausetheyaddressthesamemarket (e.g., Telecom, Banking) orhavesimilarfunctionsorboth. However, mostorganisationsdonottakeadvantageofthissimilarityandconducteachprojectfromscratchleadingtolessermarginsandlowerquality. Ourkeyobservationisthatthesimilarityamongtheprojectsalludestotheexistenceofaveritabledomainofdiscoursewhoseontology, ifcreated, wouldmakethesimilarityacrosstheprojectsexplicit. Designdiagramsareanintegralpartofanycommercialsoftwareprojectdeliverablesastheydocumentcrucialfacetsofthesoftwaresolution. WeproposeanapproachtoextractontologicalinformationfromUMLdesigndiagrams (classandsequencediagrams) andrepresentitasdomainontologyinaconvenientrepresentation. Thisontologynotonlyhelpsindevelopingabetterunderstandingofthedomainbutalsofosterssoftwarereuseforfuturesoftwareprojectsinthatdomain. Initialresultsonextractingontologyfromthousandsofmodelfrompublicrepositoryshowthatthecreatedontologiesareaccurateandhelpinbettersoftwarereusefornewsolutions. endabstract"}, {"id": "conf/sigsoft/MaddilaBN19", "title": "Predicting pull request completion time: a case study on large scale cloud services.", "authors": ["Chandra Shekhar Maddila", "Chetan Bansal", "Nachiappan Nagappan"], "DOIs": ["https://doi.org/10.1145/3338906.3340457"], "tag": ["Industry Papers"], "abstract": "ABSTRACTEffort estimation models have been long studied in software engineering research. Effort estimation models help organizations and individuals plan and track progress of their software projects and individual tasks to help plan delivery milestones better. Towards this end, there is a large body of work that has been done on effort estimation for projects but little work on an individual checkin (Pull Request) level. In this paper we present a methodology that provides effort estimates on individual developer check-ins which is displayed to developers to help them track their work items. Given the cloud development infrastructure pervasive in companies, it has enabled us to deploy our Pull Request Lifetime prediction system to several thousand developers across multiple software families. We observe from our deployment that the pull request lifetime prediction system conservatively helps save 44.61% of the developer time by accelerating Pull Requests to completion."}, {"id": "conf/sigsoft/YuFMRPC19", "title": "TERMINATOR: better automated UI test case prioritization.", "authors": ["Zhe Yu", "Fahmid M. Fahid", "Tim Menzies", "Gregg Rothermel", "Kyle Patrick", "Snehit Cherian"], "DOIs": ["https://doi.org/10.1145/3338906.3340448"], "tag": ["Industry Papers"], "abstract": "ABSTRACTAutomated UI testing is an important component of the continuous integration process of software development. A modern web-based UI is an amalgam of reports from dozens of microservices written by multiple teams. Queries on a page that opens up another will fail if any of that page's microservices fails. As a result, the overall cost for automated UI testing is high since the UI elements cannot be tested in isolation. For example, the entire automated UI testing suite at LexisNexis takes around 30 hours (3-5 hours on the cloud) to execute, which slows down the continuous integration process.  To mitigate this problem and give developers faster feedback on their code, test case prioritization techniques are used to reorder the automated UI test cases so that more failures can be detected earlier. Given that much of the automated UI testing is \"black box\" in nature, very little information (only the test case descriptions and testing results) can be utilized to prioritize these automated UI test cases. Hence, this paper evaluates 17 \"black box\" test case prioritization approaches that do not rely on source code information. Among these, we propose a novel TCP approach, that dynamically re-prioritizes the test cases when new failures are detected, by applying and adapting a state of the art framework from the total recall problem. Experimental results on LexisNexis automated UI testing data show that our new approach (which we call TERMINATOR), outperformed prior state of the art approaches in terms of failure detection rates with negligible CPU overhead."}, {"id": "conf/sigsoft/0001F19", "title": "Risks and assets: a qualitative study of a software ecosystem in the mining industry.", "authors": ["Thomas Olsson", "Ulrik Franke"], "DOIs": ["https://doi.org/10.1145/3338906.3340443"], "tag": ["Industry Papers"], "abstract": "ABSTRACTDigitalization and servitization are impacting many domains, including the mining industry. As the equipment becomes connected and technical infrastructure evolves, business models and risk management need to adapt. In this paper, we present a study on how changes in asset and risk distribution are evolving for the actors in a software ecosystem (SECO) and system-of-systems (SoS) around a mining operation. We have performed a survey to understand how Service Level Agreements (SLAs) -- a common mechanism for managing risk -- are used in other domains. Furthermore, we have performed a focus group study with companies. There is an overall trend in the mining industry to move the investment cost (CAPEX) from the mining operator to the vendors. Hence, the mining operator instead leases the equipment (as operational expense, OPEX) or even acquires a service. This change in business model impacts operation, as knowledge is moved from the mining operator to the suppliers. Furthermore, as the infrastructure becomes more complex, this implies that the mining operator is more and more reliant on the suppliers for the operation and maintenance. As this change is still in an early stage, there is no formalized risk management, e.g. through SLAs, in place. Rather, at present, the companies in the ecosystem rely more on trust and the incentives created by the promise of mutual future benefits of innovation activities. We believe there is a need to better understand how to manage risk in SECO as it is established and evolves. At the same time, in a SECO, the focus is on cooperation and innovation, the companies do not have incentives to address this unless there is an incident. Therefore, industry need, we believe, help in systematically understanding risk and defining quality aspects such as reliability and performance in the new business environment."}, {"id": "conf/sigsoft/NguyenSCMBL19", "title": "Using microservices for non-intrusive customization of multi-tenant SaaS.", "authors": ["Phu H. Nguyen", "Hui Song", "Franck Chauvel", "Roy M\u00fcller", "Seref Boyar", "Erik Levin"], "DOIs": ["https://doi.org/10.1145/3338906.3340452"], "tag": ["Industry Papers"], "abstract": "ABSTRACTEnterprise software vendors often need to support their customer companies to customize the enterprise software products deployed on-premises of customers. But when software vendors are migrating their products to cloud-based Software-as-a-Service (SaaS), deep customization that used to be done on-premises is not applicable to the cloud-based multi-tenant context in which all tenants share the same SaaS. Enabling tenant-specific customization in cloud-based multi-tenant SaaS requires a novel approach. This paper proposes a Microservices-based non-intrusive Customization framework for multi-tenant Cloud-based SaaS, called MiSC-Cloud. Non-intrusive deep customization means that the microservices for customization of each tenant are isolated from the main software product and other microservices for customization of other tenants. MiSC-Cloud makes deep customization possible via authorized API calls through API gateways to the APIs of the customization microservices and the APIs of the main software product. We have implemented a proof-of-concept of our approach to enable non-intrusive deep customization of an open-source cloud native reference application of Microsoft called eShopOnContainers. Based on this work, we provide some lessons learned and directions for future work."}, {"id": "conf/sigsoft/ChenCCHCM19", "title": "Predicting breakdowns in cloud services (with SPIKE).", "authors": ["Jianfeng Chen", "Joymallya Chakraborty", "Philip Clark", "Kevin Haverlock", "Snehit Cherian", "Tim Menzies"], "DOIs": ["https://doi.org/10.1145/3338906.3340450"], "tag": ["Industry Papers"], "abstract": "ABSTRACTMaintaining web-services is a mission-critical task where any down- time means loss of revenue and reputation (of being a reliable service provider). In the current competitive web services market, such a loss of reputation causes extensive loss of future revenue.  To address this issue, we developed SPIKE, a data mining tool which can predict upcoming service breakdowns, half an hour into the future. Such predictions let an organization alert and assemble the tiger team to address the problem (e.g. by reconguring cloud hardware in order to reduce the likelihood of that breakdown).  SPIKE utilizes (a) regression tree learning (with CART); (b) synthetic minority over-sampling (to handle how rare spikes are in our data); (c) hyperparameter optimization (to learn best settings for our local data) and (d) a technique we called \u201ctopology sampling\u201d where training vectors are built from extensive details of an individual node plus summary details on all their neighbors.  In the experiments reported here, SPIKE predicted service spikes 30 minutes into future with recalls and precision of 75% and above. Also, SPIKE performed relatively better than other widely-used learning methods (neural nets, random forests, logistic regression)."}, {"id": "conf/sigsoft/0001RJGA19", "title": "DeepDelta: learning to repair compilation errors.", "authors": ["Ali Mesbah", "Andrew Rice", "Emily Johnston", "Nick Glorioso", "Edward Aftandilian"], "DOIs": ["https://doi.org/10.1145/3338906.3340455"], "tag": ["Industry Papers"], "abstract": "ABSTRACTProgrammers spend a substantial amount of time manually repairing code that does not compile. We observe that the repairs for any particular error class typically follow a pattern and are highly mechanical. We propose a novel approach that automatically learns these patterns with a deep neural network and suggests program repairs for the most costly classes of build-time compilation failures. We describe how we collect all build errors and the human-authored, in-progress code changes that cause those failing builds to transition to successful builds at Google. We generate an AST diff from the textual code changes and transform it into a domain-specific language called Delta that encodes the change that must be made to make the code compile. We then feed the compiler diagnostic information (as source) and the Delta changes that resolved the diagnostic (as target) into a Neural Machine Translation network for training. For the two most prevalent and costly classes of Java compilation errors, namely missing symbols and mismatched method signatures, our system called DeepDelta, generates the correct repair changes for 19,314 out of 38,788 (50%) of unseen compilation errors. The correct changes are in the top three suggested fixes 86% of the time on average."}, {"id": "conf/sigsoft/AsthanaKBBBMMA19", "title": "WhoDo: automating reviewer suggestions at scale.", "authors": ["Sumit Asthana", "Rahul Kumar", "Ranjita Bhagwan", "Christian Bird", "Chetan Bansal", "Chandra Shekhar Maddila", "Sonu Mehta", "B. Ashok"], "DOIs": ["https://doi.org/10.1145/3338906.3340449"], "tag": ["Industry Papers"], "abstract": "ABSTRACTToday's software development is distributed and involves continuous changes for new features and yet, their development cycle has to be fast and agile. An important component of enabling this agility is selecting the right reviewers for every code-change - the smallest unit of the development cycle. Modern tool-based code review is proven to be an effective way to achieve appropriate code review of software changes. However, the selection of reviewers in these code review systems is at best manual. As software and teams scale, this poses the challenge of selecting the right reviewers, which in turn determines software quality over time. While previous work has suggested automatic approaches to code reviewer recommendations, it has been limited to retrospective analysis. We not only deploy a reviewer suggestions algorithm - WhoDo - and evaluate its effect but also incorporate load balancing as part of it to address one of its major shortcomings: of recommending experienced developers very frequently. We evaluate the effect of this hybrid recommendation + load balancing system on five repositories within Microsoft. Our results are based around various aspects of a commit and how code review affects that. We attempt to quantitatively answer questions which are supposed to play a vital role in effective code review through our data and substantiate it through qualitative feedback of partner repositories."}, {"id": "conf/sigsoft/MiryeganehAH19", "title": "An IR-based approach towards automated integration of geo-spatial datasets in map-based software systems.", "authors": ["Nima Miryeganeh", "Mehdi Amoui", "Hadi Hemmati"], "DOIs": ["https://doi.org/10.1145/3338906.3340454"], "tag": ["Industry Papers"], "abstract": "ABSTRACTData is arguably the most valuable asset of the modern world. In this era, the success of any data-intensive solution relies on the quality of data that drives it. Among vast amount of data that are captured, managed, and analyzed everyday, geospatial data are one of the most interesting class of data that hold geographical information of real-world phenomena and can be visualized as digital maps. Geo-spatial data is the source of many enterprise solutions that provide local information and insights. Companies often aggregate geospacial datasets from various sources in order to increase the quality of such solutions. However, a lack of a global standard model for geospatial datasets makes the task of merging and integrating datasets difficult and error prone. Traditionally, this aggregation was accomplished by domain experts manually validating the data integration process by merging new data sources and/or new versions of previous data against conflicts and other requirement violations. However, this manual approach is not scalable is a hinder toward rapid release when dealing with big datasets which change frequently. Thus more automated approaches with limited interaction with domain experts is required. As a first step to tackle this problem, we have leveraged Information Retrieval (IR) and geospatial search techniques to propose a systematic and automated conflict identification approach. To evaluate our approach, we conduct a case study in which we measure the accuracy of our approach in several real-world scenarios and followed by interviews with Localintel Inc. software developers to get their feedbacks."}, {"id": "conf/sigsoft/IvankovicPJF19", "title": "Code coverage at Google.", "authors": ["Marko Ivankovic", "Goran Petrovic", "Ren\u00e9 Just", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1145/3338906.3340459"], "tag": ["Industry Papers"], "abstract": "ABSTRACTCode coverage is a measure of the degree to which a test suite exercises a software system. Although coverage is well established in software engineering research, deployment in industry is often inhibited by the perceived usefulness and the computational costs of analyzing coverage at scale. At Google, coverage information is computed for one billion lines of code daily, for seven programming languages. A key aspect of making coverage information actionable is to apply it at the level of changesets and code review. This paper describes Google\u2019s code coverage infrastructure and how the computed code coverage information is visualized and used. It also describes the challenges and solutions for adopting code coverage at scale. To study how code coverage is adopted and perceived by developers, this paper analyzes adoption rates, error rates, and average code coverage ratios over a five-year period, and it reports on 512 responses, received from surveying 3000 developers. Finally, this paper provides concrete suggestions for how to implement and use code coverage in an industrial setting."}, {"id": "conf/sigsoft/CambroneroLKS019", "title": "When deep learning met code search.", "authors": ["Jos\u00e9 Cambronero", "Hongyu Li", "Seohyun Kim", "Koushik Sen", "Satish Chandra"], "DOIs": ["https://doi.org/10.1145/3338906.3340458"], "tag": ["Industry Papers"], "abstract": "ABSTRACTThere have been multiple recent proposals on using deep neural networks for code search using natural language. Common across these proposals is the idea of embedding code and natural language queries into real vectors and then using vector distance to approximate semantic correlation between code and the query. Multiple approaches exist for learning these embeddings, including unsupervised techniques, which rely only on a corpus of code examples, and supervised techniques, which use an aligned corpus of paired code and natural language descriptions. The goal of this supervision is to produce embeddings that are more similar for a query and the corresponding desired code snippet.  Clearly, there are choices in whether to use supervised techniques at all, and if one does, what sort of network and training to use for supervision. This paper is the first to evaluate these choices systematically. To this end, we assembled implementations of state-of-the-art techniques to run on a common platform, training and evaluation corpora. To explore the design space in network complexity, we also introduced a new design point that is a minimal supervision extension to an existing unsupervised technique.  Our evaluation shows that: 1. adding supervision to an existing unsupervised technique can improve performance, though not necessarily by much; 2. simple networks for supervision can be more effective that more sophisticated sequence-based networks for code search; 3. while it is common to use docstrings to carry out supervision, there is a sizeable gap between the effectiveness of docstrings and a more query-appropriate supervision corpus."}, {"id": "conf/sigsoft/BabicBCIKKLSW19", "title": "FUDGE: fuzz driver generation at scale.", "authors": ["Domagoj Babic", "Stefan Bucur", "Yaohui Chen", "Franjo Ivancic", "Tim King", "Markus Kusano", "Caroline Lemieux", "L\u00e1szl\u00f3 Szekeres", "Wei Wang"], "DOIs": ["https://doi.org/10.1145/3338906.3340456"], "tag": ["Industry Papers"], "abstract": "ABSTRACTAt Google we have found tens of thousands of security and robustness bugs by fuzzing C and C++ libraries. To fuzz a library, a fuzzer requires a fuzz driver\u2014which exercises some library code\u2014to which it can pass inputs. Unfortunately, writing fuzz drivers remains a primarily manual exercise, a major hindrance to the widespread adoption of fuzzing. In this paper, we address this major hindrance by introducing the Fudge system for automated fuzz driver generation. Fudge automatically generates fuzz driver candidates for libraries based on existing client code. We have used Fudge to generate thousands of new drivers for a wide variety of libraries. Each generated driver includes a synthesized C/C++ program and a corresponding build script, and is automatically analyzed for quality. Developers have integrated over 200 of these generated drivers into continuous fuzzing services and have committed to address reported security bugs. Further, several of these fuzz drivers have been upstreamed to open source projects and integrated into the OSS-Fuzz fuzzing infrastructure. Running these fuzz drivers has resulted in over 150 bug fixes, including the elimination of numerous exploitable security vulnerabilities."}, {"id": "conf/sigsoft/ShiWFWSJSJS19", "title": "Industry practice of coverage-guided enterprise Linux kernel fuzzing.", "authors": ["Heyuan Shi", "Runzhe Wang", "Ying Fu", "Mingzhe Wang", "Xiaohai Shi", "Xun Jiao", "Houbing Song", "Yu Jiang", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3338906.3340460"], "tag": ["Industry Papers"], "abstract": "ABSTRACTCoverage-guided kernel fuzzing is a widely-used technique that has helped kernel developers and testers discover numerous vulnerabilities. However, due to the high complexity of application and hardware environment, there is little study on deploying fuzzing to the enterprise-level Linux kernel. In this paper, collaborating with the enterprise developers, we present the industry practice to deploy kernel fuzzing on four different enterprise Linux distributions that are responsible for internal business and external services of the company. We have addressed the following outstanding challenges when deploying a popular kernel fuzzer, syzkaller, to these enterprise Linux distributions: coverage support absence, kernel configuration inconsistency, bugs in shallow paths, and continuous fuzzing complexity. This leads to a vulnerability detection of 41 reproducible bugs which are previous unknown in these enterprise Linux kernel and 6 bugs with CVE IDs in U.S. National Vulnerability Database, including flaws that cause general protection fault, deadlock, and use-after-free."}, {"id": "conf/sigsoft/RueckertBKSMF19", "title": "Architectural decision forces at work: experiences in an industrial consultancy setting.", "authors": ["Julius Rueckert", "Andreas Burger", "Heiko Koziolek", "Thanikesavan Sivanthi", "Alexandru Moga", "Carsten Franke"], "DOIs": ["https://doi.org/10.1145/3338906.3340461"], "tag": ["Industry Papers"], "abstract": "ABSTRACTThe concepts of decision forces and the decision forces viewpoint were proposed to help software architects to make architectural decisions more transparent and the documentation of their rationales more explicit. However, practical experience reports and guidelines on how to use the viewpoint in typical industrial project setups are not available. Existing works mainly focus on basic tool support for the documentation of the viewpoint or show how forces can be used as part of focused architecture review sessions. With this paper, we share experiences and lessons learned from applying the decision forces viewpoint in a distributed industrial project setup, which involves consultants supporting architects during the re-design process of an existing large software system. Alongside our findings, we describe new forces that can serve as template for similar projects, discuss challenges applying them in a distributed consultancy project, and share ideas for potential extensions."}, {"id": "conf/sigsoft/Gamez-Diaz0RMKB19", "title": "The role of limitations and SLAs in the API industry.", "authors": ["Antonio Gamez-Diaz", "Pablo Fernandez", "Antonio Ruiz-Cort\u00e9s", "Pedro J. Molina", "Nikhil Kolekar", "Prithpal Bhogill", "Madhurranjan Mohaan", "Francisco M\u00e9ndez"], "DOIs": ["https://doi.org/10.1145/3338906.3340445"], "tag": ["Industry Papers"], "abstract": "ABSTRACTAs software architecture design is evolving to a microservice paradigm, RESTful APIs are being established as the preferred choice to build applications. In such a scenario, there is a shift towards a growing market of APIs where providers offer different service levels with tailored limitations typically based on the cost.  In this context, while there are well established standards to describe the functional elements of APIs (such as the OpenAPI Specification), having a standard model for Service Level Agreements (SLAs) for APIs may boost an open ecosystem of tools that would represent an improvement for the industry by automating certain tasks during the development such as: SLA-aware scaffolding, SLA-aware testing, or SLA-aware requesters.  Unfortunately, despite there have been several proposals to describe SLAs for software in general and web services in particular during the past decades, there is an actual lack of a widely used standard due to the complex landscape of concepts surrounding the notion of SLAs and the multiple perspectives that can be addressed.  In this paper, we aim to analyze the landscape for SLAs for APIs in two different directions: i) Clarifying the SLA-driven API development lifecycle: its activities and participants; 2) Developing a catalog of relevant concepts and an ulterior prioritization based on different perspectives from both Industry and Academia.  As a main result, we present a scored list of concepts that paves the way to establish a concrete road-map for a standard industry-aligned specification to describe SLAs in APIs."}, {"id": "conf/sigsoft/NejatiGMBFW19", "title": "Evaluating model testing and model checking for finding requirements violations in Simulink models.", "authors": ["Shiva Nejati", "Khouloud Gaaloul", "Claudio Menghi", "Lionel C. Briand", "Stephen Foster", "David Wolfe"], "DOIs": ["https://doi.org/10.1145/3338906.3340444"], "tag": ["Industry Papers"], "abstract": "ABSTRACTMatlab/Simulink is a development and simulation language that is widely used by the Cyber-Physical System (CPS) industry to model dynamical systems. There are two mainstream approaches to verify CPS Simulink models: model testing that attempts to identify failures in models by executing them for a number of sampled test inputs, and model checking that attempts to exhaustively check the correctness of models against some given formal properties. In this paper, we present an industrial Simulink model benchmark, provide a categorization of different model types in the benchmark, describe the recurring logical patterns in the model requirements, and discuss the results of applying model checking and model testing approaches to identify requirements violations in the benchmarked models. Based on the results, we discuss the strengths and weaknesses of model testing and model checking. Our results further suggest that model checking and model testing are complementary and by combining them, we can significantly enhance the capabilities of each of these approaches individually. We conclude by providing guidelines as to how the two approaches can be best applied together."}, {"id": "conf/sigsoft/LangP19", "title": "Model checking a C++ software framework: a case study.", "authors": ["John L\u00e5ng", "I. S. W. B. Prasetya"], "DOIs": ["https://doi.org/10.1145/3338906.3340453"], "tag": ["Industry Papers"], "abstract": "ABSTRACTThis paper presents a case study on applying two model checkers, Spin and Divine, to verify key properties of a C++ software framework, known as ADAPRO, originally developed at CERN. Spin was used for verifying properties on the design level. Divine was used for verifying simple test applications that interacted with the implementation. Both model checkers were found to have their own respective sets of pros and cons, but the overall experience was positive. Because both model checkers were used in a complementary manner, they provided valuable new insights into the framework, which would arguably have been hard to gain by traditional testing and analysis tools only. Translating the C++ source code into the modeling language of the Spin model checker helped to find flaws in the original design. With Divine, defects were found in parts of the code base that had already been subject to hundreds of hours of unit tests, integration tests, and acceptance tests. Most importantly, model checking was found to be easy to integrate into the workflow of the software project and bring added value, not only as verification, but also validation methodology. Therefore, using model checking for developing library-level code seems realistic and worth the effort."}, {"id": "conf/sigsoft/Morales-Trujillo19", "title": "Evolving with patterns: a 31-month startup experience report.", "authors": ["Miguel Eh\u00e9catl Morales-Trujillo", "Gabriel Alberto Garc\u00eda-Mireles"], "DOIs": ["https://doi.org/10.1145/3338906.3340447"], "tag": ["Industry Papers"], "abstract": "ABSTRACTSoftware startups develop innovative products under extreme conditions of uncertainty. At the same time they represent a fast-growing sector in the economy and scale up research and technological advancement. This paper describes findings after observing a startup during its first 31 months of life. The data was collected through observations, unstructured interviews as well as from technical and managerial documentation of the startup. The findings are based on a deductive analysis and summarized in 24 contextualized patterns that concern communication, interaction with customer, teamwork, and management. Furthermore, 13 lessons learned are presented with the aim of sharing experience with other startups. This industry report contributes to understanding the applicability and usefulness of startups' patterns, providing valuable knowledge for the startup software engineering community."}, {"id": "conf/sigsoft/BarashFJRTZ19", "title": "Bridging the gap between ML solutions and their business requirements using feature interactions.", "authors": ["Guy Barash", "Eitan Farchi", "Ilan Jayaraman", "Orna Raz", "Rachel Tzoref-Brill", "Marcel Zalmanovici"], "DOIs": ["https://doi.org/10.1145/3338906.3340442"], "tag": ["Industry Papers"], "abstract": "ABSTRACTMachine Learning (ML) based solutions are becoming increasingly popular and pervasive. When testing such solutions, there is a tendency to focus on improving the ML metrics such as the F1-score and accuracy at the expense of ensuring business value and correctness by covering business requirements. In this work, we adapt test planning methods of classical software to ML solutions. We use combinatorial modeling methodology to define the space of business requirements and map it to the ML solution data, and use the notion of data slices to identify the weaker areas of the ML solution and strengthen them. We apply our approach to three real-world case studies and demonstrate its value."}, {"id": "conf/sigsoft/DobrigkeitP19", "title": "Design thinking in practice: understanding manifestations of design thinking in software engineering.", "authors": ["Franziska Dobrigkeit", "Danielly de Paula"], "DOIs": ["https://doi.org/10.1145/3338906.3340451"], "tag": ["Industry Papers"], "abstract": "ABSTRACTThis industry case study explores where and how Design Thinking supports software development teams in their endeavour to create innovative software solutions. Design Thinking has found its way into software companies ranging from startups to SMEs and multinationals. It is mostly seen as a human centered innovation approach or a way to elicit requirements in a more agile fashion. However, research in Design Thinking suggests that being exposed to DT changes the mindset of employees. Thus this article aims to explore the wider use of DT within software companies through a case study in a multinational organization. Our results indicate, that once trained in DT, employees find various ways to implement it not only as a pre-phase to software development but throughout their projects even applying it to aspects of their surroundings such as the development process, team spaces and team work. Specifically we present a model of how DT manifests itself in a software development company."}, {"id": "conf/sigsoft/CorreiaASN19", "title": "MOTSD: a multi-objective test selection tool using test suite diagnosability.", "authors": ["Daniel Correia", "Rui Abreu", "Pedro Santos", "Jo\u00e3o Nadkarni"], "DOIs": ["https://doi.org/10.1145/3338906.3341187"], "tag": ["Demonstrations"], "abstract": "ABSTRACTPerforming regression testing on large software systems becomes unfeasible as it takes too long to run all the test cases every time a change is made. The main motivation of this work was to provide a faster and earlier feedback loop to the developers at OutSystems when a change is made. The developed tool, MOTSD, implements a multi-objective test selection approach in a C# code base using a test suite diagnosability metric and historical metrics as objectives and it is powered by a particle swarm optimization algorithm. We present implementation challenges, current experimental results and limitations of the tool when applied in an industrial context. Screencast demo link: <a>https://www.youtube.com/watch?v=CYMfQTUu2BE</a>"}, {"id": "conf/sigsoft/CaiWH0X019", "title": "BIKER: a tool for Bi-information source based API method recommendation.", "authors": ["Liang Cai", "Haoye Wang", "Qiao Huang", "Xin Xia", "Zhenchang Xing", "David Lo"], "DOIs": ["https://doi.org/10.1145/3338906.3341174"], "tag": ["Demonstrations"], "abstract": "ABSTRACTApplication Programming Interfaces (APIs) in software libraries play an important role in modern software development. Although most libraries provide API documentation as a reference, developers may find it difficult to directly search for appropriate APIs in documentation using the natural language description of the programming tasks. We call such phenomenon as knowledge gap, which refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes. In this paper, we propose a Java API recommendation tool named BIKER (Bi-Information source based KnowledgE Recommendation) to bridge the knowledge gap. We implement BIKER as a search engine website. Given a query in natural language, instead of directly searching API documentation, BIKER first searches for similar API-related questions on Stack Overflow to extract candidate APIs. Then, BIKER ranks them by considering the query\u2019s similarity with both Stack Overflow posts and API documentation. Finally, to help developers better understand why each API is recommended and how to use them in practice, BIKER summarizes and presents supplementary information (e.g., API description, code examples in Stack Overflow posts) for each recommended API. Our quantitative evaluation and user study demonstrate that BIKER can help developers find appropriate APIs more efficiently and precisely."}, {"id": "conf/sigsoft/ChekamPT19", "title": "Mart: a mutant generation tool for LLVM.", "authors": ["Thierry Titcheu Chekam", "Mike Papadakis", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3338906.3341180"], "tag": ["Demonstrations"], "abstract": "ABSTRACTProgram mutation makes small syntactic alterations to programs' code in order to artificially create faulty programs (mutants). Mutants creation (generation) tools are often characterized by their mutation operators and the way they create and represent the mutants. This paper presents Mart, a mutants generation tool, for LLVM bitcode, that supports the fine-grained definition of mutation operators (as matching rule - replacing pattern pair; uses 816 defined pairs by default) and the restriction of the code parts to mutate. New operators are implemented in Mart by implementing their matching rules and replacing patterns. Mart also implements in-memory Trivial Compiler Equivalence to eliminate equivalent and duplicate mutants during mutants generation. Mart generates mutant code as separated mutant files, meta-mutants file, weak mutation and mutant coverage instrumented files. Mart is publicly available (https://github.com/thierry-tct/mart). Mart has been applied to generate mutants for several research experiments and generated more than 4,000,000 mutants."}, {"id": "conf/sigsoft/TundoMORGM19", "title": "VARYS: an agnostic model-driven monitoring-as-a-service framework for the cloud.", "authors": ["Alessandro Tundo", "Marco Mobilio", "Matteo Orr\u00f9", "Oliviero Riganelli", "Michell Guzm\u00e1n", "Leonardo Mariani"], "DOIs": ["https://doi.org/10.1145/3338906.3341185"], "tag": ["Demonstrations"], "abstract": "ABSTRACTCloud systems are large scalable distributed systems that must be carefully monitored to timely detect problems and anomalies. While a number of cloud monitoring frameworks are available, only a few solutions address the problem of adaptively and dynamically selecting the indicators that must be collected, based on the actual needs of the operator. Unfortunately, these solutions are either limited to infrastructure-level indicators or technology-specific, for instance, they are designed to work with OpenStack but not with other cloud platforms. This paper presents the VARYS monitoring framework, a technology-agnostic Monitoring-as-a-Service solution that can address KPI monitoring at all levels of the Cloud stack, including the application-level. Operators use VARYS to indicate their monitoring goals declaratively, letting the framework to perform all the operations necessary to achieve a requested monitoring configuration automatically. Interestingly, the VARYS architecture is general and extendable, and can thus be used to support increasingly more platforms and probing technologies."}, {"id": "conf/sigsoft/StallenbergP19", "title": "JCOMIX: a search-based tool to detect XML injection vulnerabilities in web applications.", "authors": ["Dimitri Michel Stallenberg", "Annibale Panichella"], "DOIs": ["https://doi.org/10.1145/3338906.3341178"], "tag": ["Demonstrations"], "abstract": "ABSTRACTInput sanitization and validation of user inputs are well-established protection mechanisms for microservice architectures against XML injection attacks (XMLi). The effectiveness of the protection mechanisms strongly depends on the quality of the sanitization and validation rule sets (e.g., regular expressions) and, therefore, security analysts have to test them thoroughly. In this demo, we introduce JCOMIX, a penetration testing tool that generates XMLi attacks (test cases) exposing XML vulnerabilities in front-end web applications. JCOMIX implements various search algorithms, including random search (traditional fuzzing), genetic algorithms (GAs), and the more recent co-operative, co-evolutionary algorithm designed explicitly for the XMLi testing (COMIX). We also show the results of an empirical study showing the effectiveness of JCOMIX in testing an open-source front-end web application."}, {"id": "conf/sigsoft/SuiZZZX19", "title": "Event trace reduction for effective bug replay of Android apps via differential GUI state analysis.", "authors": ["Yulei Sui", "Yifei Zhang", "Wei Zheng", "Manqing Zhang", "Jingling Xue"], "DOIs": ["https://doi.org/10.1145/3338906.3341183"], "tag": ["Demonstrations"], "abstract": "ABSTRACTExisting Android testing tools, such as Monkey, generate a large quantity and a wide variety of user events to expose latent GUI bugs in Android apps. However, even if a bug is found, a majority of the events thus generated are often redundant and bug-irrelevant. In addition, it is also time-consuming for developers to localize and replay the bug given a long and tedious event sequence (trace). This paper presents ECHO, an event trace reduction tool for effective bug replay by using a new differential GUI state analysis. Given a sequence of events (trace), ECHO aims at removing bug-irrelevant events by exploiting the differential behavior between the GUI states collected when their corresponding events are triggered. During dynamic testing, ECHO injects at most one lightweight inspection event after every event to collect its corresponding GUI state. A new adaptive model is proposed to selectively inject inspection events based on sliding windows to differentiate the GUI states on-the-fly in a single testing process. The experimental results show that ECHO improves the effectiveness of bug replay by removing 85.11% redundant events on average while also revealing the same bugs as those detected when full event sequences are used."}, {"id": "conf/sigsoft/AnBPY19", "title": "PyGGI 2.0: language independent genetic improvement framework.", "authors": ["Gabin An", "Aymeric Blot", "Justyna Petke", "Shin Yoo"], "DOIs": ["https://doi.org/10.1145/3338906.3341184"], "tag": ["Demonstrations"], "abstract": "ABSTRACTPyGGI is a research tool for Genetic Improvement (GI), that is designed to be versatile and easy to use. We present version 2.0 of PyGGI, the main feature of which is an XML-based intermediate program representation. It allows users to easily define GI operators and algorithms that can be reused with multiple target languages. Using the new version of PyGGI, we present two case studies. First, we conduct an Automated Program Repair (APR) experiment with the QuixBugs benchmark, one that contains defective programs in both Python and Java. Second, we replicate an existing work on runtime improvement through program specialisation for the MiniSAT satisfiability solver. PyGGI 2.0 was able to generate a patch for a bug not previously fixed by any APR tool. It was also able to achieve 14% runtime improvement in the case of MiniSAT. The presented results show the applicability and the expressiveness of the new version of PyGGI. A video of the tool demo is at: https://youtu.be/PxRUdlRDS40."}, {"id": "conf/sigsoft/MostaeenSRRS19", "title": "CloneCognition: machine learning based code clone validation tool.", "authors": ["Golam Mostaeen", "Jeffrey Svajlenko", "Banani Roy", "Chanchal K. Roy", "Kevin A. Schneider"], "DOIs": ["https://doi.org/10.1145/3338906.3341182"], "tag": ["Demonstrations"], "abstract": "ABSTRACTA code clone is a pair of similar code fragments, within or between software systems. To detect each possible clone pair from a software system while handling the complex code structures, the clone detection tools undergo a lot of generalization of the original source codes. The generalization often results in returning code fragments that are only coincidentally similar and not considered clones by users, and hence requires manual validation of the reported possible clones by users which is often both time-consuming and challenging. In this paper, we propose a machine learning based tool 'CloneCognition' (Open Source Codes: https://github.com/pseudoPixels/CloneCognition ; Video Demonstration: https://www.youtube.com/watch?v=KYQjmdr8rsw) to automate the laborious manual validation process. The tool runs on top of any code clone detection tools to facilitate the clone validation process. The tool shows promising clone classification performance with an accuracy of up to 87.4%. The tool also exhibits significant improvement in the results when compared with state-of-the-art techniques for code clone validation."}, {"id": "conf/sigsoft/FuRMSYJLS19", "title": "EVMFuzzer: detect EVM vulnerabilities via fuzz testing.", "authors": ["Ying Fu", "Meng Ren", "Fuchen Ma", "Heyuan Shi", "Xin Yang", "Yu Jiang", "Huizhong Li", "Xiang Shi"], "DOIs": ["https://doi.org/10.1145/3338906.3341175"], "tag": ["Demonstrations"], "abstract": "ABSTRACTEthereum Virtual Machine (EVM) is the run-time environment for smart contracts and its vulnerabilities may lead to serious problems to the Ethereum ecology. With lots of techniques being continuously developed for the validation of smart contracts, the testing of EVM remains challenging because of the special test input format and the absence of oracles. In this paper, we propose EVMFuzzer, the first tool that uses differential fuzzing technique to detect vulnerabilities of EVM. The core idea is to continuously generate seed contracts and feed them to the target EVM and the benchmark EVMs, so as to find as many inconsistencies among execution results as possible, eventually discover vulnerabilities with output cross-referencing. Given a target EVM and its APIs, EVMFuzzer generates seed contracts via a set of predefined mutators, and then employs dynamic priority scheduling algorithm to guide seed contracts selection and maximize the inconsistency. Finally, EVMFuzzer leverages benchmark EVMs as cross-referencing oracles to avoid manual checking. With EVMFuzzer, we have found several previously unknown security bugs in four widely used EVMs, and 5 of which had been included in Common Vulnerabilities and Exposures (CVE) IDs in U.S. National Vulnerability Database. The video is presented at https://youtu.be/9Lejgf2GSOk."}, {"id": "conf/sigsoft/FuC19", "title": "A dynamic taint analyzer for distributed systems.", "authors": ["Xiaoqin Fu", "Haipeng Cai"], "DOIs": ["https://doi.org/10.1145/3338906.3341179"], "tag": ["Demonstrations"], "abstract": "ABSTRACTAs in other software domains, information flow security is a fundamental aspect of code security in distributed systems. However, most existing solutions to information flow security are limited to centralized software. For distributed systems, such solutions face multiple challenges, including technique applicability, tool portability, and analysis scalability. To overcome these challenges, we present DistTaint, a dynamic information flow (taint) analyzer for distributed systems. By partial-ordering method-execution events, DistTaint infers implicit dependencies in distributed programs, so as to resolve the applicability challenge. It resolves the portability challenge by working fully at application level, without customizing the runtime platform. To achieve scalability, it reduces analysis costs using a multi-phase analysis, where the pre-analysis phase generates method-level results to narrow down the scope of the following statement-level analysis. We evaluated DistTaint against eight real-world distributed systems. Empirical results showed DistTaint\u2019s applicability to, portability with, and scalability for industry-scale distributed systems, along with its capability of discovering known and unknown vulnerabilities. A demo video for DistTaint can be downloaded from https://www.dropbox.com/l/scl/AAAkrm4p63Ffx0rZqblY3zlLFuaohbRxs0 or viewed here https://youtu.be/fy4yMIaKzPE online. The tool package is here: https://www.dropbox.com/sh/kfr9ixucyny1jp2/AAC00aI-I8Od4ywZCqwZ1uaa?dl=0"}, {"id": "conf/sigsoft/Gamez-Diaz0R19", "title": "Governify for APIs: SLA-driven ecosystem for API governance.", "authors": ["Antonio Gamez-Diaz", "Pablo Fernandez", "Antonio Ruiz-Cort\u00e9s"], "DOIs": ["https://doi.org/10.1145/3338906.3341176"], "tag": ["Demonstrations"], "abstract": "ABSTRACTAs software architecture design is evolving to a microservice paradigm, RESTful APIs are being established as the preferred choice to build applications. In such a scenario, there is a shift towards a growing market of APIs where providers offer different service levels with tailored limitations typically based on the cost. In such a context, while there are well-established standards to describe the functional elements of APIs (such as the OpenAPI Specification), having a standard model for Service Level Agreements (SLAs) for APIs may boost an open ecosystem of tools that would represent an improvement for the industry by automating certain tasks during the development. In this paper, we introduce Governify for APIs, an ecosystem of tools aimed to support the user during the SLA-Driven RESTful APIs\u2019 development process. Namely, an SLA Editor, an SLA Engine and an SLA Instrumentation Library. We also present a fully operational SLA-Driven API Gateway built on the top of our ecosystem of tools. To evaluate our proposal, we used three sources for gathering validation feedback: industry, teaching and research. Website: <a>links.governify.io/link/GovernifyForAPIs</a> Video: <a>links.governify.io/link/GovernifyForAPIsVideo</a>"}, {"id": "conf/sigsoft/AtzeiBLYZ19", "title": "Developing secure bitcoin contracts with BitML.", "authors": ["Nicola Atzei", "Massimo Bartoletti", "Stefano Lande", "Nobuko Yoshida", "Roberto Zunino"], "DOIs": ["https://doi.org/10.1145/3338906.3341173"], "tag": ["Demonstrations"], "abstract": "ABSTRACTWe present a toolchain for developing and verifying smart contracts that can be executed on Bitcoin. The toolchain is based on BitML, a recent domain-specific language for smart contracts with a computationally sound embedding into Bitcoin. Our toolchain automatically verifies relevant properties of contracts, among which liquidity, ensuring that funds do not remain frozen within a contract forever. A compiler is provided to translate BitML contracts into sets of standard Bitcoin transactions: executing a contract corresponds to appending these transactions to the blockchain. We assess our toolchain through a benchmark of representative contracts."}, {"id": "conf/sigsoft/AwadhutkarSHK19", "title": "DISCOVER: detecting algorithmic complexity vulnerabilities.", "authors": ["Payas Awadhutkar", "Ganesh Ram Santhanam", "Benjamin Holland", "Suresh Kothari"], "DOIs": ["https://doi.org/10.1145/3338906.3341177"], "tag": ["Demonstrations"], "abstract": "ABSTRACTAlgorithmic Complexity Vulnerabilities (ACV) are a class of vulnerabilities that enable Denial of Service Attacks. ACVs stem from asymmetric consumption of resources due to complex loop termination logic, recursion, and/or resource intensive library APIs. Completely automated detection of ACVs is intractable and it calls for tools that assist human analysts.  We present DISCOVER, a suite of tools that facilitates human-on-the-loop detection of ACVs. DISCOVER's workflow can be broken into three phases - (1) Automated characterization of loops, (2) Selection of suspicious loops, and (3) Interactive audit of selected loops. We demonstrate DISCOVER using a case study using a DARPA challenge app. DISCOVER supports analysis of Java source code and Java bytecode. We demonstrate it for Java bytecode."}, {"id": "conf/sigsoft/CaiWXH00X19", "title": "AnswerBot: an answer summary generation tool based on stack overflow.", "authors": ["Liang Cai", "Haoye Wang", "Bowen Xu", "Qiao Huang", "Xin Xia", "David Lo", "Zhenchang Xing"], "DOIs": ["https://doi.org/10.1145/3338906.3341186"], "tag": ["Demonstrations"], "abstract": "ABSTRACTSoftware Q&A sites (like Stack Overflow) play an essential role in developers\u2019 day-to-day work for problem-solving. Although search engines (like Google) are widely used to obtain a list of relevant posts for technical problems, we observed that the redundant relevant posts and sheer amount of information barriers developers to digest and identify the useful answers. In this paper, we propose a tool AnswerBot which enables to automatically generate an answer summary for a technical problem. AnswerBot consists of three main stages, (1) relevant question retrieval, (2) useful answer paragraph selection, (3) diverse answer summary generation. We implement it in the form of a search engine website. To evaluate AnswerBot, we first build a repository includes a large number of Java questions and their corresponding answers from Stack Overflow. Then, we conduct a user study that evaluates the answer summary generated by AnswerBot and two baselines (based on Google and Stack Overflow search engine) for 100 queries. The results show that the answer summaries generated by AnswerBot are more relevant, useful, and diverse. Moreover, we also substantially improved the efficiency of AnswerBot (from 309 to 8 seconds per query)."}, {"id": "conf/sigsoft/GuerreroFJF0MR19", "title": "Eagle: a team practices audit framework for agile software development.", "authors": ["Alejandro Guerrero", "Rafael Fresno", "An Ju", "Armando Fox", "Pablo Fernandez", "Carlos M\u00fcller", "Antonio Ruiz-Cort\u00e9s"], "DOIs": ["https://doi.org/10.1145/3338906.3341181"], "tag": ["Demonstrations"], "abstract": "ABSTRACTAgile/XP (Extreme Programming) software teams are expected to follow a number of specific practices in each iteration, such as estimating the effort (\u201dpoints\u201d) required to complete user stories, properly using branches and pull requests to coordinate merging multiple contributors\u2019 code, having frequent \u201dstandups\u201d to keep all team members in sync, and conducting retrospectives to identify areas of improvement for future iterations. We combine two observations in developing a methodology and tools to help teams monitor their performance on these practices. On the one hand, many Agile practices are increasingly supported by web-based tools whose \u201ddata exhaust\u201d can provide insight into how closely the teams are following the practices. On the other hand, some of the practices can be expressed in terms similar to those developed for expressing service level objectives (SLO) in software as a service; as an example, a typical SLO for an interactive Web site might be \u201dover any 5-minute window, 99% of requests to the main page must be delivered within 200ms\u201d and, analogously, a potential Team Practice (TP) for an Agile/XP team might be \u201dover any 2-week iteration, 75% of stories should be \u20191-point\u2019 stories\u201d. Following this similarity, we adapt a system originally developed for monitoring and visualizing service level agreement (SLA) compliance to monitor selected TPs for Agile/XP software teams. Specifically, the system consumes and analyzes the data exhaust from widely-used tools such as GitHub and Pivotal Tracker and provides team(s) and coach(es) a \u201ddashboard\u201d summarizing the teams\u2019 adherence to various practices. As a qualitative initial investigation of its usefulness, we deployed it to twenty student teams in a four-sprint software engineering project course. We find an improvement of the adherence to team practice and a positive students\u2019 self-evaluations of their team practices when using the tool, compared to previous experiences using an Agile/XP methodology. The demo video is located at <a>https://youtu.be/A4xwJMEQh9c</a> and a landing page with a live demo at <a>https://isa-group.github.io/2019-05-eagle-demo/</a>."}, {"id": "conf/sigsoft/Caulo19", "title": "A taxonomy of metrics for software fault prediction.", "authors": ["Maria Caulo"], "DOIs": ["https://doi.org/10.1145/3338906.3341462"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTIn the field of Software Fault Prediction (SFP), researchers exploit software metrics to build predictive models using machine learning and/or statistical techniques. SFP has existed for several decades and the number of metrics used has increased dramatically. Thus, the need for a taxonomy of metrics for SFP arises firstly to standardize the lexicon used in this field so that the communication among researchers is simplified and then to organize and systematically classify the used metrics. In this doctoral symposium paper, I present my ongoing work which aims not only to build such a taxonomy as comprehensive as possible, but also to provide a global understanding of the metrics for SFP in terms of detailed information: acronym(s), extended name, univocal description, granularity of the fault prediction (e.g., method and class), category, and research papers in which they were used."}, {"id": "conf/sigsoft/Coviello19", "title": "Distributed execution of test cases and continuous integration.", "authors": ["Carmen Coviello"], "DOIs": ["https://doi.org/10.1145/3338906.3341460"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTI present here a part of the research conducted in my Ph.D. course. In particular, I focus on my ongoing work on how to support testing in the context of Continuous Integration (CI) development by distributing the execution of test cases (TCs) on geographically dispersed servers. I show how to find a trade-off between the cost of leased servers and the time to execute a given test suite (TS). The distribution and the execution of TCs on servers is modeled as a multi-objective optimization problem, where the goal is to balance the cost to lease servers and the time to execute TCs. The preliminary results : (i) show evidence of the existence of a Pareto Front (trade-off between costs to lease servers and TCs time) and (ii) suggest that the found solutions are worthwhile as compared to a traditional non-distributed TS execution (i.e., a single server/PC). Although the obtained results cannot be considered conclusive, it seems that the solutions are worth to speed up the testing activities in the context of CI."}, {"id": "conf/sigsoft/Denkers19", "title": "A longitudinal field study on creation and use of domain-specific languages in industry.", "authors": ["Jasper Denkers"], "DOIs": ["https://doi.org/10.1145/3338906.3341463"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTDomain-specific languages (DSLs) have extensively been investigated in research and have frequently been applied in practice for over 20 years. While DSLs have been attributed improvements in terms of productivity, maintainability, and taming accidental complexity, surprisingly, we know little about their actual impact on the software engineering practice. This PhD project, that is done in close collaboration with our industrial partner Oc\u00e9 - A Canon Company, offers a unique opportunity to study the application of DSLs using a longitudinal field study. In particular, we focus on introducing DSLs with language workbenches, i.e., infrastructures for designing and deploying DSLs, for projects that are already running for several years and for which extensive domain analysis outcomes are available. In doing so, we expect to gain a novel perspective on DSLs in practice. Additionally, we aim to derive best practices for DSL development and to identify and overcome limitations in the current state-of-the-art tooling for DSLs."}, {"id": "conf/sigsoft/Ginelli19", "title": "Failure-driven program repair.", "authors": ["Davide Ginelli"], "DOIs": ["https://doi.org/10.1145/3338906.3341464"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTProgram repair techniques can dramatically reduce the cost of program debugging by automatically generating program fixes. Although program repair has been already successful with several classes of faults, it also turned out to be quite limited in the complexity of the fixes that can be generated.  This Ph.D. thesis addresses the problem of cost-effectively generating fixes of higher complexity by investigating how to exploit failure information to directly shape the repair process. In particular, this thesis proposes Failure-Driven Program Repair, which is a novel approach to program repair that exploits its knowledge about both the possible failures and the corresponding repair strategies, to produce highly specialized repair tasks that can effectively generate non-trivial fixes."}, {"id": "conf/sigsoft/Greiner19", "title": "On extending single-variant model transformations for reuse in software product line engineering.", "authors": ["Sandra Greiner"], "DOIs": ["https://doi.org/10.1145/3338906.3341467"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTSoftware product line engineering (SPLE) aims at increasing productivity by following the principles of variability and organized reuse. Combining the discipline with model-driven software engineering (MDSE) seeks to intensify this effect by raising the level of abstraction. Typically, a product line developed in a model-driven way is composed of various kinds of models, like class diagrams and database schemata. To automatically generate further necessary representations from a initial (source) model, model transformations may create a respective target model. In annotative approaches to SPLE, variability annotations, which are boolean expressions over the features of the product line, state in which products a (model) element is visible. State-of-the-art single-variant model transformations (SVMT), however, do not consider variability annotations additionally associated with model elements. Thus, multi-variant model transformations (MVMT) should bridge the gap between existing SPLE and MDSE approaches by reusing already existing technology to propagate annotations additionally to the the target. The present contribution gives an overview on the research we conduct to reuse SVMTs in model-driven SPLE and provides a plan on which steps are still to be taken."}, {"id": "conf/sigsoft/Karlsson19", "title": "Exploratory test agents for stateful software systems.", "authors": ["Stefan Karlsson"], "DOIs": ["https://doi.org/10.1145/3338906.3341458"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTThe adequate testing of stateful software systems is a hard and costly activity. Failures that result from complex stateful interactions can be of high impact, and it can be hard to replicate failures resulting from erroneous stateful interactions. Addressing this problem in an automatic way would save cost and time and increase the quality of software systems in the industry. In this paper, we propose an approach that uses agents to explore software systems with the intention to find faults and gain knowledge."}, {"id": "conf/sigsoft/Marques19", "title": "Helping developers search and locate task-relevant information in natural language documents.", "authors": ["Arthur Marques"], "DOIs": ["https://doi.org/10.1145/3338906.3341459"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTWhile performing a task, software developers interact with a myriad of natural language documents. Not all information in these documents is relevant to a developer's task forcing them to filter relevant information from large amounts of irrelevant information. If a developer misses some of the necessary information for her task, she will have an incomplete or incorrect basis from which to complete the task. Many approaches mine relevant text fragments from natural language artifacts. However, existing approaches mine information for pre-defined tasks and from a restricted set of artifacts. I hypothesize that it is possible to design a more generalizable approach that can identify, for a particular task, relevant text across different artifact types establishing relationships between them and facilitating how developers search and locate task-relevant information. To investigate this hypothesis, I propose to match a developer's task to text fragments in natural language artifacts according to their semantics. By semantically matching textual pieces to a developer's task we aim to more precisely identify fragments relevant to a task. To help developers in thoroughly navigating through the identified fragments I also propose to synthesize and group them. Ultimately, this research aims to help developers make more informed decisions regarding their software development task. Dr. Gail C. Murphy supervises this work."}, {"id": "conf/sigsoft/Melegati19", "title": "Improving requirements engineering practices to support experimentation in software startups.", "authors": ["Jorge Melegati"], "DOIs": ["https://doi.org/10.1145/3338906.3341465"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTThe importance of startups to economic development is indisputable. Software startups are startups that develop an innovative software-intensive product or service. In spite of the rising of several methodologies to improve their efficiency, most of software startups still fail. There are several possible reasons to failure including under or over-engineering the product because of not-suitable engineering practices, wasted resources, and missed market opportunities. The literature argues that experimentation is essential to innovation and entrepreneurship. Even though well-known startup development methodologies employ it, studies revealed that practitioners still do not use it. Given that requirements engineering is in between software engineering and business, in this study, I aim to improve these practices to foster experimentation in software startups. To achieve that, first I investigated how requirements engineering activities are performed in software startups. Then, my goal is to propose new requirements engineering practices to foster experimentation in this context."}, {"id": "conf/sigsoft/Muller19", "title": "Managing the open cathedral.", "authors": ["Matthias M\u00fcller"], "DOIs": ["https://doi.org/10.1145/3338906.3341461"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTAlready early in the history of open source projects it became apparent that they are driven by only a few contributors, creating the biggest portion of code. Whereas this has already been shown in previous research, this work adds a time perspective and considers the dynamics and evolution of communities. These aspects become increasingly important with the growing involvement of firms in such communities. Open source software is today used in many commercial applications, but also gets actively developed by businesses. Therefore, understanding and managing such projects into a common direction gets of increasing interest. The author\u2019s work is intended to build a better understanding of these communities, their dynamics over time, key players and dependencies on them."}, {"id": "conf/sigsoft/Sonnekalb19", "title": "Machine-learning supported vulnerability detection in source code.", "authors": ["Tim Sonnekalb"], "DOIs": ["https://doi.org/10.1145/3338906.3341466"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTThe awareness of writing secure code rises with the increasing number of attacks and their resultant damage. But often, software developers are no security experts and vulnerabilities arise unconsciously during the development process. They use static analysis tools for bug detection, which often come with a high false positive rate. The developers, therefore, need a lot of resources to mind about all alarms, if they want to consistently take care of the security of their software project. We want to investigate, if machine learning techniques could point the user to the position of a security weak point in the source code with a higher accuracy than ordinary methods with static analysis. For this purpose, we focus on current machine learning on code approaches for our initial studies to evolve an efficient way for finding security-related software bugs. We will create a configuration interface to discover certain vulnerabilities, categorized in CWEs. We want to create a benchmark tool to compare existing source code representations and machine learning architectures for vulnerability detection and develop a customizable feature model. At the end of this PhD project, we want to have an easy-to-use vulnerability detection tool based on machine learning on code."}, {"id": "conf/sigsoft/Papachristou19", "title": "Software clusterings with vector semantics and the call graph.", "authors": ["Marios Papachristou"], "DOIs": ["https://doi.org/10.1145/3338906.3342483"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTIn this paper, we propose a novel method to determine a software's modules without knowledge of its architectural structure, and empirically validate the method's performance. We cluster files by combining document embeddings, generated with the Doc2Vec algorithm, and the call graph, provided by Static Graph Analyzers to an augmented graph. We use the Louvain Algorithm to determine its community structure and propose a module-level clustering. Our method performs better in terms of stability, authoritativeness, and extremity over other state-of-the-art clustering methods proposed in the literature and is able to decently recover the ground truth clustering of the Linux Kernel. Finally, we conclude that semantic information from vector semantics as well as the call graph can produce accurate results for software clusterings of large systems."}, {"id": "conf/sigsoft/Moghadam19", "title": "Machine learning-assisted performance testing.", "authors": ["Mahshid Helali Moghadam"], "DOIs": ["https://doi.org/10.1145/3338906.3342484"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTAutomated testing activities like automated test case generation imply a reduction in human effort and cost, with the potential to impact the test coverage positively. If the optimal policy, i.e., the course of actions adopted, for performing the intended test activity could be learnt by the testing system, i.e., a smart tester agent, then the learnt policy could be reused in analogous situations which leads to even more efficiency in terms of required efforts. Performance testing under stress execution conditions, i.e., stress testing, which involves providing extreme test conditions to find the performance breaking points, remains a challenge, particularly for complex software systems. Some common approaches for generating stress test conditions are based on source code or system model analysis, or use-case based design approaches. However, source code or precise system models might not be easily available for testing. Moreover, drawing a precise performance model is often difficult, particularly for complex systems. In this research, I have used model-free reinforcement learning to build a self-adaptive autonomous stress testing framework which is able to learn the optimal policy for stress test case generation without having a model of the system under test. The conducted experimental analysis shows that the proposed smart framework is able to generate the stress test conditions for different software systems efficiently and adaptively without access to performance models."}, {"id": "conf/sigsoft/Stepanov19", "title": "File tracing by intercepting disk requests.", "authors": ["Vladislav Stepanov"], "DOIs": ["https://doi.org/10.1145/3338906.3342485"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTExisting file operation tracing methods are always OS-specific. It is a problem for file monitoring in some exotic operating systems and other programs running in privileged mode. We present a file system specific solution that could be work with any guest OS on a virtual machine. This solution works on the basis of intercepting requests to a block device. Our implementation is based on the QEMU emulator and supports ext2 and ext3 file systems."}, {"id": "conf/sigsoft/Abid19", "title": "Recommending related functions from API usage-based function clone structures.", "authors": ["Shamsa Abid"], "DOIs": ["https://doi.org/10.1145/3338906.3342486"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTDevelopers need to be able to find reusable code for desired software features in a way that supports opportunistic programming for increased developer productivity. Our objective is to develop a recommendation system that provides a developer with function recommendations having functionality relevant to her development task. We employ a combination of information retrieval, static code analysis and data mining techniques to build the proposed recommendation system called FACER (Feature-driven API usage-based Code Examples Recommender). We performed an experimental evaluation on 122 projects from GitHub from selected categories to determine the accuracy of the retrieved code for related features. FACER recommended functions with a precision of 54% and 75% when evaluated using automated and manual methods respectively."}, {"id": "conf/sigsoft/Cetin19", "title": "Identifying the most valuable developers using artifact traceability graphs.", "authors": ["H. Alperen Cetin"], "DOIs": ["https://doi.org/10.1145/3338906.3342487"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTFinding the most valuable and indispensable developers is a crucial task in software development. We categorize these valuable developers into two categories: connector and maven. A typical connector represents a developer who connects different groups of developers in a large-scale project. Mavens represent the developers who are the sole experts in specific modules of the project.  To identify the connectors and mavens, we propose an approach using graph centrality metrics and connections of traceability graphs. We conducted a preliminary study on this approach by using two open source projects: QT 3D Studio and Android. Initial results show that the approach leads to identify the essential developers."}, {"id": "conf/sigsoft/Ren19", "title": "Automated patch porting across forked projects.", "authors": ["Luyao Ren"], "DOIs": ["https://doi.org/10.1145/3338906.3342488"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTForking projects provides a straightforward method for developers to reuse existing source code and tailor it to their own application scenarios, which can significantly reduce developers' burden. However, this process makes forked projects (upstream projects and their forks) share the same defects on reused code as well. With the independent development of forked projects, some defects can only be repaired in one of them, where the patches need to be ported to others as well. Manually tracking all such activities among them is hard. Previous studies reveal that porting patches across forked projects is imperative and call research in this direction. Targeting at this problem, we conducted an empirical study to analyze the characteristics of patches in forked projects. We found that 20.5% patches need to be ported among all analyzed patches, which is a non-negligible portion. Among all those patches that need to be ported, 73.2% can be easily ported by simple syntactic code transformations. However, it is still challenging for other 26.8% patches since the corresponding code has experienced different modifications in the forked projects. As a result, according to the insights from the study, we proposed a new approach, which aims to automatically identify and port patches across forked projects."}, {"id": "conf/sigsoft/Mitropoulos19", "title": "Employing different program analysis methods to study bug evolution.", "authors": ["Charalambos Mitropoulos"], "DOIs": ["https://doi.org/10.1145/3338906.3342489"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTThe evolution of software bugs has been a well-studied topic in software engineering. We used three different program analysis tools to examine the different versions of two popular sets of programming tools (gnu Binary and Core utilities), and check if their bugs increase or decrease over time. Each tool is based on a different approach, namely: static analysis, symbolic execution, and fuzzing. In this way we can observe potential differences on the kinds of bugs that each tool detects and examine their effectiveness. To do so, we have performed a qualitative analysis on the results. Overall, our results indicate that we cannot say if bugs either decrease or increase over time and that the tools identify different bug types based on the method they follow."}, {"id": "conf/sigsoft/Tan19", "title": "Reducing the workload of the Linux kernel maintainers: multiple-committer model.", "authors": ["Xin Tan"], "DOIs": ["https://doi.org/10.1145/3338906.3342490"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTWith the increasing scale and complexity of software, the traditional development workflow may be inapplicable, which is harmful to the sustainable development of projects. In this study, we explored a new workflow \u2014 multiple-committer model that was applied by a subsystem of the Linux kernel to confront the heavy workload of the maintainers. We designed four dimensions of metrics toevaluate the model effect and found that this model conspicuouslyreduces the workload of the maintainers. We also obtained thecrucial factors for implementing this model."}, {"id": "conf/sigsoft/Loukeris19", "title": "Efficient computing in a safe environment.", "authors": ["Michail Loukeris"], "DOIs": ["https://doi.org/10.1145/3338906.3342491"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTModern computer systems are facing security challenges and thus are forced to employ various encryption, mitigation mechanisms, and other measures that affect significantly their performance. In this study, we aim to identify the energy and run-time performance implications of Meltdown and Spectre mitigation mechanisms. To achieve our goal, we experiment on server platform using different test cases. Our results highlight that request handling and memory operations are noticeably affected from mitigation mechanisms, both in terms of energy and run-time performance."}, {"id": "conf/sigsoft/Nurgalieva19", "title": "The lessons software engineers can extract from painters to improve the software development process.", "authors": ["Milana Nurgalieva"], "DOIs": ["https://doi.org/10.1145/3338906.3342492"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTThe way software is produced is very similar to an artistic and creative process. This fact is well known and has been acknowledged from the very early era of computers.  Moreover, there are even further similarities between the software development process and painting. This similarity can consequently lead to the assumption that software engineers can utilise the knowledge the artists employ, since the people have created artistic works for centuries. This paper focuses on the following questions: what similarities exist between the software development process and painting, what artistic practices could be profitably transferred to software development, and, in particular, with reference to pair programming, how do artists paint together and is there something that could be learned by software developers and engineers.  The uniqueness of the proposed approach lies in the exploration of the novel idea with the use of a complex way to examine the topic, and considering developers primarily as creative people, not ordinary industrial workers."}, {"id": "conf/sigsoft/Correia19", "title": "An industrial application of test selection using test suite diagnosability.", "authors": ["Daniel Correia"], "DOIs": ["https://doi.org/10.1145/3338906.3342493"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTPerforming full regression testing every time a change is made on large software systems tends to be unfeasible as it takes too long to run all the test cases. The main motivation of this work was to provide a shorter and earlier feedback loop to the developers at OutSystems when a change is made (instead of having to wait for slower feedback from a CI pipeline). The developed tool, MOTSD, implements a multi-objective test selection approach in a C# code base using a test suite diagnosability metric and historical metrics as objectives and it is powered by a particle swarm optimization algorithm. This paper presents implementation challenges, current experimental results and limitations of the developed approach when applied in an industrial context."}, {"id": "conf/sigsoft/He19", "title": "Understanding source code comments at large-scale.", "authors": ["Hao He"], "DOIs": ["https://doi.org/10.1145/3338906.3342494"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTSource code comments are important for any software, but the basic patterns of writing comments across domains and programming languages remain unclear. In this paper, we take a first step toward understanding differences in commenting practices by analyzing the comment density of 150 projects in 5 different programming languages. We have found that there are noticeable differences in comment density, which may be related to the programming language used in the project and the purpose of the project."}, {"id": "conf/sigsoft/Vandenbogaerde19", "title": "A graph-based framework for analysing the design of smart contracts.", "authors": ["Bram Vandenbogaerde"], "DOIs": ["https://doi.org/10.1145/3338906.3342495"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTUsed as a platform for executing smart contracts, Blockchain technology has yielded new programming languages. We propose a graph-based framework for computing software design metrics for the Solidity programming language, and use this framework in a preliminary study on 505 smart contracts mined from GitHub. The results show that most of the smart contracts are rather straightforward from an objected-oriented point of view and that new design metrics specific to smart contracts should be developed."}, {"id": "conf/sigsoft/Valdes19", "title": "Finding the shortest path to reproduce a failure found by TESTAR.", "authors": ["Olivia Rodriguez Valdes"], "DOIs": ["https://doi.org/10.1145/3338906.3342496"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTTESTAR is a tool for automated testing via the GUI. It uses dynamic analysis during automated GUI exploration and generates the test sequences during the execution. TESTAR saves all kind of information about the tests in a Graph database that can be queried or traversed during or after the tests using a traversal language. Test sequences leading to a failure can be excessively long, making the root-cause analysis of the failure difficult. This paper proposes an initial approach to find the shortest path to reproduce an error found by TESTAR"}, {"id": "conf/sigsoft/Golzadeh19", "title": "Analysing socio-technical congruence in the package dependency network of Cargo.", "authors": ["Mehdi Golzadeh"], "DOIs": ["https://doi.org/10.1145/3338906.3342497"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTSoftware package distributions form large dependency networks maintained by large communities of contributors. My PhD research will consist of analysing the evolution of the socio-technical congruence of these package dependency networks, and studying its impact on the health of the ecosystem and its community. I have started a longitudinal empirical study of Cargo's dependency network and the social (commenting) and technical (development) activities in Cargo's package repositories on GitHub, and present some preliminary findings."}, {"id": "conf/sigsoft/He19a", "title": "Tuning backfired? not (always) your fault: understanding and detecting configuration-related performance bugs.", "authors": ["Haochen He"], "DOIs": ["https://doi.org/10.1145/3338906.3342498"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTPerformance bugs (PBugs) are often hard to detect due to their non fail-stop symptoms. Existing debugging techniques can only detect PBugs with known patterns (e.g. inefficient loops). The key reason behind this incapability is the lack of a general test oracle. Here, we argue that the configuration tuning can serve as a strong candidate for PBugs detection. First, prior work shows that most performance bugs are related to configurations. Second, the tuning reflects users\u2019 expectation of performance changes. If the actual performance behaves differently from the users\u2019 intuition, the related code segment is likely to be problematic.  In this paper, we first conduct a comprehensive study on configuration related performance bugs(CPBugs) from 7 representative softwares (i.e., MySQL, MariaDB, MongoDB, RocksDB, PostgreSQL, Apache and Nginx) and collect 135 real-world CPBugs. Next, by further analyzing the symptoms and root causes of the collected bugs, we identify 7 counter-intuitive patterns. Finally, by integrating the counter-intuitive patterns, we build a general test framework for detecting performance bugs."}, {"id": "conf/sigsoft/SangleM19", "title": "On the use of lambda expressions in 760 open source Python projects.", "authors": ["Shubham Sangle", "Sandeep Muvva"], "DOIs": ["https://doi.org/10.1145/3338906.3342499"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTLambdas as anonymous functions have gained significant prominence in programming languages such as Java, C++, Python and so on as developers tend to use them. With the dominant use of Python as backend language in many projects and large number of open source projects available, we set out to investigate the use of lambdas in Python projects and obtained 19 categories to classify lambda usages as preliminary results. Our study could help language designers to improve the state of the art libraries for lambda expressions and developers to use lambda expressions effectively."}, {"id": "conf/sigsoft/Pecorelli19", "title": "Test-related factors and post-release defects: an empirical study.", "authors": ["Fabiano Pecorelli"], "DOIs": ["https://doi.org/10.1145/3338906.3342500"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTTesting is a very important activity whose purpose is to ensure software quality. Recent studies have studied the effects of test-related factors (e.g., code coverage) on software code quality, showing that they have good predictive power on post-release defects. Despite these studies demonstrated the existence of a relation between test-related factors and software code quality, they considered different factors separately. That led us to conduct an additional empirical study in which we considered these factors all together. The key findings of the study show that, while post-release defects are strongly related to process and code metrics of the production classes, test-related factors have a limited prediction impact."}, {"id": "conf/sigsoft/Pan19", "title": "Static deep neural network analysis for robustness.", "authors": ["Rangeet Pan"], "DOIs": ["https://doi.org/10.1145/3338906.3342502"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTThis work studies the static structure of deep neural network models using white box based approach and utilizes that knowledge to find the susceptible classes which can be misclassified easily. With the knowledge of susceptible classes, our work has proposed to retrain the model for those classes to achieve increased robustness. Our preliminary result has been evaluated on MNIST, F-MNIST, and CIFAR-10 (ImageNet and ResNet-32 model) based datasets and have been compared with two state-of-the-art detectors."}, {"id": "conf/sigsoft/Khanve19", "title": "Are existing code smells relevant in web games? an empirical study.", "authors": ["Vaishali Khanve"], "DOIs": ["https://doi.org/10.1145/3338906.3342504"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTIn software applications, code smells are considered as bad coding practices acquired at the time of development. The presence of such code smells in games may affect the process of game development adversely. Our preliminary study aims at investigating the existence of code smells in the games. To achieve this, we used JavaScript code smells detection tool JSNose against 361 JavaScript web games to find occurrences of JavaScript smells in games. Further, we conducted a manual study to find violations of known game programming patterns in 8 web games to verify the necessity of game-specific code smells detection tool. Our results shows that existing JavaScript code smells detection tool is not sufficient to find game-specific code smells in web games."}, {"id": "conf/sigsoft/Kruger19", "title": "Tackling knowledge needs during software evolution.", "authors": ["Jacob Kr\u00fcger"], "DOIs": ["https://doi.org/10.1145/3338906.3342505"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTDevelopers use a large amount of their time to understand the system they work on, an activity referred to as program comprehension. Especially software evolution and forgetting over time lead to developers becoming unfamiliar with a system. To support them during program comprehension, we can employ knowledge recovery to reverse engineer implicit information from the system and the platform (e.g., GitHub) it is hosted on. However, to recover useful knowledge and to provide it in a useful way, we first need to understand what knowledge developers forget to what extent, what sources are reliable to recover knowledge, and how to trace knowledge to the features in a system. We tackle these three issues, aiming to provide empirical insights and tooling to support developers during software evolution and maintenance. The results help practitioners, as we support the analysis and understanding of systems, as well as researchers, showing opportunities to automate, for example, reverse-engineering techniques."}, {"id": "conf/sigsoft/Fu19", "title": "On the scalable dynamic taint analysis for distributed systems.", "authors": ["Xiaoqin Fu"], "DOIs": ["https://doi.org/10.1145/3338906.3342506"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTTo protect the privacy and search sensitive data leaks, we must solve multiple challenges (e.g., applicability, portability, and scalability) for developing an appropriate taint analysis for distributed systems.We hence present DistTaint, a dynamic taint analysis for distributed systems against these challenges. It could infer implicit dependencies from partial-ordering method events in executions to resolve the applicability challenge. DistTaint fully works at application-level without any customization of platforms to overcome the portability challenge. It exploits a multi-phase analysis to achieve scalability. By proposing a pre-analysis, DistTaint narrows down the following fine-grained analysis\u2019 scope to reduce the overall cost significantly. Empirical results showed DistTaint\u2019s practical applicability, portability, and scalability to industry-scale distributed programs, and its capability of discovering security vulnerabilities in real-world distributed systems. The tool package can be downloaded here: https://www.dropbox.com/sh/kfr9ixucyny1jp2/AAC00aI-I8O-d4ywZCqwZ1uaa?dl=0"}, {"id": "conf/sigsoft/Sulun19", "title": "Suggesting reviewers of software artifacts using traceability graphs.", "authors": ["Emre S\u00fcl\u00fcn"], "DOIs": ["https://doi.org/10.1145/3338906.3342507"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTDuring the lifecycle of a software project, software artifacts constantly change. A change should be peer-reviewed to ensure the software quality. To maximize the benefit of review, the reviewer(s) should be chosen appropriately. However, choosing the right reviewer(s) might not be trivial especially in large projects. Researchers developed different methods to recommend reviewers. In this study, we introduce a novel approach for reviewer recommendation problem. Our approach utilizes the traceability graph of a software project and assigns a know-about score to each developer, then recommends the developers who have the maximum know-about score for an artifact. We tested our approach on an open source project and achieved top-3 recall of 0.85 with an MRR (mean reciprocal ranking) of 0.73."}, {"id": "conf/sigsoft/Radavelli19", "title": "Using software testing to repair models.", "authors": ["Marco Radavelli"], "DOIs": ["https://doi.org/10.1145/3338906.3342508"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTSoftware testing is an important phase in the software development process, aiming at locating faults in artifacts, and achieve some confidence that the software behaves according to specification. There exists many software testing techniques applied to debugging, fault-localization, and repair of code, however, to the best of our knowledge, the application of software testing to locating faults in models and automatically repair them, is still an open issue. We present a project that investigates the use of software testing methods to automatically repair model artifacts, to support engineers in maintaining them consistent with the implementation and specification. We describe the research approach, the structure of the devised test-driven repair processes, present results in the cases of combinatorial models and feature models, and finally discuss future work of applying testing to repair models for other scenarios, such as timed automata."}, {"id": "conf/sigsoft/Davis19", "title": "Rethinking Regex engines to address ReDoS.", "authors": ["James C. Davis"], "DOIs": ["https://doi.org/10.1145/3338906.3342509"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTRegular expressions (regexes) are a powerful string manipulation tool. Unfortunately, in programming languages like Python, Java, and JavaScript, they are unnecessarily dangerous, implemented with worst-case exponential matching behavior. This high time complexity exposes software services to regular expression denial of service (ReDoS) attacks.  We propose a data-driven redesign of regex engines, to reflect how regexes are used and what they typically look like. We report that about 95% of regexes in popular programming languages can be evaluated in linear time. The regex engine is a fundamental component of a programming language, and any changes risk introducing compatibility problems. We believe a full redesign is therefore impractical, and so we describe how the vast majority of regex matches can be made linear-time with minor, not major, changes to existing algorithms. Our prototype shows that on a kernel of the regex language, we can trade space for time to make regex matches safe"}, {"id": "conf/sigsoft/Sun19", "title": "Context-aware test case adaptation.", "authors": ["Peiyi Sun"], "DOIs": ["https://doi.org/10.1145/3338906.3342510"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTDuring software evolution, both production code and test cases evolve frequently. To assure software quality, test cases should evolve in time. However, test case evolution is usually delayed and error-prone. To facilitate this process, this paper proposes a context-aware test case adaptation approach (CAT), which first identifies and generalizes test case adaptation patterns, and then applies these patterns to automatically evolve test cases by analyzing their context. We conducted a preliminary study on three open-source projects and found that CAT correctly adapts 71.91% of test cases."}, {"id": "conf/sigsoft/Gizzatullina19", "title": "Empirical study of customer communication problem in agile requirements engineering.", "authors": ["Ilyuza Gizzatullina"], "DOIs": ["https://doi.org/10.1145/3338906.3342511"], "tag": ["Student Research Competition"], "abstract": "ABSTRACTAs Agile principles and values become an integral part of the soft-ware development culture, development processes experience significant changes. Requirements engineering, an individual phase occurring at the beginning of the traditional development, is distributed between various activities according to agile. However, how customer communication related problems are solved within the context of agile requirements engineering (RE)? Empirical study of that problem is done using 2 methods: systematic literature review and semi-structured interviews. Problems related to customer communication in agile RE are revealed and composed into patterns. Patterns are to be supplemented with the solutions in the further research."}]}, "kbse/ase": {"2017": [{"id": "conf/kbse/Holzmann17", "title": "Cobra - an interactive static code analyzer.", "authors": ["Gerard J. Holzmann"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115610", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115610", "http://dl.acm.org/citation.cfm?id=3155564"], "tag": ["Keynotes"], "abstract": "Sadly we know that virtually all software of any significance has residual errors. Some of those errors can be traced back to requirements flaws or faulty design assumptions; others are just plain coding mistakes. Static analyzers have become quite good at spotting these types of errors, but they don't scale very well. If, for instance, you need to check a code base of a few million lines you better be prepared to wait for the result; sometimes hours. Eyeballing a large code base to find flaws is clearly not an option, so what is missing is a static analysis capability that can be used to answer common types of queries interactively, even for large code bases. I will describe the design and use of such a tool in this talk."}, {"id": "conf/kbse/Han17", "title": "Mining structures from massive text data: will it help software engineering?", "authors": ["Jiawei Han"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115611", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115611", "http://dl.acm.org/citation.cfm?id=3155565"], "tag": ["Keynotes"], "abstract": "The real-world big data are largely unstructured, interconnected text data. One of the grand challenges is to turn such massive unstructured text data into structured, actionable knowledge. We propose a text mining approach that requires only distant or minimal supervision but relies on massive text data. We show quality phrases can be mined from such massive text data, types can be extracted from massive text data with distant supervision, and entities/attributes/values can be discovered by meta-path directed pattern discovery. We show text-rich and structure-rich networks can be constructed from massive unstructured data. Finally, we speculate whether such a paradigm could be useful for turning massive software repositories into multi-dimensional structures to help searching and mining software repositories."}, {"id": "conf/kbse/Deursen17", "title": "Software engineering without borders.", "authors": ["Arie van Deursen"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115612", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115612", "http://dl.acm.org/citation.cfm?id=3155566"], "tag": ["Keynotes"], "abstract": "DevOps approaches software engineering by advocating the removal of borders between development and operations. DevOps emphasizes operational resilience, continuous feedback from operations back to development, and rapid deployment of features developed. In this talk we will look at selected (automation) aspects related to DevOps, based on our collaborations with various industrial partners. For example, we will explore (automated) methods for analyzing log data to support deployments and monitor REST API integrations, (search-based) test input generation for reproducing crashes and testing complex database queries, and zero downtime database schema evolution and deployment. We will close by looking at borders beyond those between development and operations, in order to see whether there are other borders we need to remove in order to strengthen the impact of software engineering research."}, {"id": "conf/kbse/ZhangLLC17", "title": "Systematically testing background services of mobile apps.", "authors": ["Li Lyna Zhang", "Chieh-Jan Mike Liang", "Yunxin Liu", "Enhong Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115613", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115613", "http://dl.acm.org/citation.cfm?id=3155568"], "tag": ["Test Generation"], "abstract": "Contrary to popular belief, mobile apps can spend a large fraction of time running \"hidden\" as background services. And, bugs in services can translate into crashes, energy depletion, device slow-down, etc. Unfortunately, without necessary testing tools, developers can only resort to telemetries from user devices in the wild. To this end, Snowdrop is a testing framework that systematically identifies and automates background services in Android apps. Snowdrop realizes a service-oriented approach that does not assume all inter-component communication messages are explicitly coded in the app bytecode. Furthermore, to improve the completeness of test inputs generated, Snowdrop infers field values by exploiting the similarity in how developers name variables. We evaluate Snowdrop by testing 848 commercially available mobile apps. Empirical results show that Snowdrop can achieve 20.91% more code path coverage than pathwise test input generators, and 64.11% more coverage than random test input generators."}, {"id": "conf/kbse/MaoHJ17", "title": "Crowd intelligence enhances automated mobile testing.", "authors": ["Ke Mao", "Mark Harman", "Yue Jia"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115614", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115614", "http://dl.acm.org/citation.cfm?id=3155569"], "tag": ["Test Generation"], "abstract": "We show that information extracted from crowd-based testing can enhance automated mobile testing. We introduce Polariz, which generates replicable test scripts from crowd-based testing, extracting cross-app `motif' events: automatically-inferred reusable higher-level event sequences composed of lower-level observed event actions. Our empirical study used 434 crowd workers from Mechanical Turk to perform 1,350 testing tasks on 9 popular Google Play apps, each with at least 1 million user installs. The findings reveal that the crowd was able to achieve 60.5% unique activity coverage and proved to be complementary to automated search-based testing in 5 out of the 9 subjects studied. Our leave-one-out evaluation demonstrates that coverage attainment can be improved (6 out of 9 cases, with no disimprovement on the remaining 3) by combining crowd-based and search-based testing."}, {"id": "conf/kbse/SongQH17", "title": "EHBDroid: beyond GUI testing for Android applications.", "authors": ["Wei Song", "Xiangxing Qian", "Jeff Huang"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115615", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115615", "http://dl.acm.org/citation.cfm?id=3155570"], "tag": ["Test Generation"], "abstract": "With the prevalence of Android-based mobile devices, automated testing for Android apps has received increasing attention. However, owing to the large variety of events that Android supports, test input generation is a challenging task. In this paper, we present a novel approach and an open source tool called EHBDroid for testing Android apps. In contrast to conventional GUI testing approaches, a key novelty of EHBDroid is that it does not generate events from the GUI, but directly invokes callbacks of event handlers. By doing so, EHBDroid can efficiently simulate a large number of events that are difficult to generate by traditional UI-based approaches. We have evaluated EHBDroid on a collection of 35 real-world large-scale Android apps and compared its performance with two state-of-the-art UI-based approaches, Monkey and Dynodroid. Our experimental results show that EHBDroid is significantly more effective and efficient than Monkey and Dynodroid: in a much shorter time, EHBDroid achieves as much as 22.3% higher statement coverage (11.1% on average) than the other two approaches, and found 12 bugs in these benchmarks, including 5 new bugs that the other two failed to find."}, {"id": "conf/kbse/ZhangCTCBL17", "title": "Sketch-guided GUI test generation for mobile applications.", "authors": ["Chucheng Zhang", "Haoliang Cheng", "Enyi Tang", "Xin Chen", "Lei Bu", "Xuandong Li"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115616", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115616", "http://dl.acm.org/citation.cfm?id=3155571"], "tag": ["Test Generation"], "abstract": "Mobile applications with complex GUIs are very popular today. However, generating test cases for these applications is often tedious professional work. On the one hand, manually designing and writing elaborate GUI scripts requires expertise. On the other hand, generating GUI scripts with record and playback techniques usually depends on repetitive work that testers need to interact with the application over and over again, because only one path is recorded in an execution. Automatic GUI testing focuses on exploring combinations of GUI events. As the number of combinations is huge, it is still necessary to introduce a test interface for testers to reduce its search space. This paper presents a sketch-guided GUI test generation approach for testing mobile applications, which provides a simple but expressive interface for testers to specify their testing purposes. Testers just need to draw a few simple strokes on the screenshots. Then our approach translates the strokes to a testing model and initiates a model-based automatic GUI testing. We evaluate our sketch-guided approach on a few real-world Android applications collected from the literature. The results show that our approach can achieve higher coverage than existing automatic GUI testing techniques with just 10-minute sketching for an application."}, {"id": "conf/kbse/ToffolaSP17", "title": "Saying 'hi!' is not enough: mining inputs for effective test generation.", "authors": ["Luca Della Toffola", "Cristian-Alexandru Staicu", "Michael Pradel"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115617", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115617", "http://dl.acm.org/citation.cfm?id=3155572"], "tag": ["Test Generation"], "abstract": "Automatically generating unit tests is a powerful approach to exercise complex software. Unfortunately, current techniques often fail to provide relevant input values, such as strings that bypass domain-specific sanity checks. As a result, state-of-the-art techniques are effective for generic classes, such as collections, but less successful for domain-specific software. This paper presents TestMiner, the first technique for mining a corpus of existing tests for input values to be used by test generators for effectively testing software not in the corpus. The main idea is to extract literals from thousands of tests and to adapt information retrieval techniques to find values suitable for a particular domain. Evaluating the approach with 40 Java classes from 18 different projects shows that TestMiner improves test coverage by 21% over an existing test generator. The approach can be integrated into various test generators in a straightforward way, increasing their effectiveness on previously difficult-to-test classes."}, {"id": "conf/kbse/GodefroidPS17", "title": "Learn&Fuzz: machine learning for input fuzzing.", "authors": ["Patrice Godefroid", "Hila Peleg", "Rishabh Singh"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115618", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115618", "http://dl.acm.org/citation.cfm?id=3155573"], "tag": ["Test Generation"], "abstract": "Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss and measure the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn&fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs."}, {"id": "conf/kbse/ZhaoSZFV17", "title": "The impact of continuous integration on other software development practices: a large-scale empirical study.", "authors": ["Yangyang Zhao", "Alexander Serebrenik", "Yuming Zhou", "Vladimir Filkov", "Bogdan Vasilescu"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115619", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115619", "http://dl.acm.org/citation.cfm?id=3155575"], "tag": ["Developersz Practice and Behavior"], "abstract": "Continuous Integration (CI) has become a disruptive innovation in software development: with proper tool support and adoption, positive effects have been demonstrated for pull request throughput and scaling up of project sizes. As any other innovation, adopting CI implies adapting existing practices in order to take full advantage of its potential, and \"best practices\" to that end have been proposed. Here we study the adaptation and evolution of code writing and submission, issue and pull request closing, and testing practices as TRAVIS CI is adopted by hundreds of established projects on GITHUB. To help essentialize the quantitative results, we also survey a sample of GITHUB developers about their experiences with adopting TRAVIS CI. Our findings suggest a more nuanced picture of how GITHUB teams are adapting to, and benefiting from, continuous integration technology than suggested by prior work."}, {"id": "conf/kbse/KavalerSHAF17", "title": "Perceived language complexity in GitHub issue discussions and their effect on issue resolution.", "authors": ["David Kavaler", "Sasha Sirovica", "Vincent Hellendoorn", "Ra\u00fal Aranovich", "Vladimir Filkov"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115620", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115620", "http://dl.acm.org/citation.cfm?id=3155576"], "tag": ["Developersz Practice and Behavior"], "abstract": "Modern software development is increasingly collaborative. Open Source Software (OSS) are the bellwether; they support dynamic teams, with tools for code sharing, communication, and issue tracking. The success of an OSS project is reliant on team communication. E.g., in issue discussions, individuals rely on rhetoric to argue their position, but also maintain technical relevancy. Rhetoric and technical language are on opposite ends of a language complexity spectrum: the former is stylistically natural; the latter is terse and concise. Issue discussions embody this duality, as developers use rhetoric to describe technical issues. The style mix in any discussion can define group culture and affect performance, e.g., issue resolution times may be longer if discussion is imprecise. Using GitHub, we studied issue discussions to understand whether project-specific language differences exist, and to what extent users conform to a language norm. We built project-specific and overall GitHub language models to study the effect of perceived language complexity on multiple responses. We find that experienced users conform to project-specific language norms, popular individuals use overall GitHub language rather than project-specific language, and conformance to project-specific language norms reduces issue resolution times. We also provide a tool to calculate project-specific perceived language complexity."}, {"id": "conf/kbse/MirhosseiniP17", "title": "Can automated pull requests encourage software developers to upgrade out-of-date dependencies?", "authors": ["Samim Mirhosseini", "Chris Parnin"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115621", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115621", "http://dl.acm.org/citation.cfm?id=3155577"], "tag": ["Developersz Practice and Behavior"], "abstract": "Developers neglect to update legacy software dependencies, resulting in buggy and insecure software. One explanation for this neglect is the difficulty of constantly checking for the availability of new software updates, verifying their safety, and addressing any migration efforts needed when upgrading a dependency. Emerging tools attempt to address this problem by introducing automated pull requests and project badges to inform the developer of stale dependencies. To understand whether these tools actually help developers, we analyzed 7,470 GitHub projects that used these notification mechanisms to identify any change in upgrade behavior. Our results find that, on average, projects that use pull request notifications upgraded 1.6\u00d7 as often as projects that did not use any tools. Badge notifications were slightly less effective: users upgraded 1.4\u00d7 more frequently. Unfortunately, although pull request notifications are useful, developers are often overwhelmed by notifications: only a third of pull requests were actually merged. Through a survey, 62 developers indicated that their most significant concerns are breaking changes, understanding the implications of changes, and migration effort. The implications of our work suggests ways in which notifications can be improved to better align with developers' expectations and the need for new mechanisms to reduce notification fatigue and improve confidence in automated pull requests."}, {"id": "conf/kbse/PaixaoKHRH17", "title": "Are developers aware of the architectural impact of their changes?", "authors": ["Matheus Paix\u00e3o", "Jens Krinke", "DongGyun Han", "Chaiyong Ragkhitwetsagul", "Mark Harman"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115622", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115622", "http://dl.acm.org/citation.cfm?id=3155578"], "tag": ["Developersz Practice and Behavior"], "abstract": "Although considered one of the most important decisions in a software development lifecycle, empirical evidence on how developers perform and perceive architectural changes is still scarce. Given the large implications of architectural decisions, we do not know whether developers are aware of their changes' impact on the software's architecture, whether awareness leads to better changes, and whether automatically making developers aware would prevent degradation. Therefore, we use code review data of 4 open source systems to investigate the intent and awareness of developers when performing changes. We extracted 8,900 reviews for which the commits are available. 2,152 of the commits have changes in their computed architectural metrics, and 338 present significant changes to the architecture. We manually inspected all reviews for commits with significant changes and found that only in 38% of the time developers are discussing the impact of their changes on the architectural structure, suggesting a lack of awareness. Finally, we observed that developers tend to be more aware of the architectural impact of their changes when the architectural structure is improved, suggesting that developers should be automatically made aware when their changes degrade the architectural structure."}, {"id": "conf/kbse/AhmedBIR17", "title": "SentiCR: a customized sentiment analysis tool for code review interactions.", "authors": ["Toufique Ahmed", "Amiangshu Bosu", "Anindya Iqbal", "Shahram Rahimi"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115623", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115623", "http://dl.acm.org/citation.cfm?id=3155579"], "tag": ["Developersz Practice and Behavior"], "abstract": "Sentiment Analysis tools, developed for analyzing social media text or product reviews, work poorly on a Software Engineering (SE) dataset. Since prior studies have found developers expressing sentiments during various SE activities, there is a need for a customized sentiment analysis tool for the SE domain. On this goal, we manually labeled 2000 review comments to build a training dataset and used our dataset to evaluate seven popular sentiment analysis tools. The poor performances of the existing sentiment analysis tools motivated us to build SentiCR, a sentiment analysis tool especially designed for code review comments. We evaluated SentiCR using one hundred 10-fold cross-validations of eight supervised learning algorithms. We found a model, trained using the Gradient Boosting Tree (GBT) algorithm, providing the highest mean accuracy (83%), the highest mean precision (67.8%), and the highest mean recall (58.4%) in identifying negative review comments."}, {"id": "conf/kbse/RatolR17", "title": "Detecting fragile comments.", "authors": ["Inderjot Kaur Ratol", "Martin P. Robillard"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115624", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115624", "http://dl.acm.org/citation.cfm?id=3155581"], "tag": ["Documentation"], "abstract": "Refactoring is a common software development practice and many simple refactorings can be performed automatically by tools. Identifier renaming is a widely performed refactoring activity. With tool support, rename refactorings can rely on the program structure to ensure correctness of the code transformation. Unfortunately, the textual references to the renamed identifier present in the unstructured comment text cannot be formally detected through the syntax of the language, and are thus fragile with respect to identifier renaming. We designed a new rule-based approach to detect fragile comments. Our approach, called Fraco, takes into account the type of identifier, its morphology, the scope of the identifier and the location of comments. We evaluated the approach by comparing its precision and recall against hand-annotated benchmarks created for six target Java systems, and compared the results against the performance of Eclipse's automated in-comment identifier replacement feature. Fraco performed with near-optimal precision and recall on most components of our evaluation data set, and generally outperformed the baseline Eclipse feature. As part of our evaluation, we also noted that more than half of the total number of identifiers in our data set had fragile comments after renaming, which further motivates the need for research on automatic comment refactoring."}, {"id": "conf/kbse/LinZZX17", "title": "Improving software text retrieval using conceptual knowledge in source code.", "authors": ["Zeqi Lin", "Yanzhen Zou", "Junfeng Zhao", "Bing Xie"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115625", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115625", "http://dl.acm.org/citation.cfm?id=3155582"], "tag": ["Documentation"], "abstract": "A large software project usually has lots of various textual learning resources about its API, such as tutorials, mailing lists, user forums, etc. Text retrieval technology allows developers to search these API learning resources for related documents using free-text queries, but it suffers from the lexical gap between search queries and documents. In this paper, we propose a novel approach for improving the retrieval of API learning resources through leveraging software-specific conceptual knowledge in software source code. The basic idea behind this approach is that the semantic relatedness between queries and documents could be measured according to software-specific concepts involved in them, and software source code contains a large amount of software-specific conceptual knowledge. In detail, firstly we extract an API graph from software source code and use it as software-specific conceptual knowledge. Then we discover API entities involved in queries and documents, and infer semantic document relatedness through analyzing structural relationships between these API entities. We evaluate our approach in three popular open source software projects. Comparing to the state-of-the-art text retrieval approaches, our approach lead to at least 13.77% improvement with respect to mean average precision (MAP)."}, {"id": "conf/kbse/JiangAM17", "title": "Automatically generating commit messages from diffs using neural machine translation.", "authors": ["Siyuan Jiang", "Ameer Armaly", "Collin McMillan"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115626", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115626", "http://dl.acm.org/citation.cfm?id=3155583"], "tag": ["Documentation"], "abstract": "Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically \"translate\" diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead."}, {"id": "conf/kbse/SunCWB17", "title": "Improving missing issue-commit link recovery using positive and unlabeled data.", "authors": ["Yan Sun", "Celia Chen", "Qing Wang", "Barry W. Boehm"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115627", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115627", "http://dl.acm.org/citation.cfm?id=3155584"], "tag": ["Documentation"], "abstract": "Links between issue reports and corresponding fix commits are widely used in software maintenance. The quality of links directly affects maintenance costs. Currently, such links are mainly maintained by error-prone manual efforts, which may result in missing links. To tackle this problem, automatic link recovery approaches have been proposed by building traditional classifiers with positive and negative links. However, these traditional classifiers may not perform well due to the inherent characteristics of missing links. Positive links, which can be used to build link recovery model, are quite limited as the result of missing links. Since the construction of negative links depends on the number of positive links in many existing approaches, the available negative links also become restricted. In this paper, we point out that it is better to consider the missing link problem as a model learning problem by using positive and unlabeled data, rather than the construction of traditional classifier. We propose PULink, an approach that constructs the link recovery model with positive and unlabeled links. Our experiment results show that compared to existing state-of-the-art technologies built on traditional classifier, PULink can achieve competitive performance by utilizing only 70% positive links that are used in those approaches."}, {"id": "conf/kbse/TianTSL17", "title": "APIBot: question answering bot for API documentation.", "authors": ["Yuan Tian", "Ferdian Thung", "Abhishek Sharma", "David Lo"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115628", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115628", "http://dl.acm.org/citation.cfm?id=3155585"], "tag": ["Documentation"], "abstract": "As the carrier of Application Programming Interfaces (APIs) knowledge, API documentation plays a crucial role in how developers learn and use an API. It is also a valuable information resource for answering API-related questions, especially when developers cannot find reliable answers to their questions online/offline. However, finding answers to API-related questions from API documentation might not be easy because one may have to manually go through multiple pages before reaching the relevant page, and then read and understand the information inside the relevant page to figure out the answers. To deal with this challenge, we develop APIBot, a bot that can answer API questions given API documentation as an input. APIBot is built on top of SiriusQA, the QA system from Sirius, a state of the art intelligent personal assistant. To make SiriusQA work well under software engineering scenario, we make several modifications over SiriusQA by injecting domain specific knowledge. We evaluate APIBot on 92 API questions, answers of which are known to be present in Java 8 documentation. Our experiment shows that APIBot can achieve a Hit@5 score of 0.706."}, {"id": "conf/kbse/UddinK17", "title": "Automatic summarization of API reviews.", "authors": ["Gias Uddin", "Foutse Khomh"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115629", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115629", "http://dl.acm.org/citation.cfm?id=3155586"], "tag": ["Documentation"], "abstract": "With the proliferation of online developer forums as informal documentation, developers often share their opinions about the APIs they use. However, given the plethora of opinions available for an API in various online developer forums, it can be challenging for a developer to make informed decisions about the APIs. While automatic summarization of opinions have been explored for other domains (e.g., cameras, cars), we found little research that investigates the benefits of summaries of public API reviews. In this paper, we present two algorithms (statistical and aspect-based) to summarize opinions about APIs. To investigate the usefulness of the techniques, we developed, Opiner, an online opinion summarization engine that presents summaries of opinions using both our proposed techniques and existing six off-the-shelf techniques. We investigated the usefulness of Opiner using two case studies, both involving professional software engineers. We found that developers were interested to use our proposed summaries much more frequently than other summaries (daily vs once a year) and that while combined with Stack Overflow, Opiner helped developers to make the right decision with more accuracy and confidence and in less time."}, {"id": "conf/kbse/CelikPG17", "title": "iCoq: regression proof selection for large-scale verification projects.", "authors": ["Ahmet \u00c7elik", "Karl Palmskog", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115630", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115630", "http://dl.acm.org/citation.cfm?id=3155588"], "tag": ["Formal Verification"], "abstract": "Proof assistants such as Coq are used to construct and check formal proofs in many large-scale verification projects. As proofs grow in number and size, the need for tool support to quickly find failing proofs after revising a project increases. We present a technique for large-scale regression proof selection, suitable for use in continuous integration services, e.g., Travis CI. We instantiate the technique in a tool dubbed ICOQ. ICOQ tracks fine-grained dependencies between Coq definitions, propositions, and proofs, and only checks those proofs affected by changes between two revisions. ICOQ additionally saves time by ignoring changes with no impact on semantics. We applied ICOQ to track dependencies across many revisions in several large Coq projects and measured the time savings compared to proof checking from scratch and when using Coq's timestamp-based toolchain for incremental checking. Our results show that proof checking with ICOQ is up to 10 times faster than the former and up to 3 times faster than the latter."}, {"id": "conf/kbse/TianDDO17", "title": "More effective interpolations in software model checking.", "authors": ["Cong Tian", "Zhao Duan", "Zhenhua Duan", "C.-H. Luke Ong"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115631", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115631", "http://dl.acm.org/citation.cfm?id=3155589"], "tag": ["Formal Verification"], "abstract": "An approach to CEGAR-based model checking which has proved to be successful on large models employs Craig interpolation to efficiently construct parsimonious abstractions. Following this design, we introduce new applications, universal safety interpolant and existential error interpolant, of Craig interpolation that can systematically reduce the program state space to be explored for safety verification. Whenever the universal safety interpolant is implied by the current path, all paths emanating from that location are guaranteed to be safe. Dually whenever the existential error interpolant is implied by the current path, there is guaranteed to be an unsafe path from the location. We show how these interpolants are computed and applied in safety verification. We have implemented our approach in a tool named InterpChecker by building on an open source software model checker. Experiments on a large number of benchmark programs show that both the interpolations and the auxiliary optimization strategies are effective in improving scalability of software model checking."}, {"id": "conf/kbse/GhassabaniGWHW17", "title": "Proof-based coverage metrics for formal verification.", "authors": ["Elaheh Ghassabani", "Andrew Gacek", "Michael W. Whalen", "Mats Per Erik Heimdahl", "Lucas G. Wagner"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115632", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115632", "http://dl.acm.org/citation.cfm?id=3155590"], "tag": ["Formal Verification"], "abstract": "When using formal verification on critical software, an important question involves whether we have we specified enough properties for a given implementation model. To address this question, coverage metrics for property-based formal verification have been proposed. Existing metrics are usually based on mutation, where the implementation model is repeatedly modified and re-analyzed to determine whether mutant models are \"killed\" by the property set. These metrics tend to be very expensive to compute, as they involve many additional verification problems. This paper proposes an alternate family of metrics that can be computed using the recently introduced idea of Inductive Validity Cores (IVCs). IVCs determine a minimal set of model elements necessary to establish a proof. One of the proposed metrics is both rigorous and substantially cheaper to compute than mutation-based metrics. In addition, unlike the mutation-based techniques, the design elements marked as necessary by the metric are guaranteed to preserve provability. We demonstrate the metrics on a large corpus of examples."}, {"id": "conf/kbse/CastanoBGU17", "title": "Model checker execution reports.", "authors": ["Rodrigo Casta\u00f1o", "V\u00edctor A. Braberman", "Diego Garbervetsky", "Sebasti\u00e1n Uchitel"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115633", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115633", "http://dl.acm.org/citation.cfm?id=3155591"], "tag": ["Formal Verification"], "abstract": "Software model checking constitutes an undecidable problem and, as such, even an ideal tool will in some cases fail to give a conclusive answer. In practice, software model checkers fail often and usually do not provide any information on what was effectively checked. The purpose of this work is to provide a conceptual framing to extend software model checkers in a way that allows users to access information about incomplete checks. We characterize the information that model checkers themselves can provide, in terms of analyzed traces, i.e. sequences of statements, and safe canes, and present the notion of execution reports (ERs), which we also formalize. We instantiate these concepts for a family of techniques based on Abstract Reachability Trees and implement the approach using the software model checker CPAchecker. We evaluate our approach empirically and provide examples to illustrate the ERs produced and the information that can be extracted."}, {"id": "conf/kbse/SungKW17", "title": "Modular verification of interrupt-driven software.", "authors": ["Chungha Sung", "Markus Kusano", "Chao Wang"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115634", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115634", "http://dl.acm.org/citation.cfm?id=3155592"], "tag": ["Formal Verification"], "abstract": "Interrupts have been widely used in safety-critical computer systems to handle outside stimuli and interact with the hardware, but reasoning about interrupt-driven software remains a difficult task. Although a number of static verification techniques have been proposed for interrupt-driven software, they often rely on constructing a monolithic verification model. Furthermore, they do not precisely capture the complete execution semantics of interrupts such as nested invocations of interrupt handlers. To overcome these limitations, we propose an abstract interpretation framework for static verification of interrupt-driven software that first analyzes each interrupt handler in isolation as if it were a sequential program, and then propagates the result to other interrupt handlers. This iterative process continues until results from all interrupt handlers reach a fixed point. Since our method never constructs the global model, it avoids the up-front blowup in model construction that hampers existing, non-modular, verification techniques. We have evaluated our method on 35 interrupt-driven applications with a total of 22,541 lines of code. Our results show the method is able to quickly and more accurately analyze the behavior of interrupts."}, {"id": "conf/kbse/CorradiniFP0TV17", "title": "BProVe: a formal verification framework for business process models.", "authors": ["Flavio Corradini", "Fabrizio Fornari", "Andrea Polini", "Barbara Re", "Francesco Tiezzi", "Andrea Vandin"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115635", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115635", "http://dl.acm.org/citation.cfm?id=3155593"], "tag": ["Formal Verification"], "abstract": "Business Process Modelling has acquired increasing relevance in software development. Available notations, such as BPMN, permit to describe activities of complex organisations. On the one hand, this shortens the communication gap between domain experts and IT specialists. On the other hand, this permits to clarify the characteristics of software systems introduced to provide automatic support for such activities. Nevertheless, the lack of formal semantics hinders the automatic verification of relevant properties. This paper presents a novel verification framework for BPMN 2.0, called BProVe. It is based on an operational semantics, implemented using MAUDE, devised to make the verification general and effective. A complete tool chain, based on the Eclipse modelling environment, allows for rigorous modelling and analysis of Business Processes. The approach has been validated using more than one thousand models available on a publicly accessible repository. Besides showing the performance of BProVe, this validation demonstrates its practical benefits in identifying correctness issues in real models."}, {"id": "conf/kbse/ChenODL17", "title": "Static detection of asymptotic resource side-channel vulnerabilities in web applications.", "authors": ["Jia Chen", "Oswaldo Olivo", "Isil Dillig", "Calvin Lin"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115636", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115636", "http://dl.acm.org/citation.cfm?id=3155595"], "tag": ["Security"], "abstract": "Web applications can leak confidential user information due to the presence of unintended side-channel vulnerabilities in code. One particularly subtle class of side-channel vulnerabilities arises due to resource usage imbalances along different execution paths of a program. Such side-channel vulnerabilities are especially severe if the resource usage imbalance is asymptotic. This paper formalizes the notion of asymptotic resource side-channels and presents a lightweight static analysis algorithm for automatically detecting them. Based on these ideas, we have developed a tool called SCANNER that detects resource-related side-channel vulnerabilities in PHP applications. SCANNER has found 18 zero-day security vulnerabilities in 10 different web applications and reports only 2 false positives. The vulnerabilities uncovered by SCANNER can be exploited using cross-site search attacks to extract various kinds of confidential information, such as a user's medications or purchase history."}, {"id": "conf/kbse/WangKZAKLLMZE17", "title": "PAD: programming third-party web advertisement censorship.", "authors": ["Weihang Wang", "Yonghwi Kwon", "Yunhui Zheng", "Yousra Aafer", "I Luk Kim", "Wen-Chuan Lee", "Yingqi Liu", "Weijie Meng", "Xiangyu Zhang", "Patrick Eugster"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115637", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115637", "http://dl.acm.org/citation.cfm?id=3155596"], "tag": ["Security"], "abstract": "In the current online advertisement delivery, an ad slot on a publisher's website may go through multiple layers of bidding and reselling until the final ad content is delivered. The publishers have little control on the ads being displayed on their web pages. As a result, website visitors may suffer from unwanted ads such as malvertising, intrusive ads, and information disclosure ads. Unfortunately, the visitors often blame the publisher for their unpleasant experience and switch to competitor websites. In this paper, we propose a novel programming support system for ad delivery, called PAD, for publisher programmers, who specify their policies on regulating third-party ads shown on their websites. PAD features an expressive specification language and a novel persistent policy enforcement runtime that can self-install and self-protect throughout the entire ad delegation chain. It also provides an ad-specific memory protection scheme that prevents malvertising by corrupting malicious payloads. Our experiments show that PAD has negligible runtime overhead. It effectively suppresses a set of malvertising cases and unwanted ad behaviors reported in the real world, without affecting normal functionalities and regular ads."}, {"id": "conf/kbse/LeeHR17", "title": "All about activity injection: threats, semantics, and detection.", "authors": ["Sungho Lee", "Sungjae Hwang", "Sukyoung Ryu"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115638", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115638", "http://dl.acm.org/citation.cfm?id=3155597"], "tag": ["Security"], "abstract": "Android supports seamless user experience by maintaining activities from different apps in the same activity stack. While such close inter-app communication is essential in the Android framework, the powerful inter-app communication contains vulnerabilities that can inject malicious activities into a victim app's activity stack to hijack user interaction flows. In this paper, we demonstrate activity injection attacks with a simple malware, and formally specify the activity activation mechanism using operational semantics. Based on the operational semantics, we develop a static analysis tool, which analyzes Android apps to detect activity injection attacks. Our tool is fast enough to analyze real-world Android apps in 6 seconds on average, and our experiments found that 1,761 apps out of 129,756 real-world Android apps inject their activities into other apps' tasks."}, {"id": "conf/kbse/MathisASBZ17", "title": "Detecting information flow by mutating input data.", "authors": ["Bj\u00f6rn Mathis", "Vitalii Avdiienko", "Ezekiel O. Soremekun", "Marcel B\u00f6hme", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115639", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115639", "http://dl.acm.org/citation.cfm?id=3155598"], "tag": ["Security"], "abstract": "Analyzing information flow is central in assessing the security of applications. However, static and dynamic analyses of information flow are easily challenged by non-available or obscure code. We present a lightweight mutation-based analysis that systematically mutates dynamic values returned by sensitive sources to assess whether the mutation changes the values passed to sensitive sinks. If so, we found a flow between source and sink. In contrast to existing techniques, mutation-based flow analysis does not attempt to identify the specific path of the flow and is thus resilient to obfuscation. In its evaluation, our MUTAFLOW prototype for Android programs showed that mutation-based flow analysis is a lightweight yet effective complement to existing tools. Compared to the popular FlowDroid static analysis tool, MutaFlow requires less than 10% of source code lines but has similar accuracy; on 20 tested real-world apps, it is able to detect 75 flows that FlowDroid misses."}, {"id": "conf/kbse/HeCHSLYHYJF17", "title": "Automatically assessing crashes from heap overflows.", "authors": ["Liang He", "Yan Cai", "Hong Hu", "Purui Su", "Zhenkai Liang", "Yi Yang", "Huafeng Huang", "Jia Yan", "Xiangkun Jia", "Dengguo Feng"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115640", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115640", "http://dl.acm.org/citation.cfm?id=3155599"], "tag": ["Security"], "abstract": "Heap overflow is one of the most widely exploited vulnerabilities, with a large number of heap overflow instances reported every year. It is important to decide whether a crash caused by heap overflow can be turned into an exploit. Efficient and effective assessment of exploitability of crashes facilitates to identify severe vulnerabilities and thus prioritize resources. In this paper, we propose the first metrics to assess heap overflow crashes based on both the attack aspect and the feasibility aspect. We further present HCSIFTER, a novel solution to automatically assess the exploitability of heap overflow instances under our metrics. Given a heap-based crash, HCSIFTER accurately detects heap overflows through dynamic execution without any source code or debugging information. Then it uses several novel methods to extract program execution information needed to quantify the severity of the heap overflow using our metrics. We have implemented a prototype HCSIFTER and applied it to assess nine programs with heap overflow vulnerabilities. HCSIFTER successfully reports that five heap overflow vulnerabilities are highly exploitable and two overflow vulnerabilities are unlikely exploitable. It also gave quantitatively assessments for other two programs. On average, it only takes about two minutes to assess one heap overflow crash. The evaluation result demonstrates both effectiveness and efficiency of HC Sifter."}, {"id": "conf/kbse/RafiqDRBYSLCPN17", "title": "Learning to share: engineering adaptive decision-support for online social networks.", "authors": ["Yasmin Rafiq", "Luke Dickens", "Alessandra Russo", "Arosha K. Bandara", "Mu Yang", "Avelie Stuart", "Mark Levine", "Gul Calikli", "Blaine A. Price", "Bashar Nuseibeh"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115641", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115641", "http://dl.acm.org/citation.cfm?id=3155600"], "tag": ["Security"], "abstract": "Some online social networks (OSNs) allow users to define friendship-groups as reusable shortcuts for sharing information with multiple contacts. Posting exclusively to a friendship-group gives some privacy control, while supporting communication with (and within) this group. However, recipients of such posts may want to reuse content for their own social advantage, and can bypass existing controls by copy-pasting into a new post; this cross-posting poses privacy risks. This paper presents a learning to share approach that enables the incorporation of more nuanced privacy controls into OSNs. Specifically, we propose a reusable, adaptive software architecture that uses rigorous runtime analysis to help OSN users to make informed decisions about suitable audiences for their posts. This is achieved by supporting dynamic formation of recipient-groups that benefit social interactions while reducing privacy risks. We exemplify the use of our approach in the context of Facebook."}, {"id": "conf/kbse/HuangAPZT17", "title": "UI driven Android application reduction.", "authors": ["Jianjun Huang", "Yousra Aafer", "David Mitchel Perry", "Xiangyu Zhang", "Chen Tian"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115642", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115642", "http://dl.acm.org/citation.cfm?id=3155602"], "tag": ["Mobile Development"], "abstract": "While smartphones and mobile apps have been an integral part of our life, modern mobile apps tend to contain a lot of rarely used functionalities. For example, applications contain advertisements and offer extra features such as recommended news stories in weather apps. While these functionalities are not essential to an app, they nonetheless consume power, CPU cycles and bandwidth. In this paper, we design a UI driven approach that allows customizing an Android app by removing its unwanted functionalities. In particular, our technique displays the UI and allows the user to select elements denoting functionalities that she wants to remove. Using this information, our technique automatically removes all the code elements related to the selected functionalities, including all the relevant background tasks. The underlying analysis is a type system, in which each code element is tagged with a type indicating if it should be removed. From the UI hints, our technique infers types for all other code elements and reduces the app accordingly. We implement a prototype and evaluate it on 10 real-world Android apps. The results show that our approach can accurately discover the removable code elements and lead to substantial resource savings in the reduced apps."}, {"id": "conf/kbse/JiangWLC17", "title": "SimplyDroid: efficient event sequence simplification for Android application.", "authors": ["Bo Jiang", "Yuxuan Wu", "Teng Li", "W. K. Chan"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115643", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115643", "http://dl.acm.org/citation.cfm?id=3155603"], "tag": ["Mobile Development"], "abstract": "To ensure the quality of Android applications, many automatic test case generation techniques have been proposed. Among them, the Monkey fuzz testing tool and its variants are simple, effective and widely applicable. However, one major drawback of those Monkey tools is that they often generate many events in a failure-inducing input trace, which makes the follow-up debugging activities hard to apply. It is desirable to simplify or reduce the input event sequence while triggering the same failure. In this paper, we propose an efficient event trace representation and the SimplyDroid tool with three hierarchical delta-debugging algorithms each operating on this trace representation to simplify crash traces. We have evaluated SimplyDroid on a suite of real-life Android applications with 92 crash traces. The empirical result shows that our new algorithms in SimplyDroid are both efficient and effective in reducing these event traces."}, {"id": "conf/kbse/FazziniO17", "title": "Automated cross-platform inconsistency detection for mobile apps.", "authors": ["Mattia Fazzini", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115644", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115644", "http://dl.acm.org/citation.cfm?id=3155604"], "tag": ["Mobile Development"], "abstract": "Testing of Android apps is particularly challenging due to the fragmentation of the Android ecosystem in terms of both devices and operating system versions. Developers must in fact ensure not only that their apps behave as expected, but also that the apps' behavior is consistent across platforms. To support this task, we propose DiffDroid, a new technique that helps developers automatically find cross-platform inconsistencies (CPIs) in mobile apps. DiffDroid combines input generation and differential testing to compare the behavior of an app on different platforms and identify possible inconsistencies. Given an app, DiffDroid (1) generates test inputs for the app, (2) runs the app with these inputs on a reference device and builds a model of the app behavior, (3) runs the app with the same inputs on a set of other devices, and (4) compares the behavior of the app on these different devices with the model of its behavior on the reference device. We implemented DiFFDRoiD and performed an evaluation of our approach on 5 benchmarks and over 130 platforms. our results show that DiFFDRoiD can identify CPis on real apps efficiently and with a limited number of false positives. DiFFDRoiD and our experimental infrastructure are publicly available."}, {"id": "conf/kbse/WangW17", "title": "In-memory fuzzing for binary code similarity analysis.", "authors": ["Shuai Wang", "Dinghao Wu"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115645", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115645", "http://dl.acm.org/citation.cfm?id=3155606"], "tag": ["Binary Analysis"], "abstract": "Detecting similar functions in binary executables serves as a foundation for many binary code analysis and reuse tasks. By far, recognizing similar components in binary code remains a challenge. Existing research employs either static or dynamic approaches to capture program syntax or semantics-level features for comparison. However, there exist multiple design limitations in previous work, which result in relatively high cost, low accuracy and scalability, and thus severely impede their practical use. In this paper, we present a novel method that leverages in-memory fuzzing for binary code similarity analysis. Our prototype tool IMF-SIM applies in-memory fuzzing to launch analysis towards every function and collect traces of different kinds of program behaviors. The similarity score of two behavior traces is computed according to their longest common subsequence. To compare two functions, a feature vector is generated, whose elements are the similarity scores of the behavior trace-level comparisons. We train a machine learning model through labeled feature vectors; later, for a given feature vector by comparing two functions, the trained model gives a final score, representing the similarity score of the two functions. We evaluate IMF-SIM against binaries compiled by different compilers, optimizations, and commonly-used obfuscation methods, in total over one thousand binary executables. Our evaluation shows that IMF-SIM notably outperforms existing tools with higher accuracy and broader application scopes."}, {"id": "conf/kbse/RupprechtC0BLB17", "title": "DSIbin: identifying dynamic data structures in C/C++ binaries.", "authors": ["Thomas Rupprecht", "Xi Chen", "David H. White", "Jan H. Boockmann", "Gerald L\u00fcttgen", "Herbert Bos"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115646", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115646", "http://dl.acm.org/citation.cfm?id=3155607"], "tag": ["Binary Analysis"], "abstract": "Reverse engineering binary code is notoriously difficult and, especially, understanding a binary's dynamic data structures. Existing data structure analyzers are limited wrt. program comprehension: they do not detect complex structures such as skip lists, or lists running through nodes of different types such as in the Linux kernel's cyclic doubly-linked list. They also do not reveal complex parent-child relationships between structures. The tool DSI remedies these shortcomings but requires source code, where type information on heap nodes is available. We present DSIbin, a combination of DSI and the type excavator Howard for the inspection of C/C++ binaries. While a naive combination already improves upon related work, its precision is limited because Howard's inferred types are often too coarse. To address this we auto-generate candidates of refined types based on speculative nested-struct detection and type merging; the plausibility of these hypotheses is then validated by DSI. We demonstrate via benchmarking that DSIbin detects data structures with high precision."}, {"id": "conf/kbse/KargenS17", "title": "Towards robust instruction-level trace alignment of binary code.", "authors": ["Ulf Karg\u00e9n", "Nahid Shahmehri"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115647", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115647", "http://dl.acm.org/citation.cfm?id=3155608"], "tag": ["Binary Analysis"], "abstract": "Program trace alignment is the process of establishing a correspondence between dynamic instruction instances in executions of two semantically similar but syntactically different programs. In this paper we present what is, to the best of our knowledge, the first method capable of aligning realistically long execution traces of real programs. To maximize generality, our method works entirely on the machine code level, i.e. it does not require access to source code. Moreover, the method is based entirely on dynamic analysis, which avoids the many challenges associated with static analysis of binary code, and which additionally makes our approach inherently resilient to e.g. static code obfuscation. Therefore, we believe that our trace alignment method could prove to be a useful aid in many program analysis tasks, such as debugging, reverse-engineering, investigating plagiarism, and malware analysis. We empirically evaluate our method on 11 popular Linux programs, and show that it is capable of producing meaningful alignments in the presence of various code transformations such as optimization or obfuscation, and that it easily scales to traces with tens of millions of instructions."}, {"id": "conf/kbse/KimFJJOLC17", "title": "Testing intermediate representations for binary analysis.", "authors": ["Soomin Kim", "Markus Faerevaag", "Minkyu Jung", "SeungIl Jung", "DongYeop Oh", "JongHyup Lee", "Sang Kil Cha"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115648", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115648", "http://dl.acm.org/citation.cfm?id=3155609"], "tag": ["Binary Analysis"], "abstract": "Binary lifting, which is to translate a binary executable to a high-level intermediate representation, is a primary step in binary analysis. Despite its importance, there are only few existing approaches to testing the correctness of binary lifters. Furthermore, the existing approaches suffer from low test coverage, because they largely depend on random test case generation. In this paper, we present the design and implementation of the first systematic approach to testing binary lifters. We have evaluated the proposed system on 3 state-of-the-art binary lifters, and found 24 previously unknown semantic bugs. Our result demonstrates that writing a precise binary lifter is extremely difficult even for those heavily tested projects."}, {"id": "conf/kbse/GerrardD17", "title": "Comprehensive failure characterization.", "authors": ["Mitchell J. Gerrard", "Matthew B. Dwyer"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115649", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115649", "http://dl.acm.org/citation.cfm?id=3155611"], "tag": ["From Failures to Faults"], "abstract": "There is often more than one way to trigger a fault. Standard static and dynamic approaches focus on exhibiting a single witness for a failing execution. In this paper, we study the problem of computing a comprehensive characterization which safely bounds all failing program behavior while exhibiting a diversity of witnesses for those failures. This information can be used to facilitate software engineering tasks ranging from fault localization and repair to quantitative program analysis for reliability. Our approach combines the results of overapproximating and underapproximating static analyses in an alternating iterative framework to produce upper and lower bounds on the failing input space of a program, which we call a comprehensive failure characterization (CFC). We evaluated a prototype implementation of this alternating framework on a set of 168 C programs from the SV-COMP benchmarks, and the data indicate that it is possible to efficiently, accurately, and safely characterize failure spaces."}, {"id": "conf/kbse/CoppikSWS17", "title": "TrEKer: tracing error propagation in operating system kernels.", "authors": ["Nicolas Coppik", "Oliver Schwahn", "Stefan Winter", "Neeraj Suri"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115650", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115650", "http://dl.acm.org/citation.cfm?id=3155612"], "tag": ["From Failures to Faults"], "abstract": "Modern operating systems (OSs) consist of numerous interacting components, many of which are developed and maintained independently of one another. In monolithic systems, the boundaries of and interfaces between such components are not strictly enforced at runtime. Therefore, faults in individual components may directly affect other parts of the system in various ways. Software fault injection (SFI) is a testing technique to assess the resilience of a software system in the presence of faulty components. Unfortunately, SFI tests of OSs are inconclusive if they do not lead to observable failures, as corruptions of the internal software state may not be visible at its interfaces and, yet, affect the subsequent execution of the OS beyond the duration of the test. In this paper we present TrEKer, a fully automated approach for identifying how faulty OS components affect other parts of the system. TrEKer combines static and dynamic analyses to achieve efficient tracing on the granularity of memory accesses. We demonstrate TrEKer's ability to support SFI oracles by accurately tracing the effects of faults injected into three widely used Linux kernel modules."}, {"id": "conf/kbse/SulirP17", "title": "RuntimeSearch: Ctrl+F for a running program.", "authors": ["Mat\u00fas Sul\u00edr", "Jaroslav Porub\u00e4n"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115651", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115651", "http://dl.acm.org/citation.cfm?id=3155613"], "tag": ["From Failures to Faults"], "abstract": "Developers often try to find occurrences of a certain term in a software system. Traditionally, a text search is limited to static source code files. In this paper, we introduce a simple approach, RuntimeSearch, where the given term is searched in the values of all string expressions in a running program. When a match is found, the program is paused and its runtime properties can be explored with a traditional debugger. The feasibility and usefulness of RuntimeSearch is demonstrated on a medium-sized Java project."}, {"id": "conf/kbse/LinMXXSPLZD17", "title": "Mining implicit design templates for actionable code reuse.", "authors": ["Yun Lin", "Guozhu Meng", "Yinxing Xue", "Zhenchang Xing", "Jun Sun", "Xin Peng", "Yang Liu", "Wenyun Zhao", "Jin Song Dong"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115652", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115652", "http://dl.acm.org/citation.cfm?id=3155615"], "tag": ["Program Comprehension"], "abstract": "In this paper, we propose an approach to detecting project-specific recurring designs in code base and abstracting them into design templates as reuse opportunities. The mined templates allow programmers to make further customization for generating new code. The generated code involves the code skeleton of recurring design as well as the semi-implemented code bodies annotated with comments to remind programmers of necessary modification. We implemented our approach as an Eclipse plugin called MICoDe. We evaluated our approach with a reuse simulation experiment and a user study involving 16 participants. The results of our simulation experiment on 10 open source Java projects show that, to create a new similar feature with a design template, (1) on average 69% of the elements in the template can be reused and (2) on average 60% code of the new feature can be adopted from the template. Our user study further shows that, compared to the participants adopting the copy-paste-modify strategy, the ones using MICoDe are more effective to understand a big design picture and more efficient to accomplish the code reuse task."}, {"id": "conf/kbse/ChapmanWS17", "title": "Exploring regular expression comprehension.", "authors": ["Carl Chapman", "Peipei Wang", "Kathryn T. Stolee"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115653", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115653", "http://dl.acm.org/citation.cfm?id=3155616"], "tag": ["Program Comprehension"], "abstract": "The regular expression (regex) is a powerful tool employed in a large variety of software engineering tasks. However, prior work has shown that regexes can be very complex and that it could be difficult for developers to compose and understand them. This work seeks to identify code smells that impact comprehension. We conduct an empirical study on 42 pairs of behaviorally equivalent but syntactically different regexes using 180 participants and evaluate the understandability of various regex language features. We further analyze regexes in GitHub to find the community standards or the common usages of various features. We found that some regex expression representations are more understandable than others. For example, using a range (e.g., [0-9]) is often more understandable than a default character class (e.g., [\\d]). We also found that the DFA size of a regex significantly affects comprehension for the regexes studied. The larger the DFA of a regex (up to size eight), the more understandable it was. Finally, we identify smelly and non-smelly regex representations based on a combination of community standards and understandability metrics."}, {"id": "conf/kbse/ScalabrinoBVVPO17", "title": "Automatically assessing code understandability: how far are we?", "authors": ["Simone Scalabrino", "Gabriele Bavota", "Christopher Vendome", "Mario Linares V\u00e1squez", "Denys Poshyvanyk", "Rocco Oliveto"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115654", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115654", "http://dl.acm.org/citation.cfm?id=3155617"], "tag": ["Program Comprehension"], "abstract": "Program understanding plays a pivotal role in software maintenance and evolution: a deep understanding of code is the stepping stone for most software-related activities, such as bug fixing or testing. Being able to measure the understandability of a piece of code might help in estimating the effort required for a maintenance activity, in comparing the quality of alternative implementations, or even in predicting bugs. Unfortunately, there are no existing metrics specifically designed to assess the understandability of a given code snippet. In this paper, we perform a first step in this direction, by studying the extent to which several types of metrics computed on code, documentation, and developers correlate with code understandability. To perform such an investigation we ran a study with 46 participants who were asked to understand eight code snippets each. We collected a total of 324 evaluations aiming at assessing the perceived understandability, the actual level of understanding, and the time needed to understand a code snippet. Our results demonstrate that none of the (existing and new) metrics we considered is able to capture code understandability, not even the ones assumed to assess quality attributes strongly related with it, such as code readability and complexity."}, {"id": "conf/kbse/0001R17", "title": "Improved query reformulation for concept location using CodeRank and document structures.", "authors": ["Mohammad Masudur Rahman", "Chanchal K. Roy"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115655", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115655", "http://dl.acm.org/citation.cfm?id=3155618"], "tag": ["Program Comprehension"], "abstract": "During software maintenance, developers usually deal with a significant number of software change requests. As a part of this, they often formulate an initial query from the request texts, and then attempt to map the concepts discussed in the request to relevant source code locations in the software system (a.k.a., concept location). Unfortunately, studies suggest that they often perform poorly in choosing the right search terms for a change task. In this paper, we propose a novel technique-ACER-that takes an initial query, identifies appropriate search terms from the source code using a novel term weight-CodeRank, and then suggests effective reformulation to the initial query by exploiting the source document structures, query quality analysis and machine learning. Experiments with 1,675 baseline queries from eight subject systems report that our technique can improve 71% of the baseline queries which is highly promising. Comparison with five closely related existing techniques in query reformulation not only validates our empirical findings but also demonstrates the superiority of our technique."}, {"id": "conf/kbse/ShiCWLB17", "title": "Understanding feature requests by leveraging fuzzy method and linguistic analysis.", "authors": ["Lin Shi", "Celia Chen", "Qing Wang", "Shoubin Li", "Barry W. Boehm"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115656", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115656", "http://dl.acm.org/citation.cfm?id=3155619"], "tag": ["Program Comprehension"], "abstract": "In open software development environment, a large number of feature requests with mixed quality are often posted by stakeholders and usually managed in issue tracking systems. Thoroughly understanding and analyzing the real intents that feature requests imply is a labor-intensive and challenging task. In this paper, we introduce an approach to understand feature requests automatically. We generate a set of fuzzy rules based on natural language processing techniques that classify each sentence in feature requests into a set of categories: Intent, Explanation, Benefit, Drawback, Example and Trivia. Consequently, the feature requests can be automatically structured based on the classification results. We conduct experiments on 2,112 sentences taken from 602 feature requests of nine popular open source projects. The results show that our method can reach a high performance on classifying sentences from feature requests. Moreover, when applying fuzzy rules on machine learning methods, the performance can be improved significantly."}, {"id": "conf/kbse/QianP0YNZ17", "title": "O2O service composition with social collaboration.", "authors": ["Wenyi Qian", "Xin Peng", "Jun Sun", "Yijun Yu", "Bashar Nuseibeh", "Wenyun Zhao"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115657", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115657", "http://dl.acm.org/citation.cfm?id=3155621"], "tag": ["Models"], "abstract": "In Online-to-Offline (O2O) commerce, customer services may need to be composed from online and offline services. Such composition is challenging, as it requires effective selection of appropriate services that, in turn, support optimal combination of both online and offline services. In this paper, we address this challenge by proposing an approach to O2O service composition which combines offline route planning and social collaboration to optimize service selection. We frame general O2O service composition problems using timed automata and propose an optimization procedure that incorporates: (1) a Markov Chain Monte Carlo (MCMC) algorithm to stochastically select a concrete composite service, and (2) a model checking approach to searching for an optimal collaboration plan with the lowest cost given certain time constraint. Our procedure has been evaluated using the simulation of a rich scenario on effectiveness and scalability."}, {"id": "conf/kbse/DanielJSC17", "title": "Gremlin-ATL: a scalable model transformation framework.", "authors": ["Gwendal Daniel", "Fr\u00e9d\u00e9ric Jouault", "Gerson Suny\u00e9", "Jordi Cabot"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115658", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115658", "http://dl.acm.org/citation.cfm?id=3155622"], "tag": ["Models"], "abstract": "Industrial use of Model Driven Engineering techniques has emphasized the need for efficiently store, access, and transform very large models. While scalable persistence frameworks, typically based on some kind of NoSQL database, have been proposed to solve the model storage issue, the same level of performance improvement has not been achieved for the model transformation problem. Existing model transformation tools (such as the well-known ATL) often require the input models to be loaded in memory prior to the start of the transformation and are not optimized to benefit from lazy-loading mechanisms, mainly due to their dependency on current low-level APIs offered by the most popular modeling frameworks nowadays. In this paper we present Gremlin-ATL, a scalable and efficient model-to-model transformation framework that translates ATL transformations into Gremlin, a query language supported by several NoSQL databases. With Gremlin-ATL, the transformation is computed within the database itself, bypassing the modeling framework limitations and improving its performance both in terms of execution time and memory consumption. Tool support is available online."}, {"id": "conf/kbse/RahimiXCL17", "title": "Diagnosing assumption problems in safety-critical products.", "authors": ["Mona Rahimi", "Wandi Xiong", "Jane Cleland-Huang", "Robyn R. Lutz"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115659", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115659", "http://dl.acm.org/citation.cfm?id=3155623"], "tag": ["Models"], "abstract": "Problems with the correctness and completeness of environmental assumptions contribute to many accidents in safety-critical systems. The problem is exacerbated when products are modified in new releases or in new products of a product line. In such cases existing sets of environmental assumptions are often carried forward without sufficiently rigorous analysis. This paper describes a new technique that exploits the traceability required by many certifying bodies to reason about the likelihood that environmental assumptions are omitted or incorrectly retained in new products. An analysis of over 150 examples of environmental assumptions in historical systems informs the approach. In an evaluation on three safety-related product lines the approach caught all but one of the assumption-related problems. It also provided clearly defined steps for mitigating the identified issues. The contribution of the work is to arm the safety analyst with useful information for assessing the validity of environmental assumptions for a new product."}, {"id": "conf/kbse/IncertoTT17", "title": "Software performance self-adaptation through efficient model predictive control.", "authors": ["Emilio Incerto", "Mirco Tribastone", "Catia Trubiani"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115660", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115660", "http://dl.acm.org/citation.cfm?id=3155624"], "tag": ["Models"], "abstract": "A key challenge in software systems that are exposed to runtime variabilities, such as workload fluctuations and service degradation, is to continuously meet performance requirements. In this paper we present an approach that allows performance self-adaptation using a system model based on queuing networks (QNs), a well-assessed formalism for software performance engineering. Software engineers can select the adaptation knobs of a QN (routing probabilities, service rates, and concurrency level) and we automatically derive a Model Predictive Control (MPC) formulation suitable to continuously configure the selected knobs and track the desired performance requirements. Previous MPC approaches have two main limitations: i) high computational cost of the optimization, due to nonlinearity of the models; ii) focus on long-run performance metrics only, due to the lack of tractable representations of the QN's time-course evolution. As a consequence, these limitations allow adaptations with coarse time granularities, neglecting the system's transient behavior. Our MPC adaptation strategy is efficient since it is based on mixed integer programming, which uses a compact representation of a QN with ordinary differential equations. An extensive evaluation on an implementation of a load balancer demonstrates the effectiveness of the adaptation and compares it with traditional methods based on probabilistic model checking."}, {"id": "conf/kbse/JamshidiSVKPA17", "title": "Transfer learning for performance modeling of configurable systems: an exploratory analysis.", "authors": ["Pooyan Jamshidi", "Norbert Siegmund", "Miguel Velez", "Christian K\u00e4stner", "Akshay Patel", "Yuvraj Agarwal"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115661", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115661", "http://dl.acm.org/citation.cfm?id=3155625"], "tag": ["Models"], "abstract": "Modern software systems provide many configuration options which significantly influence their non-functional properties. To understand and predict the effect of configuration options, several sampling and learning strategies have been proposed, albeit often with significant cost to cover the highly dimensional configuration space. Recently, transfer learning has been applied to reduce the effort of constructing performance models by transferring knowledge about performance behavior across environments. While this line of research is promising to learn more accurate models at a lower cost, it is unclear why and when transfer learning works for performance modeling. To shed light on when it is beneficial to apply transfer learning, we conducted an empirical study on four popular software systems, varying software configurations and environmental conditions, such as hardware, workload, and software versions, to identify the key knowledge pieces that can be exploited for transfer learning. Our results show that in small environmental changes (e.g., homogeneous workload change), by applying a linear transformation to the performance model, we can understand the performance behavior of the target environment, while for severe environmental changes (e.g., drastic workload change) we can transfer only knowledge that makes sampling more efficient, e.g., by reducing the dimensionality of the configuration space."}, {"id": "conf/kbse/FrancoGR17", "title": "A comprehensive study of real-world numerical bug characteristics.", "authors": ["Anthony Di Franco", "Hui Guo", "Cindy Rubio-Gonz\u00e1lez"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115662", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115662", "http://dl.acm.org/citation.cfm?id=3155627"], "tag": ["Reliability and Bugs"], "abstract": "Numerical software is used in a wide variety of applications including safety-critical systems, which have stringent correctness requirements, and whose failures have catastrophic consequences that endanger human life. Numerical bugs are known to be particularly difficult to diagnose and fix, largely due to the use of approximate representations of numbers such as floating point. Understanding the characteristics of numerical bugs is the first step to combat them more effectively. In this paper, we present the first comprehensive study of real-world numerical bugs. Specifically, we identify and carefully examine 269 numerical bugs from five widely-used numerical software libraries: NumPy, SciPy, LAPACK, GNU Scientific Library, and Elemental. We propose a categorization of numerical bugs, and discuss their frequency, symptoms and fixes. Our study opens new directions in the areas of program analysis, testing, and automated program repair of numerical software, and provides a collection of real-world numerical bugs."}, {"id": "conf/kbse/WangDGGQYW17", "title": "A comprehensive study on real world concurrency bugs in Node.js.", "authors": ["Jie Wang", "Wensheng Dou", "Yu Gao", "Chushu Gao", "Feng Qin", "Kang Yin", "Jun Wei"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115663", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115663", "http://dl.acm.org/citation.cfm?id=3155628"], "tag": ["Reliability and Bugs"], "abstract": "Node.js becomes increasingly popular in building server-side JavaScript applications. It adopts an event-driven model, which supports asynchronous I/O and non-deterministic event processing. This asynchrony and non-determinism can introduce intricate concurrency bugs, and leads to unpredictable behaviors. An in-depth understanding of real world concurrency bugs in Node.js applications will significantly promote effective techniques in bug detection, testing and fixing for Node.js. In this paper, we present NodeCB, a comprehensive study on real world concurrency bugs in Node.js applications. Specifically, we have carefully studied 57 real bug cases from open-source Node.js applications, and have analyzed their bug characteristics, e.g., bug patterns and root causes, bug impacts, bug manifestation, and fix strategies. Through this study, we obtain several interesting findings, which may open up many new research directions in combating concurrency bugs in Node.js. For example, one finding is that two thirds of the bugs are caused by atomicity violation. However, due to lack of locks and transaction mechanism, Node.js cannot easily express and guarantee the atomic intention."}, {"id": "conf/kbse/HigoOK17", "title": "Generating simpler AST edit scripts by considering copy-and-paste.", "authors": ["Yoshiki Higo", "Akio Ohtani", "Shinji Kusumoto"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115664", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115664", "http://dl.acm.org/citation.cfm?id=3155630"], "tag": ["Source Code Analysis"], "abstract": "In software development, there are many situations in which developers need to understand given source code changes in detail. Until now, a variety of techniques have been proposed to support understanding source code changes. Tree-based differencing techniques are expected to have better understandability than text-based ones, which are widely used nowadays (e.g., diff in Unix). In this paper, we propose to consider copy-and-paste as a kind of editing action forming tree-based edit script, which is an editing sequence that transforms a tree to another one. Software developers often perform copy- and-paste when they are writing source code. Introducing copy- and-paste action into edit script contributes to not only making simpler (more easily understandable) edit scripts but also making edit scripts closer to developers' actual editing sequences. We conducted experiments on an open dataset. As a result, we confirmed that our technique made edit scripts shorter for 18% of the code changes with a little more computational time. For the other 82% code changes, our technique generated the same edit scripts as an existing technique. We also confirmed that our technique provided more helpful visualizations."}, {"id": "conf/kbse/LessenichAKSS17", "title": "Renaming and shifted code in structured merging: looking ahead for precision and performance.", "authors": ["Olaf Le\u00dfenich", "Sven Apel", "Christian K\u00e4stner", "Georg Seibt", "Janet Siegmund"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115665", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115665", "http://dl.acm.org/citation.cfm?id=3155631"], "tag": ["Source Code Analysis"], "abstract": "Diffing and merging of source-code artifacts is an essential task when integrating changes in software versions. While state-of-the-art line-based merge tools (e.g., git merge) are fast and independent of the programming language used, they have only a low precision. Recently, it has been shown that the precision of merging can be substantially improved by using a language-aware, structured approach that works on abstract syntax trees. But, precise structured merging is NP hard, especially, when considering the notoriously difficult scenarios of renamings and shifted code. To address these scenarios without compromising scalability, we propose a syntax-aware, heuristic optimization for structured merging that employs a lookahead mechanism during tree matching. The key idea is that renamings and shifted code are not arbitrarily distributed, but their occurrence follows patterns, which we address with a syntax-specific lookahead. Our experiments with 48 real-world open-source projects (4,878 merge scenarios with over 400 million lines of code) demonstrate that we can significantly improve matching precision in 28 percent of cases while maintaining performance."}, {"id": "conf/kbse/MenariniYG17", "title": "Semantics-assisted code review: an efficient toolchain and a user study.", "authors": ["Massimiliano Menarini", "Yan Yan", "William G. Griswold"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115666", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115666", "http://dl.acm.org/citation.cfm?id=3155632"], "tag": ["Source Code Analysis"], "abstract": "Code changes are often reviewed before they are deployed. Popular source control systems aid code review by presenting textual differences between old and new versions of the code, leaving developers with the difficult task of determining whether the differences actually produced the desired behavior. Fortunately, we can mine such information from code repositories. We propose aiding code review with inter-version semantic differential analysis. During review of a new commit, a developer is presented with summaries of both code differences and behavioral differences, which are expressed as diffs of likely invariants extracted by running the system's test cases. As a result, developers can more easily determine that the code changes produced the desired effect. We created an invariant-mining tool chain, Getty, to support our concept of semantically-assisted code review. To validate our approach, 1) we applied Getty to the commits of 6 popular open source projects, 2) we assessed the performance and cost of running Getty in different configurations, and 3) we performed a comparative user study with 18 developers. Our results demonstrate that semantically-assisted code review is feasible, effective, and that real programmers can leverage it to improve the quality of their reviews."}, {"id": "conf/kbse/OcarizaP017", "title": "Detecting unknown inconsistencies in web applications.", "authors": ["Frolin S. Ocariza Jr.", "Karthik Pattabiraman", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115667", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115667", "http://dl.acm.org/citation.cfm?id=3155633"], "tag": ["Source Code Analysis"], "abstract": "Although there has been increasing demand for more reliable web applications, JavaScript bugs abound in web applications. In response to this issue, researchers have proposed automated fault detection tools, which statically analyze the web application code to find bugs. While useful, these tools either only target a limited set of bugs based on predefined rules, or they do not detect bugs caused by cross-language interactions, which occur frequently in web application code. To address this problem, we present an anomaly-based inconsistency detection approach, implemented in a tool called HOLOCRON. The main novelty of our approach is that it does not look for hard-coded inconsistency classes. Instead, it applies subtree pattern matching to infer inconsistency classes and association rule mining to detect inconsistencies that occur both within a single language, and between two languages. We evaluated HOLOCRON, and it successfully detected 51 previously unreported inconsistencies - including 18 bugs and 33 code smells - in 12 web applications."}, {"id": "conf/kbse/TomasdottirAD17", "title": "Why and how JavaScript developers use linters.", "authors": ["Krist\u00edn Fj\u00f3la T\u00f3masd\u00f3ttir", "Mauricio Finavaro Aniche", "Arie van Deursen"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115668", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115668", "http://dl.acm.org/citation.cfm?id=3155634"], "tag": ["Source Code Analysis"], "abstract": "Automatic static analysis tools help developers to automatically spot code issues in their software. They can be of extreme value in languages with dynamic characteristics, such as JavaScript, where developers can easily introduce mistakes which can go unnoticed for a long time, e.g. a simple syntactic or spelling mistake. Although research has already shown how developers perceive such tools for strongly-typed languages such as Java, little is known about their perceptions when it comes to dynamic languages. In this paper, we investigate what motivates and how developers make use of such tools in JavaScript projects. To that goal, we apply a qualitative research method to conduct and analyze a series of 15 interviews with developers responsible for the linter configuration in reputable OSS JavaScript projects that apply the most commonly used linter, ESLint. The results describe the benefits that developers obtain when using ESLint, the different ways one can configure the tool and prioritize its rules, and the existing challenges in applying linters in the real world. These results have direct implications for developers, tool makers, and researchers, such as tool improvements, and a research agenda that aims to increase our knowledge about the usefulness of such analyzers."}, {"id": "conf/kbse/KapusC17", "title": "Automatic testing of symbolic execution engines via program generation and differential testing.", "authors": ["Timotej Kapus", "Cristian Cadar"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115669", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115669", "http://dl.acm.org/citation.cfm?id=3155636"], "tag": ["Symbolic Execution"], "abstract": "Symbolic execution has attracted significant attention in recent years, with applications in software testing, security, networking and more. Symbolic execution tools, like CREST, KLEE, FuzzBALL, and Symbolic PathFinder, have enabled researchers and practitioners to experiment with new ideas, scale the technique to larger applications and apply it to new application domains. Therefore, the correctness of these tools is of critical importance. In this paper, we present our experience extending compiler testing techniques to find errors in both the concrete and symbolic execution components of symbolic execution engines. The approach used relies on a novel way to create program versions, in three different testing modes-concrete, single-path and multi-path-each exercising different features of symbolic execution engines. When combined with existing program generation techniques and appropriate oracles, this approach enables differential testing within a single symbolic execution engine. We have applied our approach to the KLEE, CREST and FuzzBALL symbolic execution engines, where it has discovered 20 different bugs exposing a variety of important errors having to do with the handling of structures, division, modulo, casting, vector instructions and more, as well as issues related to constraint solving, compiler optimisations and test input replay."}, {"id": "conf/kbse/LiewSCDZW17", "title": "Floating-point symbolic execution: a case study in n-version programming.", "authors": ["Daniel Liew", "Daniel Schemmel", "Cristian Cadar", "Alastair F. Donaldson", "Rafael Z\u00e4hl", "Klaus Wehrle"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115670", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115670", "http://dl.acm.org/citation.cfm?id=3155637"], "tag": ["Symbolic Execution"], "abstract": "Symbolic execution is a well-known program analysis technique for testing software, which makes intensive use of constraint solvers. Recent support for floating-point constraint solving has made it feasible to support floating-point reasoning in symbolic execution tools. In this paper, we present the experience of two research teams that independently added floating-point support to KLEE, a popular symbolic execution engine. Since the two teams independently developed their extensions, this created the rare opportunity to conduct a rigorous comparison between the two implementations, essentially a modern case study on N-version programming. As part of our comparison, we report on the different design and implementation decisions taken by each team, and show their impact on a rigorously assembled and tested set of benchmarks, itself a contribution of the paper."}, {"id": "conf/kbse/CoppaDD17", "title": "Rethinking pointer reasoning in symbolic execution.", "authors": ["Emilio Coppa", "Daniele Cono D'Elia", "Camil Demetrescu"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115671", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115671", "http://dl.acm.org/citation.cfm?id=3155638"], "tag": ["Symbolic Execution"], "abstract": "Symbolic execution is a popular program analysis technique that allows seeking for bugs by reasoning over multiple alternative execution states at once. As the number of states to explore may grow exponentially, a symbolic executor may quickly run out of space. For instance, a memory access to a symbolic address may potentially reference the entire address space, leading to a combinatorial explosion of the possible resulting execution states. To cope with this issue, state-of-the-art executors concretize symbolic addresses that span memory intervals larger than some threshold. Unfortunately, this could result in missing interesting execution states, e.g., where a bug arises. In this paper we introduce MEMSIGHT, a new approach to symbolic memory that reduces the need for concretization, hence offering the opportunity for broader state explorations and more precise pointer reasoning. Rather than mapping address instances to data as previous tools do, our technique maps symbolic address expressions to data, maintaining the possible alternative states resulting from the memory referenced by a symbolic address in a compact, implicit form. A preliminary experimental investigation on prominent benchmarks from the DARPA Cyber Grand Challenge shows that MemSight enables the exploration of states unreachable by previous techniques."}, {"id": "conf/kbse/AlatawiSM17", "title": "Leveraging abstract interpretation for efficient dynamic symbolic execution.", "authors": ["Eman Alatawi", "Harald S\u00f8ndergaard", "Tim Miller"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115672", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115672", "http://dl.acm.org/citation.cfm?id=3155639"], "tag": ["Symbolic Execution"], "abstract": "Dynamic Symbolic Execution (DSE) is a technique to automatically generate test inputs by executing a program with concrete and symbolic values simultaneously. A key challenge in DSE is scalability; executing all feasible program paths is not possible, owing to the potentially exponential or infinite number of paths. Loops are a main source of path explosion, in particular where the number of iterations depends on a program's input. Problems arise because DSE maintains symbolic values that capture only the dependencies on symbolic inputs. This ignores control dependencies, including loop dependencies that depend indirectly on the inputs. We propose a method to increase the coverage achieved by DSE in the presence of input-data dependent loops and loop dependent branches. We combine DSE with abstract interpretation to find indirect control dependencies, including loop and branch indirect dependencies. Preliminary results show that this results in better coverage, within considerably less time compared to standard DSE."}, {"id": "conf/kbse/WeissGB17", "title": "Tortoise: interactive system configuration repair.", "authors": ["Aaron Weiss", "Arjun Guha", "Yuriy Brun"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115673", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115673", "http://dl.acm.org/citation.cfm?id=3155641"], "tag": ["Program Repair"], "abstract": "System configuration languages provide powerful abstractions that simplify managing large-scale, networked systems. Thousands of organizations now use configuration languages, such as Puppet. However, specifications written in configuration languages can have bugs and the shell remains the simplest way to debug a misconfigured system. Unfortunately, it is unsafe to use the shell to fix problems when a system configuration language is in use: a fix applied from the shell may cause the system to drift from the state specified by the configuration language. Thus, despite their advantages, configuration languages force system administrators to give up the simplicity and familiarity of the shell. This paper presents a synthesis-based technique that allows administrators to use configuration languages and the shell in harmony. Administrators can fix errors using the shell and the technique automatically repairs the higher-level specification written in the configuration language. The approach (1) produces repairs that are consistent with the fix made using the shell; (2) produces repairs that are maintainable by minimizing edits made to the original specification; (3) ranks and presents multiple repairs when relevant; and (4) supports all shells the administrator may wish to use. We implement our technique for Puppet, a widely used system configuration language, and evaluate it on a suite of benchmarks under 42 repair scenarios. The top-ranked repair is selected by humans 76% of the time and the human-equivalent repair is ranked 1.31 on average."}, {"id": "conf/kbse/Chen0F17", "title": "Contract-based program repair without the contracts.", "authors": ["Liushan Chen", "Yu Pei", "Carlo A. Furia"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115674", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115674", "http://dl.acm.org/citation.cfm?id=3155642"], "tag": ["Program Repair"], "abstract": "Automated program repair (APR) is a promising approach to automatically fixing software bugs. Most APR techniques use tests to drive the repair process; this makes them readily applicable to realistic code bases, but also brings the risk of generating spurious repairs that overfit the available tests. Some techniques addressed the overfitting problem by targeting code using contracts (such as pre- and postconditions), which provide additional information helpful to characterize the states of correct and faulty computations; unfortunately, mainstream programming languages do not normally include contract annotations, which severely limits the applicability of such contract-based techniques. This paper presents JAID, a novel APR technique for Java programs, which is capable of constructing detailed state abstractions-similar to those employed by contract-based techniques-that are derived from regular Java code without any special annotations. Grounding the repair generation and validation processes on rich state abstractions mitigates the overfitting problem, and helps extend APR's applicability: in experiments with the DEFECTS4J benchmark, a prototype implementation of JAID produced genuinely correct repairs, equivalent to those written by programmers, for 25 bugs-improving over the state of the art of comparable Java APR techniques in the number and kinds of correct fixes."}, {"id": "conf/kbse/SahaLYP17", "title": "ELIXIR: effective object oriented program repair.", "authors": ["Ripon K. Saha", "Yingjun Lyu", "Hiroaki Yoshida", "Mukul R. Prasad"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115675", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115675", "http://dl.acm.org/citation.cfm?id=3155643"], "tag": ["Program Repair"], "abstract": "This work is motivated by the pervasive use of method invocations in object-oriented (OO) programs, and indeed their prevalence in patches of OO-program bugs. We propose a generate-and-validate repair technique, called ELIXIR designed to be able to generate such patches. ELIXIR aggressively uses method calls, on par with local variables, fields, or constants, to construct more expressive repair-expressions, that go into synthesizing patches. The ensuing enlargement of the repair space, on account of the wider use of method calls, is effectively tackled by using a machine-learnt model to rank concrete repairs. The machine-learnt model relies on four features derived from the program context, i.e., the code surrounding the potential repair location, and the bug report. We implement ELIXIR and evaluate it on two datasets, the popular Defects4J dataset and a new dataset Bugs.jar created by us, and against 2 baseline versions of our technique, and 5 other techniques representing the state of the art in program repair. Our evaluation shows that ELIXIR is able to increase the number of correctly repaired bugs in Defects4J by 85% (from 14 to 26) and by 57% in Bugs.jar (from 14 to 22), while also significantly out-performing other state-of-the-art repair techniques including ACS, HD-Repair, NOPOL, PAR, and jGenProg."}, {"id": "conf/kbse/XinR17", "title": "Leveraging syntax-related code for automated program repair.", "authors": ["Qi Xin", "Steven P. Reiss"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115676", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115676", "http://dl.acm.org/citation.cfm?id=3155644"], "tag": ["Program Repair"], "abstract": "We present our automated program repair technique ssFix which leverages existing code (from a code database) that is syntax-related to the context of a bug to produce patches for its repair. Given a faulty program and a fault-exposing test suite, ssFix does fault localization to identify suspicious statements that are likely to be faulty. For each such statement, ssFix identifies a code chunk (or target chunk) including the statement and its local context. ssFix works on the target chunk to produce patches. To do so, it first performs syntactic code search to find candidate code chunks that are syntax-related, i.e., structurally similar and conceptually related, to the target chunk from a code database (or codebase) consisting of the local faulty program and an external code repository. ssFix assumes the correct fix to be contained in the candidate chunks, and it leverages each candidate chunk to produce patches for the target chunk. To do so, ssFix translates the candidate chunk by unifying the names used in the candidate chunk with those in the target chunk; matches the chunk components (expressions and statements) between the translated candidate chunk and the target chunk; and produces patches for the target chunk based on the syntactic differences that exist between the matched components and in the unmatched components. ssFix finally validates the patched programs generated against the test suite and reports the first one that passes the test suite. We evaluated ssFix on 357 bugs in the Defects4J bug dataset. Our results show that ssFix successfully repaired 20 bugs with valid patches generated and that it outperformed five other repair techniques for Java."}, {"id": "conf/kbse/ZhongW17", "title": "Boosting complete-code tool for partial program.", "authors": ["Hao Zhong", "Xiaoyin Wang"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115677", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115677", "http://dl.acm.org/citation.cfm?id=3155646"], "tag": ["Recommender Systems"], "abstract": "To improve software quality, researchers and practitioners have proposed static analysis tools for various purposes (e.g., detecting bugs, anomalies, and vulnerabilities). Although many such tools are powerful, they typically need complete programs where all the code names (e.g., class names, method names) are resolved. In many scenarios, researchers have to analyze partial programs in bug fixes (the revised source files can be viewed as a partial program), tutorials, and code search results. As a partial program is a subset of a complete program, many code names in partial programs are unknown. As a result, despite their syntactical correctness, existing complete-code tools cannot analyze partial programs, and existing partial-code tools are limited in both their number and analysis capability. Instead of proposing another tool for analyzing partial programs, we propose a general approach, called GRAPA, that boosts existing tools for complete programs to analyze partial programs. Our major insight is that after unknown code names are resolved, tools for complete programs can analyze partial programs with minor modifications. In particular, GRAPA locates Java archive files to resolve unknown code names, and resolves the remaining unknown code names from resolved code names. To illustrate GRAPA, we implement a tool that leverages the state-of-the-art tool, WALA, to analyze Java partial programs. We thus implemented the first tool that is able to build system dependency graphs for partial programs, complementing existing tools. We conduct an evaluation on 8,198 partial-code commits from four popular open source projects. Our results show that GRAPA fully resolved unknown code names for 98.5% bug fixes, with an accuracy of 96.1% in total. Furthermore, our results show the significance of GRAPA's internal techniques, which provides insights on how to integrate with more complete-code tools to analyze partial programs."}, {"id": "conf/kbse/YangJ0SGL17", "title": "A language model for statements of software code.", "authors": ["Yixiao Yang", "Yu Jiang", "Ming Gu", "Jiaguang Sun", "Jian Gao", "Han Liu"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115678", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115678", "http://dl.acm.org/citation.cfm?id=3155647"], "tag": ["Recommender Systems"], "abstract": "Building language models for source code enables a large set of improvements on traditional software engineering tasks. One promising application is automatic code completion. State-of-the-art techniques capture code regularities at token level with lexical information. Such language models are more suitable for predicting short token sequences, but become less effective with respect to long statement level predictions. In this paper, we have proposed PCC to optimize the token-level based language modeling. Specifically, PCC introduced an intermediate representation (IR) for source code, which puts tokens into groups using lexeme and variable relative order. In this way, PCC is able to handle long token sequences, i.e., group sequences, to suggest a complete statement with the precise synthesizer. Further more, PCC employed a fuzzy matching technique which combined genetic and longest common subsequence algorithms to make the prediction more accurate. We have implemented a code completion plugin for Eclipse and evaluated it on open-source Java projects. The results have demonstrated the potential of PCC in generating precise long statement level predictions. In 30%-60% of the cases, it can correctly suggest the complete statement with only six candidates, and 40%-90% of the cases with ten candidates."}, {"id": "conf/kbse/GasparicG017", "title": "Context-aware integrated development environment command recommender systems.", "authors": ["Marko Gasparic", "Tural Gurbanov", "Francesco Ricci"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115679", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115679", "http://dl.acm.org/citation.cfm?id=3155648"], "tag": ["Recommender Systems"], "abstract": "Integrated development environments (IDEs) are complex applications that integrate multiple tools for creating and manipulating software project artifacts. To improve users' knowledge and the effectiveness of usage of the available functionality, the inclusion of recommender systems into IDEs has been proposed. We present a novel IDE command recommendation algorithm that, by taking into account the contexts in which a developer works and in which different commands are usually executed, is able to provide relevant recommendations. We performed an empirical comparison of the proposed algorithm with state-of-the-art IDE command recommenders on a real-world data set. The algorithms were evaluated in terms of precision, recall, F1, k-tail, and with a new evaluation metric that is specifically measuring the usefulness of contextual recommendations. The experiments revealed that in terms of the contextual relevance and usefulness of recommendations the proposed algorithm outperforms existing algorithms."}, {"id": "conf/kbse/RolfsnesMB17", "title": "Predicting relevance of change recommendations.", "authors": ["Thomas Rolfsnes", "Leon Moonen", "David W. Binkley"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115680", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115680", "http://dl.acm.org/citation.cfm?id=3155649"], "tag": ["Recommender Systems"], "abstract": "Software change recommendation seeks to suggest artifacts (e.g., files or methods) that are related to changes made by a developer, and thus identifies possible omissions or next steps. While one obvious challenge for recommender systems is to produce accurate recommendations, a complimentary challenge is to rank recommendations based on their relevance. In this paper, we address this challenge for recommendation systems that are based on evolutionary coupling. Such systems use targeted association-rule mining to identify relevant patterns in a software system's change history. Traditionally, this process involves ranking artifacts using interestingness measures such as confidence and support. However, these measures often fall short when used to assess recommendation relevance. We propose the use of random forest classification models to assess recommendation relevance. This approach improves on past use of various interestingness measures by learning from previous change recommendations. We empirically evaluate our approach on fourteen open source systems and two systems from our industry partners. Furthermore, we consider complimenting two mining algorithms: Co-Change and Tarmaq. The results find that random forest classification significantly outperforms previous approaches, receives lower Brier scores, and has superior trade-off between precision and recall. The results are consistent across software system and mining algorithm."}, {"id": "conf/kbse/XuXXL17", "title": "AnswerBot: automated generation of answer summary to developers\u017a technical questions.", "authors": ["Bowen Xu", "Zhenchang Xing", "Xin Xia", "David Lo"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115681", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115681", "http://dl.acm.org/citation.cfm?id=3155650"], "tag": ["Recommender Systems"], "abstract": "The prevalence of questions and answers on domain-specific Q&A sites like Stack Overflow constitutes a core knowledge asset for software engineering domain. Although search engines can return a list of questions relevant to a user query of some technical question, the abundance of relevant posts and the sheer amount of information in them makes it difficult for developers to digest them and find the most needed answers to their questions. In this work, we aim to help developers who want to quickly capture the key points of several answer posts relevant to a technical question before they read the details of the posts. We formulate our task as a query-focused multi-answer-posts summarization task for a given technical question. Our proposed approach AnswerBot contains three main steps : 1) relevant question retrieval, 2) useful answer paragraph selection, 3) diverse answer summary generation. To evaluate our approach, we build a repository of 228,817 Java questions and their corresponding answers from Stack Overflow. We conduct user studies with 100 randomly selected Java questions (not in the question repository) to evaluate the quality of the answer summaries generated by our approach, and the effectiveness of its relevant question retrieval and answer paragraph selection components. The user study results demonstrate that answer summaries generated by our approach are relevant, useful and diverse; moreover, the two components are able to effectively retrieve relevant questions and select salient answer paragraphs for summarization."}, {"id": "conf/kbse/WangSFY17", "title": "Recommending crowdsourced software developers in consideration of skill improvement.", "authors": ["Zizhe Wang", "Hailong Sun", "Yang Fu", "Luting Ye"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115682", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115682", "http://dl.acm.org/citation.cfm?id=3155651"], "tag": ["Recommender Systems"], "abstract": "Finding suitable developers for a given task is critical and challenging for successful crowdsourcing software development. In practice, the development skills will be improved as developers accomplish more development tasks. Prior studies on crowdsourcing developer recommendation do not consider the changing of skills, which can underestimate developers' skills to fulfill a task. In this work, we first conducted an empirical study of the performance of 74 developers on Topcoder. With a difficulty-weighted algorithm, we re-compute the scores of each developer by eliminating the effect of task difficulty from the performance. We find out that the skill improvement of Topcoder developers can be fitted well with the negative exponential learning curve model. Second, we design a skill prediction method based on the learning curve. Then we propose a skill improvement aware framework for recommending developers for software development with crowdsourcing."}, {"id": "conf/kbse/Perez-SolerGLJ17", "title": "The rise of the (modelling) bots: towards assisted modelling via social networks.", "authors": ["Sara P\u00e9rez-Soler", "Esther Guerra", "Juan de Lara", "Francisco Jurado"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115683", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115683", "http://dl.acm.org/citation.cfm?id=3155652"], "tag": ["Recommender Systems"], "abstract": "We are witnessing a rising role of mobile computing and social networks to perform all sorts of tasks. This way, social networks like Twitter or Telegram are used for leisure, and they frequently serve as a discussion media for work-related activities. In this paper, we propose taking advantage of social networks to enable the collaborative creation of models by groups of users. The process is assisted by modelling bots that orchestrate the collaboration and interpret the users' inputs (in natural language) to incrementally build a (meta-)model. The advantages of this modelling approach include ubiquity of use, automation, assistance, natural user interaction, traceability of design decisions, possibility to incorporate coordination protocols, and seamless integration with the user's normal daily usage of social networks. We present a prototype implementation called SOCIO, able to work over several social networks like Twitter and Telegram, and a preliminary evaluation showing promising results."}, {"id": "conf/kbse/ZhouSLCL17", "title": "UNDEAD: detecting and preventing deadlocks in production software.", "authors": ["Jinpeng Zhou", "Sam Silvestro", "Hongyu Liu", "Yan Cai", "Tongping Liu"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115684", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115684", "http://dl.acm.org/citation.cfm?id=3155654"], "tag": ["Concurrency"], "abstract": "Deadlocks are critical problems afflicting parallel applications, causing software to hang with no further progress. Existing detection tools suffer not only from significant recording performance overhead, but also from excessive memory and/or storage overhead. In addition, they may generate numerous false alarms. Subsequently, after problems have been reported, tremendous manual effort is required to confirm and fix these deadlocks. This paper designs a novel system, UnDead, that helps defeat deadlocks in production software. Different from existing detection tools, UnDead imposes negligible runtime performance overhead (less than 3 % on average) and small memory overhead (around 6%), without any storage consumption. After detection, UnDead automatically strengthens erroneous programs to prevent future occurrences of both existing and potential deadlocks, which is similar to the existing work-Dimmunix. However, UnDead exceeds Dimmunix with several orders of magnitude lower performance overhead, while eliminating numerous false positives. Extremely low runtime and memory overhead, convenience, and automatic prevention make UnDead an always-on detection tool, and a \"band-aid\" prevention system for production software."}, {"id": "conf/kbse/Abdelrasoul17", "title": "Promoting secondary orders of event pairs in randomized scheduling using a randomized stride.", "authors": ["Mahmoud Abdelrasoul"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115685", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115685", "http://dl.acm.org/citation.cfm?id=3155655"], "tag": ["Concurrency"], "abstract": "Because of the wide use of randomized scheduling in concurrency testing research, it is important to understand randomized scheduling and its limitations. This work analyzes how randomized scheduling discovers concurrency bugs by focusing on the probabilities of the two possible orders of a pair of events. Analysis shows that the disparity between probabilities can be large for programs that encounter a large number of events during execution. Because sets of ordered event pairs define conditions for discovering concurrency bugs, this disparity can make some concurrency bugs highly unlikely. The complementary nature of the two possible orders also indicates a potential trade-off between the probability of discovering frequently-occurring and infrequently-occurring concurrency bugs. To help address this trade-off in a more balanced way, randomized-stride scheduling is proposed, where scheduling granularity for each thread is adjusted using a randomized stride calculated based on thread length. With some assumptions, strides can be calculated to allow covering the least likely event pair orders. Experiments confirm the analysis results and also suggest that randomized-stride scheduling is more effective for discovering concurrency bugs compared to the original randomized scheduling implementation, and compared to other algorithms in recent literature."}, {"id": "conf/kbse/NguyenS0TP17", "title": "Parallel bug-finding in concurrent programs via reduced interleaving instances.", "authors": ["Truc L. Nguyen", "Peter Schrammel", "Bernd Fischer", "Salvatore La Torre", "Gennaro Parlato"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115686", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115686", "http://dl.acm.org/citation.cfm?id=3155656"], "tag": ["Concurrency"], "abstract": "Concurrency poses a major challenge for program verification, but it can also offer an opportunity to scale when subproblems can be analysed in parallel. We exploit this opportunity here and use a parametrizable code-to-code translation to generate a set of simpler program instances, each capturing a reduced set of the original program's interleavings. These instances can then be checked independently in parallel. Our approach does not depend on the tool that is chosen for the final analysis, is compatible with weak memory models, and amplifies the effectiveness of existing tools, making them find bugs faster and with fewer resources. We use Lazy-CSeq as an off-the-shelf final verifier to demonstrate that our approach is able, already with a small number of cores, to find bugs in the hardest known concurrency benchmarks in a matter of minutes, whereas other dynamic and static tools fail to do so in hours."}, {"id": "conf/kbse/PintoCCXL17", "title": "Understanding and overcoming parallelism bottlenecks in ForkJoin applications.", "authors": ["Gustavo Pinto", "Anthony Canino", "Fernando Castor", "Guoqing (Harry) Xu", "Yu David Liu"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115687", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115687", "http://dl.acm.org/citation.cfm?id=3155657"], "tag": ["Concurrency"], "abstract": "ForkJoin framework is a widely used parallel programming framework upon which both core concurrency libraries and real-world applications are built. Beneath its simple and user-friendly APIs, ForkJoin is a sophisticated managed parallel runtime unfamiliar to many application programmers: the framework core is a work-stealing scheduler, handles fine-grained tasks, and sustains the pressure from automatic memory management. ForkJoin poses a unique gap in the compute stack between high-level software engineering and low-level system optimization. Understanding and bridging this gap is crucial for the future of parallelism support in JVM-supported applications. This paper describes a comprehensive study on parallelism bottlenecks in ForkJoin applications, with a unique focus on how they interact with underlying system-level features, such as work stealing and memory management. We identify 6 bottlenecks, and found that refactoring them can significantly improve performance and energy efficiency. Our field study includes an in-depth analysis of Akka - a real-world actor framework - and 30 additional open-source ForkJoin projects. We sent our patches to the developers of 15 projects, and 7 out of the 9 projects that replied to our patches have accepted them."}, {"id": "conf/kbse/MetzlerSBS17", "title": "Quick verification of concurrent programs by iteratively relaxed scheduling.", "authors": ["Patrick Metzler", "Habib Saissi", "P\u00e9ter Bokor", "Neeraj Suri"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115688", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115688", "http://dl.acm.org/citation.cfm?id=3155658"], "tag": ["Concurrency"], "abstract": "The most prominent advantage of software verification over testing is a rigorous check of every possible software behavior. However, large state spaces of concurrent systems, due to non-deterministic scheduling, result in a slow automated verification process. Therefore, verification introduces a large delay between completion and deployment of concurrent software. This paper introduces a novel iterative approach to verification of concurrent programs that drastically reduces this delay. By restricting the execution of concurrent programs to a small set of admissible schedules, verification complexity and time is drastically reduced. Iteratively adding admissible schedules after their verification eventually restores non-deterministic scheduling. Thereby, our framework allows to find a sweet spot between a low verification delay and sufficient execution time performance. Our evaluation of a prototype implementation on well-known benchmark programs shows that after verifying only few schedules of the program, execution time overhead is competitive to existing deterministic multi-threading frameworks."}, {"id": "conf/kbse/LiSLLL17", "title": "Automatic loop-invariant generation and refinement through selective sampling.", "authors": ["Jiaying Li", "Jun Sun", "Li Li", "Quang Loc Le", "Shang-Wei Lin"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115689", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115689", "http://dl.acm.org/citation.cfm?id=3155660"], "tag": ["Program Synthesis"], "abstract": "Automatic loop-invariant generation is important in program analysis and verification. In this paper, we propose to generate loop-invariants automatically through learning and verification. Given a Hoare triple of a program containing a loop, we start with randomly testing the program, collect program states at run-time and categorize them based on whether they satisfy the invariant to be discovered. Next, classification techniques are employed to generate a candidate loop-invariant automatically. Afterwards, we refine the candidate through selective sampling so as to overcome the lack of sufficient test cases. Only after a candidate invariant cannot be improved further through selective sampling, we verify whether it can be used to prove the Hoare triple. If it cannot, the generated counterexamples are added as new tests and we repeat the above process. Furthermore, we show that by introducing a path-sensitive learning, i.e., partitioning the program states according to program locations they visit and classifying each partition separately, we are able to learn disjunctive loop-invariants. In order to evaluate our idea, a prototype tool has been developed and the experiment results show that our approach complements existing approaches."}, {"id": "conf/kbse/LinSXLSH17", "title": "FiB: squeezing loop invariants by interpolation between Forward/Backward predicate transformers.", "authors": ["Shang-Wei Lin", "Jun Sun", "Hao Xiao", "Yang Liu", "David San\u00e1n", "Henri Hansen"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115690", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115690", "http://dl.acm.org/citation.cfm?id=3155661"], "tag": ["Program Synthesis"], "abstract": "Loop invariant generation is a fundamental problem in program analysis and verification. In this work, we propose a new approach to automatically constructing inductive loop invariants. The key idea is to aggressively squeeze an inductive invariant based on Craig interpolants between forward and backward reachability analysis. We have evaluated our approach by a set of loop benchmarks, and experimental results show that our approach is promising."}, {"id": "conf/kbse/NguyenDV17", "title": "SymInfer: inferring program invariants using symbolic states.", "authors": ["ThanhVu Nguyen", "Matthew B. Dwyer", "Willem Visser"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115691", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115691", "http://dl.acm.org/citation.cfm?id=3155662"], "tag": ["Program Synthesis"], "abstract": "We introduce a new technique for inferring program invariants that uses symbolic states generated by symbolic execution. Symbolic states, which consist of path conditions and constraints on local variables, are a compact description of sets of concrete program states and they can be used for both invariant inference and invariant verification. Our technique uses a counterexample-based algorithm that creates concrete states from symbolic states, infers candidate invariants from concrete states, and then verifies or refutes candidate invariants using symbolic states. The refutation case produces concrete counterexamples that prevent spurious results and allow the technique to obtain more precise invariants. This process stops when the algorithm reaches a stable set of invariants. We present Symlnfer, a tool that implements these ideas to automatically generate invariants at arbitrary locations in a Java program. The tool obtains symbolic states from Symbolic PathFinder and uses existing algorithms to infer complex (potentially nonlinear) numerical invariants. Our preliminary results show that Symlnfer is effective in using symbolic states to generate precise and useful invariants for proving program safety and analyzing program runtime complexity. We also show that Symlnfer outperforms existing invariant generation systems."}, {"id": "conf/kbse/LeungL17", "title": "Parsimony: an IDE for example-guided synthesis of lexers and parsers.", "authors": ["Alan Leung", "Sorin Lerner"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115692", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115692", "http://dl.acm.org/citation.cfm?id=3155663"], "tag": ["Program Synthesis"], "abstract": "We present Parsimony, a programming-by-example development environment for synthesizing lexers and parsers by example. Parsimony provides a graphical interface in which the user presents examples simply by selecting and labeling sample text in a text editor. An underlying synthesis engine then constructs syntactic rules to solve the system of constraints induced by the supplied examples. Parsimony is more expressive and usable than prior programming-by-example systems for parsers in several ways: Parsimony can (1) synthesize lexer rules in addition to productions, (2) solve for much larger constraint systems over multiple examples, rather than handling examples one-at-a-time, and (3) infer much more complex sets of productions, such as entire algebraic expression grammars, by detecting instances of well-known grammar design patterns. The results of a controlled user study across 18 participants show that users are able to perform lexing and parsing tasks faster and with fewer mistakes when using Parsimony as compared to a traditional parsing workflow."}, {"id": "conf/kbse/KrismayerRG17", "title": "Mining constraints for event-based monitoring in systems of systems.", "authors": ["Thomas Krismayer", "Rick Rabiser", "Paul Gr\u00fcnbacher"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115693", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115693", "http://dl.acm.org/citation.cfm?id=3155664"], "tag": ["Program Synthesis"], "abstract": "The full behavior of software-intensive systems of systems (SoS) emerges during operation only. Runtime monitoring approaches have thus been proposed to detect deviations from the expected behavior. They commonly rely on temporal logic or domain-specific languages to formally define requirements, which are then checked by analyzing the stream of monitored events and event data. Some approaches also allow developers to generate constraints from declarative specifications of the expected behavior. However, independent of the approach, deep domain knowledge is required to specify the desired behavior. This knowledge is often not accessible in SoS environments with multiple development teams independently working on different, heterogeneous systems. In this New Ideas Paper we thus describe an approach that automatically mines constraints for runtime monitoring from event logs recorded in SoS. Our approach builds on ideas from specification mining, process mining, and machine learning to mine different types of constraints on event occurrence, event timing, and event data. The approach further presents the mined constraints to users in an existing constraint language and it ranks the constraints using different criteria. We demonstrate the feasibility of our approach by applying it to event logs from a real-world industrial SoS."}, {"id": "conf/kbse/ZamaniradBBC017", "title": "Programming bots by synthesizing natural language expressions into API invocations.", "authors": ["Shayan Zamanirad", "Boualem Benatallah", "Moshe Chai Barukh", "Fabio Casati", "Carlos Rodr\u00edguez"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115694", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115694", "http://dl.acm.org/citation.cfm?id=3155665"], "tag": ["Program Synthesis"], "abstract": "At present, bots are still in their preliminary stages of development. Many are relatively simple, or developed ad-hoc for a very specific use-case. For this reason, they are typically programmed manually, or utilize machine-learning classifiers to interpret a fixed set of user utterances. In reality, real world conversations with humans require support for dynamically capturing users expressions. Moreover, bots will derive immeasurable value by programming them to invoke APIs for their results. Today, within the Web and Mobile development community, complex applications are being stringed together with a few lines of code - all made possible by APIs. Yet, developers today are not as empowered to program bots in much the same way. To overcome this, we introduce BotBase, a bot programming platform that dynamically synthesizes natural language user expressions into API invocations. Our solution is two faceted: Firstly, we construct an API knowledge graph to encode and evolve APIs; secondly, leveraging the above we apply techniques in NLP, ML and Entity Recognition to perform the required synthesis from natural language user expressions into API calls."}, {"id": "conf/kbse/CandidoMd17", "title": "Test suite parallelization in open-source projects: a study on its usage and impact.", "authors": ["Jeanderson C\u00e2ndido", "Luis Melo", "Marcelo d'Amorim"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115695", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115695", "http://dl.acm.org/citation.cfm?id=3155667"], "tag": ["Testing"], "abstract": "Dealing with high testing costs remains an important problem in Software Engineering. Test suite parallelization is an important approach to address this problem. This paper reports our findings on the usage and impact of test suite parallelization in open-source projects. It provides recommendations to practitioners and tool developers to speed up test execution. Considering a set of 468 popular Java projects we analyzed, we found that 24% of the projects contain costly test suites but parallelization features still seem underutilized in practice - only 19.1% of costly projects use parallelization. The main reported reason for adoption resistance was the concern to deal with concurrency issues. Results suggest that, on average, developers prefer high predictability than high performance in running tests."}, {"id": "conf/kbse/ChengYW17", "title": "Systematic reduction of GUI test sequences.", "authors": ["Lin Cheng", "Zijiang Yang", "Chao Wang"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115696", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115696", "http://dl.acm.org/citation.cfm?id=3155668"], "tag": ["Testing"], "abstract": "Graphic user interface (GUI) is an integral part of many software applications. However, GUI testing remains a challenging task. The main problem is to generate a set of high-quality test cases, i.e., sequences of user events to cover the often large input space. Since manually crafting event sequences is labor-intensive and automated testing tools often have poor performance, we propose a new GUI testing framework to efficiently generate progressively longer event sequences while avoiding redundant sequences. Our technique for identifying the redundancy among these sequences relies on statically checking a set of simple and syntactic-level conditions, whose reduction power matches and sometimes exceeds that of classic techniques based on partial order reduction. We have evaluated our method on 17 Java Swing applications. Our experimental results show the new technique, while being sound and systematic, can achieve more than 10X reduction in the number of test sequences compared to the state-of-the-art GUI testing tools."}, {"id": "conf/kbse/HerfertPP17", "title": "Automatically reducing tree-structured test inputs.", "authors": ["Satia Herfert", "Jibesh Patra", "Michael Pradel"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115697", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115697", "http://dl.acm.org/citation.cfm?id=3155669"], "tag": ["Testing"], "abstract": "Reducing the test input given to a program while preserving some property of interest is important, e.g., to localize faults or to reduce test suites. The well-known delta debugging algorithm and its derivatives automate this task by repeatedly reducing a given input. Unfortunately, these approaches are limited to blindly removing parts of the input and cannot reduce the input by restructuring it. This paper presents the Generalized Tree Reduction (GTR) algorithm, an effective and efficient technique to reduce arbitrary test inputs that can be represented as a tree, such as program code, PDF files, and XML documents. The algorithm combines tree transformations with delta debugging and a greedy backtracking algorithm. To reduce the size of the considered search space, the approach automatically specializes the tree transformations applied by the algorithm based on examples of input trees. We evaluate GTR by reducing Python files that cause interpreter crashes, JavaScript files that cause browser inconsistencies, PDF documents with malicious content, and XML files used to tests an XML validator. The GTR algorithm reduces the trees of these files to 45.3%, 3.6%, 44.2%, and 1.3% of the original size, respectively, outperforming both delta debugging and another state-of-the-art algorithm."}, {"id": "conf/kbse/SoltanaSB17", "title": "Synthetic data generation for statistical testing.", "authors": ["Ghanem Soltana", "Mehrdad Sabetzadeh", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115698", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115698", "http://dl.acm.org/citation.cfm?id=3155670"], "tag": ["Testing"], "abstract": "Usage-based statistical testing employs knowledge about the actual or anticipated usage profile of the system under test for estimating system reliability. For many systems, usage-based statistical testing involves generating synthetic test data. Such data must possess the same statistical characteristics as the actual data that the system will process during operation. Synthetic test data must further satisfy any logical validity constraints that the actual data is subject to. Targeting data-intensive systems, we propose an approach for generating synthetic test data that is both statistically representative and logically valid. The approach works by first generating a data sample that meets the desired statistical characteristics, without taking into account the logical constraints. Subsequently, the approach tweaks the generated sample to fix any logical constraint violations. The tweaking process is iterative and continuously guided toward achieving the desired statistical characteristics. We report on a realistic evaluation of the approach, where we generate a synthetic population of citizens' records for testing a public administration IT system. Results suggest that our approach is scalable and capable of simultaneously fulfilling the statistical representativeness and logical validity requirements."}, {"id": "conf/kbse/LeeYSNM17", "title": "SEALANT: a detection and visualization tool for inter-app security vulnerabilities in Android.", "authors": ["Youn Kyu Lee", "Peera Yoodee", "Arman Shahbazian", "Daye Nam", "Nenad Medvidovic"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115699", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115699", "http://dl.acm.org/citation.cfm?id=3155672"], "tag": ["Visualization, Models, and Synthesis"], "abstract": "Android's flexible communication model allows interactions among third-party apps, but it also leads to inter-app security vulnerabilities. Specifically, malicious apps can eavesdrop on interactions between other apps or exploit the functionality of those apps, which can expose a user's sensitive information to attackers. While the state-of-the-art tools have focused on detecting inter-app vulnerabilities in Android, they neither accurately analyze realistically large numbers of apps nor effectively deliver the identified issues to users. This paper presents SEALANT, a novel tool that combines static analysis and visualization techniques that, together, enable accurate identification of inter-app vulnerabilities as well as their systematic visualization. SEALANT statically analyzes architectural information of a given set of apps, infers vulnerable communication channels where inter-app attacks can be launched, and visualizes the identified information in a compositional representation. SEALANT has been demonstrated to accurately identify inter-app vulnerabilities from hundreds of real-world Android apps and to effectively deliver the identified information to users. (Demo Video: https://youtu.be/E4lLQonOdUw)"}, {"id": "conf/kbse/KritzingerKVRG17", "title": "Visualization support for requirements monitoring in systems of systems.", "authors": ["Lisa Maria Kritzinger", "Thomas Krismayer", "Michael Vierhauser", "Rick Rabiser", "Paul Gr\u00fcnbacher"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115700", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115700", "http://dl.acm.org/citation.cfm?id=3155673"], "tag": ["Visualization, Models, and Synthesis"], "abstract": "Industrial software systems are often systems of systems (SoS) whose full behavior only emerges at runtime. The systems and their interactions thus need to be continuously monitored and checked during operation to determine compliance with requirements. Many requirements monitoring approaches have been proposed. However, only few of these come with tools that present and visualize monitoring results and details on requirements violations to end users such as industrial engineers. In this tool demo paper we present visualization capabilities we have been developing motivated by industrial scenarios. Our tool complements ReMinds, an existing requirements monitoring framework, which supports collecting, aggregating, and analyzing events and event data in architecturally heterogeneous SoS. Our visualizations support a `drill-down' scenario for monitoring and diagnosis: starting from a graphical status overview of the monitored systems and their relations, engineers can view trends and statistics about performed analyses and diagnose the root cause of problems by inspecting the events and event data that led to a specific violation. Initial industry feedback we received confirms the usefulness of our tool support. Demo video: https://youtu.be/iv7kWzeNkdk.."}, {"id": "conf/kbse/ReissX17", "title": "A demonstration of simultaneous execution and editing in a development environment.", "authors": ["Steven P. Reiss", "Qi Xin"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115701", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115701", "http://dl.acm.org/citation.cfm?id=3155674"], "tag": ["Visualization, Models, and Synthesis"], "abstract": "We introduce a tool within the Code Bubbles development environment that allows for continuous execution as the programmer edits. The tool, SEEDE, shows both the intermediate and final results of execution in terms of variables, control flow, output, and graphics. These results are updated as the user edits. The user can explore the execution to find or fix bugs or use the intermediate values to help write appropriate code. A demonstration video is available at https://www.you-tube.com/watch?v=GpibSxX3Wlw."}, {"id": "conf/kbse/SchmidtNF17", "title": "TREM: a tool for mining timed regular specifications from system traces.", "authors": ["Lukas Schmidt", "Apurva Narayan", "Sebastian Fischmeister"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115702", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115702", "http://dl.acm.org/citation.cfm?id=3155675"], "tag": ["Visualization, Models, and Synthesis"], "abstract": "Software specifications are useful for software validation, model checking, runtime verification, debugging, monitoring, etc. In context of safety-critical real-time systems, temporal properties play an important role. However, temporal properties are rarely present due to the complexity and evolutionary nature of software systems. We propose Timed Regular Expression Mining (TREM) a hosted tool for specification mining using timed regular expressions (TREs). It is designed for easy and robust mining of dominant temporal properties. TREM uses an abstract structure of the property; the framework constructs a finite state machine to serve as an acceptor. TREM is scalable, easy to access/use, and platform independent specification mining framework. The tool is tested on industrial strength software system traces such as the QNX real-time operating system using traces with more than 1.5 Million entries. The tool demonstration video can be accessed here: youtu.be/cSd_aj3_LH8."}, {"id": "conf/kbse/ErataGGSLTKM17", "title": "ModelWriter: text and model-synchronized document engineering platform.", "authors": ["Ferhat Erata", "Claire Gardent", "Bikash Gyawali", "Anastasia Shimorina", "Yvan Lussaud", "Bedir Tekinerdogan", "Geylani Kardas", "Anne Monceaux"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115703", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115703", "http://dl.acm.org/citation.cfm?id=3155676"], "tag": ["Visualization, Models, and Synthesis"], "abstract": "The ModelWriter platform provides a generic framework for automated traceability analysis. In this paper, we demonstrate how this framework can be used to trace the consistency and completeness of technical documents that consist of a set of System Installation Design Principles used by Airbus to ensure the correctness of aircraft system installation. We show in particular, how the platform allows the integration of two types of reasoning: reasoning about the meaning of text using semantic parsing and description logic theorem proving; and reasoning about document structure using first-order relational logic and finite model finding for traceability analysis."}, {"id": "conf/kbse/PietschOKK17", "title": "Incrementally slicing editable submodels.", "authors": ["Christopher Pietsch", "Manuel Ohrndorf", "Udo Kelter", "Timo Kehrer"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115704", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115704", "http://dl.acm.org/citation.cfm?id=3155677"], "tag": ["Visualization, Models, and Synthesis"], "abstract": "Model slicers are tools which provide two services: (a) finding parts of interest in a model and (b) displaying these parts somehow or extract these parts as a new, autonomous model, which is referred to as slice or sub-model. This paper focuses on the creation of editable slices, which can be processed by model editors, analysis tools, model management tools etc. Slices are useful if, e.g., only a part of a large model shall be analyzed, compared or processed by time-consuming algorithms, or if sub-models shall be modified independently. We present a new generic incremental slicer which can slice models of arbitrary type and which creates slices which are consistent in the sense that they are editable by standard editors. It is built on top of a model differencing framework and does not require additional configuration data beyond those available in the differencing framework. The slicer can incrementally extend or reduce an existing slice if model elements shall be added or removed, even if the slice has been edited meanwhile. We demonstrate the usefulness of our slicer in several scenarios using a large UML model. A screencast of the demonstrated scenarios is provided at http://pi.informatik.uni-siegen.de/projects/SiLift/ase2017."}, {"id": "conf/kbse/AbateBCCCDKKP17", "title": "DSSynth: an automated digital controller synthesis tool for physical plants.", "authors": ["Alessandro Abate", "Iury Bessa", "Dario Cattaruzza", "Lennon C. Chaves", "Lucas C. Cordeiro", "Cristina David", "Pascal Kesseli", "Daniel Kroening", "Elizabeth Polgreen"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115705", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115705", "http://dl.acm.org/citation.cfm?id=3155678"], "tag": ["Visualization, Models, and Synthesis"], "abstract": "We present an automated MATLAB Toolbox, named DSSynth (Digital-System Synthesizer), to synthesize sound digital controllers for physical plants that are represented as linear timeinvariant systems with single input and output. In particular, DSSynth synthesizes digital controllers that are sound w.r.t. stability and safety specifications. DSSynth considers the complete range of approximations, including time discretization, quantization effects and finite-precision arithmetic (and its rounding errors). We demonstrate the practical value of this toolbox by automatically synthesizing stable and safe controllers for intricate physical plant models from the digital control literature. The resulting toolbox enables the application of program synthesis to real-world control engineering problems. A demonstration can be found at https://youtu.be_hLQslRcee8."}, {"id": "conf/kbse/WangZJS0S17", "title": "A static analysis tool with optimizations for reachability determination.", "authors": ["Yuexing Wang", "Min Zhou", "Yu Jiang", "Xiaoyu Song", "Ming Gu", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115706", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115706", "http://dl.acm.org/citation.cfm?id=3155680"], "tag": ["Analysis and Testing"], "abstract": "To reduce the false positives of static analysis, many tools collect path constraints and integrate SMT solvers to filter unreachable execution paths. However, the accumulated calling and computing of SMT solvers are time and resource consuming. This paper presents TsmartLW, an alternate static analysis tool in which we implement a path constraint solving engine to speed up reachability determination. Within the engine, typical types of constraint-patterns are firstly defined based on an empirical study of a large number of code repositories. For each pattern, a constraint solving algorithm is designed and implemented. For each program, the engine predicts the most suitable strategy and then applies the strategy to solve path constraints. The experimental results on some well-known benchmarks and real-world applications show that TsmartLW is faster than some state-of-the-art static analysis tools. For example, it is 1.32\u00d7 faster than CPAchecker and our engine is 369\u00d7 faster than SMT solvers in solving path constraints. The demo video is available at https://www.youtube.com/watch?v=5c3ARhFclHA&t=2s."}, {"id": "conf/kbse/KrugerNRAMBGGWD17", "title": "CogniCrypt: supporting developers in using cryptography.", "authors": ["Stefan Kr\u00fcger", "Sarah Nadi", "Michael Reif", "Karim Ali", "Mira Mezini", "Eric Bodden", "Florian G\u00f6pfert", "Felix G\u00fcnther", "Christian Weinert", "Daniel Demmler", "Ram Kamath"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115707", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115707", "http://dl.acm.org/citation.cfm?id=3155681"], "tag": ["Analysis and Testing"], "abstract": "Previous research suggests that developers often struggle using low-level cryptographic APIs and, as a result, produce insecure code. When asked, developers desire, among other things, more tool support to help them use such APIs. In this paper, we present CogniCrypt, a tool that supports developers with the use of cryptographic APIs. CogniCrypt assists the developer in two ways. First, for a number of common cryptographic tasks, CogniCrypt generates code that implements the respective task in a secure manner. Currently, CogniCrypt supports tasks such as data encryption, communication over secure channels, and long-term archiving. Second, CogniCrypt continuously runs static analyses in the background to ensure a secure integration of the generated code into the developer's workspace. This video demo showcases the main features of CogniCrypt: youtube.com/watch?v=JUq5mRHfAWY."}, {"id": "conf/kbse/CorradiniFP0TV17a", "title": "BProVe: tool support for business process verification.", "authors": ["Flavio Corradini", "Fabrizio Fornari", "Andrea Polini", "Barbara Re", "Francesco Tiezzi", "Andrea Vandin"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115708", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115708", "http://dl.acm.org/citation.cfm?id=3155682"], "tag": ["Analysis and Testing"], "abstract": "This demo introduces BProVe, a tool supporting automated verification of Business Process models. BProVe analysis is based on a formal operational semantics defined for the BPMN 2.0 modelling language, and is provided as a freely accessible service that uses open standard formats as input data. Furthermore a plug-in for the Eclipse platform has been developed making available a tool chain supporting users in modelling and visualising, in a friendly manner, the results of the verification. Finally we have conducted a validation through more than one thousand models, showing the effectiveness of our verification tool in practice. (Demo video: https://youtu.be/iF5OM7vKtDA)."}, {"id": "conf/kbse/KjolstadCLKA17", "title": "taco: a tool to generate tensor algebra kernels.", "authors": ["Fredrik Kjolstad", "Stephen Chou", "David Lugato", "Shoaib Kamil", "Saman P. Amarasinghe"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115709", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115709", "http://dl.acm.org/citation.cfm?id=3155683"], "tag": ["Analysis and Testing"], "abstract": "Tensor algebra is an important computational abstraction that is increasingly used in data analytics, machine learning, engineering, and the physical sciences. However, the number of tensor expressions is unbounded, which makes it hard to develop and optimize libraries. Furthermore, the tensors are often sparse (most components are zero), which means the code has to traverse compressed formats. To support programmers we have developed taco, a code generation tool that generates dense, sparse, and mixed kernels from tensor algebra expressions. This paper describes the taco web and command-line tools and discusses the benefits of a code generator over a traditional library. See also the demo video at tensor-compiler.org/ase2017."}, {"id": "conf/kbse/LegunsenSM17", "title": "STARTS: STAtic regression test selection.", "authors": ["Owolabi Legunsen", "August Shi", "Darko Marinov"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115710", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115710", "http://dl.acm.org/citation.cfm?id=3155684"], "tag": ["Analysis and Testing"], "abstract": "Regression testing is an important part of software development, but it can be very time consuming. Regression test selection (RTS) aims to speed up regression testing by running only impacted tests-the subset of tests that can change behavior due to code changes. We present STARTS, a tool for STAtic Regression Test Selection. Unlike dynamic RTS, STARTS requires no code instrumentation or runtime information to find impacted tests; instead, STARTS uses only compile-time information. Specifically, STARTS builds a dependency graph of program types and finds, as impacted, tests that can reach some changed type in the transitive closure of the dependency graph. STARTS is a Maven plugin that can be easily integrated into any Maven-based Java project. We find that STARTS selects on average 35.2% of tests, leading to an end-to-end runtime that is on average 81.0% of running all the tests. A video demo of STARTS can be found at https://youtu.be/PCNtk8jphrM."}, {"id": "conf/kbse/SaddlerC17", "title": "EventFlowSlicer: a tool for generating realistic goal-driven GUI tests.", "authors": ["Jonathan A. Saddler", "Myra B. Cohen"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115711", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115711", "http://dl.acm.org/citation.cfm?id=3155685"], "tag": ["Analysis and Testing"], "abstract": "Most automated testing techniques for graphical user interfaces (GUIs) produce test cases that are only concerned with covering the elements (widgets, menus, etc.) on the interface, or the underlying program code, with little consideration of test case semantics. This is effective for functional testing where the aim is to find as many faults as possible. However, when one wants to mimic a real user for evaluating usability, or when it is necessary to extensively test important end-user tasks of a system, or to generate examples of how to use an interface, this generation approach fails. Capture and replay techniques can be used, however there are often multiple ways to achieve a particular goal, and capturing all of these is usually too time consuming and unrealistic. Prior work on human performance regression testing introduced a constraint based method to filter test cases created by a functional test case generator, however that work did not capture the specifications, or directly generate only the required tests and considered only a single type of test goal. In this paper we present EventFlowSlicer, a tool that allows the GUI tester to specify and generate all realistic test cases relevant to achieve a stated goal. The user first captures relevant events on the interface, then adds constraints to provide restrictions on the task. An event flow graph is extracted containing only the widgets of interest for that goal. Next all test cases are generated for edges in the graph which respect the constraints. The test cases can then be replayed using a modified version of GUITAR. A video demonstration of EventFlowSlicer can be found at https://youtu.be/hw7WYz8WYVU."}, {"id": "conf/kbse/MeftahGRC17", "title": "ANDROFLEET: testing WiFi peer-to-peer mobile apps in the large.", "authors": ["Lakhdar Meftah", "Mar\u00eda G\u00f3mez", "Romain Rouvoy", "Isabelle Chrisment"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115712", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115712", "http://dl.acm.org/citation.cfm?id=3155686"], "tag": ["Analysis and Testing"], "abstract": "WiFi P2P allows mobile apps to connect to each other via WiFi without an intermediate access point. This communication mode is widely used by mobile apps to support interactions with one or more devices simultaneously. However, testing such P2P apps remains a challenge for app developers as i) existing testing frameworks lack support for WiFi P2P, and ii) WiFi P2P testing fails to scale when considering a deployment on more than two devices. In this paper, we therefore propose an acceptance testing framework, named Androfleet, to automate testing of WiFi P2P mobile apps at scale. Beyond the capability of testing point-to-point interactions under various conditions, An-drofleet supports the deployment and the emulation of a fleet of mobile devices as part of an alpha testing phase in order to assess the robustness of a WiFi P2P app once deployed in the field. To validate Androfleet, we demonstrate the detection of failing black-box acceptance tests for WiFi P2P apps and we capture the conditions under which such a mobile app can correctly work in the field. The demo video of Androfleet is made available from https://youtu.be/gJ5_Ed7XL04."}, {"id": "conf/kbse/AsaduzzamanRSH17", "title": "FEMIR: a tool for recommending framework extension examples.", "authors": ["Muhammad Asaduzzaman", "Chanchal K. Roy", "Kevin A. Schneider", "Daqing Hou"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115713", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115713", "http://dl.acm.org/citation.cfm?id=3155688"], "tag": ["Search and Editing"], "abstract": "Software frameworks enable developers to reuse existing well tested functionalities instead of taking the burden of implementing everything from scratch. However, to meet application specific requirements, the frameworks need to be customized via extension points. This is often done by passing a framework related object as an argument to an API call. To enable such customizations, the object can be created by extending a framework class, implementing an interface, or changing the properties of the object via API calls. However, it is both a common and non-trivial task to find all the details related to the customizations. In this paper, we present a tool, called FEMIR, that utilizes partial program analysis and graph mining technique to detect, group, and rank framework extension examples. The tool extends existing code completion infrastructure to inform developers about customization choices, enabling them to browse through extension points of a framework, and frequent usages of each point in terms of code examples. A video demo is made available at https://asaduzzamanparvez.wordpress.com/femir."}, {"id": "conf/kbse/LinL0CGLLMR17", "title": "TiQi: a natural language interface for querying software project data.", "authors": ["Jinfeng Lin", "Yalin Liu", "Jin Guo", "Jane Cleland-Huang", "William Goss", "Wenchuang Liu", "Sugandha Lohar", "Natawut Monaikul", "Alexander Rasin"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115714", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115714", "http://dl.acm.org/citation.cfm?id=3155689"], "tag": ["Search and Editing"], "abstract": "Software projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo."}, {"id": "conf/kbse/UddinK17a", "title": "Opiner: an opinion search and summarization engine for APIs.", "authors": ["Gias Uddin", "Foutse Khomh"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115715", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115715", "http://dl.acm.org/citation.cfm?id=3155690"], "tag": ["Search and Editing"], "abstract": "Opinions are key determinants to many of the activities related to software development. The perceptions of developers about an API, and the choices they make about whether and how they should use it, may, to a considerable degree, be conditioned upon how other developers see and evaluate the API. Given the plethora of APIs available for a given development task and the advent of developer forums as the media to share opinions about those APIs, it can be challenging for a developer to make informed decisions about an API to support the task. We introduce Opiner, our opinion search and summarization engine for API reviews. The server side of Opiner collects and summarizes opinions about APIs by crawling online developer forums and by associating the opinions found in the forum posts to the APIs discussed in the posts. The client side of Opiner is a Website that presents different summarized viewpoints of the opinions about the APIs in an online search engine. We evaluated Opiner by asking Industrial developers to select APIs for two development tasks. We found that developers were interested to use our proposed summaries of API reviews and that while combined with Stack Overflow, Opiner helped developers to make the right decision with more accuracy and confidence. The Opiner online search engine is available at: http://opiner.polymtl.ca. A video demo is available at: https://youtu.be/XAXpfmg5Lqs."}, {"id": "conf/kbse/KhatchadourianM17", "title": "Defaultification refactoring: a tool for automatically converting Java methods to default.", "authors": ["Raffi Khatchadourian", "Hidehiko Masuhara"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115716", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115716", "http://dl.acm.org/citation.cfm?id=3155691"], "tag": ["Search and Editing"], "abstract": "Enabling interfaces to declare (instance) method implementations, Java 8 default methods can be used as a substitute for the ubiquitous skeletal implementation software design pattern. Performing this transformation on legacy software manually, though, may be non-trivial. The refactoring requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods. All of this is necessary to preserve type-correctness and confirm semantics preservation. We demonstrate an automated refactoring tool called MIGRATE Skeletal Implementation to Interface for transforming legacy Java code to use the new default construct. The tool, implemented as an Eclipse plug-in, is driven by an efficient, fully-automated, type constraint-based refactoring approach. It features an extensive rule set covering various corner-cases where default methods cannot be used. The resulting code is semantically equivalent to the original, more succinct, easier to comprehend, less complex, and exhibits increased modularity. A demonstration can be found at http://youtu.be/YZHIy0yePh8."}, {"id": "conf/kbse/GrigeraGR17", "title": "Kobold: web usability as a service.", "authors": ["Juli\u00e1n Grigera", "Alejandra Garrido", "Gustavo Rossi"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115717", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115717", "http://dl.acm.org/citation.cfm?id=3155692"], "tag": ["Search and Editing"], "abstract": "While Web applications have become pervasive in today's business, social interaction and information exchange, their usability is often deficient, even being a key factor for a website success. Usability problems repeat across websites, and many of them have been catalogued, but usability evaluation and repair still remains expensive. There are efforts from both the academy and industry to automate usability testing or to provide automatic statistics, but they rarely offer concrete solutions. These solutions appear as guidelines or patterns that developers can follow manually. This paper presents Kobold, a tool that detects usability problems from real user interaction (UI) events and repairs them automatically when possible, at least suggesting concrete solutions. By using the refactoring technique and its associated concept of bad smell, Kobold mines UI events to detect usability smells and applies usability refactorings on the client to correct them. The purpose of Kobold is to deliver usability advice and solutions as a service (SaaS) for developers, allowing them to respond to feedback of the real use of their applications and improve usability incrementally, even when there are no usability experts on the team. Kobold is available at: http://autorefactoring.lifia.info.unlp.edu.ar. A screencast is available at https://youtu.be/c-myYPMUh0Q."}, {"id": "conf/kbse/ChengZS0S17", "title": "IntPTI: automatic integer error repair with proper-type inference.", "authors": ["Xi Cheng", "Min Zhou", "Xiaoyu Song", "Ming Gu", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115718", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115718", "http://dl.acm.org/citation.cfm?id=3155693"], "tag": ["Search and Editing"], "abstract": "Integer errors in C/C++ are caused by arithmetic operations yielding results which are unrepresentable in certain type. They can lead to serious safety and security issues. Due to the complicated semantics of C/C++ integers, integer errors are widely harbored in real-world programs and it is error-prone to repair them even for experts. An automatic tool is desired to 1) automatically generate fixes which assist developers to correct the buggy code, and 2) provide sufficient hints to help developers review the generated fixes and better understand integer types in C/C++. In this paper, we present a tool IntPTI that implements the desired functionalities for C programs. IntPTI infers appropriate types for variables and expressions to eliminate representation issues, and then utilizes the derived types with fix patterns codified from the successful human-written patches. IntPTI provides a user-friendly web interface which allows users to review and manage the fixes. We evaluate IntPTI on 7 real-world projects and the results show its competitive repair accuracy and its scalability on large code bases. The demo video for IntPTI is available at: https://youtu.be/9Tgd4A_FgZM."}, {"id": "conf/kbse/Krishna17", "title": "Learning effective changes for software projects.", "authors": ["Rahul Krishna"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115719", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115719", "http://dl.acm.org/citation.cfm?id=3155695"], "tag": ["Doctoral Symposium"], "abstract": "The primary motivation of much of software analytics is decision making. How to make these decisions? Should one make decisions based on lessons that arise from within a particular project? Or should one generate these decisions from across multiple projects? This work is an attempt to answer these questions. Our work was motivated by a realization that much of the current generation software analytics tools focus primarily on prediction. Indeed prediction is a useful task, but it is usually followed by \"planning\" about what actions need to be taken. This research seeks to address the planning task by seeking methods that support actionable analytics by offering clear guidance on what to do. Specifically, we propose XTREE and BELLTREE algorithms for generating a set of actionable plans within and across projects. Each of these plans, if followed will improve the quality of the software project."}, {"id": "conf/kbse/Wang17", "title": "Characterizing and taming non-deterministic bugs in JavaScript applications.", "authors": ["Jie Wang"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115720", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115720", "http://dl.acm.org/citation.cfm?id=3155696"], "tag": ["Doctoral Symposium"], "abstract": "JavaScript has become one of the most popular programming languages for both client-side and server-side applications. In JavaScript applications, events may be generated, triggered and consumed non-deterministically. Thus, JavaScript applications may suffer from non-deterministic bugs, when events are triggered and consumed in an unexpected order. In this proposal, we aim to characterize and combat non-deterministic bugs in JavaScript applications. Specifically, we first perform a comprehensive study about real-world non-deterministic bugs in server-side JavaScript applications. In order to facilitate bug diagnosis, we further propose approaches to isolate the necessary events that are responsible for the occurrence of a failure. We also plan to design new techniques in detecting non-deterministic bugs in JavaScript applications."}, {"id": "conf/kbse/Nielebock17", "title": "Towards API-specific automatic program repair.", "authors": ["Sebastian Nielebock"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115721", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115721", "http://dl.acm.org/citation.cfm?id=3155697"], "tag": ["Doctoral Symposium"], "abstract": "The domain of Automatic Program Repair (APR) had many research contributions in recent years. So far, most approaches target fixing generic bugs in programs (e.g., off-by-one errors). Nevertheless, recent studies reveal that about 50% of real bugs require API-specific fixes (e.g., adding missing API method calls or correcting method ordering), for which existing APR approaches are not designed. In this paper, we address this problem and introduce the notion of an API-specific program repair mechanism. This mechanism detects erroneous code in a similar way to existing APR approaches. However, to fix such bugs, it uses API-specific information from the erroneous code to search for API usage patterns in other software, with which we could fix the bug. We provide first insights on the applicability of this mechanism and discuss upcoming research challenges."}, {"id": "conf/kbse/Li17", "title": "Managing software evolution through semantic history slicing.", "authors": ["Yi Li"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115722", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115722", "http://dl.acm.org/citation.cfm?id=3155698"], "tag": ["Doctoral Symposium"], "abstract": "Software change histories are results of incremental updates made by developers. As a side-effect of the software development process, version history is a surprisingly useful source of information for understanding, maintaining and reusing software. However, traditional commit-based sequential organization of version histories lacks semantic structure and thus are insufficient for many development tasks that require high-level, semantic understanding of program functionality, such as locating feature implementations and porting hot fixes. In this work, we propose to use well-organized unit tests as identifiers for corresponding software functionalities. We then present a family of automated techniques which analyze the semantics of historical changes and assist developers in many everyday practical settings. For validation, we evaluate our approaches on a benchmark of developer-annotated version history instances obtained from real-world open source software projects on GitHub."}, {"id": "conf/kbse/Mills17", "title": "Towards the automatic classification of traceability links.", "authors": ["Chris Mills"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115723", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115723", "http://dl.acm.org/citation.cfm?id=3155699"], "tag": ["Doctoral Symposium"], "abstract": "A wide range of text-based artifacts contribute to software projects (e.g., source code, test cases, use cases, project requirements, interaction diagrams, etc.). Traceability Link Recovery (TLR) is the software task in which relevant documents in these various sets are linked to one another, uncovering information about the project that is not available when considering only the documents themselves. This information is helpful for enabling other tasks such as improving test coverage, impact analysis, and ensuring that system or regulatory requirements are met. However, while traceability links are useful, performing TLR manually is time consuming and fraught with error. Previous work has applied Information Retrieval (IR) and other techniques to reduce the human effort involved; however, that effort remains significant. In this research we seek to take the next step in reducing it by using machine learning (ML) classification models to predict whether a candidate link is valid or invalid without human oversight. Preliminary results show that this approach has promise for accurately recommending valid links; however, there are several challenges that still must be addressed in order to achieve a technique with high enough performance to consider it a viable, completely automated solution."}, {"id": "conf/kbse/Sultana17", "title": "Towards a software vulnerability prediction model using traceable code patterns and software metrics.", "authors": ["Kazi Zakia Sultana"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115724", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115724", "http://dl.acm.org/citation.cfm?id=3155700"], "tag": ["Doctoral Symposium"], "abstract": "Software security is an important aspect of ensuring software quality. The goal of this study is to help developers evaluate software security using traceable patterns and software metrics during development. The concept of traceable patterns is similar to design patterns but they can be automatically recognized and extracted from source code. If these patterns can better predict vulnerable code compared to traditional software metrics, they can be used in developing a vulnerability prediction model to classify code as vulnerable or not. By analyzing and comparing the performance of traceable patterns with metrics, we propose a vulnerability prediction model. This study explores the performance of some code patterns in vulnerability prediction and compares them with traditional software metrics. We use the findings to build an effective vulnerability prediction model. We evaluate security vulnerabilities reported for Apache Tomcat, Apache CXF and three stand-alone Java web applications. We use machine learning and statistical techniques for predicting vulnerabilities using traceable patterns and metrics as features. We found that patterns have a lower false negative rate and higher recall in detecting vulnerable code than the traditional software metrics."}, {"id": "conf/kbse/Busari17", "title": "Towards search-based modelling and analysis of requirements and architecture decisions.", "authors": ["Saheed A. Busari"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115725", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115725", "http://dl.acm.org/citation.cfm?id=3155701"], "tag": ["Doctoral Symposium"], "abstract": "Many requirements engineering and software architecture decisions are complicated by uncertainty and multiple conflicting stakeholders objectives. Using quantitative decision models helps clarify these decisions and allows the use of multi-objective simulation optimisation techniques in analysing the impact of decisions on objectives. Existing requirements and architecture decision support methods that use quantitative decision models are limited by the difficulty in elaborating problem-specific decision models and/or lack integrated tool support for automated decision analysis under uncertainty. To address these problems and facilitate requirements and architecture decision analysis, this research proposes a novel modelling language and automated decision analysis technique, implemented in a tool called RADAR. The modelling language is a simplified version of quantitative AND/OR goal models used in requirements engineering and similar to feature models used in software product lines. This research involves developing the RADAR tool and evaluating the tool's applicability, usefulness and scalability on a set of real-world examples."}, {"id": "conf/kbse/Guerriero17", "title": "Privacy-aware data-intensive applications.", "authors": ["Michele Guerriero"], "DOIs": ["https://doi.org/10.1109/ASE.2017.8115726", "http://doi.ieeecomputersociety.org/10.1109/ASE.2017.8115726", "http://dl.acm.org/citation.cfm?id=3155702"], "tag": ["Doctoral Symposium"], "abstract": "The rise of Big Data is leading to an increasing demand for data-intensive applications (DIAs), which, in many cases, are expected to process massive amounts of sensitive data. In this context, ensuring data privacy becomes paramount. While the way we design and develop DIAs has radically changed over the last few years in order to deal with Big Data, there has been relatively little effort to make such design privacy-aware. As a result, enforcing privacy policies in large-scale data processing is currently an open research problem. This thesis proposal makes one step towards this investigation: after identifying the dataflow model as the reference computational model for large-scale DIAs, (1) we propose a novel language for specifying privacy policies on dataflow applications along with (2) a dataflow rewriting mechanism to enforce such policies during DIA execution. Although a systematic evaluation still needs to be carried out, preliminary results are promising. We plan to implement our approach within a model-driven solution to ultimately simplify the design and development of privacy-aware DIAs, i.e. DIAs that ensure privacy policies at runtime."}], "2018": [{"id": "conf/kbse/Cosmo18", "title": "Software heritage: collecting, preserving, and sharing all our source code (keynote).", "authors": ["Roberto Di Cosmo"], "DOIs": ["https://doi.org/10.1145/3238147.3241985"], "tag": ["Keynotes"], "abstract": "ABSTRACTSoftware Heritage is a non profit initiative whose ambitious goal is to collect, preserve and share the source code of all software ever written, with its full development history, building a universal source code software knowledge base. Software Heritage addresses a variety of needs: preserving our scientific and technological knowledge, enabling better software development and reuse for society and industry, fostering better science, and building an essential infrastructure for large scale, reproducible software studies. We have already collected over 4 billions unique source files from over 80 millions repositories, and organised them into a giant Merkle graph, with full deduplication across all repositories. This allows us to cope with the growth of collaborative software development, and provides a unique vantage point for observing its evolution. In this talk, we will highlight the new challenges and opportunities that Software Heritage brings up."}, {"id": "conf/kbse/Cleland-Huang18", "title": "Automated requirements engineering challenges with examples from small unmanned aerial systems (keynote).", "authors": ["Jane Cleland-Huang"], "DOIs": ["https://doi.org/10.1145/3238147.3241986"], "tag": ["Keynotes"], "abstract": "ABSTRACTRequirements Engineering includes various activities aimed at discovering, analyzing, validating, evolving, and managing software and systems requirements. Many of these activities are human facing, effort intensive, and sometimes error prone. They could benefit greatly from cutting edge advances in automation. However, the software engineering community has primarily focused on automating other aspects of the development process such as testing, code analytics, and mining software respositories. As a result, advances in software analytics have had superficial impact upon advancing the state of art and practice in the field of requirements engineering. Two primary inhibitors are the lack of publicly available datasets and poorly publicized industry-relevant open requirements analytic challenges. To empower the Automated Software Engineering community to tackle open Requirements Engineering challenges, the talk will describe the rapidly evolving landscape of requirements engineering, clearly articulate open challenges, draw upon examples from an ongoing, agile, safety-critical project in the domain of Unmanned Aerial Vehicles, and introduce Dronology as a new community dataset."}, {"id": "conf/kbse/Herckis18", "title": "Implementation science for software engineering: bridging the gap between research and practice (keynote).", "authors": ["Lauren Herckis"], "DOIs": ["https://doi.org/10.1145/3238147.3264581"], "tag": ["Keynotes"], "abstract": "ABSTRACTSoftware engineering research rarely impacts practitioners in the field. A desire to facilitate transfer alone is not sufficient to bridge the gap between research and practice. Fields from medicine to education have acknowledged a similar challenge over the past 25 years. An empirical approach to the translation of research into evidence-based practice has emerged from the resulting discussion. Implementation science has fundamentally changed the way that biomedical research is conducted, and has revolutionized the daily practice of doctors, social workers, epidemiologists, early childhood educators, and more. In this talk we will explore the methods, frameworks, and practices of implementation science and their application to novel disciplines, including software engineering research. We will close by proposing some directions for future software engineering research to facilitate transfer."}, {"id": "conf/kbse/Murphy18", "title": "The need for context in software engineering (IEEE CS Harlan Mills award keynote).", "authors": ["Gail C. Murphy"], "DOIs": ["https://doi.org/10.1145/3238147.3241987"], "tag": ["Keynotes"], "abstract": "ABSTRACTThe development of a software system requires the orchestration of many different people using many different tools. Despite the need for a developer who is performing a task to understand the context in which that task is undertaken, the tools we imagine, build and provide to support software developers are typically isolated. Instead of the tools helping a developer work within the appropriate context, it is the developer who must bring the context to the tools. In this talk, I will argue that the lack of context in the software engineering tools we build limits the effectiveness of developers and of our software development practices."}, {"id": "conf/kbse/HabchiBR18", "title": "On adopting linters to deal with performance concerns in Android apps.", "authors": ["Sarra Habchi", "Xavier Blanc", "Romain Rouvoy"], "DOIs": ["https://doi.org/10.1145/3238147.3238197"], "tag": ["Performance"], "abstract": "ABSTRACTWith millions of applications (apps) distributed through mobile markets, engaging and retaining end-users challenge Android developers to deliver a nearly perfect user experience. As mobile apps run in resource-limited devices, performance is a critical criterion for the quality of experience. Therefore, developers are expected to pay much attention to limit performance bad practices. On the one hand, many studies already identified such performance bad practices and showed that they can heavily impact app performance. Hence, many static analysers, a.k.a. linters, have been proposed to detect and fix these bad practices. On the other hand, other studies have shown that Android developers tend to deal with performance reactively and they rarely build on linters to detect and fix performance bad practices. In this paper, we therefore perform a qualitative study to investigate this gap between research and development community. In particular, we performed interviews with 14 experienced Android developers to identify the perceived benefits and constraints of using linters to identify performance bad practices in Android apps. Our observations can have a direct impact on developers and the research community. Specifically, we describe why and how developers leverage static source code analysers to improve the performance of their apps. On top of that, we bring to light important challenges faced by developers when it comes to adopting static analysis for performance purposes."}, {"id": "conf/kbse/HanYL18", "title": "PerfLearner: learning from bug reports to understand and generate performance test frames.", "authors": ["Xue Han", "Tingting Yu", "David Lo"], "DOIs": ["https://doi.org/10.1145/3238147.3238204"], "tag": ["Performance"], "abstract": "ABSTRACTSoftware performance is important for ensuring the quality of software products. Performance bugs, defined as programming errors that cause significant performance degradation, can lead to slow systems and poor user experience. While there has been some research on automated performance testing such as test case generation, the main idea is to select workload values to increase the program execution times. These techniques often assume the initial test cases have the right combination of input parameters and focus on evolving values of certain input parameters. However, such an assumption may not hold for highly configurable real-word applications, in which the combinations of input parameters can be very large. In this paper, we manually analyze 300 bug reports from three large open source projects - Apache HTTP Server, MySQL, and Mozilla Firefox. We found that 1) exposing performance bugs often requires combinations of multiple input parameters, and 2) certain input parameters are frequently involved in exposing performance bugs. Guided by these findings, we designed and evaluated an automated approach, PerfLearner, to extract execution commands and input parameters from descriptions of performance bug reports and use them to generate test frames for guiding actual performance test case generation."}, {"id": "conf/kbse/BaoLXF18", "title": "AutoConfig: automatic configuration tuning for distributed message systems.", "authors": ["Liang Bao", "Xin Liu", "Ziheng Xu", "Baoyin Fang"], "DOIs": ["https://doi.org/10.1145/3238147.3238175"], "tag": ["Performance"], "abstract": "ABSTRACTDistributed message systems (DMSs) serve as the communication backbone for many real-time streaming data processing applications. To support the vast diversity of such applications, DMSs provide a large number of parameters to configure. However, It overwhelms for most users to configure these parameters well for better performance. Although many automatic configuration approaches have been proposed to address this issue, critical challenges still remain: 1) to train a better and robust performance prediction model using a limited number of samples, and 2) to search for a high-dimensional parameter space efficiently within a time constraint. In this paper, we propose AutoConfig -- an automatic configuration system that can optimize producer-side throughput on DMSs. AutoConfig constructs a novel comparison-based model (CBM) that is more robust that the prediction-based model (PBM) used by previous learning-based approaches. Furthermore, AutoConfig uses a weighted Latin hypercube sampling (wLHS) approach to select a set of samples that can provide a better coverage over the high-dimensional parameter space. wLHS allows AutoConfig to search for more promising configurations using the trained CBM. We have implemented AutoConfig on the Kafka platform, and evaluated it using eight different testing scenarios deployed on a public cloud. Experimental results show that our CBM can obtain better results than that of PBM under the same random forests based model. Furthermore, AutoConfig outperforms default configurations by 215.40% on average, and five state-of-the-art configuration algorithms by 7.21%-64.56%."}, {"id": "conf/kbse/HabibP18", "title": "Is this class thread-safe? inferring documentation using graph-based learning.", "authors": ["Andrew Habib", "Michael Pradel"], "DOIs": ["https://doi.org/10.1145/3238147.3238212"], "tag": ["Performance"], "abstract": "ABSTRACTThread-safe classes are pervasive in concurrent, object-oriented software. However, many classes lack documentation regarding their safety guarantees under multi-threaded usage. This lack of documentation forces developers who use a class in a concurrent program to either carefully inspect the implementation of the class, to conservatively synchronize all accesses to it, or to optimistically assume that the class is thread-safe. To overcome the lack of documentation, we present TSFinder, an approach to automatically classify classes as supposedly thread-safe or thread-unsafe. The key idea is to combine a lightweight static analysis that extracts a graph representation from classes with a graph-based classifier. After training the classifier with classes known to be thread-safe and thread-unsafe, it achieves an accuracy of 94.5% on previously unseen classes, enabling the approach to infer thread safety documentation with high confidence. The classifier takes about 3 seconds per class, i.e., it is efficient enough to infer documentation for many classes."}, {"id": "conf/kbse/Hilton0M18", "title": "A large-scale study of test coverage evolution.", "authors": ["Michael Hilton", "Jonathan Bell", "Darko Marinov"], "DOIs": ["https://doi.org/10.1145/3238147.3238183"], "tag": ["Testing Studies"], "abstract": "ABSTRACTStatement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change --- coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We find that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going beyond considering statement coverage as a simple ratio, we examine how the set of statements covered evolves between project revisions. We present and study new ways to assess the impact of a patch on a project's test suite quality that both separates coverage of the patch from coverage of the non-patch, and separates changes in coverage from changes in the set of statements covered."}, {"id": "conf/kbse/TerragniP18", "title": "Effectiveness and challenges in generating concurrent tests for thread-safe classes.", "authors": ["Valerio Terragni", "Mauro Pezz\u00e8"], "DOIs": ["https://doi.org/10.1145/3238147.3238224"], "tag": ["Testing Studies"], "abstract": "ABSTRACTDeveloping correct and efficient concurrent programs is difficult and error-prone, due to the complexity of thread synchronization. Often, developers alleviate such problem by relying on thread-safe classes, which encapsulate most synchronization-related challenges. Thus, testing such classes is crucial to ensure the reliability of the concurrency aspects of programs. Some recent techniques and corresponding tools tackle the problem of testing thread-safe classes by automatically generating concurrent tests. In this paper, we present a comprehensive study of the state-of-the-art techniques and an independent empirical evaluation of the publicly available tools. We conducted the study by executing all tools on the JaConTeBe benchmark that contains 47 well-documented concurrency faults. Our results show that 8 out of 47 faults (17%) were detected by at least one tool. By studying the issues of the tools and the generated tests, we derive insights to guide future research on improving the effectiveness of automated concurrent test generation."}, {"id": "conf/kbse/KonatEV18", "title": "Scalable incremental building with dynamic task dependencies.", "authors": ["Gabri\u00ebl Konat", "Sebastian Erdweg", "Eelco Visser"], "DOIs": ["https://doi.org/10.1145/3238147.3238196"], "tag": ["Build and Test Automation"], "abstract": "ABSTRACTIncremental build systems are essential for fast, reproducible software builds. Incremental build systems enable short feedback cycles when they capture dependencies precisely and selectively execute build tasks efficiently. A much overlooked feature of build systems is the expressiveness of the scripting language, which directly influences the maintainability of build scripts. In this paper, we present a new incremental build algorithm that allows build engineers to use a full-fledged programming language with explicit task invocation, value and file inspection facilities, and conditional and iterative language constructs. In contrast to prior work on incrementality for such programmable builds, our algorithm scales with the number of tasks affected by a change and is independent of the size of the software project being built. Specifically, our algorithm accepts a set of changed files, transitively detects and re-executes affected build tasks, but also accounts for new task dependencies discovered during building. We have evaluated the performance of our algorithm in a real-world case study and confirm its scalability."}, {"id": "conf/kbse/GallabaM0M18", "title": "Noise and heterogeneity in historical build data: an empirical study of Travis CI.", "authors": ["Keheliya Gallaba", "Christian Macho", "Martin Pinzger", "Shane McIntosh"], "DOIs": ["https://doi.org/10.1145/3238147.3238171"], "tag": ["Build and Test Automation"], "abstract": "ABSTRACTAutomated builds, which may pass or fail, provide feedback to a development team about changes to the codebase. A passing build indicates that the change compiles cleanly and tests (continue to) pass. A failing (a.k.a., broken) build indicates that there are issues that require attention. Without a closer analysis of the nature of build outcome data, practitioners and researchers are likely to make two critical assumptions: (1) build results are not noisy; however, passing builds may contain failing or skipped jobs that are actively or passively ignored; and (2) builds are equal; however, builds vary in terms of the number of jobs and configurations.  To investigate the degree to which these assumptions about build breakage hold, we perform an empirical study of 3.7 million build jobs spanning 1,276 open source projects. We find that: (1) 12% of passing builds have an actively ignored failure; (2) 9% of builds have a misleading or incorrect outcome on average; and (3) at least 44% of the broken builds contain passing jobs, i.e., the breakage is local to a subset of build variants. Like other software archives, build data is noisy and complex. Analysis of build data requires nuance."}, {"id": "conf/kbse/UdeshiAC18", "title": "Automated directed fairness testing.", "authors": ["Sakshi Udeshi", "Pryanshu Arora", "Sudipta Chattopadhyay"], "DOIs": ["https://doi.org/10.1145/3238147.3238165"], "tag": ["Quality Assurance for Machine Learning Techniques"], "abstract": "ABSTRACTFairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aeqitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aeqitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aeqitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aeqitas and we have evaluated it on six stateof- the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aeqitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aeqitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%."}, {"id": "conf/kbse/SunWRHKK18", "title": "Concolic testing for deep neural networks.", "authors": ["Youcheng Sun", "Min Wu", "Wenjie Ruan", "Xiaowei Huang", "Marta Kwiatkowska", "Daniel Kroening"], "DOIs": ["https://doi.org/10.1145/3238147.3238172"], "tag": ["Quality Assurance for Machine Learning Techniques"], "abstract": "ABSTRACTConcolic testing combines program execution and symbolic analysis to explore the execution paths of a software program. In this paper, we develop the first concolic testing approach for Deep Neural Networks (DNNs). More specifically, we utilise quantified linear arithmetic over rationals to express test requirements that have been studied in the literature, and then develop a coherent method to perform concolic testing with the aim of better coverage. Our experimental results show the effectiveness of the concolic testing approach in both achieving high coverage and finding adversarial examples."}, {"id": "conf/kbse/MaJZSXLCSLLZW18", "title": "DeepGauge: multi-granularity testing criteria for deep learning systems.", "authors": ["Lei Ma", "Felix Juefei-Xu", "Fuyuan Zhang", "Jiyuan Sun", "Minhui Xue", "Bo Li", "Chunyang Chen", "Ting Su", "Li Li", "Yang Liu", "Jianjun Zhao", "Yadong Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3238202"], "tag": ["Quality Assurance for Machine Learning Techniques"], "abstract": "ABSTRACTDeep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems."}, {"id": "conf/kbse/ZhangZZ0K18", "title": "DeepRoad: GAN-based metamorphic testing and input validation framework for autonomous driving systems.", "authors": ["Mengshi Zhang", "Yuqun Zhang", "Lingming Zhang", "Cong Liu", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1145/3238147.3238187"], "tag": ["Quality Assurance for Machine Learning Techniques"], "abstract": "ABSTRACTWhile Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness.  In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well."}, {"id": "conf/kbse/AbdessalemPNBS18", "title": "Testing autonomous cars for feature interaction failures using many-objective search.", "authors": ["Raja Ben Abdessalem", "Annibale Panichella", "Shiva Nejati", "Lionel C. Briand", "Thomas Stifter"], "DOIs": ["https://doi.org/10.1145/3238147.3238192"], "tag": ["Variability"], "abstract": "ABSTRACTComplex systems such as autonomous cars are typically built as a composition of features that are independent units of functionality. Features tend to interact and impact one another's behavior in unknown ways. A challenge is to detect and manage feature interactions, in particular, those that violate system requirements, hence leading to failures. In this paper, we propose a technique to detect feature interaction failures by casting this problem into a search-based test generation problem. We define a set of hybrid test objectives (distance functions) that combine traditional coverage-based heuristics with new heuristics specifically aimed at revealing feature interaction failures. We develop a new search-based test generation algorithm, called FITEST, that is guided by our hybrid test objectives. FITEST extends recently proposed many-objective evolutionary algorithms to reduce the time required to compute fitness values. We evaluate our approach using two versions of an industrial self-driving system. Our results show that our hybrid test objectives are able to identify more than twice as many feature interaction failures as two baseline test objectives used in the software testing literature (i.e., coverage-based and failure-based test objectives). Further, the feedback from domain experts indicates that the detected feature interaction failures represent real faults in their systems that were not previously identified based on analysis of the system features and their requirements."}, {"id": "conf/kbse/MukelabaiNMBS18", "title": "Tackling combinatorial explosion: a study of industrial needs and practices for analyzing highly configurable systems.", "authors": ["Mukelabai Mukelabai", "Damir Nesic", "Salome Maro", "Thorsten Berger", "Jan-Philipp Stegh\u00f6fer"], "DOIs": ["https://doi.org/10.1145/3238147.3238201"], "tag": ["Variability"], "abstract": "ABSTRACTHighly configurable systems are complex pieces of software. To tackle this complexity, hundreds of dedicated analysis techniques have been conceived, many of which able to analyze system properties for all possible system configurations, as opposed to traditional, single-system analyses. Unfortunately, it is largely unknown whether these techniques are adopted in practice, whether they address actual needs, or what strategies practitioners actually apply to analyze highly configurable systems. We present a study of analysis practices and needs in industry. It relied on a survey with 27 practitioners engineering highly configurable systems and follow-up interviews with 15 of them, covering 18 different companies from eight countries. We confirm that typical properties considered in the literature (e.g., reliability) are relevant, that consistency between variability models and artifacts is critical, but that the majority of analyses for specifications of configuration options (a.k.a., variability model analysis) is not perceived as needed. We identified rather pragmatic analysis strategies, including practices to avoid the need for analysis. For instance, testing with experience-based sampling is the most commonly applied strategy, while systematic sampling is rarely applicable. We discuss analyses that are missing and synthesize our insights into suggestions for future research."}, {"id": "conf/kbse/He0WZLX18", "title": "Understanding and detecting evolution-induced compatibility issues in Android apps.", "authors": ["Dongjie He", "Lian Li", "Lei Wang", "Hengjie Zheng", "Guangwei Li", "Jingling Xue"], "DOIs": ["https://doi.org/10.1145/3238147.3238185"], "tag": ["Variability"], "abstract": "ABSTRACTThe frequent release of Android OS and its various versions bring many compatibility issues to Android Apps. This paper studies and addresses such evolution-induced compatibility problems. We conduct an extensive empirical study over 11 different Android versions and 4,936 Android Apps. Our study shows that there are drastic API changes between adjacent Android versions, with averagely 140.8 new types, 1,505.6 new methods, and 979.2 new fields being introduced in each release. However, the Android Support Library (provided by the Android OS) only supports less than 23% of the newly added methods, with much less support for new types and fields. As a result, 91.84% of Android Apps write additional code to support different OS versions. Furthermore, 88.65% of the supporting codes share a common pattern, which directly compares variable android.os.Build.VERSION.SDK_INT with a constant version number, to use an API of particular versions. Based on our findings, we develop a new tool called IctApiFinder, to detect incompatible API usages in Android applications. IctApiFinder effectively computes the OS versions on which an API may be invoked, using an inter-procedural data-flow analysis framework. It detects numerous incompatible API usages in 361 out of 1,425 Apps. Compared to Android Lint, IctApiFinder is sound and able to reduce the false positives by 82.1%. We have reported the issues to 13 Apps developers. At present, 5 of them have already been confirmed by the original developers and 3 of them have already been fixed."}, {"id": "conf/kbse/HeCHL18", "title": "Characterizing the natural language descriptions in software logging statements.", "authors": ["Pinjia He", "Zhuangbin Chen", "Shilin He", "Michael R. Lyu"], "DOIs": ["https://doi.org/10.1145/3238147.3238193"], "tag": ["Mining and Crowd Sourcing"], "abstract": "ABSTRACTLogging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C# projects, which contain 28,532,975 LOC and 115,159 logging statements in total. Furthermore, our study demonstrates the potential of automated description text generation for logging statements by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L score using a simple information retrieval method. To facilitate future research in this field, the datasets have been publicly released."}, {"id": "conf/kbse/OreEDK18", "title": "Assessing the type annotation burden.", "authors": ["John-Paul Ore", "Sebastian G. Elbaum", "Carrick Detweiler", "Lambros Karkazis"], "DOIs": ["https://doi.org/10.1145/3238147.3238173"], "tag": ["Mining and Crowd Sourcing"], "abstract": "ABSTRACTType annotations provide a link between program variables and domain-specific types. When combined with a type system, these annotations can enable early fault detection. For type annotations to be cost-effective in practice, they need to be both accurate and affordable for developers. We lack, however, an understanding of how burdensome type annotation is for developers. Hence, this work explores three fundamental questions: 1) how accurately do developers make type annotations; 2) how long does a single annotation take; and, 3) if a system could automatically suggest a type annotation, how beneficial to accuracy are correct suggestions and how detrimental are incorrect suggestions? We present results of a study of 71 programmers using 20 random code artifacts that contain variables with physical unit types that must be annotated. Subjects choose a correct type annotation only 51% of the time and take an average of 136 seconds to make a single correct annotation. Our qualitative analysis reveals that variable names and reasoning over mathematical operations are the leading clues for type selection. We find that suggesting the correct type boosts accuracy to 73%, while making a poor suggestion decreases accuracy to 28%. We also explore what state-of-the-art automated type annotation systems can and cannot do to help developers with type annotations, and identify implications for tool developers."}, {"id": "conf/kbse/KovalenkoPB18", "title": "Mining file histories: should we consider branches?", "authors": ["Vladimir Kovalenko", "Fabio Palomba", "Alberto Bacchelli"], "DOIs": ["https://doi.org/10.1145/3238147.3238169"], "tag": ["Mining and Crowd Sourcing"], "abstract": "ABSTRACTModern distributed version control systems, such as Git, offer support for branching \u2014 the possibility to develop parts of software outside the master trunk. Consideration of the repository structure in Mining Software Repository (MSR) studies requires a thorough approach to mining, but there is no well-documented, widespread methodology regarding the handling of merge commits and branches. Moreover, there is still a lack of knowledge of the extent to which considering branches during MSR studies impacts the results of the studies. In this study, we set out to evaluate the importance of proper handling of branches when calculating file modification histories. We analyze over 1,400 Git repositories of four open source ecosystems and compute modification histories for over two million files, using two different algorithms. One algorithm only follows the first parent of each commit when traversing the repository, the other returns the full modification history of a file across all branches. We show that the two algorithms consistently deliver different results, but the scale of the difference varies across projects and ecosystems. Further, we evaluate the importance of accurate mining of file histories by comparing the performance of common techniques that rely on file modification history \u2014 reviewer recommendation, change recommendation, and defect prediction \u2014 for two algorithms of file history retrieval. We find that considering full file histories leads to an increase in the techniques\u2019 performance that is rather modest."}, {"id": "conf/kbse/HuangCXLL18", "title": "Tell them apart: distilling technology differences from crowd-scale comparison discussions.", "authors": ["Yi Huang", "Chunyang Chen", "Zhenchang Xing", "Tian Lin", "Yang Liu"], "DOIs": ["https://doi.org/10.1145/3238147.3238208"], "tag": ["Mining and Crowd Sourcing"], "abstract": "ABSTRACTDevelopers can use different technologies for many software development tasks in their work. However, when faced with several technologies with comparable functionalities, it is not easy for developers to select the most appropriate one, as comparisons among technologies are time-consuming by trial and error. Instead, developers can resort to expert articles, read official documents or ask questions in QA sites for technology comparison, but it is opportunistic to get a comprehensive comparison as online information is often fragmented or contradictory. To overcome these limitations, we propose the diffTech system that exploits the crowdsourced discussions from Stack Overflow, and assists technology comparison with an informative summary of different comparison aspects. We first build a large database of comparable technologies in software engineering by mining tags in Stack Overflow, and then locate comparative sentences about comparable technologies with natural language processing methods. We further mine prominent comparison aspects by clustering similar comparative sentences and representing each cluster with its keywords. The evaluation demonstrates both the accuracy and usefulness of our model and we implement our approach into a practical website for public use."}, {"id": "conf/kbse/Shen000ML18", "title": "ReScue: crafting regular expression DoS attacks.", "authors": ["Yuju Shen", "Yanyan Jiang", "Chang Xu", "Ping Yu", "Xiaoxing Ma", "Jian Lu"], "DOIs": ["https://doi.org/10.1145/3238147.3238159"], "tag": ["Security"], "abstract": "ABSTRACTRegular expression (regex) with modern extensions is one of the most popular string processing tools. However, poorly-designed regexes can yield exponentially many matching steps, and lead to regex Denial-of-Service (ReDoS) attacks under well-conceived string inputs. This paper presents Rescue, a three-phase gray-box analytical technique, to automatically generate ReDoS strings to highlight vulnerabilities of given regexes. Rescue systematically seeds (by a genetic search), incubates (by another genetic search), and finally pumps (by a regex-dedicated algorithm) for generating strings with maximized search time. We implemenmted the Rescue tool and evaluated it against 29,088 practical regexes in real-world projects. The evaluation results show that Rescue found 49% more attack strings compared with the best existing technique, and applying Rescue to popular GitHub projects discovered ten previously unknown ReDoS vulnerabilities."}, {"id": "conf/kbse/LiuWX18", "title": "TDroid: exposing app switching attacks in Android with control flow specialization.", "authors": ["Jie Liu", "Diyu Wu", "Jingling Xue"], "DOIs": ["https://doi.org/10.1145/3238147.3238188"], "tag": ["Security"], "abstract": "ABSTRACTThe Android multitasking mechanism can be plagued with app switching attacks, in which a malicious app replaces the legitimate top activity of the focused app with one of its own, thus mounting, e.g., phishing and denial-of-service attacks. Existing market-level defenses are still ineffective, as static analysis is fundamentally unable to reason about the intention of an app and dynamic analysis has low coverage. We introduce TDroid, a new market-level approach to detecting app switching attacks. The challenge lies in how to handle a plethora of input-dependent branch predicates (forming an exponential number of paths) that control the execution of the code responsible for launching such attacks. TDroid tackles this challenge by combining static and dynamic analysis to analyze an app without producing any false positives. In its static analysis, TDroid transforms the app into runnable slices containing potentially app switching attacks, one slice per attack. In its dynamic analysis, TDroid executes these slices on an Android phone or emulator to expose their malicious GUIs. The novelty lies in the use of a new trigger-oriented slicing technique in producing runnable slices so that certain input-dependent branch predicates are specialized to execute always some fixed branches. Evaluated with a large set of malware apps, TDroid is shown to outperform the state of the art, by detecting substantially more app switching attacks, in a few minutes per app, on average."}, {"id": "conf/kbse/FadhelBB18", "title": "Model-driven run-time enforcement of complex role-based access control policies.", "authors": ["Ameni Ben Fadhel", "Domenico Bianculli", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1145/3238147.3238167"], "tag": ["Security"], "abstract": "ABSTRACTA Role-based Access Control (RBAC) mechanism prevents unauthorized users to perform an operation, according to authorization policies which are defined on the user\u2019s role within an enterprise. Several models have been proposed to specify complex RBAC policies. However, existing approaches for policy enforcement do not fully support all the types of policies that can be expressed in these models, which hinders their adoption among practitioners. In this paper we propose a model-driven enforcement framework for complex policies captured by GemRBAC+CTX, a comprehensive RBAC model proposed in the literature. We reduce the problem of making an access decision to checking whether a system state (from an RBAC point of view), expressed as an instance of the GemRBAC+CTX model, satisfies the constraints corresponding to the RBAC policies to be enforced at run time. We provide enforcement algorithms for various types of access requests and events, and a prototype tool (MORRO) implementing them. We also show how to integrate MORRO into an industrial Web application. The evaluation results show the applicability of our approach on a industrial system and its scalability with respect to the various parameters characterizing an AC configuration."}, {"id": "conf/kbse/0001LC18", "title": "ContractFuzzer: fuzzing smart contracts for vulnerability detection.", "authors": ["Bo Jiang", "Ye Liu", "W. K. Chan"], "DOIs": ["https://doi.org/10.1145/3238147.3238177"], "tag": ["Security"], "abstract": "ABSTRACTDecentralized cryptocurrencies feature the use of blockchain to transfer values among peers on networks without central agency. Smart contracts are programs running on top of the blockchain consensus protocol to enable people make agreements while minimizing trusts. Millions of smart contracts have been deployed in various decentralized applications. The security vulnerabilities within those smart contracts pose significant threats to their applications. Indeed, many critical security vulnerabilities within smart contracts on Ethereum platform have caused huge financial losses to their users. In this work, we present ContractFuzzer, a novel fuzzer to test Ethereum smart contracts for security vulnerabilities. ContractFuzzer generates fuzzing inputs based on the ABI specifications of smart contracts, defines test oracles to detect security vulnerabilities, instruments the EVM to log smart contracts runtime behaviors, and analyzes these logs to report security vulnerabilities. Our fuzzing of 6991 smart contracts has flagged more than 459 vulnerabilities with high precision. In particular, our fuzzing tool successfully detects the vulnerability of the DAO contract that leads to USD 60 million loss and the vulnerabilities of Parity Wallet that have led to the loss of USD 30 million and the freezing of USD 150 million worth of Ether."}, {"id": "conf/kbse/ReissXH18", "title": "SEEDE: simultaneous execution and editing in a development environment.", "authors": ["Steven P. Reiss", "Qi Xin", "Jeff Huang"], "DOIs": ["https://doi.org/10.1145/3238147.3238182"], "tag": ["Developer Tools"], "abstract": "ABSTRACTWe introduce a tool within the Code Bubbles development environment that allows for continuous execution as the programmer edits. The tool, SEEDE, shows both the intermediate and final results of execution in terms of variables, control and data flow, output, and graphics. These results are updated as the user edits. The tool can be used to help the user write new code or to find and fix bugs. The tool is explicitly designed to let the user quickly explore the execution of a method along with all the code it invokes, possibly while writing or modifying the code. The user can start continuous execution either at a breakpoint or for a test case. This paper describes the tool, its implementation, and its user interface. It presents an initial user study of the tool demonstrating its potential utility."}, {"id": "conf/kbse/LiuHN18", "title": "Effective API recommendation without historical software repositories.", "authors": ["Xiaoyu Liu", "LiGuo Huang", "Vincent Ng"], "DOIs": ["https://doi.org/10.1145/3238147.3238216"], "tag": ["Developer Tools"], "abstract": "ABSTRACTIt is time-consuming and labor-intensive to learn and locate the correct API for programming tasks. Thus, it is beneficial to perform API recommendation automatically. The graph-based statistical model has been shown to recommend top-10 API candidates effectively. It falls short, however, in accurately recommending an actual top-1 API. To address this weakness, we propose RecRank, an approach and tool that applies a novel ranking-based discriminative approach leveraging API usage path features to improve top-1 API recommendation. Empirical evaluation on a large corpus of (1385+8) open source projects shows that RecRank significantly improves top-1 API recommendation accuracy and mean reciprocal rank when compared to state-of-the-art API recommendation approaches."}, {"id": "conf/kbse/HuangXXLW18", "title": "API method recommendation without worrying about the task-API knowledge gap.", "authors": ["Qiao Huang", "Xin Xia", "Zhenchang Xing", "David Lo", "Xinyu Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3238191"], "tag": ["Developer Tools"], "abstract": "ABSTRACTDevelopers often need to search for appropriate APIs for their programming tasks. Although most libraries have API reference documentation, it is not easy to find appropriate APIs due to the lexical gap and knowledge gap between the natural language description of the programming task and the API description in API documentation. Here, the lexical gap refers to the fact that the same semantic meaning can be expressed by different words, and the knowledge gap refers to the fact that API documentation mainly describes API functionality and structure but lacks other types of information like concepts and purposes, which are usually the key information in the task description. In this paper, we propose an API recommendation approach named BIKER (Bi-Information source based KnowledgE Recommendation) to tackle these two gaps. To bridge the lexical gap, BIKER uses word embedding technique to calculate the similarity score between two text descriptions. Inspired by our survey findings that developers incorporate Stack Overflow posts and API documentation for bridging the knowledge gap, BIKER leverages Stack Overflow posts to extract candidate APIs for a program task, and ranks candidate APIs by considering the query\u2019s similarity with both Stack Overflow posts and API documentation. It also summarizes supplementary information (e.g., API description, code examples in Stack Overflow posts) for each API to help developers select the APIs that are most relevant to their tasks. Our evaluation with 413 API-related questions confirms the effectiveness of BIKER for both class- and method-level API recommendation, compared with state-of-the-art baselines. Our user study with 28 Java developers further demonstrates the practicality of BIKER for API search."}, {"id": "conf/kbse/ChenSXHJ18", "title": "An automated approach to estimating code coverage measures via execution logs.", "authors": ["Boyuan Chen", "Jian Song", "Peng Xu", "Xing Hu", "Zhen Ming (Jack) Jiang"], "DOIs": ["https://doi.org/10.1145/3238147.3238214"], "tag": ["Developer Tools"], "abstract": "ABSTRACTSoftware testing is a widely used technique to ensure the quality of software systems. Code coverage measures are commonly used to evaluate and improve the existing test suites. Based on our industrial and open source studies, existing state-of-the-art code coverage tools are only used during unit and integration testing due to issues like engineering challenges, performance overhead, and incomplete results. To resolve these issues, in this paper we have proposed an automated approach, called LogCoCo, to estimating code coverage measures using the readily available execution logs. Using program analysis techniques, LogCoCo matches the execution logs with their corresponding code paths and estimates three different code coverage criteria: method coverage, statement coverage, and branch coverage. Case studies on one open source system (HBase) and five commercial systems from Baidu and systems show that: (1) the results of LogCoCo are highly accurate (>96% in seven out of nine experiments) under a variety of testing activities (unit testing, integration testing, and benchmarking); and (2) the results of LogCoCo can be used to evaluate and improve the existing test suites. Our collaborators at Baidu are currently considering adopting LogCoCo and use it on a daily basis."}, {"id": "conf/kbse/HabibP18a", "title": "How many of all bugs do we find? a study of static bug detectors.", "authors": ["Andrew Habib", "Michael Pradel"], "DOIs": ["https://doi.org/10.1145/3238147.3238213"], "tag": ["Static Analysis"], "abstract": "ABSTRACTStatic bug detectors are becoming increasingly popular and are widely used by professional software developers. While most work on bug detectors focuses on whether they find bugs at all, and on how many false positives they report in addition to legitimate warnings, the inverse question is often neglected: How many of all real-world bugs do static bug detectors find? This paper addresses this question by studying the results of applying three widely used static bug detectors to an extended version of the Defects4J dataset that consists of 15 Java projects with 594 known bugs. To decide which of these bugs the tools detect, we use a novel methodology that combines an automatic analysis of warnings and bugs with a manual validation of each candidate of a detected bug. The results of the study show that: (i) static bug detectors find a non-negligible amount of all bugs, (ii) different tools are mostly complementary to each other, and (iii) current bug detectors miss the large majority of the studied bugs. A detailed analysis of bugs missed by the static detectors shows that some bugs could have been found by variants of the existing detectors, while others are domain-specific problems that do not match any existing bug pattern. These findings help potential users of such tools to assess their utility, motivate and outline directions for future work on static bug detection, and provide a basis for future comparisons of static bug detection with other bug finding techniques, such as manual and automated testing."}, {"id": "conf/kbse/SharifAGZ18", "title": "TRIMMER: application specialization for code debloating.", "authors": ["Hashim Sharif", "Muhammad Abubakar", "Ashish Gehani", "Fareed Zaffar"], "DOIs": ["https://doi.org/10.1145/3238147.3238160"], "tag": ["Static Analysis"], "abstract": "ABSTRACTWith the proliferation of new hardware architectures and ever-evolving user requirements, the software stack is becoming increasingly bloated. In practice, only a limited subset of the supported functionality is utilized in a particular usage context, thereby presenting an opportunity to eliminate unused features. In the past, program specialization has been proposed as a mechanism for enabling automatic software debloating. In this work, we show how existing program specialization techniques lack the analyses required for providing code simplification for real-world programs. We present an approach that uses stronger analysis techniques to take advantage of constant configuration data, thereby enabling more effective debloating. We developed Trimmer, an application specialization tool that leverages user-provided configuration data to specialize an application to its deployment context. The specialization process attempts to eliminate the application functionality that is unused in the user-defined context. Our evaluation demonstrates Trimmer can effectively reduce code bloat. For 13 applications spanning various domains, we observe a mean binary size reduction of 21% and a maximum reduction of 75%. We also show specialization reduces the surface for code-reuse attacks by reducing the number of exploitable gadgets. For the evaluated programs, we observe a 20% mean reduction in the total gadget count and a maximum reduction of 87%."}, {"id": "conf/kbse/HelmKERM18", "title": "A unified lattice model and framework for purity analyses.", "authors": ["Dominik Helm", "Florian K\u00fcbler", "Michael Eichberg", "Michael Reif", "Mira Mezini"], "DOIs": ["https://doi.org/10.1145/3238147.3238226"], "tag": ["Static Analysis"], "abstract": "ABSTRACTAnalyzing methods in object-oriented programs whether they are side-effect free and also deterministic, i.e., mathematically pure, has been the target of extensive research. Identifying such methods helps to find code smells and security related issues, and also helps analyses detecting concurrency bugs. Pure methods are also used by formal verification approaches as the foundations for specifications and proving the pureness is necessary to ensure correct specifications. However, so far no common terminology exists which describes the purity of methods. Furthermore, some terms (e.g., pure or side-effect free) are also used inconsistently. Further, all current approaches only report selected purity information making them only suitable for a smaller subset of the potential use cases. In this paper, we present a fine-grained unified lattice model which puts the purity levels found in the literature into relation and which adds a new level that generalizes existing definitions. We have also implemented a scalable, modularized purity analysis which produces significantly more precise results for real-world programs than the best-performing related work. The analysis shows that all defined levels are found in real-world projects."}, {"id": "conf/kbse/ChenH18", "title": "Control flow-guided SMT solving for program verification.", "authors": ["Jianhui Chen", "Fei He"], "DOIs": ["https://doi.org/10.1145/3238147.3238218"], "tag": ["Verification 1"], "abstract": "ABSTRACTSatisfiability modulo theories (SMT) solvers have been widely applied as the reasoning engine for diverse software analysis and verification technologies. The efficiency of the SMT solver has significant effects on the performance of these technologies. However, the current SMT solvers are designed for the general purpose of constraint solving. Many useful knowledge of programs cannot be utilized during the SMT solving. As a result, the SMT solver may spend a lot of effort to explore redundant search space. In this paper, we propose a novel approach for utilizing control-flow knowledge in SMT solving. With this technique, the search space can be considerably reduced and the efficiency of SMT solving is observably improved. We conducted extensive experiments on credible benchmarks, the results show orders of magnitude improvements of our approach."}, {"id": "conf/kbse/NagashimaH18", "title": "PaMpeR: proof method recommendation system for Isabelle/HOL.", "authors": ["Yutaka Nagashima", "Yilun He"], "DOIs": ["https://doi.org/10.1145/3238147.3238210"], "tag": ["Verification 1"], "abstract": "ABSTRACTDeciding which sub-tool to use for a given proof state requires expertise specific to each interactive theorem prover (ITP). To mitigate this problem, we present <pre>PaMpeR</pre>, a <U>p</U>roof <U>m</U>ethod <U>r</U>ecommendation system for Isabelle/HOL. Given a proof state, <pre>PaMpeR</pre> recommends proof methods to discharge the proof goal and provides qualitative explanations as to why it suggests these methods. <pre>PaMpeR</pre> generates these recommendations based on existing hand-written proof corpora, thus transferring experienced users\u2019 expertise to new users. Our evaluation shows that <pre>PaMpeR</pre> correctly predicts experienced users\u2019 proof methods invocation especially when it comes to special purpose proof methods."}, {"id": "conf/kbse/LiuXHLXW18", "title": "Neural-machine-translation-based commit message generation: how far are we?", "authors": ["Zhongxin Liu", "Xin Xia", "Ahmed E. Hassan", "David Lo", "Zhenchang Xing", "Xinyu Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3238190"], "tag": ["Maintenance and Machine Learning"], "abstract": "ABSTRACTCommit messages can be regarded as the documentation of software changes. These messages describe the content and purposes of changes, hence are useful for program comprehension and software maintenance. However, due to the lack of time and direct motivation, commit messages sometimes are neglected by developers. To address this problem, Jiang et al. proposed an approach (we refer to it as NMT), which leverages a neural machine translation algorithm to automatically generate short commit messages from code. The reported performance of their approach is promising, however, they did not explore why their approach performs well. Thus, in this paper, we first perform an in-depth analysis of their experimental results. We find that (1) Most of the test <pre>diffs</pre> from which NMT can generate high-quality messages are similar to one or more training <pre>diffs</pre> at the token level. (2) About 16% of the commit messages in Jiang et al.\u2019s dataset are noisy due to being automatically generated or due to them describing repetitive trivial changes. (3) The performance of NMT declines by a large amount after removing such noisy commit messages. In addition, NMT is complicated and time-consuming. Inspired by our first finding, we proposed a simpler and faster approach, named NNGen (Nearest Neighbor Generator), to generate concise commit messages using the nearest neighbor algorithm. Our experimental results show that NNGen is over 2,600 times faster than NMT, and outperforms NMT in terms of BLEU (an accuracy measure that is widely used to evaluate machine translation systems) by 21%. Finally, we also discuss some observations for the road ahead for automated commit message generation to inspire other researchers."}, {"id": "conf/kbse/LiuXZ18", "title": "Deep learning based feature envy detection.", "authors": ["Hui Liu", "Zhifeng Xu", "Yanzhen Zou"], "DOIs": ["https://doi.org/10.1145/3238147.3238166"], "tag": ["Maintenance and Machine Learning"], "abstract": "ABSTRACTSoftware refactoring is widely employed to improve software quality. A key step in software refactoring is to identify which part of the software should be refactored. To facilitate the identification, a number of approaches have been proposed to identify certain structures in the code (called code smells) that suggest the possibility of refactoring. Most of such approaches rely on manually designed heuristics to map manually selected source code metrics to predictions. However, it is challenging to manually select the best features, especially textual features. It is also difficult to manually construct the optimal heuristics. To this end, in this paper we propose a deep learning based novel approach to detecting feature envy, one of the most common code smells. The key insight is that deep neural networks and advanced deep learning techniques could automatically select features (especially textual features) of source code for feature envy detection, and could automatically build the complex mapping between such features and predictions. We also propose an automatic approach to generating labeled training data for the neural network based classifier, which does not require any human intervention. Evaluation results on open-source applications suggest that the proposed approach significantly improves the state-of-the-art in both detecting feature envy smells and recommending destinations for identified smelly methods."}, {"id": "conf/kbse/WanZYXY0Y18", "title": "Improving automatic source code summarization via deep reinforcement learning.", "authors": ["Yao Wan", "Zhou Zhao", "Min Yang", "Guandong Xu", "Haochao Ying", "Jian Wu", "Philip S. Yu"], "DOIs": ["https://doi.org/10.1145/3238147.3238206"], "tag": ["Maintenance and Machine Learning"], "abstract": "ABSTRACTCode summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization; b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an exposure bias issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods."}, {"id": "conf/kbse/ChaLO18", "title": "Template-guided concolic testing via online learning.", "authors": ["Sooyoung Cha", "Seonho Lee", "Hakjoo Oh"], "DOIs": ["https://doi.org/10.1145/3238147.3238227"], "tag": ["Symbolic Execution"], "abstract": "ABSTRACTWe present template-guided concolic testing, a new technique for effectively reducing the search space in concolic testing. Addressing the path-explosion problem has been a significant challenge in concolic testing. Diverse search heuristics have been proposed to mitigate this problem but using search heuristics alone is not sufficient to substantially improve code coverage for real-world programs. The goal of this paper is to complement existing techniques and achieve higher coverage by exploiting templates in concolic testing. In our approach, a template is a partially symbolized input vector whose job is to reduce the search space. However, choosing a right set of templates is nontrivial and significantly affects the final performance of our approach. We present an algorithm that automatically learns useful templates online, based on data collected from previous runs of concolic testing. The experimental results with open-source programs show that our technique achieves greater branch coverage and finds bugs more effectively than conventional concolic testing."}, {"id": "conf/kbse/GaoTDR18", "title": "Android testing via synthetic symbolic execution.", "authors": ["Xiang Gao", "Shin Hwei Tan", "Zhen Dong", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1145/3238147.3238225"], "tag": ["Symbolic Execution"], "abstract": "ABSTRACTSymbolic execution of Android applications is challenging as it involves either building a customized VM for Android or modeling the Android libraries. Since the Android Runtime evolves from one version to another, building a high-fidelity symbolic execution engine involves modeling the effect of the libraries and their evolved versions. Without simulating the behavior of Android libraries, path divergence may occur due to constraint loss when the symbolic values flow into Android framework and these values later affect the subsequent path taken. Previous works such as JPF-Android have relied on the modeling of execution environment such as libraries. In this work, we build a dynamic symbolic execution engine for Android apps, without any manual modeling of execution environment. Environment (or library) dependent control flow decisions in the application will trigger an on-demand program synthesis step to automatically deduce a representation of the library.This representation is refined on-the-fly by running the corresponding library multiple times.The overarching goal of the refinement is to enhance behavioral coverage and to alleviate the path divergence problem during symbolic execution. Moreover, our library synthesis can be made context-specific. Compared to traditional synthesis approaches which aim to synthesize the complete library code, our context-specific synthesis engine can generate more precise expressions for a given context. The evaluation of our dynamic symbolic execution engine, built on top of JDART, shows that the library models obtained from program synthesis are often more accurate than the semi-manual models in JPF-Android. Furthermore, our symbolic execution engine could reach more branch targets, as compared to using the JPF-Android models."}, {"id": "conf/kbse/DustmannWC18", "title": "PARTI: a multi-interval theory solver for symbolic execution.", "authors": ["Oscar Soria Dustmann", "Klaus Wehrle", "Cristian Cadar"], "DOIs": ["https://doi.org/10.1145/3238147.3238179"], "tag": ["Symbolic Execution"], "abstract": "ABSTRACTSymbolic execution is an effective program analysis technique whose scalability largely depends on the ability to quickly solve large numbers of first-order logic queries. We propose an effective general technique for speeding up the solving of queries in the theory of arrays and bit-vectors with a specific structure, while otherwise falling back to a complete solver.  The technique has two stages: a learning stage that determines the solution sets of each symbolic variable, and a decision stage that uses this information to quickly determine the satisfiability of certain types of queries. The main challenges involve deciding which operators to support and precisely dealing with integer type casts and arithmetic underflow and overflow.  We implemented this technique in an incomplete solver called PARTI (``PARtial Theory solver for Intervals''), directly integrating it into the popular KLEE symbolic execution engine. We applied KLEE with PARTI and a state-of-the-art SMT solver to synthetic and real-world benchmarks. We found that PARTI practically does not hurt performance while many times achieving order-of-magnitude speedups."}, {"id": "conf/kbse/Mora0RC18", "title": "Client-specific equivalence checking.", "authors": ["Federico Mora", "Yi Li", "Julia Rubin", "Marsha Chechik"], "DOIs": ["https://doi.org/10.1145/3238147.3238178"], "tag": ["Maintenance"], "abstract": "ABSTRACTSoftware is often built by integrating components created by different teams or even different organizations. With little understanding of changes in dependent components, it is challenging to maintain correctness and robustness of the entire system. In this paper, we investigate the effect of component changes on the behavior of their clients. We observe that changes in a component are often irrelevant to a particular client and thus can be adopted without any delays or negative effects. Following this observation, we formulate the notion of client-specific equivalence checking (CSE) and develop an automated technique optimized for checking such equivalence. We evaluate our technique on a set of benchmarks, including those from the existing literature on equivalence checking, and show its applicability and effectiveness."}, {"id": "conf/kbse/Arora0IKR18", "title": "Replay without recording of production bugs for service oriented applications.", "authors": ["Nipun Arora", "Jonathan Bell", "Franjo Ivancic", "Gail E. Kaiser", "Baishakhi Ray"], "DOIs": ["https://doi.org/10.1145/3238147.3238186"], "tag": ["Maintenance"], "abstract": "ABSTRACTShort time-to-localize and time-to-fix for production bugs is extremely important for any 24x7 service-oriented application (SOA). Debugging buggy behavior in deployed applications is hard, as it requires careful reproduction of a similar environment and workload. Prior approaches for automatically reproducing production failures do not scale to large SOA systems. Our key insight is that for many failures in SOA systems (e.g., many semantic and performance bugs), a failure can automatically be reproduced solely by relaying network packets to replicas of suspect services, an insight that we validated through a manual study of 16 real bugs across five different systems. This paper presents Parikshan, an application monitoring framework that leverages user-space virtualization and network proxy technologies to provide a sandbox \u201cdebug\u201d environment. In this \u201cdebug\u201d environment, developers are free to attach debuggers and analysis tools without impacting performance or correctness of the production environment. In comparison to existing monitoring solutions that can slow down production applications, Parikshan allows application monitoring at significantly lower overhead."}, {"id": "conf/kbse/AlizadehK18", "title": "Reducing interactive refactoring effort via clustering-based multi-objective search.", "authors": ["Vahid Alizadeh", "Marouane Kessentini"], "DOIs": ["https://doi.org/10.1145/3238147.3238217"], "tag": ["Maintenance"], "abstract": "ABSTRACTRefactoring is nowadays widely adopted in the industry because bad design decisions can be very costly and extremely risky. On the one hand, automated refactoring does not always lead to the desired design. On the other hand, manual refactoring is error-prone, time-consuming and not practical for radical changes. Thus, recent research trends in the field focused on integrating developers feedback into automated refactoring recommendations because developers understand the problem domain intuitively and may have a clear target design in mind. However, this interactive process can be repetitive, expensive, and tedious since developers must evaluate recommended refactorings, and adapt them to the targeted design especially in large systems where the number of possible strategies can grow exponentially.  In this paper, we propose an interactive approach combining the use of multi-objective and unsupervised learning to reduce the developer's interaction effort when refactoring systems. We generate, first, using multi-objective search different possible refactoring strategies by finding a trade-off between several conflicting quality attributes. Then, an unsupervised learning algorithm clusters the different trade-off solutions, called the Pareto front, to guide the developers in selecting their region of interests and reduce the number of refactoring options to explore. The feedback from the developer, both at the cluster and solution levels, are used to automatically generate constraints to reduce the search space in the next iterations and focus on the region of developer preferences. We selected 14 active developers to manually evaluate the effectiveness our tool on 5 open source projects and one industrial system. The results show that the participants found their desired refactorings faster and more accurate than the current state of the art."}, {"id": "conf/kbse/LemieuxS18", "title": "FairFuzz: a targeted mutation strategy for increasing greybox fuzz testing coverage.", "authors": ["Caroline Lemieux", "Koushik Sen"], "DOIs": ["https://doi.org/10.1145/3238147.3238176"], "tag": ["Software Quality"], "abstract": "ABSTRACTIn recent years, fuzz testing has proven itself to be one of the most effective techniques for finding correctness bugs and security vulnerabilities in practice. One particular fuzz testing tool, American Fuzzy Lop (AFL), has become popular thanks to its ease-of-use and bug-finding power. However, AFL remains limited in the bugs it can find since it simply does not cover large regions of code. If it does not cover parts of the code, it will not find bugs there. We propose a two-pronged approach to increase the coverage achieved by AFL. First, the approach automatically identifies branches exercised by few AFL-produced inputs (rare branches), which often guard code that is empirically hard to cover by naively mutating inputs. The second part of the approach is a novel mutation mask creation algorithm, which allows mutations to be biased towards producing inputs hitting a given rare branch. This mask is dynamically computed during fuzz testing and can be adapted to other testing targets. We implement this approach on top of AFL in a tool named FairFuzz. We conduct evaluation on real-world programs against state-of-the-art versions of AFL. We find that on these programs FairFuzz achieves high branch coverage at a faster rate that state-of-the-art versions of AFL. In addition, on programs with nested conditional structure, it achieves sustained increases in branch coverage after 24 hours (average 10.6% increase). In qualitative analysis, we find that FairFuzz has an increased capacity to automatically discover keywords."}, {"id": "conf/kbse/FanSCMLXP18", "title": "Efficiently manifesting asynchronous programming errors in Android apps.", "authors": ["Lingling Fan", "Ting Su", "Sen Chen", "Guozhu Meng", "Yang Liu", "Lihua Xu", "Geguang Pu"], "DOIs": ["https://doi.org/10.1145/3238147.3238170"], "tag": ["Software Quality"], "abstract": "ABSTRACTAndroid, the #1 mobile app framework, enforces the single-GUI-thread model, in which a single UI thread manages GUI rendering and event dispatching. Due to this model, it is vital to avoid blocking the UI thread for responsiveness. One common practice is to offload long-running tasks into async threads. To achieve this, Android provides various async programming constructs, and leaves evelopers themselves to obey the rules implied by the model. However, as our study reveals, more than 25% apps violate these rules and introduce hard-to-detect, fail-stop errors, which we term as aysnc programming errors (APEs). To this end, this paper introduces APEChecker, a technique to automatically and efficiently manifest APEs. The key idea is to characterize APEs as specific fault patterns, and synergistically combine static analysis and dynamic UI exploration to detect and verify such errors. Among the 40 real-world Android apps, APEChecker unveils and processes 61 APEs, of which 51 are confirmed (83.6% hit rate). Specifically, APEChecker detects 3X more APEs than the state-of-art testing tools (Monkey, Sapienz and Stoat), and reduces testing time from half an hour to a few minutes. On a specific type of APEs, APEChecker confirms 5X more errors than the data race detection tool, EventRacer, with very few false alarms."}, {"id": "conf/kbse/DouHXZ018", "title": "Expandable group identification in spreadsheets.", "authors": ["Wensheng Dou", "Shi Han", "Liang Xu", "Dongmei Zhang", "Jun Wei"], "DOIs": ["https://doi.org/10.1145/3238147.3238222"], "tag": ["Software Quality"], "abstract": "ABSTRACTSpreadsheets are widely used in various business tasks. Spreadsheet users may put similar data and computations by repeating a block of cells (a unit) in their spreadsheets. We name the unit and all its expanding ones as an expandable group. All units in an expandable group share the same or similar formats and semantics. As a data storage and management tool, expandable groups represent the fundamental structure in spreadsheets. However, existing spreadsheet systems do not recognize any expandable groups. Therefore, other spreadsheet analysis tools, e.g., data integration and fault detection, cannot utilize this structure of expandable groups to perform precise analysis. In this paper, we propose ExpCheck to automatically extract expandable groups in spreadsheets. We observe that continuous units that share the similar formats and semantics are likely to be an expandable group. Inspired by this, we inspect the format of each cell and its corresponding semantics, and further classify them into expandable groups according to their similarity. We evaluate ExpCheck on 120 spreadsheets randomly sampled from the EUSES and VEnron corpora. The experimental results show that ExpCheck is effective. ExpCheck successfully detect expandable groups with F1-measure of 73.1%, significantly outperforming the state-of-the-art techniques (F1-measure of 13.3%)."}, {"id": "conf/kbse/Lin0TBWD18", "title": "Break the dead end of dynamic slicing: localizing data and control omission bug.", "authors": ["Yun Lin", "Jun Sun", "Lyly Tran", "Guangdong Bai", "Haijun Wang", "Jin Song Dong"], "DOIs": ["https://doi.org/10.1145/3238147.3238163"], "tag": ["Software Quality"], "abstract": "ABSTRACTDynamic slicing is a common way of identifying the root cause when a program fault is revealed. With the dynamic slicing technique, the programmers can follow data and control flow along the program execution trace to the root cause. However, the technique usually fails to work on omission bugs, i.e., the faults which are caused by missing executing some code. In many cases, dynamic slicing over-skips the root cause when an omission bug happens, leading the debugging process to a dead end. In this work, we conduct an empirical study on the omission bugs in the Defects4J bug repository. Our study shows that (1) omission bugs are prevalent (46.4%) among all the studied bugs; (2) there are repeating patterns on causes and fixes of the omission bugs; (3) the patterns of fixing omission bugs serve as a strong hint to break the slicing dead end. Based on our findings, we train a neural network model on the omission bugs in Defects4J repository to recommend where to approach when slicing can no long work. We conduct an experiment by applying our approach on 3193 mutated omission bugs which slicing fails to locate. The results show that our approach outperforms random benchmark on breaking the dead end and localizing the mutated omission bugs (63.8% over 2.8%)."}, {"id": "conf/kbse/DegiovanniMRA18", "title": "A genetic algorithm for goal-conflict identification.", "authors": ["Renzo Degiovanni", "Facundo Molina", "Germ\u00e1n Regis", "Nazareno Aguirre"], "DOIs": ["https://doi.org/10.1145/3238147.3238220"], "tag": ["Architecture and Requirements"], "abstract": "ABSTRACTGoal-conflict analysis has been widely used as an abstraction for risk analysis in goal-oriented requirements engineering approaches. In this context, where the expected behaviour of the system-to-be is captured in terms of domain properties and goals, identifying combinations of circumstances that may make the goals diverge, i.e., not to be satisfied as a whole, is of most importance. Various approaches have been proposed in order to automatically identify boundary conditions, i.e., formulas capturing goal-divergent situations, but they either apply only to some specific goal expressions, or are affected by scalability issues that make them applicable only to relatively small specifications. In this paper, we present a novel approach to automatically identify boundary conditions, using evolutionary computation. More precisely, we develop a genetic algorithm that, given the LTL formulation of the domain properties and the goals, it searches for formulas that capture divergences in the specification. We exploit a modern LTL satisfiability checker to successfully guide our genetic algorithm to the solutions. We assess our technique on a set of case studies, and show that our genetic algorithm is able to find boundary conditions that cannot be generated by related approaches, and is able to efficiently scale to LTL specifications that other approaches are unable to deal with."}, {"id": "conf/kbse/HuangWLC18", "title": "Understanding and detecting callback compatibility issues for Android applications.", "authors": ["Huaxun Huang", "Lili Wei", "Yepang Liu", "Shing-Chi Cheung"], "DOIs": ["https://doi.org/10.1145/3238147.3238181"], "tag": ["Mobile Analysis"], "abstract": "ABSTRACTThe control flows of Android apps are largely driven by the protocols that govern how callback APIs are invoked in response to various events. When these callback APIs evolve along with the Android framework, the changes in their invocation protocols can induce unexpected control flows to existing Android apps, causing various compatibility issues. We refer to these issues as callback compatibility issues. While Android framework updates have received due attention, little is known about their impacts on app control flows and the callback compatibility issues thus induced. To bridge the gap, we examined Android documentations and conducted an empirical study on 100 real-world callback compatibility issues to investigate how these issues were induced by callback API evolutions. Based on our empirical findings, we propose a graph-based model to capture the control flow inconsistencies caused by API evolutions and devise a static analysis technique, Cider, to detect callback compatibility issues. Our evaluation of Cider\u00a0on 20 popular open-source Android apps shows that Cider\u00a0is effective. It detected 13 new callback compatibility issues in these apps, among which 12 issues were confirmed and 9 issues were fixed."}, {"id": "conf/kbse/MoranWHPP18", "title": "Detecting and summarizing GUI changes in evolving mobile apps.", "authors": ["Kevin Moran", "Cody Watson", "John Hoskins", "George Purnell", "Denys Poshyvanyk"], "DOIs": ["https://doi.org/10.1145/3238147.3238203"], "tag": ["Mobile Analysis"], "abstract": "ABSTRACTMobile applications have become a popular software development domain in recent years due in part to a large user base, capable hardware, and accessible platforms. However, mobile developers also face unique challenges, including pressure for frequent releases to keep pace with rapid platform evolution, hardware iteration, and user feedback. Due to this rapid pace of evolution, developers need automated support for documenting the changes made to their apps in order to aid in program comprehension. One of the more challenging types of changes to document in mobile apps are those made to the graphical user interface (GUI) due to its abstract, pixel-based representation. In this paper, we present a fully automated approach, called GCAT, for detecting and summarizing GUI changes during the evolution of mobile apps. GCAT leverages computer vision techniques and natural language generation to accurately and concisely summarize changes made to the GUI of a mobile app between successive commits or releases. We evaluate the performance of our approach in terms of its precision and recall in detecting GUI changes compared to developer specified changes, and investigate the utility of the generated change reports in a controlled user study. Our results indicate that GCAT is capable of accurately detecting and classifying GUI changes - outperforming developers - while providing useful documentation."}, {"id": "conf/kbse/ZhaoWLM18", "title": "Empirically assessing opportunities for prefetching and caching in mobile apps.", "authors": ["Yixue Zhao", "Paul Wat", "Marcelo Schmitt Laser", "Nenad Medvidovic"], "DOIs": ["https://doi.org/10.1145/3238147.3238215"], "tag": ["Mobile Analysis"], "abstract": "ABSTRACTNetwork latency in mobile software has a large impact on user experience, with potentially severe economic consequences. Prefetching and caching have been shown effective in reducing the latencies in browser-based systems. However, those techniques cannot be directly applied to the emerging domain of mobile apps because of the differences in network interactions. Moreover, there is a lack of research on prefetching and caching techniques that may be suitable for the mobile app domain, and it is not clear whether such techniques can be effective or whether they are even feasible. This paper takes the first step toward answering these questions by conducting a comprehensive study to understand the characteristics of HTTP requests in over 1,000 popular Android apps. Our work focuses on the prefetchability of requests using static program analysis techniques and cacheability of resulting responses. We find that there is a substantial opportunity to leverage prefetching and caching in mobile apps, but that suitable techniques must take into account the nature of apps\u2019 network interactions and idiosyncrasies such as untrustworthy HTTP header information. Our observations provide guidelines for developers to utilize prefetching and caching schemes in app development, and motivate future research in this area."}, {"id": "conf/kbse/SteinCSC18", "title": "Safe stream-based programming with refinement types.", "authors": ["Benno Stein", "Lazaro Clapp", "Manu Sridharan", "Bor-Yuh Evan Chang"], "DOIs": ["https://doi.org/10.1145/3238147.3238174"], "tag": ["Mobile Analysis"], "abstract": "ABSTRACTIn stream-based programming, data sources are abstracted as a stream of values that can be manipulated via callback functions. Stream-based programming is exploding in popularity, as it provides a powerful and expressive paradigm for handling asynchronous data sources in interactive software. However, high-level stream abstractions can also make it difficult for developers to reason about control- and data-flow relationships in their programs. This is particularly impactful when asynchronous stream-based code interacts with thread-limited features such as UI frameworks that restrict UI access to a single thread, since the threading behavior of streaming constructs is often non-intuitive and insufficiently documented.  In this paper, we present a type-based approach that can statically prove the thread-safety of UI accesses in stream-based software. Our key insight is that the fluent APIs of stream-processing frameworks enable the tracking of threads via type-refinement, making it possible to reason automatically about what thread a piece of code runs on -- a difficult problem in general.  We implement the system as an annotation-based Java typechecker for Android programs built upon the popular ReactiveX framework and evaluate its efficacy by annotating and analyzing 8 open-source apps, where we find 33 instances of unsafe UI access while incurring an annotation burden of only one annotation per 186 source lines of code. We also report on our experience applying the typechecker to two much larger apps from the Uber Technologies, Inc. codebase, where it currently runs on every code change and blocks changes that introduce potential threading bugs."}, {"id": "conf/kbse/WangSK18", "title": "Automated model repair for Alloy.", "authors": ["Kaiyuan Wang", "Allison Sullivan", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1145/3238147.3238162"], "tag": ["Repair"], "abstract": "ABSTRACTAutomated program repair is an active research area. However, existing research focuses mostly on imperative code, e.g. in Java. In this paper, we study the problem of repairing declarative models in Alloy -- a first order relational logic with transitive closure. We introduce ARepair, the first technique for repairing Alloy models. ARepair follows the spirit of traditional automated program repair techniques. Specifically, ARepair takes as input a faulty Alloy model and a test suite that contains some failing test, and outputs a repaired model that is correct with respect to the given tests. ARepair integrates ideas from mutation testing and program synthesis to provide an effective solution for repairing Alloy models. The experimental results show that ARepair can fix 28 out of 38 real-world faulty models we collected."}, {"id": "conf/kbse/LinWLSZW18", "title": "PFix: fixing concurrency bugs based on memory access patterns.", "authors": ["Huarui Lin", "Zan Wang", "Shuang Liu", "Jun Sun", "Dongdi Zhang", "Guangning Wei"], "DOIs": ["https://doi.org/10.1145/3238147.3238198"], "tag": ["Repair"], "abstract": "ABSTRACTConcurrency bugs of a multi-threaded program may only manifest with certain scheduling, i.e., they are heisenbugs which are observed only from time to time if we execute the same program with the same input multiple times. They are notoriously hard to fix. In this work, we propose an approach to automatically fix concurrency bugs. Compared to previous approaches, our key idea is to systematically fix concurrency bugs by inferring locking policies from failure inducing memory-access patterns. That is, we automatically identify memory-access patterns which are correlated with the manifestation of the bug, and then conjecture what is the intended locking policy of the program. Afterwards, we fix the program by implementing the locking policy so that the failure inducing memory-access patterns are made impossible. We have implemented our approach in a toolkit called PFix which supports Java programs. We applied PFix to a set of 23 concurrency bugs and are able to automatically fix 19 of them. In comparison, Grail which is the state-of-the-art tool for fixing concurrency bugs in Java programs can only fix 3 of them correctly."}, {"id": "conf/kbse/BajammalM018", "title": "Generating reusable web components from mockups.", "authors": ["Mohammad Bajammal", "Davood Mazinanian", "Ali Mesbah"], "DOIs": ["https://doi.org/10.1145/3238147.3238194"], "tag": ["Repair"], "abstract": "ABSTRACTThe transformation of a user interface mockup designed by a graphic designer to web components in the final app built by a web developer is often laborious, involving manual and time consuming steps. We propose an approach to automate this aspect of web development by generating reusable web components from a mockup. Our approach employs visual analysis of the mockup, and unsupervised learning of visual cues to create reusable web components (e.g., React components). We evaluated our approach, implemented in a tool called VizMod, on five real-world web mockups, and assessed the transformations and generated components through comparison with web development experts. The results show that VizMod achieves on average 94% precision and 75% recall in terms of agreement with the developers' assessment. Furthermore, the refactorings yielded 22% code reusability, on average."}, {"id": "conf/kbse/TonderKG18", "title": "Semantic crash bucketing.", "authors": ["Rijnard van Tonder", "John Kotheimer", "Claire Le Goues"], "DOIs": ["https://doi.org/10.1145/3238147.3238200"], "tag": ["Repair"], "abstract": "ABSTRACTPrecise crash triage is important for automated dynamic testing tools, like fuzzers. At scale, fuzzers produce millions of crashing inputs. Fuzzers use heuristics, like stack hashes, to cut down on duplicate bug reports. These heuristics are fast, but often imprecise: even after deduplication, hundreds of uniquely reported crashes can still correspond to the same bug. Remaining crashes must be inspected manually, incurring considerable effort. In this paper we present Semantic Crash Bucketing, a generic method for precise crash bucketing using program transformation. Semantic Crash Bucketing maps crashing inputs to unique bugs as a function of changing a program (i.e., a semantic delta). We observe that a real bug fix precisely identifies crashes belonging to the same bug. Our insight is to approximate real bug fixes with lightweight program transformation to obtain the same level of precision. Our approach uses (a) patch templates and (b) semantic feedback from the program to automatically generate and apply approximate fixes for general bug classes. Our evaluation shows that approximate fixes are competitive with using true fixes for crash bucketing, and significantly outperforms built-in deduplication techniques for three state of the art fuzzers."}, {"id": "conf/kbse/WangCYJ18", "title": "A symbolic model checking approach to the analysis of string and length constraints.", "authors": ["Hung-En Wang", "Shih-Yu Chen", "Fang Yu", "Jie-Hong R. Jiang"], "DOIs": ["https://doi.org/10.1145/3238147.3238189"], "tag": ["Verification 2"], "abstract": "ABSTRACTStrings with length constraints are prominent in software security analysis. Recent endeavors have made significant progress in developing constraint solvers for strings and integers. Most prior methods are based on deduction with inference rules or analysis using automata. The former may be inefficient when the constraints involve complex string manipulations such as language replacement; the latter may not be easily extended to handle length constraints and may be inadequate for counterexample generation due to approximation. Inspired by recent work on string analysis with logic circuit representation, we propose a new method for solving string with length constraints by an implicit representation of automata with length encoding. The length-encoded automata are of infinite states and can represent languages beyond regular expressions. By converting string and length constraints into a dependency graph of manipulations over length-encoded automata, a symbolic model checker for infinite state systems can be leveraged as an engine for the analysis of string and length constraints. Experiments show that our method has its unique capability of handling complex string and length constraints not solvable by existing methods."}, {"id": "conf/kbse/0001F18", "title": "Domain-independent multi-threaded software model checking.", "authors": ["Dirk Beyer", "Karlheinz Friedberger"], "DOIs": ["https://doi.org/10.1145/3238147.3238195"], "tag": ["Verification 2"], "abstract": "ABSTRACTRecent development of software aims at massively parallel execution, because of the trend to increase the number of processing units per CPU socket. But many approaches for program analysis are not designed to benefit from a multi-threaded execution and lack support to utilize multi-core computers. Rewriting existing algorithms is difficult and error-prone, and the design of new parallel algorithms also has limitations. An orthogonal problem is the granularity: computing each successor state in parallel seems too fine-grained, so the open question is to find the right structural level for parallel execution. We propose an elegant solution to these problems: Block summaries should be computed in parallel. Many successful approaches to software verification are based on summaries of control-flow blocks, large blocks, or function bodies. Block-abstraction memoization is a successful domain-independent approach for summary-based program analysis. We redesigned the verification approach of block-abstraction memoization starting from its original recursive definition, such that it can run in a parallel manner for utilizing the available computation resources without losing its advantages of being independent from a certain abstract domain. We present an implementation of our new approach for multi-core shared-memory machines. The experimental evaluation shows that our summary-based approach has no significant overhead compared to the existing sequential approach and that it has a significant speedup when using multi-threading."}, {"id": "conf/kbse/Yin0LW18", "title": "Scheduling constraint based abstraction refinement for weak memory models.", "authors": ["Liangze Yin", "Wei Dong", "Wanwei Liu", "Ji Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3238223"], "tag": ["Verification 2"], "abstract": "ABSTRACTScheduling constraint based abstraction refinement (SCAR) is one of the most efficient methods for verifying programs under sequential consistency (SC). However, most multi-processor architectures implement weak memory models (WMMs) in order to improve the performance of a program. Due to the nondeterministic execution of those memory operations by the same thread, the behavior of a program under WMMs is much more complex than that under SC, which significantly increases the verification complexity. This paper elegantly extends the SCAR method to WMMs such as TSO and PSO. To capture the order requirements of an abstraction counterexample under WMMs, we have enriched the event order graph (EOG) of a counterexample such that it is competent for both SC and WMMs. We have also proposed a unified EOG generation method which can always obtain a minimal EOG efficiently. Experimental results on a large set of multi-threaded C programs show promising results of our method. It significantly outperforms state-of-the-art tools, and the time and memory it required to verify a program under TSO and PSO are roughly comparable to that under SC."}, {"id": "conf/kbse/SungLEW18", "title": "Datalog-based scalable semantic diffing of concurrent programs.", "authors": ["Chungha Sung", "Shuvendu K. Lahiri", "Constantin Enea", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3238211"], "tag": ["Code Differencing and Merging"], "abstract": "ABSTRACTWhen an evolving program is modified to address issues related to thread synchronization, there is a need to confirm the change is correct, i.e., it does not introduce unexpected behavior. However, manually comparing two programs to identify the semantic difference is labor intensive and error prone, whereas techniques based on model checking are computationally expensive. To fill the gap, we develop a fast and approximate static analysis for computing synchronization differences of two programs. The method is fast because, instead of relying on heavy-weight model checking techniques, it leverages a polynomial-time Datalog-based program analysis framework to compute differentiating data-flow edges, i.e., edges allowed by one program but not the other. Although approximation is used our method is sufficiently accurate due to careful design of the Datalog inference rules and iterative increase of the required data-flow edges for representing a difference. We have implemented our method and evaluated it on a large number of multithreaded C programs to confirm its ability to produce, often within seconds, the same differences obtained by human; in contrast, prior techniques based on model checking take minutes or even hours and thus can be 10x to 1000x slower."}, {"id": "conf/kbse/LiuHZLLPZ18", "title": "\u03b1Diff: cross-version binary code similarity detection with DNN.", "authors": ["Bingchang Liu", "Wei Huo", "Chao Zhang", "Wenchao Li", "Feng Li", "Aihua Piao", "Wei Zou"], "DOIs": ["https://doi.org/10.1145/3238147.3238199"], "tag": ["Code Differencing and Merging"], "abstract": "ABSTRACTBinary code similarity detection (BCSD) has many applications, including patch analysis, plagiarism detection, malware detection, and vulnerability search etc. Existing solutions usually perform comparisons over specific syntactic features extracted from binary code, based on expert knowledge. They have either high performance overheads or low detection accuracy. Moreover, few solutions are suitable for detecting similarities between cross-version binaries, which may not only diverge in syntactic structures but also diverge slightly in semantics. In this paper, we propose a solution \u03b1Diff, employing three semantic features, to address the cross-version BCSD challenge. It first extracts the intra-function feature of each binary function using a deep neural network (DNN). The DNN works directly on raw bytes of each function, rather than features (e.g., syntactic structures) provided by experts. \u03b1Diff further analyzes the function call graph of each binary, which are relatively stable in cross-version binaries, and extracts the inter-function and inter-module features. Then, a distance is computed based on these three features and used for BCSD. We have implemented a prototype of \u03b1Diff, and evaluated it on a dataset with about 2.5 million samples. The result shows that \u03b1Diff outperforms state-of-the-art static solutions by over 10 percentages on average in different BCSD settings."}, {"id": "conf/kbse/HuangCPZWLZ18", "title": "ClDiff: generating concise linked code differences.", "authors": ["Kaifeng Huang", "Bihuan Chen", "Xin Peng", "Daihong Zhou", "Ying Wang", "Yang Liu", "Wenyun Zhao"], "DOIs": ["https://doi.org/10.1145/3238147.3238219"], "tag": ["Code Differencing and Merging"], "abstract": "ABSTRACTAnalyzing and understanding source code changes is important in a variety of software maintenance tasks. To this end, many code differencing and code change summarization methods have been proposed. For some tasks (e.g. code review and software merging), however, those differencing methods generate too fine-grained a representation of code changes, and those summarization methods generate too coarse-grained a representation of code changes. Moreover, they do not consider the relationships among code changes. Therefore, the generated differences or summaries make it not easy to analyze and understand code changes in some software maintenance tasks.  In this paper, we propose a code differencing approach, named CLDIFF, to generate concise linked code differences whose granularity is in between the existing code differencing and code change summarization methods. The goal of CLDIFF is to generate more easily understandable code differences. CLDIFF takes source code files before and after changes as inputs, and consists of three steps. First, it pre-processes the source code files by pruning unchanged declara- tions from the parsed abstract syntax trees. Second, it generates concise code differences by grouping fine-grained code differences at or above the statement level and describing high-level changes in each group. Third, it links the related concise code differences according to five pre-defined links. Experiments with 12 Java projects (74,387 commits) and a human study with 10 participants have indicated the accuracy, conciseness, performance and usefulness of CLDIFF."}, {"id": "conf/kbse/YanDWW0Z18", "title": "Characterizing and identifying misexposed activities in Android applications.", "authors": ["Jiwei Yan", "Xi Deng", "Ping Wang", "Tianyong Wu", "Jun Yan", "Jian Zhang"], "DOIs": ["https://doi.org/10.1145/3238147.3238164"], "tag": ["Mobile Security"], "abstract": "ABSTRACTExported Activity (EA), a kind of activities in Android apps that can be launched by external components, is one of the most important inter-component communication (ICC) mechanisms to realize the interaction and cooperation among multiple apps. Existing works have pointed out that, once exposed, an activity will be vulnerable to malicious ICC attacks, such as permission leakage attack. Unfortunately, it is observed that a considerable number of activities in commercial apps are exposed inadvertently, while few works have studied the necessity and reasonability of such exposure. This work takes the first step to systematically study the exposing behavior of EAs through analyzing 13,873 Android apps. It utilizes the EA associated call relationships extracted from byte-code via data-flow analysis, as well as the launch conditions obtained from the manifest files, to guide the study on the usage and misexposure of EAs. The empirical findings are that the EA mechanism is widely adopted in development and the activities are liable to be misexposed due to the developers' misunderstanding or carelessness. Further study on subsets of apps selected according to different criteria indicates that the misexposed EAs have specific characteristics, which are manually summarized into six typical misuse patterns. As a consequence, ten heuristics are designed to decide whether an activity should be exposed or not and are implemented into an automatic tool called Mist. Experiments on the collected apps show that around one fifth EAs are unnecessarily exposed and there are more than one third EAs whose exposure may not be suggested."}, {"id": "conf/kbse/HuWLCH18", "title": "A tale of two cities: how WebView induces bugs to Android applications.", "authors": ["Jiajun Hu", "Lili Wei", "Yepang Liu", "Shing-Chi Cheung", "Huaxun Huang"], "DOIs": ["https://doi.org/10.1145/3238147.3238180"], "tag": ["Mobile Security"], "abstract": "ABSTRACTWebView is a widely used Android component that augments a native app with web browser capabilities. It eases the interactions between an app\u2019s native code and web code. However, the interaction mechanism of WebView induces new types of bugs in Android apps. Understanding the characteristics and manifestation of these WebView-induced bugs (\u03c9Bugs for short) facilitates the correct usages of WebViews in Android apps. This motivates us to conduct the first empirical study on \u03c9Bugs based on those found in popular open-source Android apps. Our study identified the major root causes and consequences of \u03c9Bugs and made interesting observations that can be leveraged for detecting and diagnosing \u03c9Bugs. Based on the empirical study, we further propose an automated testing technique \u03c9Droid to effectively expose \u03c9Bugs in Android apps. In our experiments, \u03c9Droid successfully discovered 30 unique and previously-unknown \u03c9Bugs when applied to 146 open-source Android apps. We reported the 30 \u03c9Bugs to the corresponding app developers. Out of these 30 \u03c9Bugs, 14 were confirmed and 7 of them were fixed. This shows that \u03c9Droid can effectively detect \u03c9Bugs that are of the developers\u2019 concern."}, {"id": "conf/kbse/TangZPAM0Z18", "title": "Dual-force: understanding WebView malware via cross-language forced execution.", "authors": ["Zhenhao Tang", "Juan Zhai", "Minxue Pan", "Yousra Aafer", "Shiqing Ma", "Xiangyu Zhang", "Jianhua Zhao"], "DOIs": ["https://doi.org/10.1145/3238147.3238221"], "tag": ["Mobile Security"], "abstract": "ABSTRACTModern Android malwares tend to use advanced techniques to cover their malicious behaviors. They usually feature multi-staged, condition-guarded and environment-specific payloads. An increasing number of them utilize WebView, particularly the two-way communications between Java and JavaScript, to evade detection and analysis of existing techniques. We propose Dual-Force, a forced execution technique which simultaneously forces both Java and JavaScript code of WebView applications to execute along various paths without requiring any environment setup or providing any inputs manually. As such, the hidden payloads of WebView malwares are forcefully exposed. The technique features a novel execution model that allows forced execution to suppress exceptions and continue execution. Experimental results show that Dual-Force precisely exposes malicious payload in 119 out of 150 WebView malwares. Compared to the state-of-the-art, Dual-Force can expose 23% more malicious behaviors."}, {"id": "conf/kbse/HammadGM18", "title": "Self-protection of Android systems from inter-component communication attacks.", "authors": ["Mahmoud Hammad", "Joshua Garcia", "Sam Malek"], "DOIs": ["https://doi.org/10.1145/3238147.3238207"], "tag": ["Mobile Security"], "abstract": "ABSTRACTThe current security mechanisms for Android apps, both static and dynamic analysis approaches, are insufficient for detection and prevention of the increasingly dynamic and sophisticated security attacks. Static analysis approaches suffer from false positives whereas dynamic analysis approaches suffer from false negatives. Moreover, they all lack the ability to efficiently analyze systems with incremental changes\u2014such as adding/removing apps, granting/revoking permissions, and dynamic components\u2019 communications. Each time the system changes, the entire analysis needs to be repeated, making the existing approaches inefficient for practical use. To mitigate their shortcomings, we have developed SALMA, a novel self-protecting Android software system that monitors itself and adapts its behavior at runtime to prevent a wide-range of security risks. SALMA maintains a precise architectural model, represented as a Multiple-Domain-Matrix, and incrementally and efficiently analyzes an Android system in response to incremental system changes. The maintained architecture is used to reason about the running Android system. Every time the system changes, SALMA determines (1) the impacted part of the system, and (2) the subset of the security analyses that need to be performed, thereby greatly improving the performance of the approach. Our experimental results on hundreds of real-world apps corroborate SALMA\u2019s scalability and efficiency as well as its ability to detect and prevent security attacks at runtime with minimal disruption."}, {"id": "conf/kbse/WangLYCZDX18", "title": "An empirical study of Android test generation tools in industrial cases.", "authors": ["Wenyu Wang", "Dengfeng Li", "Wei Yang", "Yurui Cao", "Zhenwen Zhang", "Yuetang Deng", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3238147.3240465"], "tag": ["Experience Papers"], "abstract": "ABSTRACTUser Interface (UI) testing is a popular approach to ensure the quality of mobile apps. Numerous test generation tools have been developed to support UI testing on mobile apps, especially for Android apps. Previous work evaluates and compares different test generation tools using only relatively simple open-source apps, while real-world industrial apps tend to have more complex functionalities and implementations. There is no direct comparison among test generation tools with regard to effectiveness and ease-of-use on these industrial apps. To address such limitation, we study existing state-of-the-art or state-of-the-practice test generation tools on 68 widely-used industrial apps. We directly compare the tools with regard to code coverage and fault-detection ability. According to our results, Monkey, a state-of-the-practice tool from Google, achieves the highest method coverage on 22 of 41 apps whose method coverage data can be obtained. Of all 68 apps under study, Monkey also achieves the highest activity coverage on 35 apps, while Stoat, a state-of-the-art tool, is able to trigger the highest number of unique crashes on 23 apps. By analyzing the experimental results, we provide suggestions for combining different test generation tools to achieve better performance. We also report our experience in applying these tools to industrial apps under study. Our study results give insights on how Android UI test generation tools could be improved to better handle complex industrial apps."}, {"id": "conf/kbse/GafurovHM18", "title": "Achieving test automation with testers without coding skills: an industrial report.", "authors": ["Davrondzhon Gafurov", "Arne Erik Hurum", "Martin Markman"], "DOIs": ["https://doi.org/10.1145/3238147.3240463"], "tag": ["Experience Papers"], "abstract": "ABSTRACTWe present a process driven test automation solution which enables delegating (part of) automation tasks from test automation engineer (expensive resource) to test analyst (non-developer, less expensive). In our approach, a test automation engineer implements test steps (or actions) which are executed automatically. Such automated test steps represent user actions in the system under test and specified by a natural language which is understandable by a non-technical person. Then, a test analyst with a domain knowledge organizes automated steps combined with test input to create an automated test case. It should be emphasized that the test analyst does not need to possess programming skills to create, modify or execute automated test cases. We refine benchmark test automation architecture to be better suitable for an effective separation and sharing of responsibilities between the test automation engineer (with coding skills) and test analyst (with a domain knowledge). In addition, we propose a metric to empirically estimate cooperation between test automation engineer and test analyst's works. The proposed automation solution has been defined based on our experience in the development and maintenance of Helsenorg, the national electronic health services in Norway which has had over one million of visits per month past year, and we still use it to automate the execution of regression tests."}, {"id": "conf/kbse/CashmanCRC18", "title": "Navigating the maze: the impact of configurability in bioinformatics software.", "authors": ["Mikaela Cashman", "Myra B. Cohen", "Priya Ranjan", "Robert W. Cottingham"], "DOIs": ["https://doi.org/10.1145/3238147.3240466"], "tag": ["Experience Papers"], "abstract": "ABSTRACTThe bioinformatics software domain contains thousands of applications for automating tasks such as the pairwise alignment of DNA sequences, building and reasoning about metabolic models or simulating growth of an organism. Its end users range from sophisticated developers to those with little computational experience. In response to their needs, developers provide many options to customize the way their algorithms are tuned. Yet there is little or no automated help for the user in determining the consequences or impact of the options they choose. In this paper we describe our experience working with configurable bioinformatics tools. We find limited documentation and help for combining and selecting options along with variation in both functionality and performance. We also find previously undetected faults. We summarize our findings with a set of lessons learned, and present a roadmap for creating automated techniques to interact with bioinformatics software. We believe these will generalize to other types of scientific software."}, {"id": "conf/kbse/BugariuWC018", "title": "Automatically testing implementations of numerical abstract domains.", "authors": ["Alexandra Bugariu", "Valentin W\u00fcstholz", "Maria Christakis", "Peter M\u00fcller"], "DOIs": ["https://doi.org/10.1145/3238147.3240464"], "tag": ["Experience Papers"], "abstract": "ABSTRACTStatic program analyses are routinely applied as the basis of code optimizations and to detect safety and security issues in software systems. For their results to be reliable, static analyses should be sound (i.e., should not produce false negatives) and precise (i.e., should report a low number of false positives). Even though it is possible to prove properties of the design of a static analysis, ensuring soundness and precision for its implementation is challenging. Complex algorithms and sophisticated optimizations make static analyzers difficult to implement and test. In this paper, we present an automatic technique to test, among other properties, the soundness and precision of abstract domains, the core of all static analyzers based on abstract interpretation. In order to cover a wide range of test data and input states, we construct inputs by applying sequences of abstract-domain operations to representative domain elements, and vary the operations through gray-box fuzzing. We use mathematical properties of abstract domains as test oracles. Our experimental evaluation demonstrates the effectiveness of our approach. We detected several previously unknown soundness and precision errors in widely-used abstract domains. Our experiments also show that our approach is more effective than dynamic symbolic execution and than fuzzing the test inputs directly."}, {"id": "conf/kbse/MoSCRKN18", "title": "Experiences applying automated architecture analysis tool suites.", "authors": ["Ran Mo", "Will Snipes", "Yuanfang Cai", "Srini Ramaswamy", "Rick Kazman", "Martin Naedele"], "DOIs": ["https://doi.org/10.1145/3238147.3240467"], "tag": ["Experience Papers"], "abstract": "ABSTRACTIn this paper, we report our experiences of applying three complementary automated software architecture analysis techniques, supported by a tool suite, called DV8, to 8 industrial projects within a large company. DV8 includes two state-of-the-art architecture-level maintainability metrics\u2014Decoupling Level and Propagation Cost, an architecture flaw detection tool, and an architecture root detection tool. We collected development process data from the project teams as input to these tools, reported the results back to the practitioners, and followed up with telephone conferences and interviews. Our experiences revealed that the metrics scores, quantitative debt analysis, and architecture flaw visualization can effectively bridge the gap between management and development, help them decide if, when, and where to refactor. In particular, the metrics scores, compared against industrial benchmarks, faithfully reflected the practitioners\u2019 intuitions about the maintainability of their projects, and enabled them to better understand the maintainability relative to other projects internal to their company, and to other industrial products. The automatically detected architecture flaws and roots enabled the practitioners to precisely pinpoint, visualize, and quantify the \u201chotspots\" within the systems that are responsible for high maintenance costs. Except for the two smallest projects for which both architecture metrics indicated high maintainability, all other projects are planning or have already begun refactorings to address the problems detected by our analyses. We are working on further automating the tool chain, and transforming the analysis suite into deployable services accessible by all projects within the company."}, {"id": "conf/kbse/VassalloPBG18", "title": "Continuous code quality: are we (really) doing that?", "authors": ["Carmine Vassallo", "Fabio Palomba", "Alberto Bacchelli", "Harald C. Gall"], "DOIs": ["https://doi.org/10.1145/3238147.3240729"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTContinuous Integration (CI) is a software engineering practice where developers constantly integrate their changes to a project through an automated build process. The goal of CI is to provide developers with prompt feedback on several quality dimensions after each change. Indeed, previous studies provided empirical evidence on a positive association between properly following CI principles and source code quality. A core principle behind CI is Continuous Code Quality (also known as CCQ, which includes automated testing and automated code inspection) may appear simple and effective, yet we know little about its practical adoption. In this paper, we propose a preliminary empirical investigation aimed at understanding how rigorously practitioners follow CCQ. Our study reveals a strong dichotomy between theory and practice: developers do not perform continuous inspection but rather control for quality only at the end of a sprint and most of the times only on the release branch. Preprint [https://doi.org/10.5281/zenodo.1341036]. Data and Materials [http://doi.org/10.5281/zenodo.1341015]."}, {"id": "conf/kbse/HassanRW18", "title": "RUDSEA: recommending updates of Dockerfiles via software environment analysis.", "authors": ["Foyzul Hassan", "Rodney Rodriguez", "Xiaoyin Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3240470"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTDockerfiles are configuration files of docker images which package all dependencies of a software to enable convenient software deployment and porting. In other words, dockerfiles list all environment assumptions of a software application's build and / or execution, so they need to be frequently updated when the environment assumptions change during fast software evolution. In this paper, we propose RUDSEA, a novel approach to recommend updates of dockerfiles to developers based on analyzing changes on software environment assumptions and their impacts. Our evaluation on 1,199 real-world instruction updates shows that RUDSEA can recommend correct update locations for 78.5% of the updates, and correct code changes for 44.1% of the updates."}, {"id": "conf/kbse/ZhouPX0LJD18", "title": "Delta debugging microservice systems.", "authors": ["Xiang Zhou", "Xin Peng", "Tao Xie", "Jun Sun", "Wenhai Li", "Chao Ji", "Dan Ding"], "DOIs": ["https://doi.org/10.1145/3238147.3240730"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTDebugging microservice systems involves the deployment and manipulation of microservice systems on a containerized environment and faces unique challenges due to the high complexity and dynamism of microservices. To address these challenges, in this paper, we propose a debugging approach for microservice systems based on the delta debugging algorithm, which is to minimize failureinducing deltas of circumstances (e.g., deployment, environmental configurations) for effective debugging. Our approach includes novel techniques for defining, deploying/manipulating, and executing deltas following the idea of delta debugging. In particular, to construct a (failing) circumstance space for delta debugging to minimize, our approach defines a set of dimensions that can affect the execution of microservice systems. Our experimental study on a medium-size microservice benchmark system shows that our approach can effectively identify failure-inducing deltas that help diagnose the root causes."}, {"id": "conf/kbse/Ye0WW18", "title": "Personalized teammate recommendation for crowdsourced software developers.", "authors": ["Luting Ye", "Hailong Sun", "Xu Wang", "Jiaruijue Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3240472"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTMost crowdsourced software development platforms adopt contest paradigm to solicit contributions from the community. To attain competitiveness in complex tasks, crowdsourced software developers often choose to work with others collaboratively. However, existing crowdsourcing platforms generally assume independent contributions from developers and do not provide effective support for team formation. Prior studies on team recommendation aim at optimizing task outcomes by recommending the most suitable team for a task instead of finding appropriate collaborators for a specific person. In this work, we are concerned with teammate recommendation for crowdsourcing developers. First, we present the results of an empirical study of Kaggle, which shows that developers\u2019personal teammate preferences are mainly affected by three factors. Second, we give a collaboration willingness model to characterize developers\u2019 teammate preferences and formulate teammate recommendation as an optimization problem. Then we design a heuristic algorithm to find suitable teammates for a developer. Finally, we have conducted a set of experiments on a Kaggle dataset to evaluate the effectiveness of our approach."}, {"id": "conf/kbse/LiuLZJS18", "title": "S-gram: towards semantic-aware security auditing for Ethereum smart contracts.", "authors": ["Han Liu", "Chao Liu", "Wenqi Zhao", "Yu Jiang", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3238147.3240728"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTSmart contracts, as a promising and powerful application on the Ethereum blockchain, have been growing rapidly in the past few years. Since they are highly vulnerable to different forms of attacks, their security becomes a top priority. However, existing security auditing techniques are either limited in fnding vulnerabilities (rely on pre-defned bug paterns) or very expensive (rely on program analysis), thus are insufcient for Ethereum.  To mitigate these limitations, we proposed a novel semanticaware security auditing technique called S-gram for Ethereum. The key insight is a combination of N-gram language modeling and lightweight static semantic labeling, which can learn statistical regularities of contract tokens and capture high-level semantics as well (e.g., flow sensitivity of a transaction). S-gram can be used to predict potential vulnerabilities by identifying irregular token sequences and optimize existing in-depth analyzers (e.g., symbolic execution engines, fuzzers etc.). We have implemented S-gram for Solidity smart contracts in Ethereum. The evaluation demonstrated the potential of S-gram in identifying possible security issues."}, {"id": "conf/kbse/WangBC18", "title": "An evolutionary approach for analyzing Alloy specifications.", "authors": ["Jianghao Wang", "Hamid Bagheri", "Myra B. Cohen"], "DOIs": ["https://doi.org/10.1145/3238147.3240468"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTFormal methods use mathematical notations and logical reasoning to precisely define a program's specifications, from which we can instantiate valid instances of a system. With these techniques we can perform a multitude of tasks to check system dependability. Despite the existence of many automated tools including ones considered lightweight, they still lack a strong adoption in practice. At the crux of this problem, is scalability and applicability to large real world applications. In this paper we show how to relax the completeness guarantee without much loss, since soundness is maintained. We have extended a popular lightweight analysis, Alloy, with a genetic algorithm. Our new tool, EvoAlloy, works at the level of finite relations generated by Kodkod and evolves the chromosomes based on the failed constraints. In a feasibility study we demonstrate that we can find solutions to a set of specifications beyond the scope where traditional Alloy fails. While small specifications take longer with EvoAlloy, the scalability means we can handle larger specifications. Our future vision is that when specifications are small we can maintain both soundness and completeness, but when this fails, EvoAlloy can switch to its genetic algorithm."}, {"id": "conf/kbse/ChenZ18", "title": "A neural framework for retrieval and summarization of source code.", "authors": ["Qingying Chen", "Minghui Zhou"], "DOIs": ["https://doi.org/10.1145/3238147.3240471"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTCode retrieval and summarization are two tasks often employed by software developers to reuse code that spreads over online repositories. In this paper, we present a neural framework that allows bidirectional mapping between source code and natural language to improve these two tasks. Our framework, BVAE, is designed to have two Variational AutoEncoders (VAEs) to model bimodal data: C-VAE for source code and L-VAE for natural language. Both VAEs are trained jointly to reconstruct their input as much as possible with regularization that captures the closeness between the latent variables of code and description. BVAE could learn semantic vector representations for both code and description and generate completely new descriptions for arbitrary code snippets. We design two instance models of BVAE for retrieval and summarization tasks respectively and evaluate their performance on a benchmark which involves two programming languages: C# and SQL. Experiments demonstrate BVAE\u2019s potential on the two tasks."}, {"id": "conf/kbse/TufanoWBPWP18", "title": "An empirical investigation into learning bug-fixing patches in the wild via neural machine translation.", "authors": ["Michele Tufano", "Cody Watson", "Gabriele Bavota", "Massimiliano Di Penta", "Martin White", "Denys Poshyvanyk"], "DOIs": ["https://doi.org/10.1145/3238147.3240732"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTMillions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. We mine millions of bug-fixes from the change histories of GitHub repositories to extract meaningful examples of such bug-fixes. Then, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. Our model is able to fix hundreds of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9% of the cases."}, {"id": "conf/kbse/GaoL18", "title": "Loop path reduction by state pruning.", "authors": ["Jianxiong Gao", "Steven S. Lumetta"], "DOIs": ["https://doi.org/10.1145/3238147.3240731"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTPath explosion has been a problem for symbolic execution for a long time. The key to avoid path explosion is to limit the number of paths generated within loops while maintaining high code coverage. Full symbolic execution creates paths for every possible execution path. Frequently, paths within loops do not contribute to code coverage. Branches within loops may generate new states at every iteration. The path explosion problem created by loops often stops symbolic execution to reach deeper parts of the code. In this paper, we propose a new path exploration method that reduces the number of states needed to achieve high coverage. Our algorithm limits the number of new states created by first prioritizing states, and then pruning the states that do not contribute to code coverage. Our algorithm does not require loop invariant inference/loop summarization, nor does it bound the number of iterations of loop exploration. The proposed algorithm can thus handle a broader set of loops than previous approaches. In fact, our algorithm is orthogonal to loop summarization techniques and search-guide heuristics, so it complements the current methods. We have implemented our algorithm using KLEE and tested with 235 student-generated versions of a classroom assignment. Our results show that our algorithm helps to achieve the same coverage with speedup of 11.8\u00d7 for 117 out of the 235 programs, while adding 15% max observed and 2% average overhead over the 50% of programs not benefiting from the technique. The maximum speedup for a single program is 52.3\u00d7."}, {"id": "conf/kbse/QuLCJCHZ18", "title": "node2defect: using network embedding to improve software defect prediction.", "authors": ["Yu Qu", "Ting Liu", "Jianlei Chi", "Yangxu Jin", "Di Cui", "Ancheng He", "Qinghua Zheng"], "DOIs": ["https://doi.org/10.1145/3238147.3240469"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTNetwork measures have been proved to be useful in predicting software defects. Leveraging the dependency relationships between software modules, network measures can capture various structural features of software systems. However, existing studies have relied on user-defined network measures (e.g., degree statistics or centrality metrics), which are inflexible and require high computation cost, to describe the structural features. In this paper, we propose a new method called node2defect which uses a newly proposed network embedding technique, node2vec, to automatically learn to encode dependency network structure into low-dimensional vector spaces to improve software defect prediction. Specifically, we firstly construct a program's Class Dependency Network. Then node2vec is used to automatically learn structural features of the network. After that, we combine the learned features with traditional software engineering features, for accurate defect prediction. We evaluate our method on 15 open source programs. The experimental results show that in average, node2defect improves the state-of-the-art approach by 9.15% in terms of F-measure."}, {"id": "conf/kbse/PatwardhanSSKG18", "title": "Towards automating disambiguation of regulations: using the wisdom of crowds.", "authors": ["Manasi Patwardhan", "Abhishek Sainani", "Richa Sharma", "Shirish Karande", "Smita Ghaisas"], "DOIs": ["https://doi.org/10.1145/3238147.3240727"], "tag": ["New Ideas Papers"], "abstract": "ABSTRACTCompliant software is a critical need of all modern businesses. Disambiguating regulations to derive requirements is therefore an important software engineering activity. Regulations however are ridden with ambiguities that make their comprehension a challenge, seemingly surmountable only by legal experts. Since legal experts' involvement in every project is expensive, approaches to automate the disambiguation need to be explored. These approaches however require a large amount of annotated data. Collecting data exclusively from experts is not a scalable and affordable solution. In this paper, we present the results of a crowd sourcing experiment to collect annotations on ambiguities in regulations from professional software engineers. We discuss an approach to automate the arduous and critical step of identifying ground truth labels by employing crowd consensus using Expectation Maximization (EM). We demonstrate that the annotations reaching a consensus match those of experts with an accuracy of 87%."}, {"id": "conf/kbse/PereiraSCS18", "title": "jStanley: placing a green thumb on Java collections.", "authors": ["Rui Pereira", "Pedro Sim\u00e3o", "J\u00e1come Cunha", "Jo\u00e3o Saraiva"], "DOIs": ["https://doi.org/10.1145/3238147.3240473"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTSoftware developers are more and more eager to understand their code\u2019s energy performance. However, even with such knowledge it is di cult to know how to improve the code. Indeed, little tool support exists to understand the energy consumption pro le of a software system and to eventually (automatically) improve its code.  In this paper we present a tool termed jStanley which automatically nds collections in Java programs that can be replaced by others with a positive impact on the energy consumption as well as on the execution time. In seconds, developers obtain information about energy-eager collection usage. jStanley will further suggest alternative collections to improve the code, making it use less time, energy, or a combination of both. The preliminary evaluation we ran using jStanley shows energy gains between 2% and 17%, and a reduction in execution time between 2% and 13%.  A video can be seen at https://greensoftwarelab.github.io/jStanley."}, {"id": "conf/kbse/HaririS18", "title": "SRCIROR: a toolset for mutation testing of C source code and LLVM intermediate representation.", "authors": ["Farah Hariri", "August Shi"], "DOIs": ["https://doi.org/10.1145/3238147.3240482"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTWe present SRCIROR (pronounced \u201csorcerer\u201c), a toolset for performing mutation testing at the levels of C/C++ source code (SRC) and the LLVM compiler intermediate representation (IR). At the SRC level, SRCIROR identifies program constructs for mutation by pattern-matching on the Clang AST. At the IR level, SRCIROR directly mutates the LLVM IR instructions through LLVM passes. Our implementation enables SRCIROR to (1) handle any program that Clang can handle, extending to large programs with a minimal overhead, and (2) have a small percentage of invalid mutants that do not compile. SRCIROR enables performing mutation testing using the same classes of mutation operators at both the SRC and IR levels, and it is easily extensible to support more operators. In addition, SRCIROR can collect coverage to generate mutants only for covered code elements. Our tool is publicly available on GitHub (https://github.com/TestingResearchIllinois/srciror). We evaluate SRCIROR on Coreutils subjects. Our evaluation shows interesting differences between SRC and IR, demonstrating the value of SRCIROR in enabling mutation testing research across different levels of code representation."}, {"id": "conf/kbse/OzdemirTEA18", "title": "Lightweight source code monitoring with Triggr.", "authors": ["Alim Ozdemir", "Ayse Tosun", "Hakan Erdogmus", "Rui Abreu"], "DOIs": ["https://doi.org/10.1145/3238147.3240486"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTExisting tools for monitoring the quality of codebases modified by multiple developers tend to be centralized and inflexible. These tools increase the visibility of quality by producing effective reports and visualizations when a change is made to the codebase and triggering alerts when undesirable situations occur. However, their configuration is invariably both (a) centrally managed in that individual maintainers cannot define local rules to receive customized feedback when a change occurs in a specific part of the code in which they are particularly interested, and (b) coarse-grained in that analyses cannot be turned on and off below the file level. Triggr, the tool proposed in this paper, addresses these limitations by allowing distributed, customized, and fine-grained monitoring. It is a lightweight re-implementation of our previous tool, CodeAware, which adopts the same paradigm. The tool listens on a codebase\u2019s shared repository using an event-based approach, and can send alerts to subscribed developers based on rules defined locally by them. Triggr is open-source and available at https://github.com/lyzerk/Triggr. A demonstration video can be found at https://youtu.be/qQs9aDwXJjY."}, {"id": "conf/kbse/ScalabrinoGNGLG18", "title": "OCELOT: a search-based test-data generation tool for C.", "authors": ["Simone Scalabrino", "Giovanni Grano", "Dario Di Nucci", "Michele Guerra", "Andrea De Lucia", "Harald C. Gall", "Rocco Oliveto"], "DOIs": ["https://doi.org/10.1145/3238147.3240477"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTAutomatically generating test cases plays an important role to reduce the time spent by developers during the testing phase. In last years, several approaches have been proposed to tackle such a problem: amongst others, search-based techniques have been shown to be particularly promising. In this paper we describe Ocelot, a search-based tool for the automatic generation of test cases in C. Ocelot allows practitioners to write skeletons of test cases for their programs and researchers to easily implement and experiment new approaches for automatic test-data generation. We show that Ocelot achieves a higher coverage compared to a competitive tool in 81% of the cases. Ocelot is publicly available to support both researchers and practitioners."}, {"id": "conf/kbse/GrigeraGGR18", "title": "Live versioning of web applications through refactoring.", "authors": ["Juli\u00e1n Grigera", "Juan Cruz Gardey", "Alejandra Garrido", "Gustavo Rossi"], "DOIs": ["https://doi.org/10.1145/3238147.3240483"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTClient-Side Web Refactorings (CSWRs) allow improving interaction aspects of web applications by applying changes to the user interface without altering the code base, even in production settings. However, developers are not always willing, or even allowed to apply external adaptations to their applications\u2019 user interface. Besides, CSWRs do not guarantee improvements in all contexts, so it may be unwise to install them in a production version. We propose a tool that allows creating private versions of a running web application almost automatically. Using this tool, developers or usability experts can easily combine CSWRs to create alternative versions of web applications, without the need of creating a cloned sandbox environment for each version. This yields many uses, such as quickly setting up user tests, showing live alternatives to Product Owners, and even performing A/B testing. The tool is built on top of Kobold, a service that allows applying CSWRs to fix usability smells. Kobold with versioning is available at: https://autorefactoring.lifia.info.unlp.edu.ar. A screencast of the tool is available at https://youtu.be/LVc3BOtVP3I."}, {"id": "conf/kbse/JanesMR18", "title": "code_call_lens: raising the developer awareness of critical code.", "authors": ["Andrea Janes", "Michael Mairegger", "Barbara Russo"], "DOIs": ["https://doi.org/10.1145/3238147.3240488"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTAs a developer, it is often complex to foresee the impact of changes in source code on usage, e.g., it is time-consuming to find out all components that will be impacted by a change or estimate the impact on the usability of a failing piece of code. It is therefore hard to decide how much effort in quality assurance is justifiable to obtain the desired business goals. In this paper, to reduce the difficulty for developers to understand the importance of source code, we propose an automated way to provide this information to developers as they are working on a given piece of code. As a proof-of-concept, we developed a plug-in for Microsoft Visual Studio Code that informs about the importance of source code methods based on the frequency of usage by the end-users of the developed software. The plug-in aims to increase the awareness developers have about the importance of source code in an unobtrusive way, helping them to prioritize their effort to quality assurance, technical excellence, and usability. code_call_lens can be downloaded from GitHub at https://github.com/xxMUROxx/vscode.code_call_lens."}, {"id": "conf/kbse/GharibiTL18", "title": "Code2graph: automatic generation of static call graphs for Python source code.", "authors": ["Gharib Gharibi", "Rashmi Tripathi", "Yugyung Lee"], "DOIs": ["https://doi.org/10.1145/3238147.3240484"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTA static call graph is an imperative prerequisite used in most interprocedural analyses and software comprehension tools. However, there is a lack of software tools that can automatically analyze the Python source-code and construct its static call graph. In this paper, we introduce a prototype Python tool, named code2graph, which automates the tasks of (1) analyzing the Python source-code and extracting its structure, (2) constructing static call graphs from the source code, and (3) generating a similarity matrix of all possible execution paths in the system. Our goal is twofold: First, assist the developers in understanding the overall structure of the system. Second, provide a stepping stone for further research that can utilize the tool in software searching and similarity detection applications. For example, clustering the execution paths into a logical workflow of the system would be applied to automate specific software tasks. Code2graph has been successfully used to generate static call graphs and similarity matrices of the paths for three popular open-source Deep Learning projects (TensorFlow, Keras, PyTorch). A tool demo is available at https://youtu.be/ecctePpcAKU."}, {"id": "conf/kbse/BrunelCCM18", "title": "The electrum analyzer: model checking relational first-order temporal specifications.", "authors": ["Julien Brunel", "David Chemouil", "Alcino Cunha", "Nuno Macedo"], "DOIs": ["https://doi.org/10.1145/3238147.3240475"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTThis paper presents the Electrum Analyzer, a free-software tool to validate and perform model checking of Electrum specifications. Electrum is an extension of Alloy that enriches its relational logic with LTL operators, thus simplifying the specification of dynamic systems. The Analyzer supports both automatic bounded model checking, with an encoding into SAT, and unbounded model checking, with an encoding into SMV. Instance, or counter-example, traces are presented back to the user in a unified visualizer. Features to speed up model checking are offered, including a decomposed parallel solving strategy and the extraction of symbolic bounds."}, {"id": "conf/kbse/GadelhaMMC0N18", "title": "ESBMC 5.0: an industrial-strength C model checker.", "authors": ["Mikhail Y. R. Gadelha", "Felipe R. Monteiro", "Jeremy Morse", "Lucas C. Cordeiro", "Bernd Fischer", "Denis A. Nicole"], "DOIs": ["https://doi.org/10.1145/3238147.3240481"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTESBMC is a mature, permissively licensed open-source context-bounded model checker for the verification of single- and multi-threaded C programs. It can verify both predefined safety properties (e.g., bounds check, pointer safety, overflow) and user-defined program assertions automatically. ESBMC provides C++ and Python APIs to access internal data structures, allowing inspection and extension at any stage of the verification process. We discuss improvements over previous versions of ESBMC, including the description of new front- and back-ends, IEEE floating-point support, and an improved k-induction algorithm. A demonstration is available at <pre>https://www.youtube.com/watch?v=YcJjXHlN1v8</pre>."}, {"id": "conf/kbse/CaoLP18", "title": "L-CMP: an automatic learning-based parameterized verification tool.", "authors": ["Jialun Cao", "Yongjian Li", "Jun Pang"], "DOIs": ["https://doi.org/10.1145/3238147.3240487"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTThis demo introduces L-CMP, an automatic learning-based parameterized verification tool. It can verify parameterized protocols by combining machine learning and model checking techniques. Given a parameterized protocol, L-CMP learns a set of auxiliary invariants and implements verification of the protocol using the invariants automatically. In particular, the learned auxiliary invariants are straightforward and readable. The experimental results show that L-CMP can successfully verify a number of cache coherence protocols, including the industrial-scale FLASH protocol. The video is available at https://youtu.be/6Dl2HiiiS4E, and L-CMPL-CMP can be downloaded at https://github.com/ ArabelaTso/Learning-Based-ParaVerifer."}, {"id": "conf/kbse/GaoYFJS18", "title": "VulSeeker: a semantic learning based vulnerability seeker for cross-platform binary.", "authors": ["Jian Gao", "Xin Yang", "Ying Fu", "Yu Jiang", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3238147.3240480"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTCode reuse improves software development efficiency, however, vulnerabilities can be introduced inadvertently. Many existing works compute the code similarity based on CFGs to determine whether a binary function contains a known vulnerability. Unfortunately, their performance in cross-platform binary search is challenged.  This paper presents VulSeeker, a semantic learning based vulnerability seeker for cross-platform binary. Given a target function and a vulnerable function, VulSeeker first constructs the labeled semantic flow graphs and extracts basic block features as numerical vectors for both of them. Then the embedding vector of the whole binary function is generated by feeding the numerical vectors of basic blocks to the customized semantics aware DNN model. Finally, the similarity of the two binary functions is measured based on the Cosine distance. The experimental results show that VulSeeker outperforms the state-of-the-art approaches in terms of accuracy. For example, compared to the most recent and related work Gemini, VulSeeker finds 50.00% more vulnerabilities in the top-10 candidates and 13.89% more in the top-50 candidates, and improves the values of AUC and ACC for 8.23% and 12.14% respectively. The video is presented at https://youtu.be/Mw0mr84gpI8."}, {"id": "conf/kbse/0001018", "title": "CPA-SymExec: efficient symbolic execution in CPAchecker.", "authors": ["Dirk Beyer", "Thomas Lemberger"], "DOIs": ["https://doi.org/10.1145/3238147.3240478"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTWe present CPA-SymExec, a tool for symbolic execution that is implemented in the open-source, configurable verification framework CPAchecker. Our implementation automatically detects which symbolic facts to track, in order to obtain a small set of constraints that are necessary to decide reachability of a program area of interest. CPA-SymExec is based on abstraction and counterexample-guided abstraction refinement (CEGAR), and uses a constraint-interpolation approach to detect symbolic facts. We show that our implementation can better mitigate the path-explosion problem than symbolic execution without abstraction, by comparing the performance to the state-of-the-art Klee-based symbolic-execution engine Symbiotic and to Klee itself. For the experiments we use two kinds of analysis tasks: one for finding an executable path to a specific location of interest (e.g., if a test vector is desired to show that a certain behavior occurs), and one for confirming that no executable path to a specific location exists (e.g., if it is desired to show that a certain behavior never occurs). CPA-SymExec is released under the Apache 2 license and available (inclusive source code) at <a href=\"https://cpachecker.sosy-lab.org\">https://cpachecker.sosy-lab.org</a>. A demonstration video is available at <a href=\"https://youtu.be/qoBHtvPKtnw\">https://youtu.be/qoBHtvPKtnw</a>."}, {"id": "conf/kbse/SungPW18", "title": "CANAL: a cache timing analysis framework via LLVM transformation.", "authors": ["Chungha Sung", "Brandon Paulsen", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3238147.3240485"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTA unified modeling framework for non-functional properties of a program is essential for research in software analysis and verification, since it reduces burdens on individual researchers to implement new approaches and compare existing approaches. We present CANAL, a framework that models the cache behaviors of a program by transforming its intermediate representation in the LLVM compiler. CANAL inserts auxiliary variables and instructions over these variables, to allow standard verification tools to handle a new class of cache related properties, e.g., for computing the worst-case execution time and detecting side-channel leaks. We demonstrate the effectiveness of using three verification tools: KLEE, SMACK and Crab-llvm. We confirm the accuracy of our cache model by comparing with CPU cycle-accurate simulation results of GEM5. CANAL is available on GitHub(https://github.com/canalcache/canal) and YouTube(https://youtu.be/JDou3F1j2nY)."}, {"id": "conf/kbse/Vera-PerezMB18", "title": "Descartes: a PITest engine to detect pseudo-tested methods: tool demonstration.", "authors": ["Oscar Luis Vera-P\u00e9rez", "Martin Monperrus", "Benoit Baudry"], "DOIs": ["https://doi.org/10.1145/3238147.3240474"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTDescartes is a tool that implements extreme mutation operators and aims at finding pseudo-tested methods in Java projects. It leverages the efficient transformation and runtime features of PITest. The demonstration compares Descartes with Gregor, the default mutation engine provided by PITest, in a set of real open source projects. It considers the execution time, number of mutants created and the relationship between the mutation scores produced by both engines. It provides some insights on the main features exposed byDescartes."}, {"id": "conf/kbse/RoohitavafK18", "title": "DKVF: a framework for rapid prototyping and evaluating distributed key-value stores.", "authors": ["Mohammad Roohitavaf", "Sandeep S. Kulkarni"], "DOIs": ["https://doi.org/10.1145/3238147.3240476"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTWe present our framework DKVF that enables one to quickly prototype and evaluate new consistency protocols for key-value stores. DKVF is designed based on the separation of concerns in creating distributed data stores. This separation of concerns allows the designers of consistency protocols to only focus on the high-level consistency protocols which gives them the opportunity to quickly deploy a consistency protocol and evaluate its performance. Moreover, the loose coupling of the different components allows us to easily change different components (e.g. storage engine) of an implementation. We demonstrate DKVF by implementing four existing protocols --eventual consistency, COPS, GentleRain and CausalSpartan-- with it. The implementation of these protocols was very convenient with DKVF, as it only required to write a piece of code for the consistency component that is very close to the pseudocode of the original papers. Hence, it was possible to achieve this in just 1-2 days per protocol. DKVF also comes with a toolset that facilitates running clusters and performing experiments. Tutorial video: <a href=\"https://www.youtube.com/watch?v=MFJQzsJkwfc&list=PLErtSVEHsnBJvoQQI6iqGn61oNrUVVuST\">https://www.youtube.com/watch?v=MFJQzsJkwfc&list=PLErtSVEHsnBJvoQQI6iqGn61oNrUVVuST</a>"}, {"id": "conf/kbse/BorgesHZ18", "title": "DroidMate-2: a platform for Android test generation.", "authors": ["Nataniel P. Borges Jr.", "Jenny Hotzkow", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1145/3238147.3240479"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTAndroid applications (apps) represent an ever increasing portion of the software market. Automated test input generators are the state of the art for testing and security analysis. We introduce DroidMate-2 (DM-2), a platform to easily assist both developers and researchers to customize, develop and test new test generators. DM-2 can be used without app instrumentation or operating system modifications, as a test generator on real devices and emulators for app testing or regression testing. Additionally, it provides sensitive resource monitoring or blocking capabilities through a lightweight app instrumentation, out-of-thebox statement coverage measurement through a fully-fledged app instrumentation and native experiment reproducibility. In our experiments we compared DM-2 against DroidBot, a state-of-the-art test generator by measuring statement coverage. Our results show that DM-2 reached 96% of its peak coverage in less than 2/3 of the time needed by DroidBot, allowing for better and more efficient tests. On short runs (5 minutes) DM-2 outperformed DroidBot by 7% while in longer runs (1 hour) this difference increases to 8%. ACM DL Artifact: https://www.doi.org/10.1145/3264864  For the details see: https://github.com/uds-se/droidmate/wiki/ASE-2018:-Data"}, {"id": "conf/kbse/Haindl18", "title": "Assessing and evaluating functional suitability of software.", "authors": ["Philipp Haindl"], "DOIs": ["https://doi.org/10.1145/3238147.3241531"], "tag": ["Session 1"], "abstract": "ABSTRACTWhile formal task models allow definition and time-based assessment of user-interaction, they have not yet been used as a baseline for assessing the variability patterns of user interaction with software. Consequently, operationalizing these variability patterns enables us to evaluate how suitable the defined task-execution paths are for the user to achieve a predefined goal. Improvements of the software could thereby be evidence-based on knowledge of changes' effects on functional suitability for the user rather than on prospective or opinion-based usage scenarios. Following a design thinking approach tailored to software engineering, understanding and observing these variability patterns are mandatory steps for continuous user-centered improvement of software. In practice however, due to the absence of an operational quality model for functional suitability it is hardly possible to effectively put these operational measures into context and to derive concrete software improvement actions. Having operational suitability metrics is even more important for software engineers as it allows to increasingly focus development activities especially on functionality which has business value for the user."}, {"id": "conf/kbse/Krismayer18", "title": "Automatic mining of constraints for monitoring systems of systems.", "authors": ["Thomas Krismayer"], "DOIs": ["https://doi.org/10.1145/3238147.3241532"], "tag": ["Session 2"], "abstract": "ABSTRACTThe behavior of complex software-intensive systems of systems often only fully emerges during operation, when all systems interact with each other and with their environment. Runtime monitoring approaches are thus used to detect deviations from the expected behavior, which is commonly defined by engineers, e.g., using temporal logic or domain-specific languages. However, the deep domain knowledge required to specify constraints is often not available during the development of systems of systems with multiple teams independently working on heterogeneous components. In this paper, we thus describe our ongoing PhD research to automatically mine constraints for runtime monitoring from recorded events. Our approach mines constraints on event occurrence, timing, data, and combinations of these properties. The approach further presents the mined constraints to users offering multiple ranking strategies and can also be used to support users in system evolution scenarios."}, {"id": "conf/kbse/Diarra18", "title": "Towards automatic restrictification of CUDA kernel arguments.", "authors": ["Rokiatou Diarra"], "DOIs": ["https://doi.org/10.1145/3238147.3241533"], "tag": ["Session 2"], "abstract": "ABSTRACTManyprocedurallanguages, suchasCandC++, havepointers. Pointersarepowerfulandconvenient, butpointeraliasingstillhinderscompileroptimizations, despiteseveralyearsofresearchonpointeraliasinganalysis. Becausealiasanalysisisadifficulttaskandresultsarenotalwaysaccurate, theISOCstandard 99 hasaddedakeyword, namedrestricttoallowtheprogrammertospecifynon\u2212aliasingasanaidtothecompiler\u2032soptimizerandtotherebypossiblyimproveperformance. Thetaskofannotatingpointerswiththerestrictkeywordisstilllefttotheprogrammer. Thistaskis, ingeneral, tediousandpronetoerrorsespeciallysincetheCdoesnotperformanyverificationtoensurethatrestrictkeywordisnotmisplaced. Inthispaperwepresentastaticanalysistoolthat (i) findsCUDAkernelscallsitesinwhichactualparametersdonotalias; (ii) clonesthekernelscalledatsuchsites; (iii) afterperforminganaliasanalysisinthesekernels, addstherestrictkeywordtotheirarguments; and (iv) replacestheoriginalkernelcallbyacalltotheoptimizedclonewheneverpossible."}, {"id": "conf/kbse/Galinier18", "title": "A DSL for requirements in the context of a seamless approach.", "authors": ["Florian Galinier"], "DOIs": ["https://doi.org/10.1145/3238147.3241538"], "tag": ["Session 2"], "abstract": "ABSTRACTReducing the lack of consistency between requirements and the system that should satisfy these requirements is one of the major issue in Requirement Engineering (RE). The objective of my thesis work is to propose a seamless approach, allowing users to express requirements, specifications and the system itself in a unique language.  The purpose of formal approaches is to reduce inconsistency. However, most developers are not familiar with these approaches, and they are not often used outside the critical systems domain. Since we want that non-experts can also use our approach to validate systems in the early stage of their development, we propose a Domain Specific Language (DSL) that is: (i) close to natural language, and (ii) based on a formal semantics. Using Model-Driven Engineering (MDE), this language bridges the gap not only between the several stakeholders that can be involved in a project, considering their different backgrounds, but also between the requirements and the code."}, {"id": "conf/kbse/Traini18", "title": "A multi-objective framework for effective performance fault injection in distributed systems.", "authors": ["Luca Traini"], "DOIs": ["https://doi.org/10.1145/3238147.3241535"], "tag": ["Session 3"], "abstract": "ABSTRACTModern distributed systems should be built to anticipate performance degradation. Often requests in these systems involve ten to thousands Remote Procedure Calls, each of which can be a source of performance degradation. The PhD programme presented here intends to address this issue by providing automated instruments to effectively drive performance fault injection in distributed systems. The envisioned approach exploits multi-objective search-based techniques to automatically find small combinations of tiny performance degradations induced by specific RPCs,which have significant impacts on the user-perceived performance. Automating the search of these events will improve the ability to inject performance issues in production in order to force developers to anticipate and mitigate them."}, {"id": "conf/kbse/Sferruzza18", "title": "Top-down model-driven engineering of web services from extended OpenAPI models.", "authors": ["David Sferruzza"], "DOIs": ["https://doi.org/10.1145/3238147.3241536"], "tag": ["Session 3"], "abstract": "ABSTRACTWeb services engineering is a crucial subject, because web services are often built to be used by other programs; thus they should have a good documentation targeting developers. Furthermore, when building a digital product, engineers need to build several programs that interact with a central instance of web services. OpenAPI, a popular industry standard, makes possible to document web services in order to quickly make a prototype of the product. It allows a top-down process where developers iterate to build an OpenAPI model that describes the web services they want, and then implement both the web services and the programs that will consume them. However, when doing such rapid prototyping, developers tend to either skip this design phase and implement web services right away, or stop updating the OpenAPI model when the product is released; in both cases they cannot take advantage of having an OpenAPI model aligned with the implementation. We show how OpenAPI can be extended to add implementation details inside models. These extensions link services to assemblies of components that describe computations. Hence a top-down development process that keeps model and implementation aligned. Moreover, this makes possible for developers to benefit from more support features while keeping the same flexibility."}, {"id": "conf/kbse/Noller18", "title": "Differential program analysis with fuzzing and symbolic execution.", "authors": ["Yannic Noller"], "DOIs": ["https://doi.org/10.1145/3238147.3241537"], "tag": ["Session 3"], "abstract": "ABSTRACTDifferential program analysis means to identify the behavioral divergences in one or multiple programs, and it can be classified into two categories: identify the behavioral divergences (1) between two program versions for the same input (aka regression analysis), and (2) for the same program with two different inputs (e.g, side-channel analysis). Most of the existent approaches for both subproblems try to solve it with single techniques, which suffer from its weaknesses like scalability issues or imprecision. This research proposes to combine two very strong techniques, namely fuzzing and symbolic execution to tackle these problems and provide scalable solutions for real-world applications. The proposed approaches will be implemented on top of state-of-the-art tools like AFL and Symbolic PathFinder to evaluate them against existent work."}, {"id": "conf/kbse/Delplanque18", "title": "Software engineering techniques applied to relational databases.", "authors": ["Julien Delplanque"], "DOIs": ["https://doi.org/10.1145/3238147.3241534"], "tag": ["Session 3"], "abstract": "ABSTRACTRelational databases play a central role in software systems. As any software artefact they are subject to software engineering related problems. That apply even more when these databases hold behaviour in the form of stored functions, triggers, etc... The purpose of this article is to present a research plan to handle this problematic."}, {"id": "conf/kbse/AlmasriTK18", "title": "Automatically quantifying the impact of a change in systems (journal-first abstract).", "authors": ["Nada Almasri", "Luay Tahat", "Bogdan Korel"], "DOIs": ["https://doi.org/10.1145/3238147.3241984"], "tag": ["Journal-First Papers"], "abstract": "ABSTRACTSoftware maintenance is becoming more challenging with the increased complexity of the software and the frequently applied changes. Performing impact analysis before the actual implementation of a change is a crucial task during system maintenance. While many tools and techniques are available to measure the impact of a change at the code level, only a few research work is done to measure the impact of a change at an earlier stage in the development process. Measuring the impact of a change at the model level speeds up the maintenance process allowing early discovery of critical components of the system before applying the actual change at the code level. In this paper, we present model-based impact analysis approach for state-based systems such as telecommunication or embedded systems. The proposed approach uses model dependencies to automatically measure the expected impact for a requested change instead of relying on the expertise of system maintainers, and it generates two impact sets representing the lower bound and the upper bound of the impact. Although it can be extended to other behavioral models, the presented approach mainly addresses extended finite-state machine (EFSM) models. An empirical study is conducted on six EFSM models to investigate the usefulness of the proposed approach. The results show that on average the size of the impact after a single modification (a change in a one EFSM transition) ranges between 14 and 38 % of the total size of the model. For a modification involving multiple transitions, the average size of the impact ranges between 30 and 64 % of the total size of the model. Additionally, we investigated the relationships (correlation) between the structure of the EFSM model, and the size of the impact sets. Upon preliminary analysis of the correlation, the concepts of model density and data density were defined, and it was found that they could be the major factors influencing the sizes of impact sets for models. As a result, these factors can be used to determine the types of models for which the proposed approach is the most appropriate."}, {"id": "conf/kbse/FalessiPCC18", "title": "Estimating the number of remaining links in traceability recovery (journal-first abstract).", "authors": ["Davide Falessi", "Massimiliano Di Penta", "Gerardo Canfora", "Giovanni Cantone"], "DOIs": ["https://doi.org/10.1145/3238147.3241982"], "tag": ["Journal-First Papers"], "abstract": "ABSTRACTAlthough very important in software engineering, establishing traceability links between software artifacts is extremely tedious, error-prone, and it requires significant effort. Even when approaches for automated traceability recovery exist, these provide the requirements analyst with a, usually very long, ranked list of candidate links that needs to be manually inspected. In this paper we introduce an approach called Estimation of the Number of Remaining Links (ENRL) which aims at estimating, via Machine Learning (ML) classifiers, the number of remaining positive links in a ranked list of candidate traceability links produced by a Natural Language Processing techniques-based recovery approach. We have evaluated the accuracy of the ENRL approach by considering several ML classifiers and NLP techniques on three datasets from industry and academia, and concerning traceability links among different kinds of software artifacts including requirements, use cases, design documents, source code, and test cases. Results from our study indicate that: (i) specific estimation models are able to provide accurate estimates of the number of remaining positive links; (ii) the estimation accuracy depends on the choice of the NLP technique, and (iii) univariate estimation models outperform multivariate ones."}, {"id": "conf/kbse/MonteiroGCF18", "title": "Bounded model checking of C++ programs based on the Qt cross-platform framework (journal-first abstract).", "authors": ["Felipe R. Monteiro", "M\u00e1rio A. P. Garcia", "Lucas C. Cordeiro", "Eddie Batista de Lima Filho"], "DOIs": ["https://doi.org/10.1145/3238147.3241981"], "tag": ["Journal-First Papers"], "abstract": "ABSTRACTThis work proposes an abstraction of the Qt framework, named as Qt Operational Model (QtOM), which is integrated into two different verification approaches: explicit-state model checking and symbolic (bounded) model checking. The proposed methodology is the first one to formally verify Qt-based applications, which has the potential to devise new directions for software verification of portable code. The full version of this paper is published in Software Testing, Verification and Reliability, on 02 March 2017 and it is available at https://doi.org/10.1002/stvr.1632."}, {"id": "conf/kbse/AcciolyBC18", "title": "Understanding Semi-structured merge conflict characteristics in open-source Java projects (journal-first abstract).", "authors": ["Paola R. G. Accioly", "Paulo Borba", "Guilherme Cavalcanti"], "DOIs": ["https://doi.org/10.1145/3238147.3241983"], "tag": ["Journal-First Papers"], "abstract": "ABSTRACTIn a collaborative development environment, tasks are commonly assigned to developers working independent from each other. As a result, when trying to integrate these contributions, one might have to deal with conflicting changes. Such conflicts might be detected when merging contributions (merge conflicts), when building the system (build conflicts), or when running tests (semantic conflicts). Regarding such conflicts, previous studies show that they occur frequently, and impair developers\u2019 productivity, as understanding and solving them is a demanding and tedious task that might introduce defects. However, despite the existing evidence in the literature, the structure of changes that lead to conflicts has not been studied yet. Understanding the underlying structure of conflicts, and the involved syntactic language elements, might shed light on how to better avoid them. For example, awareness tools that inform users about ongoing parallel changes such as Syde and Palant\u00edr can benefit from knowing the most common conflict patterns to become more efficient. With that aim, in this paper we focus on understanding the underlying structure of merge conflicts."}], "2019": [{"id": "conf/kbse/KangB019", "title": "Assessing the Generalizability of Code2vec Token Embeddings.", "authors": ["Hong Jin Kang", "Tegawend\u00e9 F. Bissyand\u00e9", "David Lo"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00011"], "tag": ["Main Track"], "abstract": "Many Natural Language Processing (NLP) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different NLP tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings."}, {"id": "conf/kbse/WanSSXZ0Y19", "title": "Multi-modal Attention Network Learning for Semantic Source Code Retrieval.", "authors": ["Yao Wan", "Jingdong Shu", "Yulei Sui", "Guandong Xu", "Zhou Zhao", "Jian Wu", "Philip S. Yu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00012"], "tag": ["Main Track"], "abstract": "Code retrieval techniques and tools have been playing a key role in facilitating software developers to retrieve existing code fragments from available open-source repositories given a user query (e.g., a short natural language text describing the functionality for retrieving a particular code snippet). Despite the existing efforts in improving the effectiveness of code retrieval, there are still two main issues hindering them from being used to accurately retrieve satisfiable code fragments from large-scale repositories when answering complicated queries. First, the existing approaches only consider shallow features of source code such as method names and code tokens, but ignoring structured features such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of source code, which contains rich and well-defined semantics of source code. Second, although the deep learning-based approach performs well on the representation of source code, it lacks the explainability, making it hard to interpret the retrieval results and almost impossible to understand which features of source code contribute more to the final results. To tackle the two aforementioned issues, this paper proposes MMAN, a novel Multi-Modal Attention Network for semantic source code retrieval. A comprehensive multi-modal representation is developed for representing unstructured and structured features of source code, with one LSTM for the sequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated Graph Neural Network) for the CFG of code. Furthermore, a multi-modal attention fusion layer is applied to assign weights to different parts of each modality of source code and then integrate them into a single hybrid representation. Comprehensive experiments and analysis on a large-scale real-world dataset show that our proposed model can accurately retrieve code snippets and outperforms the state-of-the-art methods."}, {"id": "conf/kbse/Gladisch0HOVP19", "title": "Experience Paper: Search-Based Testing in Automated Driving Control Applications.", "authors": ["Christoph Gladisch", "Thomas Heinz", "Christian Heinzemann", "Jens Oehlerking", "Anne von Vietinghoff", "Tim Pfitzer"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00013"], "tag": ["Main Track"], "abstract": "Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications."}, {"id": "conf/kbse/BuiYJ19", "title": "AutoFocus: Interpreting Attention-Based Neural Networks by Code Perturbation.", "authors": ["Nghi D. Q. Bui", "Yijun Yu", "Lingxiao Jiang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00014"], "tag": ["Main Track"], "abstract": "Despite being adopted in software engineering tasks, deep neural networks are treated mostly as a black box due to the difficulty in interpreting how the networks infer the outputs from the inputs. To address this problem, we propose AutoFocus, an automated approach for rating and visualizing the importance of input elements based on their effects on the outputs of the networks. The approach is built on our hypotheses that (1) attention mechanisms incorporated into neural networks can generate discriminative scores for various input elements and (2) the discriminative scores reflect the effects of input elements on the outputs of the networks. This paper verifies the hypotheses by applying AutoFocus on the task of algorithm classification (i.e., given a program source code as input, determine the algorithm implemented by the program). AutoFocus identifies and perturbs code elements in a program systematically, and quantifies the effects of the perturbed elements on the network's classification results. Based on evaluation on more than 1000 programs for 10 different sorting algorithms, we observe that the attention scores are highly correlated to the effects of the perturbed code elements. Such a correlation provides a strong basis for the uses of attention scores to interpret the relations between code elements and the algorithm classification results of a neural network, and we believe that visualizing code elements in an input program ranked according to their attention scores can facilitate faster program comprehension with reduced code."}, {"id": "conf/kbse/LinJM19", "title": "Test Transfer Across Mobile Apps Through Semantic Mapping.", "authors": ["Jun-Wei Lin", "Reyhaneh Jabbarvand", "Sam Malek"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00015"], "tag": ["Main Track"], "abstract": "GUI-based testing has been primarily used to examine the functionality and usability of mobile apps. Despite the numerous GUI-based test input generation techniques proposed in the literature, these techniques are still limited by (1) lack of context-aware text inputs; (2) failing to generate expressive tests; and (3) absence of test oracles. To address these limitations, we propose CraftDroid, a framework that leverages information retrieval, along with static and dynamic analysis techniques, to extract the human knowledge from an existing test suite for one app and transfer the test cases and oracles to be used for testing other apps with the similar functionalities. Evaluation of CraftDroid on real-world commercial Android apps corroborates its effectiveness by achieving 73% precision and 90% recall on average for transferring both the GUI events and oracles. In addition, 75% of the attempted transfers successfully generated valid and feature-based tests for popular features among apps in the same category."}, {"id": "conf/kbse/BehrangO19", "title": "Test Migration Between Mobile Apps with Similar Functionality.", "authors": ["Farnaz Behrang", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00016"], "tag": ["Main Track"], "abstract": "The use of mobile apps is increasingly widespread, and much effort is put into testing these apps to make sure they behave as intended. To reduce this effort, and thus the overall cost of mobile app testing, we propose APPTESTMIGRATOR, a technique for migrating test cases between apps in the same category (e.g., banking apps). The intuition behind APPTESTMIGRATOR is that many apps share similarities in their functionality, and these similarities often result in conceptually similar user interfaces (through which that functionality is accessed). APPTESTMIGRATOR leverages these commonalities between user interfaces to migrate existing tests written for an app to another similar app. Specifically, given (1) a test case for an app (source app) and (2) a second app (target app), APPTESTMIGRATOR attempts to automatically transform the sequence of events and oracles in the test for the source app to events and oracles for the target app. We implemented APPTESTMIGRATOR for Android mobile apps and evaluated it on a set of randomly selected apps from the Google Play Store in four different categories. Our initial results are promising, support our intuition that test migration is possible, and motivate further research in this direction."}, {"id": "conf/kbse/LiuW0B0X19", "title": "DaPanda: Detecting Aggressive Push Notifications in Android Apps.", "authors": ["Tianming Liu", "Haoyu Wang", "Li Li", "Guangdong Bai", "Yao Guo", "Guoai Xu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00017"], "tag": ["Main Track"], "abstract": "Mobile push notifications have been widely used in mobile platforms to deliver all sorts of information to app users. Although it offers great convenience for both app developers and mobile users, this feature was frequently reported to serve malicious and aggressive purposes, such as delivering annoying push notification advertisement. However, to the best of our knowledge, this problem has not been studied by our research community so far. To fill the void, this paper presents the first study to detect aggressive push notifications and further characterize them in the global mobile app ecosystem on a large scale. To this end, we first provide a taxonomy of mobile push notifications and identify the aggressive ones using a crowdsourcing-based method. Then we propose sc DaPanda, a novel hybrid approach, aiming at automatically detecting aggressive push notifications in Android apps. sc DaPanda leverages a guided testing approach to systematically trigger and record push notifications. By instrumenting the Android framework, sc DaPanda further collects all notification-relevant runtime information to flag the aggressive ones. Our experimental results show that sc DaPanda is capable of detecting different types of aggressive push notifications effectively in an automated way. By applying sc DaPanda to 20,000 Android apps from different app markets, it yields over 1,000 aggressive notifications, which have been further confirmed as true positives. Our in-depth analysis further reveals that aggressive notifications are prevalent across different markets and could be manifested in all the phases in the lifecycle of push notifications. It is hence urgent for our community to take actions to detect and mitigate apps involving aggressive push notifications."}, {"id": "conf/kbse/YangJ0WSLZX19", "title": "Automatic Self-Validation for Code Coverage Profilers.", "authors": ["Yibiao Yang", "Yanyan Jiang", "Zhiqiang Zuo", "Yang Wang", "Hao Sun", "Hongmin Lu", "Yuming Zhou", "Baowen Xu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00018"], "tag": ["Main Track"], "abstract": "Code coverage as the primitive dynamic program behavior information, is widely adopted to facilitate a rich spectrum of software engineering tasks, such as testing, fuzzing, debugging, fault detection, reverse engineering, and program understanding. Thanks to the widespread applications, it is crucial to ensure the reliability of the code coverage profilers. Unfortunately, due to the lack of research attention and the existence of testing oracle problem, coverage profilers are far away from being tested sufficiently. Bugs are still regularly seen in the widely deployed profilers, like gcov and llvm-cov, along with gcc and llvm, respectively. This paper proposes Cod, an automated self-validator for effectively uncovering bugs in the coverage profilers. Starting from a test program (either from a compiler's test suite or generated randomly), Cod detects profiler bugs with zero false positive using a metamorphic relation in which the coverage statistics of that program and a mutated variant are bridged. We evaluated Cod over two of the most well-known code coverage profilers, namely gcov and llvm-cov. Within a four-month testing period, a total of 196 potential bugs (123 for gcov, 73 for llvm-cov) are found, among which 23 are confirmed by the developers."}, {"id": "conf/kbse/GodioBPAF19", "title": "Efficient Test Generation Guided by Field Coverage Criteria.", "authors": ["Ariel Godio", "Valeria S. Bengolea", "Pablo Ponzio", "Nazareno Aguirre", "Marcelo Fabian Frias"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00019"], "tag": ["Main Track"], "abstract": "Field-exhaustive testing is a testing criterion suitable for object-oriented code over complex, heap-allocated, data structures. It requires test suites to contain enough test inputs to cover all feasible values for the object's fields within a certain scope (input-size bound). While previous work shows that field-exhaustive suites can be automatically generated, the generation technique required a formal specification of the inputs that can be subject to SAT-based analysis. Moreover, the restriction of producing all feasible values for inputs' fields makes test generation costly. In this paper, we deal with field coverage as testing criteria that measure the quality of a test suite in terms of coverage and mutation score, by examining to what extent the values of inputs' fields are covered. In particular, we consider field coverage in combination with test generation based on symbolic execution to produce underapproximations of field-exhaustive suites, using the Symbolic Pathfinder tool. To underapproximate these suites we use tranScoping, a technique that estimates characteristics of yet to be run analyses for large scopes, based on data obtained from analyses performed in small scopes. This provides us with a suitable condition to prematurely stop the symbolic execution. As we show, tranScoping different metrics regarding field coverage allows us to produce significantly smaller suites using a fraction of the generation time. All this while retaining the effectiveness of field exhaustive suites in terms of test suite quality."}, {"id": "conf/kbse/LuoBS19", "title": "A Qualitative Analysis of Android Taint-Analysis Results.", "authors": ["Linghui Luo", "Eric Bodden", "Johannes Sp\u00e4th"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00020"], "tag": ["Main Track"], "abstract": "In the past, researchers have developed a number of popular taint-analysis approaches, particularly in the context of Android applications. Numerous studies have shown that automated code analyses are adopted by developers only if they yield a good \"signal to noise ratio\", i.e., high precision. Many previous studies have reported analysis precision quantitatively, but this gives little insight into what can and should be done to increase precision further. To guide future research on increasing precision, we present a comprehensive study that evaluates static Android taint-analysis results on a qualitative level. To unravel the exact nature of taint flows, we have designed COVA, an analysis tool to compute partial path constraints that inform about the circumstances under which taint flows may actually occur in practice. We have conducted a qualitative study on the taint flows reported by FlowDroid in 1,022 real-world Android applications. Our results reveal several key findings: Many taint flows occur only under specific conditions, e.g., environment settings, user interaction, I/O. Taint analyses should consider the application context to discern such situations. COVA shows that few taint flows are guarded by multiple different kinds of conditions simultaneously, so tools that seek to confirm true positives dynamically can concentrate on one kind at a time, e.g., only simulating user interactions. Lastly, many false positives arise due to a too liberal source/sink configuration. Taint analyses must be more carefully configured, and their configuration could benefit from better tool assistance."}, {"id": "conf/kbse/LaiR19", "title": "Goal-Driven Exploration for Android Applications.", "authors": ["Duling Lai", "Julia Rubin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00021"], "tag": ["Main Track"], "abstract": "This paper proposes a solution for automated goal-driven exploration of Android applications - a scenario in which a user, e.g., a security auditor, needs to dynamically trigger the functionality of interest in an application, e.g., to check whether user-sensitive info is only sent to recognized third-party servers. As the auditor might need to check hundreds or even thousands of apps, manually exploring each app to trigger the desired behavior is too time-consuming to be feasible. Existing automated application exploration and testing techniques are of limited help in this scenario as well, as their goal is mostly to identify faults by systematically exploring different app paths, rather than swiftly navigating to the target functionality. The goal-driven application exploration approach proposed in this paper, called GoalExplorer, automatically generates an executable test script that directly triggers the functionality of interest. The core idea behind GoalExplorer is to first statically model the application's UI screens and transitions between these screens, producing a Screen Transition Graph (STG). Then, GoalExplorer uses the STG to guide the dynamic exploration of the application to the particular target of interest: an Android activity, API call, or a program statement. The results of our empirical evaluation on 93 benchmark applications and the 95 most popular GooglePlay applications show that the STG is substantially more accurate than other Android UI models and that GoalExplorer is able to trigger a target functionality much faster than existing application exploration techniques."}, {"id": "conf/kbse/SahinAMCE19", "title": "RANDR: Record and Replay for Android Applications via Targeted Runtime Instrumentation.", "authors": ["Onur Sahin", "Assel Aliyeva", "Hariharan Mathavan", "Ayse K. Coskun", "Manuel Egele"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00022"], "tag": ["Main Track"], "abstract": "The ability to repeat the execution of a program is a fundamental requirement in many areas of computing from computer system evaluation to software engineering. Reproducing executions of mobile apps, in particular, has proven difficult under real-life scenarios due to multiple sources of external inputs and interactive nature of the apps. Previous works that provide record/replay functionality for mobile apps are restricted to particular input sources (e.g., touchscreen events) and present deployment challenges due to intrusive modifications to the underlying software stack. Moreover, due to their reliance on record and replay of device specific events, the recorded executions cannot be reliably reproduced across different platforms. In this paper, we present a new practical approach, RandR, for record and replay of Android applications. RandR captures and replays multiple sources of input (i.e., UI and network) without requiring source code (OS or app), administrative device privileges, or any special platform support. RandR achieves these qualities by instrumenting a select set of methods at runtime within an application's own sandbox. In addition, to enable portability of recorded executions across different platforms for replay, RandR contextualizes UI events as interactions with particular UI components (e.g., a button) as opposed to relying on platform specific features (e.g., screen coordinates). We demonstrate RandR's accurate cross-platform record and replay capabilities using over 30 real-world Android apps across a variety of platforms including emulators as well as commercial off-the-shelf mobile devices deployed in real life."}, {"id": "conf/kbse/WuLZYZ019", "title": "MalScan: Fast Market-Wide Mobile Malware Scanning by Social-Network Centrality Analysis.", "authors": ["Yueming Wu", "Xiaodi Li", "Deqing Zou", "Wei Yang", "Xin Zhang", "Hai Jin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00023"], "tag": ["Main Track"], "abstract": "Malware scanning of an app market is expected to be scalable and effective. However, existing approaches use either syntax-based features which can be evaded by transformation attacks or semantic-based features which are usually extracted by performing expensive program analysis. Therefor, in this paper, we propose a lightweight graph-based approach to perform Android malware detection. Instead of traditional heavyweight static analysis, we treat function call graphs of apps as social networks and perform social-network-based centrality analysis to represent the semantic features of the graphs. Our key insight is that centrality provides a succinct and fault-tolerant representation of graph semantics, especially for graphs with certain amount of inaccurate information (e.g., inaccurate call graphs). We implement a prototype system, MalScan, and evaluate it on datasets of 15,285 benign samples and 15,430 malicious samples. Experimental results show that MalScan is capable of detecting Android malware with up to 98% accuracy under one second which is more than 100 times faster than two state-of-the-art approaches, namely MaMaDroid and Drebin. We also demonstrate the feasibility of MalScan on market-wide malware scanning by performing a statistical study on over 3 million apps. Finally, in a corpus of dataset collected from Google-Play app market, MalScan is able to identify 18 zero-day malware including malware samples that can evade detection of existing tools."}, {"id": "conf/kbse/RenX00S19", "title": "Discovering, Explaining and Summarizing Controversial Discussions in Community Q&A Sites.", "authors": ["Xiaoxue Ren", "Zhenchang Xing", "Xin Xia", "Guoqiang Li", "Jianling Sun"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00024"], "tag": ["Main Track"], "abstract": "Developers often look for solutions to programming problems in community Q&A sites like Stack Overflow. Due to the crowdsourcing nature of these Q&A sites, many user-provided answers are wrong, less optimal or out-of-date. Relying on community-curated quality indicators (e.g., accepted answer, answer vote) cannot reliably identify these answer problems. Such problematic answers are often criticized by other users. However, these critiques are not readily discoverable when reading the posts. In this paper, we consider the answers being criticized and their critique posts as controversial discussions in community Q&A sites. To help developers notice such controversial discussions and make more informed choices of appropriate solutions, we design an automatic open information extraction approach for systematically discovering and summarizing the controversies in Stack Overflow and exploiting official API documentation to assist the understanding of the discovered controversies. We apply our approach to millions of java/android-tagged Stack overflow questions and answers and discover a large scale of controversial discussions in Stack Overflow. Our manual evaluation confirms that the extracted controversy information is of high accuracy. A user study with 18 developers demonstrates the usefulness of our generated controversy summaries in helping developers avoid the controversial answers and choose more appropriate solutions to programming questions."}, {"id": "conf/kbse/GaoZX0LK19", "title": "Automating App Review Response Generation.", "authors": ["Cuiyun Gao", "Jichuan Zeng", "Xin Xia", "David Lo", "Michael R. Lyu", "Irwin King"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00025"], "tag": ["Main Track"], "abstract": "Previous studies showed that replying to a user review usually has a positive effect on the rating that is given by the user to the app. For example, Hassan et al. found that responding to a review increases the chances of a user updating their given rating by up to six times compared to not responding. To alleviate the labor burden in replying to the bulk of user reviews, developers usually adopt a template-based strategy where the templates can express appreciation for using the app or mention the company email address for users to follow up. However, reading a large number of user reviews every day is not an easy task for developers. Thus, there is a need for more automation to help developers respond to user reviews. Addressing the aforementioned need, in this work we propose a novel approach RRGen that automatically generates review responses by learning knowledge relations between reviews and their responses. RRGen explicitly incorporates review attributes, such as user rating and review length, and learns the relations between reviews and corresponding responses in a supervised way from the available training data. Experiments on 58 apps and 309,246 review-response pairs highlight that RRGen outperforms the baselines by at least 67.4% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate dialogue response generation systems). Qualitative analysis also confirms the effectiveness of RRGen in generating relevant and accurate responses."}, {"id": "conf/kbse/Liu0T0L19", "title": "Automatic Generation of Pull Request Descriptions.", "authors": ["Zhongxin Liu", "Xin Xia", "Christoph Treude", "David Lo", "Shanping Li"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00026"], "tag": ["Main Track"], "abstract": "Enabled by the pull-based development model, developers can easily contribute to a project through pull requests (PRs). When creating a PR, developers can add a free-form description to describe what changes are made in this PR and/or why. Such a description is helpful for reviewers and other developers to gain a quick understanding of the PR without touching the details and may reduce the possibility of the PR being ignored or rejected. However, developers sometimes neglect to write descriptions for PRs. For example, in our collected dataset with over 333K PRs, more than 34% of the PR descriptions are empty. To alleviate this problem, we propose an approach to automatically generate PR descriptions based on the commit messages and the added source code comments in the PRs. We regard this problem as a text summarization problem and solve it using a novel sequence-to-sequence model. To cope with out-of-vocabulary words in software artifacts and bridge the gap between the training loss function of the sequence-to-sequence model and the evaluation metric ROUGE, which has been shown to correspond to human evaluation, we integrate the pointer generator and directly optimize for ROUGE using reinforcement learning and a special loss function. We build a dataset with over 41K PRs and evaluate our approach on this dataset through ROUGE and a human evaluation. Our evaluation results show that our approach outperforms two baselines by significant margins."}, {"id": "conf/kbse/HavrikovZ19", "title": "Systematically Covering Input Structure.", "authors": ["Nikolas Havrikov", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00027"], "tag": ["Main Track"], "abstract": "Grammar-based testing uses a given grammar to produce syntactically valid inputs. To cover program features, it is necessary to also cover input features-say, all URL variants for a URL parser. Our k-path algorithm for grammar production systematically covers syntactic elements as well as their combinations. In our evaluation, we show that this results in a significantly higher code coverage than state of the art."}, {"id": "conf/kbse/SondhiP19", "title": "SEGATE: Unveiling Semantic Inconsistencies between Code and Specification of String Inputs.", "authors": ["Devika Sondhi", "Rahul Purandare"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00028"], "tag": ["Main Track"], "abstract": "Automated testing techniques are often assessed on coverage based metrics. However, despite giving good coverage, the test cases may miss the gap between functional specification and the code implementation. This gap may be subtle in nature, arising due to the absence of logical checks, either in the implementation or in the specification, resulting in inconsistencies in the input definition. The inconsistencies may be prevalent especially for structured inputs, commonly specified using string-based data types. Our study on defects reported over popular libraries reveals that such gaps may not be limited to input validation checks. We propose a test generation technique for structured string inputs where we infer inconsistencies in input definition to expose semantic gaps in the method under test and the method specification. We assess this technique using our tool SEGATE, Semantic Gap Tester. SEGATE uses static analysis and automaton modeling to infer the gap and generate test cases. On our benchmark dataset, comprising of defects reported in 15 popular open-source libraries, written in Java, SEGATE was able to generate tests to expose 80% of the defects."}, {"id": "conf/kbse/JiaLYLWLL19", "title": "Detecting Error-Handling Bugs without Error Specification Input.", "authors": ["Zhouyang Jia", "Shanshan Li", "Tingting Yu", "Xiangke Liao", "Ji Wang", "Xiaodong Liu", "Yunhuai Liu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00029"], "tag": ["Main Track"], "abstract": "Most software systems frequently encounter errors when interacting with their environments. When errors occur, error-handling code must execute flawlessly to facilitate system recovery. Implementing correct error handling is repetitive but non-trivial, and developers often inadvertently introduce bugs into error-handling code. Existing tools require correct error specifications to detect error-handling bugs. Manually generating error specifications is error-prone and tedious, while automatically mining error specifications is hard to achieve a satisfying accuracy. In this paper, we propose EH-Miner, a novel and practical tool that can automatically detect error-handling bugs without the need for error specifications. Given a function, EH-Miner mines its error-handling rules when the function is frequently checked by an equivalent condition, and handled by the same action. We applied EH-Miner to 117 applications across 15 software domains. EH-Miner mined error-handling rules with the precision of 91.1% and the recall of 46.9%. We reported 142 bugs to developers, and 106 bugs had been confirmed and fixed at the time of writing. We further applied EH-Miner to Linux kernel, and reported 68 bugs for kernel-4.17, of which 42 had been confirmed or fixed."}, {"id": "conf/kbse/Osei-OwusuAB0C19", "title": "Grading-Based Test Suite Augmentation.", "authors": ["Jonathan Osei-Owusu", "Angello Astorga", "Liia Butler", "Tao Xie", "Geoffrey Challen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00030"], "tag": ["Main Track"], "abstract": "Enrollment in introductory programming (CS1) courses continues to surge and hundreds of CS1 students can produce thousands of submissions for a single problem, all requiring timely and accurate grading. One way that instructors can efficiently grade is to construct a custom instructor test suite that compares a student submission to a reference solution over randomly generated or hand-crafted inputs. However, such test suite is often insufficient, causing incorrect submissions to be marked as correct. To address this issue, we propose the Grasa (GRAding-based test Suite Augmentation) approach consisting of two techniques. Grasa first detects and clusters incorrect submissions by approximating their behavioral equivalence to each other. To augment the existing instructor test suite, Grasa generates a minimal set of additional tests that help detect the incorrect submissions. We evaluate our Grasa approach on a dataset of CS1 student submissions for three programming problems. Our preliminary results show that Grasa can effectively identify incorrect student submissions and minimally augment the instructor test suite."}, {"id": "conf/kbse/Wang19", "title": "Emotions Extracted from Text vs. True Emotions-An Empirical Evaluation in SE Context.", "authors": ["Yi Wang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00031"], "tag": ["Main Track"], "abstract": "Emotion awareness research in SE context has been growing in recent years. Currently, researchers often rely on textual communication records to extract emotion states using natural language processing techniques. However, how well these extracted emotion states reflect people's real emotions has not been thoroughly investigated. In this paper, we report a multi-level, longitudinal empirical study with 82 individual members in 27 project teams. We collected their self-reported retrospective emotion states on a weekly basis during their year-long projects and also extracted corresponding emotions from the textual communication records. We then model and compare the dynamics of these two types of emotions using multiple statistical and time series analysis methods. Our analyses yield a rich set of findings. The most important one is that the dynamics of emotions extracted using text-based algorithms often do not well reflect the dynamics of self-reported retrospective emotions. Besides, the extracted emotions match self-reported retrospective emotions better at the team-level. Our results also suggest that individual personalities and the team's emotion display norms significantly impact the match/mismatch. Our results should warn the research community about the limitations and challenges of applying text-based emotion recognition tools in SE research."}, {"id": "conf/kbse/SaifullahAR19", "title": "Learning from Examples to Find Fully Qualified Names of API Elements in Code Snippets.", "authors": ["C. M. Khaled Saifullah", "Muhammad Asaduzzaman", "Chanchal K. Roy"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00032"], "tag": ["Main Track"], "abstract": "Developers often reuse code snippets from online forums, such as Stack Overflow, to learn API usages of software frameworks or libraries. These code snippets often contain ambiguous undeclared external references. Such external references make it difficult to learn and use those APIs correctly. In particular, reusing code snippets containing such ambiguous undeclared external references requires significant manual efforts and expertise to resolve them. Manually resolving fully qualified names (FQN) of API elements is a non-trivial task. In this paper, we propose a novel context-sensitive technique, called COSTER, to resolve FQNs of API elements in such code snippets. The proposed technique collects locally specific source code elements as well as globally related tokens as the context of FQNs, calculates likelihood scores, and builds an occurrence likelihood dictionary (OLD). Given an API element as a query, COSTER captures the context of the query API element, matches that with the FQNs of API elements stored in the OLD, and rank those matched FQNs leveraging three different scores: likelihood, context similarity, and name similarity scores. Evaluation with more than 600K code examples collected from GitHub and two different Stack Overflow datasets shows that our proposed technique improves precision by 4-6% and recall by 3-22% compared to state-of-the-art techniques. The proposed technique significantly reduces the training time compared to the StatType, a state-of-the-art technique, without sacrificing accuracy. Extensive analyses on results demonstrate the robustness of the proposed technique."}, {"id": "conf/kbse/JiangRXZ19", "title": "Inferring Program Transformations From Singular Examples via Big Code.", "authors": ["Jiajun Jiang", "Luyao Ren", "Yingfei Xiong", "Lingming Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00033"], "tag": ["Main Track"], "abstract": "Inferring program transformations from concrete program changes has many potential uses, such as applying systematic program edits, refactoring, and automated program repair. Existing work for inferring program transformations usually rely on statistical information over a potentially large set of program-change examples. However, in many practical scenarios we do not have such a large set of program-change examples. In this paper, we address the challenge of inferring a program transformation from one single example. Our core insight is that \"big code\" can provide effective guide for the generalization of a concrete change into a program transformation, i.e., code elements appearing in many files are general and should not be abstracted away. We first propose a framework for transformation inference, where programs are represented as hypergraphs to enable fine-grained generalization of transformations. We then design a transformation inference approach, GENPAT, that infers a program transformation based on code context and statistics from a big code corpus. We have evaluated GENPAT under two distinct application scenarios, systematic editing and program repair. The evaluation on systematic editing shows that GENPAT significantly outperforms a state-of-the-art approach, SYDIT, with up to 5.5x correctly transformed cases. The evaluation on program repair suggests that GENPAT has the potential to be integrated in advanced program repair tools-GENPAT successfully repaired 19 real-world bugs in the Defects4J benchmark by simply applying transformations inferred from existing patches, where 4 bugs have never been repaired by any existing technique. Overall, the evaluation results suggest that GENPAT is effective for transformation inference and can potentially be adopted for many different applications."}, {"id": "conf/kbse/HeLWMZLHLX19", "title": "Performance-Boosting Sparsification of the IFDS Algorithm with Applications to Taint Analysis.", "authors": ["Dongjie He", "Haofeng Li", "Lei Wang", "Haining Meng", "Hengjie Zheng", "Jie Liu", "Shuangwei Hu", "Lian Li", "Jingling Xue"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00034"], "tag": ["Main Track"], "abstract": "The IFDS algorithm can be compute-and memory-intensive for some large programs, often running for a long time (more than expected) or terminating prematurely after some time and/or memory budgets have been exhausted. In the latter case, the corresponding IFDS data-flow analyses may suffer from false negatives and/or false positives. To improve this, we introduce a sparse alternative to the traditional IFDS algorithm. Instead of propagating the data-flow facts across all the program points along the program's (interprocedural) control flow graph, we propagate every data-flow fact directly to its next possible use points along its own sparse control flow graph constructed on the fly, thus reducing significantly both the time and memory requirements incurred by the traditional IFDS algorithm. In our evaluation, we compare FLOWDROID, a taint analysis performed by using the traditional IFDS algorithm, with our sparse incarnation, SPARSEDROID, on a set of 40 Android apps selected. For the time budget (5 hours) and memory budget (220GB) allocated per app, SPARSEDROID can run every app to completion but FLOWDROID terminates prematurely for 9 apps, resulting in an average speedup of 22.0x. This implies that when used as a market-level vetting tool, SPARSEDROID can finish analyzing these 40 apps in 2.13 hours (by issuing 228 leak warnings) while FLOWDROID manages to analyze only 30 apps in the same time period (by issuing only 147 leak warnings)."}, {"id": "conf/kbse/WangLXMG19", "title": "Characterizing Android App Signing Issues.", "authors": ["Haoyu Wang", "Hongxuan Liu", "Xusheng Xiao", "Guozhu Meng", "Yao Guo"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00035"], "tag": ["Main Track"], "abstract": "In the app releasing process, Android requires all apps to be digitally signed with a certificate before distribution. Android uses this certificate to identify the author and ensure the integrity of an app. However, a number of signature issues have been reported recently, threatening the security and privacy of Android apps. In this paper, we present the first large-scale systematic measurement study on issues related to Android app signatures. We first create a taxonomy covering four types of app signing issues (21 anti-patterns in total), including vulnerabilities, potential attacks, release bugs and compatibility issues. Then we developed an automated tool to characterize signature-related issues in over 5 million app items (3 million distinct apks) crawled from Google Play and 24 alternative Android app markets. Our empirical findings suggest that although Google has introduced apk-level signing schemes (V2 and V3) to overcome some of the known security issues, more than 93% of the apps still use only the JAR signing scheme (V1), which poses great security threats. Besides, we also revealed that 7% to 45% of the apps in the 25 studied markets have been found containing at least one signing issue, while a large number of apps have been exposed to security vulnerabilities, attacks and compatibility issues. Among them a considerable number of apps we identified are popular apps with millions of downloads. Finally, our evolution analysis suggested that most of the issues were not mitigated after a considerable amount of time across markets. The results shed light on the emergency for detecting and repairing the app signing issues."}, {"id": "conf/kbse/RahatFT19", "title": "OAUTHLINT: An Empirical Study on OAuth Bugs in Android Applications.", "authors": ["Tamjid Al Rahat", "Yu Feng", "Yuan Tian"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00036"], "tag": ["Main Track"], "abstract": "Mobile developers use OAuth APIs to implement Single-Sign-On services. However, the OAuth protocol was originally designed for the authorization for third-party websites not to authenticate users in third-party mobile apps. As a result, it is challenging for developers to correctly implement mobile OAuth securely. These vulnerabilities due to the misunderstanding of OAuth and inexperience of developers could lead to data leakage and account breach. In this paper, we perform an empirical study on the usage of OAuth APIs in Android applications and their security implications. In particular, we develop OAUTHLINT, that incorporates a query-driven static analysis to automatically check programs on the Google Play marketplace. OAUTHLINT takes as input an anti-protocol that encodes a vulnerable pattern extracted from the OAuth specifications and a program P. Our tool then generates a counter-example if the anti-protocol can match a trace of Ps possible executions. To evaluate the effectiveness of our approach, we perform a systematic study on 600+ popular apps which have more than 10 millions of downloads. The evaluation shows that 101 (32%) out of 316 applications that use OAuth APIs make at least one security mistake."}, {"id": "conf/kbse/0003WHX0019", "title": "History-Guided Configuration Diversification for Compiler Test-Program Generation.", "authors": ["Junjie Chen", "Guancheng Wang", "Dan Hao", "Yingfei Xiong", "Hongyu Zhang", "Lu Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00037"], "tag": ["Main Track"], "abstract": "Compilers, like other software systems, contain bugs, and compiler testing is the most widely-used way to assure compiler quality. A critical task of compiler testing is to generate test programs that could effectively and efficiently discover bugs. Though we can configure test generators such as Csmith to control the features of the generated programs, it is not clear what test configuration is effective. In particular, an effective test configuration needs to generate test programs that are bug-revealing, i.e., likely to trigger bugs, and diverse, i.e., able to discover different types of bugs. It is not easy to satisfy both properties. In this paper, we propose a novel test-program generation approach, called HiCOND, which utilizes historical data for configuration diversification to solve this challenge. HiCOND first infers the range for each option in a test configuration where bug-revealing test programs are more likely to be generated based on historical data. Then, it identifies a set of test configurations that can lead to diverse test programs through a search method (particle swarm optimization). Finally, based on the set of test configurations for compiler testing, HiCOND generates test programs, which are likely to be bug-revealing and diverse. We have conducted experiments on two popular compilers GCC and LLVM, and the results confirm the effectiveness of our approach. For example, HiCOND detects 75.00%, 133.33%, and 145.00% more bugs than the three existing approaches, respectively. Moreover, HiCOND has been successfully applied to actual compiler testing in a global IT company and detected 11 bugs during the practical evaluation."}, {"id": "conf/kbse/StepanovAB19", "title": "ReduKtor: How We Stopped Worrying About Bugs in Kotlin Compiler.", "authors": ["Daniil Stepanov", "Marat Akhin", "Mikhail A. Belyaev"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00038"], "tag": ["Main Track"], "abstract": "Bug localization is well-known to be a difficult problem in software engineering, and specifically in compiler development, where it is beneficial to reduce the input program to a minimal reproducing example; this technique is more commonly known as delta debugging. What additionally contributes to the problem is that every new programming language has its own unique quirks and foibles, making it near impossible to reuse existing tools and approaches with full efficiency. In this experience paper we tackle the delta debugging problem w.r.t. Kotlin, a relatively new programming language from JetBrains. Our approach is based on a novel combination of program slicing, hierarchical delta debugging and Kotlin-specific transformations, which are synergistic to each other. We implemented it in a prototype called ReduKtor and did extensive evaluation on both synthetic and real Kotlin programs; we also compared its performance with classic delta debugging techniques. The evaluation results support the practical usability of our approach to Kotlin delta debugging and also shows the importance of using both language-agnostic and language-specific techniques to achieve best reduction efficiency and performance."}, {"id": "conf/kbse/AhmedSSK19", "title": "Targeted Example Generation for Compilation Errors.", "authors": ["Umair Z. Ahmed", "Renuka Sindhgatta", "Nisheeth Srivastava", "Amey Karkare"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00039"], "tag": ["Main Track"], "abstract": "We present TEGCER, an automated feedback tool for novice programmers. TEGCER uses supervised classification to match compilation errors in new code submissions with relevant pre-existing errors, submitted by other students before. The dense neural network used to perform this classification task is trained on 15000+ error-repair code examples. The proposed model yields a test set classification Pred@3 accuracy of 97.7% across 212 error category labels. Using this model as its base, TEGCER presents students with the closest relevant examples of solutions for their specific error on demand. A large scale (N>230) usability study shows that students who use TEGCER are able to resolve errors more than 25% faster on average than students being assisted by human tutors."}, {"id": "conf/kbse/ChenD0Q19", "title": "Understanding Exception-Related Bugs in Large-Scale Cloud Systems.", "authors": ["Haicheng Chen", "Wensheng Dou", "Yanyan Jiang", "Feng Qin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00040"], "tag": ["Main Track"], "abstract": "Exception mechanism is widely used in cloud systems. This is mainly because it separates the error handling code from main business logic. However, the huge space of potential error conditions and the sophisticated logic of cloud systems present a big hurdle to the correct use of exception mechanism. As a result, mistakes in the exception use may lead to severe consequences, such as system downtime and data loss. To address this issue, the communities direly need a better understanding of the exception-related bugs, i.e., eBugs, which are caused by the incorrect use of exception mechanism, in cloud systems. In this paper, we present a comprehensive study on 210 eBugs from six widely-deployed cloud systems, including Cassandra, HBase, HDFS, Hadoop MapReduce, YARN, and ZooKeeper. For all the studied eBugs, we analyze their triggering conditions, root causes, bug impacts, and their relations. To the best of our knowledge, this is the first study on eBugs in cloud systems, and the first one that focuses on triggering conditions. We find that eBugs are severe in cloud systems: 74% of our studied eBugs affect system availability or integrity. Luckily, exposing eBugs through testing is possible: 54% of the eBugs are triggered by non-semantic conditions, such as network errors; 40% of the eBugs can be triggered by simulating the triggering conditions at simple system states. Furthermore, we find that the triggering conditions are useful for detecting eBugs. Based on such relevant findings, we build a static analysis tool, called DIET, and apply it to the latest versions of the studied systems. Our results show that DIET reports 31 bugs and bad practices, and 23 of them are confirmed by the developers as \"previously-unknown\" ones."}, {"id": "conf/kbse/ZhengLZLZD19", "title": "iFeedback: Exploiting User Feedback for Real-Time Issue Detection in Large-Scale Online Service Systems.", "authors": ["Wujie Zheng", "Haochuan Lu", "Yangfan Zhou", "Jianming Liang", "Haibing Zheng", "Yuetang Deng"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00041"], "tag": ["Main Track"], "abstract": "Large-scale online systems are complex, fast-evolving, and hardly bug-free despite the testing efforts. Backend system monitoring cannot detect many types of issues, such as UI related bugs, bugs with small impact on backend system indicators, or errors from third-party co-operating systems, etc. However, users are good informers of such issues: They will provide their feedback for any types of issues. This experience paper discusses our design of iFeedback, a tool to perform real-time issue detection based on user feedback texts. Unlike traditional approaches that analyze user feedback with computation-intensive natural language processing algorithms, iFeedback is focusing on fast issue detection, which can serve as a system life-condition monitor. In particular, iFeedback extracts word combination-based indicators from feedback texts. This allows iFeedback to perform fast system anomaly detection with sophisticated machine learning algorithms. iFeedback then further summarizes the texts with an aim to effectively present the anomaly to the developers for root cause analysis. We present our representative experiences in successfully applying iFeedback in tens of large-scale production online service systems in ten months."}, {"id": "conf/kbse/0003HL0HGXDZ19", "title": "Continuous Incident Triage for Large-Scale Online Service Systems.", "authors": ["Junjie Chen", "Xiaoting He", "Qingwei Lin", "Hongyu Zhang", "Dan Hao", "Feng Gao", "Zhangwei Xu", "Yingnong Dang", "Dongmei Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00042"], "tag": ["Main Track"], "abstract": "In recent years, online service systems have become increasingly popular. Incidents of these systems could cause significant economic loss and customer dissatisfaction. Incident triage, which is the process of assigning a new incident to the responsible team, is vitally important for quick recovery of the affected service. Our industry experience shows that in practice, incident triage is not conducted only once in the beginning, but is a continuous process, in which engineers from different teams have to discuss intensively among themselves about an incident, and continuously refine the incident-triage result until the correct assignment is reached. In particular, our empirical study on 8 real online service systems shows that the percentage of incidents that were reassigned ranges from 5.43% to 68.26% and the number of discussion items before achieving the correct assignment is up to 11.32 on average. To improve the existing incident triage process, in this paper, we propose DeepCT, a Deep learning based approach to automated Continuous incident Triage. DeepCT incorporates a novel GRU-based (Gated Recurrent Unit) model with an attention-based mask strategy and a revised loss function, which can incrementally learn knowledge from discussions and update incident-triage results. Using DeepCT, the correct incident assignment can be achieved with fewer discussions. We conducted an extensive evaluation of DeepCT on 14 large-scale online service systems in Microsoft. The results show that DeepCT is able to achieve more accurate and efficient incident triage, e.g., the average accuracy identifying the responsible team precisely is 0.641~0.729 with the number of discussion items increasing from 1 to 5. Also, DeepCT statistically significantly outperforms the state-of-the-art bug triage approach."}, {"id": "conf/kbse/ZhangC19", "title": "Apricot: A Weight-Adaptation Approach to Fixing Deep Learning Models.", "authors": ["Hao Zhang", "W. K. Chan"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00043"], "tag": ["Main Track"], "abstract": "A deep learning (DL) model is inherently imprecise. To address this problem, existing techniques retrain a DL model over a larger training dataset or with the help of fault injected models or using the insight of failing test cases in a DL model. In this paper, we present Apricot, a novel weight-adaptation approach to fixing DL models iteratively. Our key observation is that if the deep learning architecture of a DL model is trained over many different subsets of the original training dataset, the weights in the resultant reduced DL model (rDLM) can provide insights on the adjustment direction and magnitude of the weights in the original DL model to handle the test cases that the original DL model misclassifies. Apricot generates a set of such reduced DL models from the original DL model. In each iteration, for each failing test case experienced by the input DL model (iDLM), Apricot adjusts each weight of this iDLM toward the average weight of these rDLMs correctly classifying the test case and/or away from that of these rDLMs misclassifying the same test case, followed by training the weight-adjusted iDLM over the original training dataset to generate a new iDLM for the next iteration. The experiment using five state-of-the-art DL models shows that Apricot can increase the test accuracy of these models by 0.87%-1.55% with an average of 1.08%. The experiment also reveals the complementary nature of these rDLMs in Apricot."}, {"id": "conf/kbse/HuAMLR19", "title": "Re-Factoring Based Program Repair Applied to Programming Assignments.", "authors": ["Yang Hu", "Umair Z. Ahmed", "Sergey Mechtaev", "Ben Leong", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00044"], "tag": ["Main Track"], "abstract": "Automated program repair has been used to provide feedback for incorrect student programming assignments, since program repair captures the code modification needed to make a given buggy program pass a given test-suite. Existing student feedback generation techniques are limited because they either require manual effort in the form of providing an error model, or require a large number of correct student submissions to learn from, or suffer from lack of scalability and accuracy. In this work, we propose a fully automated approach for generating student program repairs in real-time. This is achieved by first re-factoring all available correct solutions to semantically equivalent solutions. Given an incorrect program, we match the program with the closest matching refactored program based on its control flow structure. Subsequently, we infer the input-output specifications of the incorrect program's basic blocks from the executions of the correct program's aligned basic blocks. Finally, these specifications are used to modify the blocks of the incorrect program via search-based synthesis. Our dataset consists of almost 1,800 real-life incorrect Python program submissions from 361 students for an introductory programming course at a large public university. Our experimental results suggest that our method is more effective and efficient than recently proposed feedback generation approaches. About 30% of the patches produced by our tool Refactory are smaller than those produced by the state-of-art tool Clara, and can be produced given fewer correct solutions (often a single correct solution) and in a shorter time. We opine that our method is applicable not only to programming assignments, and could be seen as a general-purpose program repair method that can achieve good results with just a single correct reference solution."}, {"id": "conf/kbse/EndresSCJW19", "title": "InFix: Automatically Repairing Novice Program Inputs.", "authors": ["Madeline Endres", "Georgios Sakkas", "Benjamin Cosman", "Ranjit Jhala", "Westley Weimer"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00045"], "tag": ["Main Track"], "abstract": "This paper presents InFix, a technique for automatically fixing erroneous program inputs for novice programmers. Unlike comparable existing approaches for automatic debugging and maintenance tasks, InFix repairs input data rather than source code, does not require test cases, and does not require special annotations. Instead, we take advantage of patterns commonly used by novice programmers to automatically create helpful, high quality input repairs. InFix iteratively applies error-message based templates and random mutations based on insights about the debugging behavior of novices. This paper presents an implementation of InFix for Python. We evaluate on 29,995 unique scenarios with input-related errors collected from four years of data from Python Tutor, a free online programming tutoring environment. Our results generalize and scale; compared to previous work, we consider an order of magnitude more unique programs. Overall, InFix is able to repair 94.5% of deterministic input errors. We also present the results of a human study with 97 participants. Surprisingly, this simple approach produces high quality repairs; humans judged the output of InFix to be equally helpful and within 4% of the quality of human-generated repairs."}, {"id": "conf/kbse/CashinMWF19", "title": "Understanding Automatically-Generated Patches Through Symbolic Invariant Differences.", "authors": ["Padraic Cashin", "Carianne Martinez", "Westley Weimer", "Stephanie Forrest"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00046"], "tag": ["Main Track"], "abstract": "Developer trust is a major barrier to the deployment of automatically-generated patches. Understanding the effect of a patch is a key element of that trust. We find that differences in sets of formal invariants characterize patch differences and that implication-based distances in invariant space characterize patch similarities. When one patch is similar to another it often contains the same changes as well as additional behavior; this pattern is well-captured by logical implication. We can measure differences using a theorem prover to verify implications between invariants implied by separate programs. Although effective, theorem provers are computationally intensive; we find that string distance is an efficient heuristic for implication-based distance measurements. We propose to use distances between patches to construct a hierarchy highlighting patch similarities. We evaluated this approach on over 300 patches and found that it correctly categorizes programs into semantically similar clusters. Clustering programs reduces human effort by reducing the number of semantically distinct patches that must be considered by over 50%, thus reducing the time required to establish trust in automatically generated repairs."}, {"id": "conf/kbse/MichaelDDLS19", "title": "Regexes are Hard: Decision-Making, Difficulties, and Risks in Programming Regular Expressions.", "authors": ["Louis G. Michael IV", "James Donohue", "James C. Davis", "Dongyoon Lee", "Francisco Servant"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00047"], "tag": ["Main Track"], "abstract": "Regular expressions (regexes) are a powerful mechanism for solving string-matching problems. They are supported by all modern programming languages, and have been estimated to appear in more than a third of Python and JavaScript projects. Yet existing studies have focused mostly on one aspect of regex programming: readability. We know little about how developers perceive and program regexes, nor the difficulties that they face. In this paper, we provide the first study of the regex development cycle, with a focus on (1) how developers make decisions throughout the process, (2) what difficulties they face, and (3) how aware they are about serious risks involved in programming regexes. We took a mixed-methods approach, surveying 279 professional developers from a diversity of backgrounds (including top tech firms) for a high-level perspective, and interviewing 17 developers to learn the details about the difficulties that they face and the solutions that they prefer. In brief, regexes are hard. Not only are they hard to read, our participants said that they are hard to search for, hard to validate, and hard to document. They are also hard to master: the majority of our studied developers were unaware of critical security risks that can occur when using regexes, and those who knew of the risks did not deal with them in effective manners. Our findings provide multiple implications for future work, including semantic regex search engines for regex reuse and improved input generators for regex validation."}, {"id": "conf/kbse/DavisMKL19", "title": "Testing Regex Generalizability And Its Implications: A Large-Scale Many-Language Measurement Study.", "authors": ["James C. Davis", "Daniel Moyer", "Ayaan M. Kazerouni", "Dongyoon Lee"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00048"], "tag": ["Main Track"], "abstract": "The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines."}, {"id": "conf/kbse/ShermanH19", "title": "Accurate String Constraints Solution Counting with Weighted Automata.", "authors": ["Elena Sherman", "Andrew Harris"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00049"], "tag": ["Main Track"], "abstract": "As an important extension of symbolic execution (SE), probabilistic symbolic execution (PSE) computes execution probabilities of program paths. Using this information, PSE can prioritize path exploration strategies. To calculate the probability of a path PSE relies on solution counting approaches for the path constraint. The correctness of a solution counting approach depends on the methodology used to count solutions and whether a path constraint maintains a one-to-one relation with program input values. This work focuses on the latter aspect of the solution counting correctness for string constraints. In general, maintaining a one-to-one relation is not always possible, especially in the presence of non-linear constraints. To deal with this issue, researchers that work on PSE for numerical domains either analyze programs with linear constraints, or develop novel techniques to handle solution counting of non-linear constraints. For the string domain, however, previous work on PSE mainly focuses on efficient and accurate solution counting for automata-based string models and has not investigated whether a one-to-one relationship between the strings encoded by automata and input string values is preserved. In this work we demonstrate that traditional automata-based string models fail to maintain one-to-one relations and propose to use the weighted automata model, which preserves the one-to-one relation between the path constraint it encodes and the input string values. We use this model to implement a string constraint solver and show its correctness on a set of non-trivial synthetic benchmarks. We also present an empirical evaluation of traditional and proposed automata solvers on real-world string constraints. The evaluations show that while being less efficient than traditional automata models, the weighted automata model maintains correct solution counts."}, {"id": "conf/kbse/EiersSBB19", "title": "Subformula Caching for Model Counting and Quantitative Program Analysis.", "authors": ["William Eiers", "Seemanta Saha", "Tegan Brennan", "Tevfik Bultan"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00050"], "tag": ["Main Track"], "abstract": "Quantitative program analysis is an emerging area with applications to software reliability, quantitative information flow, side-channel detection and attack synthesis. Most quantitative program analysis techniques rely on model counting constraint solvers, which are typically the bottleneck for scalability. Although the effectiveness of formula caching in expediting expensive model-counting queries has been demonstrated in prior work, our key insight is that many subformulas are shared across non-identical constraints generated during program analyses. This has not been utilized by prior formula caching approaches. In this paper we present a subformula caching framework and integrate it into a model counting constraint solver. We experimentally evaluate its effectiveness under three quantitative program analysis scenarios: 1) model counting constraints generated by symbolic execution, 2) reliability analysis using probabilistic symbolic execution, 3) adaptive attack synthesis for side-channels. Our experimental results demonstrate that our subformula caching approach significantly improves the performance of quantitative program analysis."}, {"id": "conf/kbse/BaoLWF19", "title": "ACTGAN: Automatic Configuration Tuning for Software Systems with Generative Adversarial Networks.", "authors": ["Liang Bao", "Xin Liu", "Fangzheng Wang", "Baoyin Fang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00051"], "tag": ["Main Track"], "abstract": "Complex software systems often provide a large number of parameters so that users can configure them for their specific application scenarios. However, configuration tuning requires a deep understanding of the software system, far beyond the abilities of typical system users. To address this issue, many existing approaches focus on exploring and learning good performance estimation models. The accuracy of such models often suffers when the number of available samples is small, a thorny challenge under a given tuning-time constraint. By contrast, we hypothesize that good configurations often share certain hidden structures. Therefore, instead of trying to improve the performance estimation of a given configuration, we focus on capturing the hidden structures of good configurations and utilizing such learned structure to generate potentially better configurations. We propose ACTGAN to achieve this goal. We have implemented and evaluated ACTGAN using 17 workloads with eight different software systems. Experimental results show that ACTGAN outperforms default configurations by 76.22% on average, and six state-of-the-art configuration tuning algorithms by 6.58%-64.56%. Furthermore, the ACTGAN-generated configurations are often better than those used in training and show certain features consisting with domain knowledge, both of which supports our hypothesis."}, {"id": "conf/kbse/HortonP19", "title": "V2: Fast Detection of Configuration Drift in Python.", "authors": ["Eric Horton", "Chris Parnin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00052"], "tag": ["Main Track"], "abstract": "Code snippets are prevalent, but are hard to reuse because they often lack an accompanying environment configuration. Most are not actively maintained, allowing for drift between the most recent possible configuration and the code snippet as the snippet becomes out-of-date over time. Recent work has identified the problem of validating and detecting out-of-date code snippets as the most important consideration for code reuse. However, determining if a snippet is correct, but simply out-of-date, is a non-trivial task. In the best case, breaking changes are well documented, allowing developers to manually determine when a code snippet contains an out-of-date API usage. In the worst case, determining if and when a breaking change was made requires an exhaustive search through previous dependency versions. We present V2, a strategy for determining if a code snippet is out-of-date by detecting discrete instances of configuration drift, where the snippet uses an API which has since undergone a breaking change. Each instance of configuration drift is classified by a failure encountered during validation and a configuration patch, consisting of dependency version changes, which fixes the underlying fault. V2 uses feedback-directed search to explore the possible configuration space for a code snippet, reducing the number of potential environment configurations that need to be validated. When run on a corpus of public Python snippets from prior research, V2 identifies 248 instances of configuration drift."}, {"id": "conf/kbse/NguyenNTTN19", "title": "Feature-Interaction Aware Configuration Prioritization for Configurable Code.", "authors": ["Son Nguyen", "Hoan Nguyen", "Ngoc M. Tran", "Hieu Tran", "Tien N. Nguyen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00053"], "tag": ["Main Track"], "abstract": "Unexpected interactions among features induce most bugs in a configurable software system. Exhaustively analyzing all the exponential number of possible configurations is prohibitively costly. Thus, various sampling techniques have been proposed to systematically narrow down the exponential number of legal configurations to be analyzed. Since analyzing all selected configurations can require a huge amount of effort, fault-based configuration prioritization, that helps detect faults earlier, can yield practical benefits in quality assurance. In this paper, we propose CoPro, a novel formulation of feature-interaction bugs via common program entities enabled/disabled by the features. Leveraging from that, we develop an efficient feature-interaction-aware configuration prioritization technique for a configurable system by ranking the configurations according to their total number of potential bugs. We conducted several experiments to evaluate CoPro on the ability to detect configuration-related bugs in a public benchmark. We found that CoPro outperforms the state-of-the-art configuration prioritization techniques when we add them on advanced sampling algorithms. In 78% of the cases, CoPro ranks the buggy configurations at the top 3 positions in the resulting list. Interestingly, CoPro is able to detect 17 not-yet-discovered feature-interaction bugs."}, {"id": "conf/kbse/JiangWXCZ19", "title": "Combining Spectrum-Based Fault Localization and Statistical Debugging: An Empirical Study.", "authors": ["Jiajun Jiang", "Ran Wang", "Yingfei Xiong", "Xiangping Chen", "Lu Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00054"], "tag": ["Main Track"], "abstract": "Program debugging is a time-consuming task, and researchers have proposed different kinds of automatic fault localization techniques to mitigate the burden of manual debugging. Among these techniques, two popular families are spectrum-based fault localization (SBFL) and statistical debugging (SD), both localizing faults by collecting statistical information at runtime. Though the ideas are similar, the two families have been developed independently and their combinations have not been systematically explored. In this paper we perform a systematical empirical study on the combination of SBFL and SD. We first build a unified model of the two techniques, and systematically explore four types of variations, different predicates, different risk evaluation formulas, different granularities of data collection, and different methods of combining suspicious scores. Our study leads to several findings. First, most of the effectiveness of the combined approach contributed by a simple type of predicates: branch conditions. Second, the risk evaluation formulas of SBFL significantly outperform that of SD. Third, fine-grained data collection significantly outperforms coarse-grained data collection with a little extra execution overhead. Fourth, a linear combination of SBFL and SD predicates outperforms both individual approaches. According to our empirical study, we propose a new fault localization approach, PREDFL (Predicate-based Fault Localization), with the best configuration for each dimension under the unified model. Then, we explore its complementarity to existing techniques by integrating PREDFL with a state-of-the-art fault localization framework. The experimental results show that PREDFL can further improve the effectiveness of state-of-the-art fault localization techniques. More concretely, integrating PREDFL results in an up to 20.8% improvement w.r.t the faults successfully located at Top-1, which reveals that PREDFL complements existing techniques."}, {"id": "conf/kbse/ZamanHY19", "title": "SCMiner: Localizing System-Level Concurrency Faults from Large System Call Traces.", "authors": ["Tarannum Shaila Zaman", "Xue Han", "Tingting Yu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00055"], "tag": ["Main Track"], "abstract": "Localizing concurrency faults that occur in production is hard because, (1) detailed field data, such as user input, file content and interleaving schedule, may not be available to developers to reproduce the failure; (2) it is often impractical to assume the availability of multiple failing executions to localize the faults using existing techniques; (3) it is challenging to search for buggy locations in an application given limited runtime data; and, (4) concurrency failures at the system level often involve multiple processes or event handlers (e.g., software signals), which can not be handled by existing tools for diagnosing intra-process(thread-level) failures. To address these problems, we present SCMiner, a practical online bug diagnosis tool to help developers understand how a system-level concurrency fault happens based on the logs collected by the default system audit tools. SCMiner achieves online bug diagnosis to obviate the need for offline bug reproduction. SCMiner does not require code instrumentation on the production system or rely on the assumption of the availability of multiple failing executions. Specifically, after the system call traces are collected, SCMiner uses data mining and statistical anomaly detection techniques to identify the failure-inducing system call sequences. It then maps each abnormal sequence to specific application functions. We have conducted an empirical study on 19 real-world benchmarks. The results show that SCMiner is both effective and efficient at localizing system-level concurrency faults."}, {"id": "conf/kbse/RenLXJX19", "title": "Root Cause Localization for Unreproducible Builds via Causality Analysis Over System Call Tracing.", "authors": ["Zhilei Ren", "Changlin Liu", "Xusheng Xiao", "He Jiang", "Tao Xie"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00056"], "tag": ["Main Track"], "abstract": "Localization of the root causes for unreproducible builds during software maintenance is an important yet challenging task, primarily due to limited runtime traces from build processes and high diversity of build environments. To address these challenges, in this paper, we propose RepTrace, a framework that leverages the uniform interfaces of system call tracing for monitoring executed build commands in diverse build environments and identifies the root causes for unreproducible builds by analyzing the system call traces of the executed build commands. Specifically, from the collected system call traces, RepTrace performs causality analysis to build a dependency graph starting from an inconsistent build artifact (across two builds) via two types of dependencies: read/write dependencies among processes and parent/child process dependencies, and searches the graph to find the processes that result in the inconsistencies. To address the challenges of massive noisy dependencies and uncertain parent/child dependencies, RepTrace includes two novel techniques: (1) using differential analysis on multiple builds to reduce the search space of read/write dependencies, and (2) computing similarity of the runtime values to filter out noisy parent/child process dependencies. The evaluation results of RepTrace over a set of real-world software packages show that RepTrace effectively finds not only the root cause commands responsible for the unreproducible builds, but also the files to patch for addressing the unreproducible issues. Among its Top-10 identified commands and files, RepTrace achieves high accuracy rate of 90.00% and 90.56% in identifying the root causes, respectively."}, {"id": "conf/kbse/CelikPPAG19", "title": "Mutation Analysis for Coq.", "authors": ["Ahmet \u00c7elik", "Karl Palmskog", "Marinela Parovic", "Emilio Jes\u00fas Gallego Arias", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00057"], "tag": ["Main Track"], "abstract": "Mutation analysis, which introduces artificial defects into software systems, is the basis of mutation testing, a technique widely applied to evaluate and enhance the quality of test suites. However, despite the deep analogy between tests and formal proofs, mutation analysis has seldom been considered in the context of deductive verification. We propose mutation proving, a technique for analyzing verification projects that use proof assistants. We implemented our technique for the Coq proof assistant in a tool dubbed mCoq. mCoq applies a set of mutation operators to Coq definitions of functions and datatypes, inspired by operators previously proposed for functional programming languages. mCoq then checks proofs of lemmas affected by operator application. To make our technique feasible in practice, we implemented several optimizations in mCoq such as parallel proof checking. We applied mCoq to several medium and large scale Coq projects, and recorded whether proofs passed or failed when applying different mutation operators. We then qualitatively analyzed the mutants, finding many instances of incomplete specifications. For our evaluation, we made several improvements to serialization of Coq files and even discovered a notable bug in Coq itself, all acknowledged by developers. We believe mCoq can be useful both to proof engineers for improving the quality of their verification projects and to researchers for evaluating proof engineering techniques."}, {"id": "conf/kbse/LiuSTWY19", "title": "Verifying Arithmetic in Cryptographic C Programs.", "authors": ["Jiaxiang Liu", "Xiaomu Shi", "Ming-Hsien Tsai", "Bow-Yaw Wang", "Bo-Yin Yang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00058"], "tag": ["Main Track"], "abstract": "Cryptographic primitives are ubiquitous for modern security. The correctness of their implementations is crucial to resist malicious attacks. Typical arithmetic computation of these C programs contains large numbers of non-linear operations, hence is challenging existing automatic C verification tools. We present an automated approach to verify cryptographic C programs. Our approach successfully verifies C implementations of various arithmetic operations used in NIST P-224, P-256, P-521 and Curve25519 in OpenSSL. During verification, we expose a bug and a few anomalies that have been existing for a long time. They have been reported to and confirmed by the OpenSSL community. Our results establish the functional correctness of these C implementations for the first time."}, {"id": "conf/kbse/KimC19", "title": "Model Checking Embedded Control Software using OS-in-the-Loop CEGAR.", "authors": ["Dongwoo Kim", "Yunja Choi"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00059"], "tag": ["Main Track"], "abstract": "Verification of multitasking embedded software requires taking into account its underlying operating system w.r.t. its scheduling policy and handling of task priorities in order to achieve a higher degree of accuracy. However, such comprehensive verification of multitasking embedded software together with its underlying operating system is very costly and impractical. To reduce the verification cost while achieving the desired accuracy, we propose a variant of CEGAR, named OiL-CEGAR (OS-in-the-Loop Counterexample-Guided Abstraction Refinement), where a composition of a formal OS model and an abstracted application program is used for comprehensive verification and is successively refined using the counterexamples generated from the composition model. The refinement process utilizes the scheduling information in the counterexample, which acts as a mini-OS to check the executability of the counterexample trace on the concrete program. Our experiments using a prototype implementation of OiL-CEGAR show that OiL-CEGAR greatly improves the accuracy and efficiency of property checking in this domain. It automatically removed all false alarms and accomplished property checking within an average of 476 seconds over a set of multitasking programs, whereas model checking using existing approaches over the same set of programs either showed an accuracy of under 11.1% or was unable to finish the verification due to timeout."}, {"id": "conf/kbse/RecoulesBBMP19", "title": "Get Rid of Inline Assembly through Verification-Oriented Lifting.", "authors": ["Fr\u00e9d\u00e9ric Recoules", "S\u00e9bastien Bardin", "Richard Bonichon", "Laurent Mounier", "Marie-Laure Potet"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00060"], "tag": ["Main Track"], "abstract": "Formal methods for software development have made great strides in the last two decades, to the point that their application in safety-critical embedded software is an undeniable success. Their extension to non-critical software is one of the notable forthcoming challenges. For example, C programmers regularly use inline assembly for low-level optimizations and system primitives. This usually results in rendering state-of-the-art formal analyzers developed for C ineffective. We thus propose TINA, the first automated, generic, verification-friendly and trustworthy lifting technique turning inline assembly into semantically equivalent C code amenable to verification, in order to take advantage of existing C analyzers. Extensive experiments on real-world code (including GMP and ffmpeg) show the feasibility and benefits of TINA."}, {"id": "conf/kbse/Gu0019", "title": "CodeKernel: A Graph Kernel Based Approach to the Selection of API Usage Examples.", "authors": ["Xiaodong Gu", "Hongyu Zhang", "Sunghun Kim"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00061"], "tag": ["Main Track"], "abstract": "Developers often want to find out how to use a certain API (e.g., FileReader.read in JDK library). API usage examples are very helpful in this regard. Over the years, many automated methods have been proposed to generate code examples by clustering and summarizing relevant code snippets extracted from a code corpus. These approaches simplify source code as method invocation sequences or feature vectors. Such simplifications only model partial aspects of the code and tend to yield inaccurate examples. We propose CodeKernel, a graph kernel based approach to the selection of API usage examples. Instead of approximating source code as method invocation sequences or feature vectors, CodeKernel represents source code as object usage graphs. Then, it clusters graphs by embedding them into a continuous space using a graph kernel. Finally, it outputs code examples by selecting a representative graph from each cluster using designed ranking metrics. Our empirical evaluation shows that CodeKernel selects more accurate code examples than the related work (MUSE and eXoaDocs). A user study involving 25 developers in a multinational company also confirms the usefulness of CodeKernel in selecting API usage examples."}, {"id": "conf/kbse/JiangLJ19", "title": "Machine Learning Based Recommendation of Method Names: How Far are We.", "authors": ["Lin Jiang", "Hui Liu", "He Jiang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00062"], "tag": ["Main Track"], "abstract": "High quality method names are critical for the readability and maintainability of programs. However, constructing concise and consistent method names is often challenging, especially for inexperienced developers. To this end, advanced machine learning techniques have been recently leveraged to recommend method names automatically for given method bodies/implementation. Recent large-scale evaluations also suggest that such approaches are accurate. However, little is known about where and why such approaches work or don't work. To figure out the state of the art as well as the rationale for the success/failure, in this paper we conduct an empirical study on the state-of-the-art approach code2vec. We assess code2vec on a new dataset with more realistic settings. Our evaluation results suggest that although switching to new dataset does not significantly influence the performance, more realistic settings do significantly reduce the performance of code2vec. Further analysis on the successfully recommended method names also reveals the following findings: 1) around half (48.3%) of the accepted recommendations are made on getter/setter methods; 2) a large portion (19.2%) of the successfully recommended method names could be copied from the given bodies. To further validate its usefulness, we ask developers to manually score the difficulty in naming methods they developed. Code2vec is then applied to such manually scored methods to evaluate how often it works in need. Our evaluation results suggest that code2vec rarely works when it is really needed. Finally, to intuitively reveal the state of the art and to investigate the possibility of designing simple and straightforward alternative approaches, we propose a heuristics based approach to recommending method names. Evaluation results on large-scale dataset suggest that this simple heuristics-based approach significantly outperforms the state-of-the-art machine learning based approach, improving precision and recall by 65.25% and 22.45%, respectively. The comparison suggests that machine learning based recommendation of method names may still have a long way to go."}, {"id": "conf/kbse/NamHMMV19", "title": "MARBLE: Mining for Boilerplate Code to Identify API Usability Problems.", "authors": ["Daye Nam", "Amber Horvath", "Andrew Macvean", "Brad A. Myers", "Bogdan Vasilescu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00063"], "tag": ["Main Track"], "abstract": "Designing usable APIs is critical to developers' productivity and software quality, but is quite difficult. One of the challenges is that anticipating API usability barriers and real-world usage is difficult, due to a lack of automated approaches to mine usability data at scale. In this paper, we focus on one particular grievance that developers repeatedly express in online discussions about APIs: \"boilerplate code.\" We investigate what properties make code count as boilerplate, the reasons for boilerplate, and how programmers can reduce the need for it. We then present MARBLE, a novel approach to automatically mine boilerplate code candidates from API client code repositories. MARBLE adapts existing techniques, including an API usage mining algorithm, an AST comparison algorithm, and a graph partitioning algorithm. We evaluate MARBLE with 13 Java APIs, and show that our approach successfully identifies both already-known and new API-related boilerplate code instances."}, {"id": "conf/kbse/LacomisYSAGNV19", "title": "DIRE: A Neural Approach to Decompiled Identifier Naming.", "authors": ["Jeremy Lacomis", "Pengcheng Yin", "Edward J. Schwartz", "Miltiadis Allamanis", "Claire Le Goues", "Graham Neubig", "Bogdan Vasilescu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00064"], "tag": ["Main Track"], "abstract": "The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time."}, {"id": "conf/kbse/MuhlbauerAS19", "title": "Accurate Modeling of Performance Histories for Evolving Software Systems.", "authors": ["Stefan M\u00fchlbauer", "Sven Apel", "Norbert Siegmund"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00065"], "tag": ["Main Track"], "abstract": "Learning from the history of a software system's performance behavior does not only help discovering and locating performance bugs, but also identifying evolutionary performance patterns and general trends, such as when technical debt accumulates. Exhaustive regression testing is usually impractical, because rigorous performance benchmarking requires executing a realistic workload per revision, which results in large execution times. In this paper, we propose a novel active revision sampling approach, which aims at tracking and understanding a system's performance history by approximating the performance behavior of a software system across all of its revisions. In a nutshell, we iteratively sample and measure the performance of specific revisions that help us building an exact performance-evolution model, and we use Gaussian Process models to assess in which revision ranges our model is most uncertain with the goal to sample further revisions for measurement. We have conducted an empirical analysis of the evolutionary performance behavior modeled as a time series of the histories of six real-world software systems. Our evaluation demonstrates that Gaussian Process models are able to accurately estimate the performance-evolution history of real-world software systems with only few measurements and to reveal interesting behaviors and trends."}, {"id": "conf/kbse/ChenJML19", "title": "An Industrial Experience Report on Performance-Aware Refactoring on a Database-Centric Web Application.", "authors": ["Boyuan Chen", "Zhen Ming Jiang", "Paul Matos", "Michael Lacaria"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00066"], "tag": ["Main Track"], "abstract": "Modern web applications rely heavily on databases to query and update information. To ease the development efforts, Object Relational Mapping (ORM) frameworks provide an abstraction for developers to manage databases by writing in the same Object-Oriented programming languages. Prior studies have shown that there are various types of performance issues caused by inefficient accesses to databases via different ORM frameworks (e.g., Hibernate and ActiveRecord). However, it is not clear whether the reported performance anti-patterns (common performance issues) can be generalizable across various frameworks. In particular, there is no study focusing on detecting performance issues for applications written in PHP, which is the choice of programming languages for the majority (79%) of web applications. In this experience paper, we detail our process on conducting performance-aware refactoring of an industrial web application written in Laravel, the most popular web framework in PHP. We have derived a complete catalog of 17 performance anti-patterns based on prior research and our experimentation. We have found that some of the reported anti-patterns and refactoring techniques are framework or programming language specific, whereas others are general. The performance impact of the anti-pattern instances are highly dependent on the actual usage context (workload and database settings). When communicating the performance differences before and after refactoring, the results of the complex statistical analysis may be sometimes confusing. Instead, developers usually prefer more intuitive measures like percentage improvement. Experiments show that our refactoring techniques can reduce the response time up to 93.0% and 93.4% for the industrial and the open source application under various scenarios."}, {"id": "conf/kbse/TaoTLXQ19", "title": "How Do API Selections Affect the Runtime Performance of Data Analytics Tasks?", "authors": ["Yida Tao", "Shan Tang", "Yepang Liu", "Zhiwu Xu", "Shengchao Qin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00067"], "tag": ["Main Track"], "abstract": "As data volume and complexity grow at an unprecedented rate, the performance of data analytics programs is becoming a major concern for developers. We observed that developers sometimes use alternative data analytics APIs to improve program runtime performance while preserving functional equivalence. However, little is known on the characteristics and performance attributes of alternative data analytics APIs. In this paper, we propose a novel approach to extracting alternative implementations that invoke different data analytics APIs to solve the same tasks. A key appeal of our approach is that it exploits the comparative structures in Stack Overflow discussions to discover programming alternatives. We show that our approach is promising, as 86% of the extracted code pairs were validated as true alternative implementations. In over 20% of these pairs, the faster implementation was reported to achieve a 10x or more speedup over its slower alternative. We hope that our study offers a new perspective of API recommendation and motivates future research on optimizing data analytics programs."}, {"id": "conf/kbse/ChenSHWL19", "title": "An Experience Report of Generating Load Tests Using Log-Recovered Workloads at Varying Granularities of User Behaviour.", "authors": ["Jinfu Chen", "Weiyi Shang", "Ahmed E. Hassan", "Yong Wang", "Jiangbin Lin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00068"], "tag": ["Main Track"], "abstract": "Designing field-representative load tests is an essential step for the quality assurance of large-scale systems. Practitioners may capture user behaviour at different levels of granularity. A coarse-grained load test may miss detailed user behaviour, leading to a non-representative load test; while an extremely fine-grained load test would simply replay user actions step by step, leading to load tests that are costly to develop, execute and maintain. Workload recovery is at core of these load tests. Prior research often captures the workload as the frequency of user actions. However, there exists much valuable information in the context and sequences of user actions. Such richer information would ensure that the load tests that leverage such workloads are more field-representative. In this experience paper, we study the use of different granularities of user behaviour, i.e., basic user actions, basic user actions with contextual information and user action sequences with contextual information, when recovering workloads for use in the load testing of large-scale systems. We propose three approaches that are based on the three granularities of user behaviour and evaluate our approaches on four subject systems, namely Apache James, OpenMRS, Google Borg, and an ultra-large-scale industrial system (SA) from Alibaba. Our results show that our approach that is based on user action sequences with contextual information outperforms the other two approaches and can generate more representative load tests with similar throughput and CPU usage to the original field workload (i.e., mostly statistically insignificant or with small/trivial effect sizes). Such representative load tests are generated only based on a small number of clusters of users, leading to a low cost of conducting/maintaining such tests. Finally, we demonstrate that our approaches can detect injected users in the original field workloads with high precision and recall. Our paper demonstrates the importance of user action sequences with contextual information in the workload recovery of large-scale systems."}, {"id": "conf/kbse/TangZZLXZY19", "title": "Demystifying Application Performance Management Libraries for Android.", "authors": ["Yutian Tang", "Xian Zhan", "Hao Zhou", "Xiapu Luo", "Zhou Xu", "Yajin Zhou", "Qiben Yan"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00069"], "tag": ["Main Track"], "abstract": "Since the performance issues of apps can influence users' experience, developers leverage application performance management (APM) tools to locate the potential performance bottleneck of their apps. Unfortunately, most developers do not understand how APMs monitor their apps during the runtime and whether these APMs have any limitations. In this paper, we demystify APMs by inspecting 25 widely-used APMs that target on Android apps. We first report how these APMs implement 8 key functions as well as their limitations. Then, we conduct a large-scale empirical study on 500,000 Android apps from Google Play to explore the usage of APMs. This study has some interesting observations about existing APMs for Android, including 1) some APMs still use deprecated permissions and approaches so that they may not always work properly; 2) some app developers use APMs to collect users' privacy information."}, {"id": "conf/kbse/LiuHGN19", "title": "Predicting Licenses for Changed Source Code.", "authors": ["Xiaoyu Liu", "LiGuo Huang", "Jidong Ge", "Vincent Ng"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00070"], "tag": ["Main Track"], "abstract": "Open source software licenses regulate the circumstances under which software can be redistributed, reused and modified. Ensuring license compatibility and preventing license restriction conflicts among source code during software changes are the key to protect their commercial use. However, selecting the appropriate licenses for software changes requires lots of experience and manual effort that involve examining, assimilating and comparing various licenses as well as understanding their relationships with software changes. Worse still, there is no state-of-the-art methodology to provide this capability. Motivated by this observation, we propose in this paper Automatic License Prediction (ALP), a novel learning-based method and tool for predicting licenses as software changes. An extensive evaluation of ALP on predicting licenses in 700 open source projects demonstrate its effectiveness: ALP can achieve not only a high overall prediction accuracy (92.5% in micro F1 score) but also high accuracies across all license types."}, {"id": "conf/kbse/GongJWJ19", "title": "Empirical Evaluation of the Impact of Class Overlap on Software Defect Prediction.", "authors": ["Lina Gong", "Shujuan Jiang", "Rongcun Wang", "Li Jiang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00071"], "tag": ["Main Track"], "abstract": "Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance."}, {"id": "conf/kbse/NguyenNLW19", "title": "Combining Program Analysis and Statistical Language Model for Code Statement Completion.", "authors": ["Son Nguyen", "Tien N. Nguyen", "Yi Li", "Shaohua Wang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00072"], "tag": ["Main Track"], "abstract": "Automatic code completion helps improve developers' productivity in their programming tasks. A program contains instructions expressed via code statements, which are considered as the basic units of program execution. In this paper, we introduce AutoSC, which combines program analysis and the principle of software naturalness to fill in a partially completed statement. AutoSC benefits from the strengths of both directions, in which the completed code statement is both frequent and valid. AutoSC is first trained on a large code corpus to derive the templates of candidate statements. Then, it uses program analysis to validate and concretize the templates into syntactically and type-valid candidate statements. Finally, these candidates are ranked by using a language model trained on the lexical form of the source code in the code corpus. Our empirical evaluation on the large datasets of real-world projects shows that AutoSC achieves 38.9-41.3% top-1 accuracy and 48.2-50.1% top-5 accuracy in statement completion. It also outperforms a state-of-the-art approach from 9X-69X in top-1 accuracy."}, {"id": "conf/kbse/WangZL00L19", "title": "MAP-Coverage: A Novel Coverage Criterion for Testing Thread-Safe Classes.", "authors": ["Zan Wang", "Yingquan Zhao", "Shuang Liu", "Jun Sun", "Xiang Chen", "Huarui Lin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00073"], "tag": ["Main Track"], "abstract": "Concurrent programs must be thoroughly tested, as concurrency bugs are notoriously hard to detect. Code coverage criteria can be used to quantify the richness of a test suite (e.g., whether a program has been tested sufficiently) or provide practical guidelines on test case generation (e.g., as objective functions used in program fuzzing engines). Traditional code coverage criteria are, however, designed for sequential programs and thus ineffective for concurrent programs. In this work, we introduce a novel code coverage criterion for testing thread-safe classes called MAP-coverage (short for memory-access patterns). The motivation is that concurrency bugs are often correlated with certain memory-access patterns, and thus it is desirable to comprehensively cover all memory-access patterns. Furthermore, we propose a testing method for maximizing MAP-coverage. Our method has been implemented as a self-contained toolkit, and the experimental results on 20 benchmark programs show that our toolkit outperforms existing testing methods. Lastly, we show empirically that there exists positive correlation between MAP-coverage and the effectiveness of a set of test executions."}, {"id": "conf/kbse/ZhangYD19", "title": "Automating Non-Blocking Synchronization In Concurrent Data Abstractions.", "authors": ["Jiange Zhang", "Qing Yi", "Damian Dechev"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00074"], "tag": ["Main Track"], "abstract": "This paper investigates using compiler technology to automatically convert sequential C++ data abstractions, e.g., queues, stacks, maps, and trees, to concurrent lock-free implementations. By automatically tailoring a number of state-of-the-practice synchronization methods to the underlying sequential implementations of different data structures, our automatically synchronized code can attain performance competitive to that of manually-written concurrent data structures by experts and much better performance than heavier-weight support by software transactional memory (STM)."}, {"id": "conf/kbse/WuZ0TZ19", "title": "Automating CUDA Synchronization via Program Transformation.", "authors": ["Mingyuan Wu", "Lingming Zhang", "Cong Liu", "Shin Hwei Tan", "Yuqun Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00075"], "tag": ["Main Track"], "abstract": "While CUDA has been the most popular parallel computing platform and programming model for general purpose GPU computing, CUDA synchronization undergoes significant challenges for GPU programmers due to its intricate parallel computing mechanism and coding practices. In this paper, we propose AuCS, the first general framework to automate synchronization for CUDA kernel functions. AuCS transforms the original LLVM-level CUDA program control flow graph in a semantic-preserving manner for exploring the possible barrier function locations. Accordingly, AuCS develops mechanisms to correctly place barrier functions for automating synchronization in multiple erroneous (challenging-to-be-detected) synchronization scenarios, including data race, barrier divergence, and redundant barrier functions. To evaluate the effectiveness and efficiency of AuCS, we conduct an extensive set of experiments and the results demonstrate that AuCS can automate 20 out of 24 erroneous synchronization scenarios."}, {"id": "conf/kbse/PobeeMC19", "title": "Efficient Transaction-Based Deterministic Replay for Multi-threaded Programs.", "authors": ["Ernest Bota Pobee", "Xiupei Mei", "Wing Kwong Chan"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00076"], "tag": ["Main Track"], "abstract": "Existing deterministic replay techniques propose strategies which attempt to reduce record log sizes and achieve successful replay. However, these techniques still generate large logs and achieve replay only under certain conditions. We propose a solution based on the division of the sequence of events of each thread into sequential blocks called transactions. Our insight is that there are usually few to no atomicity violations among transactions reported during a program execution. We present TPLAY, a novel deterministic replay technique which records thread access interleavings on shared memory locations at the transactional level. TPLAY also generates an artificial pair of interleavings when an atomicity violation is reported on a transaction. We present an experiment using the Splash2x extension of the PARSEC benchmark suite. Experimental results indicate that TPLAY experiences a 13-fold improvement in record log sizes and achieves a higher replay probability in comparison to existing work."}, {"id": "conf/kbse/ZhengFXS0HMLSC19", "title": "Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning.", "authors": ["Yan Zheng", "Changjie Fan", "Xiaofei Xie", "Ting Su", "Lei Ma", "Jianye Hao", "Zhaopeng Meng", "Yang Liu", "Ruimin Shen", "Yingfeng Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00077"], "tag": ["Main Track"], "abstract": "Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs, which have been confirmed by the developers, in the commercial games."}, {"id": "conf/kbse/NejadgholiY19", "title": "A Study of Oracle Approximations in Testing Deep Learning Libraries.", "authors": ["Mahdi Nejadgholi", "Jinqiu Yang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00078"], "tag": ["Main Track"], "abstract": "Due to the increasing popularity of deep learning (DL) applications, testing DL libraries is becoming more and more important. Different from testing general software, for which output is often asserted definitely (e.g., an output is compared with an oracle for equality), testing deep learning libraries often requires to perform oracle approximations, i.e., the output is allowed to be within a restricted range of the oracle. However, oracle approximation practices have not been studied in prior empirical work that focuses on traditional testing practices. The prevalence, common practices, maintenance and evolution challenges of oracle approximations remain unknown in literature. In this work, we study oracle approximation assertions implemented to test four popular DL libraries. Our study shows that there exists a non-negligible portion of assertions that leverage oracle approximation in the test cases of DL libraries. Also, we identify the common sources of oracles on which oracle approximations are being performed through a comprehensive manual study. Moreover, we find that developers frequently modify code related to oracle approximations, i.e., using a different approximation API, modifying the oracle or the output from the code under test, and using a different approximation threshold. Last, we performed an in-depth study to understand the reasons behind the evolution of oracle approximation assertions. Our findings reveal important maintenance challenges that developers may face when maintaining oracle approximation practices as code evolves in DL libraries."}, {"id": "conf/kbse/GopinathCPT19", "title": "Property Inference for Deep Neural Networks.", "authors": ["Divya Gopinath", "Hayes Converse", "Corina S. Pasareanu", "Ankur Taly"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00079"], "tag": ["Main Track"], "abstract": "We present techniques for automatically inferring formal properties of feed-forward neural networks. We observe that a significant part (if not all) of the logic of feed forward networks is captured in the activation status (on or off) of its neurons. We propose to extract patterns based on neuron decisions as preconditions that imply certain desirable output property e.g., the prediction being a certain class. We present techniques to extract input properties, encoding convex predicates on the input space that imply given output properties and layer properties, representing network properties captured in the hidden layers that imply the desired output behavior. We apply our techniques on networks for the MNIST and ACASXU applications. Our experiments highlight the use of the inferred properties in a variety of tasks, such as explaining predictions, providing robustness guarantees, simplifying proofs, and network distillation."}, {"id": "conf/kbse/GuoCXMHLLZL19", "title": "An Empirical Study Towards Characterizing Deep Learning Development and Deployment Across Different Frameworks and Platforms.", "authors": ["Qianyu Guo", "Sen Chen", "Xiaofei Xie", "Lei Ma", "Qiang Hu", "Hongtao Liu", "Yang Liu", "Jianjun Zhao", "Xiaohong Li"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00080"], "tag": ["Main Track"], "abstract": "Deep Learning (DL) has recently achieved tremendous success. A variety of DL frameworks and platforms play a key role to catalyze such progress. However, the differences in architecture designs and implementations of existing frameworks and platforms bring new challenges for DL software development and deployment. Till now, there is no study on how various mainstream frameworks and platforms influence both DL software development and deployment in practice. To fill this gap, we take the first step towards understanding how the most widely-used DL frameworks and platforms support the DL software development and deployment. We conduct a systematic study on these frameworks and platforms by using two types of DNN architectures and three popular datasets. (1) For development process, we investigate the prediction accuracy under the same runtime training configuration or same model weights/biases. We also study the adversarial robustness of trained models by leveraging the existing adversarial attack techniques. The experimental results show that the computing differences across frameworks could result in an obvious prediction accuracy decline, which should draw the attention of DL developers. (2) For deployment process, we investigate the prediction accuracy and performance (refers to time cost and memory consumption) when the trained models are migrated/quantized from PC to real mobile devices and web browsers. The DL platform study unveils that the migration and quantization still suffer from compatibility and reliability issues. Meanwhile, we find several DL software bugs by using the results as a benchmark. We further validate the results through bug confirmation from stakeholders and industrial positive feedback to highlight the implications of our study. Through our study, we summarize practical guidelines, identify challenges and pinpoint new research directions, such as understanding the characteristics of DL frameworks and platforms, avoiding compatibility and reliability issues, detecting DL software bugs, and reducing time cost and memory consumption towards developing and deploying high quality DL systems effectively."}, {"id": "conf/kbse/AlizadehOKC19", "title": "RefBot: Intelligent Software Refactoring Bot.", "authors": ["Vahid Alizadeh", "Mohamed Amine Ouali", "Marouane Kessentini", "Meriem Chater"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00081"], "tag": ["Main Track"], "abstract": "The adoption of refactoring techniques for continuous integration received much less attention from the research community comparing to root-canal refactoring to fix the quality issues in the whole system. Several recent empirical studies show that developers, in practice, are applying refactoring incrementally when they are fixing bugs or adding new features. There is an urgent need for refactoring tools that can support continuous integration and some recent development processes such as DevOps that are based on rapid releases. Furthermore, several studies show that manual refactoring is expensive and existing automated refactoring tools are challenging to configure and integrate into the development pipelines with significant disruption cost. In this paper, we propose, for the first time, an intelligent software refactoring bot, called RefBot. Integrated into the version control system (e.g. GitHub), our bot continuously monitors the software repository, and it is triggered by any \"open\" or \"merge\" action on pull requests. The bot analyzes the files changed during that pull request to identify refactoring opportunities using a set of quality attributes then it will find the best sequence of refactorings to fix the quality issues if any. The bot recommends all these refactorings through an automatically generated pull-request. The developer can review the recommendations and their impacts in a detailed report and select the code changes that he wants to keep or ignore. After this review, the developer can close and approve the merge of the bot's pull request. We quantitatively and qualitatively evaluated the performance and effectiveness of RefBot by a survey conducted with experienced developers who used the bot on both open source and industry projects."}, {"id": "conf/kbse/KohlerS19", "title": "Automated Refactoring to Reactive Programming.", "authors": ["Mirko K\u00f6hler", "Guido Salvaneschi"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00082"], "tag": ["Main Track"], "abstract": "Reactive programming languages and libraries, such as ReactiveX, have been shown to significantly improve software design and have seen important industrial adoption over the last years. Asynchronous applications - which are notoriously error-prone to implement and to maintain - greatly benefit from reactive programming because they can be defined in a declarative style, which improves code clarity and extensibility. In this paper, we tackle the problem of refactoring existing software that has been designed with traditional abstractions for asynchronous programming. We propose 2Rx, a refactoring approach to automatically convert asynchronous code to reactive programming. Our evaluation on top-starred GitHub projects shows that 2Rx is effective with the most common asynchronous constructs, covering 12.7% of projects with asynchronous computations, and it can provide a refactoring for 91.7% of their occurrences."}, {"id": "conf/kbse/VerhaegheFGAD19", "title": "Empirical Study of Programming to an Interface.", "authors": ["Beno\u00eet Verhaeghe", "Christopher Fuhrman", "Latifa Guerrouj", "Nicolas Anquetil", "St\u00e9phane Ducasse"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00083"], "tag": ["Main Track"], "abstract": "A popular recommendation to programmers in object-oriented software is to \"program to an interface, not an implementation\" (PTI). Expected benefits include increased simplicity from abstraction, decreased dependency on implementations, and higher flexibility. Yet, interfaces must be immutable, excessive class hierarchies can be a form of complexity, and \"speculative generality\" is a known code smell. To advance the empirical knowledge of PTI, we conducted an empirical investigation that involves 126 Java projects on GitHub, aiming to measuring the decreased dependency benefits (in terms of cochange)."}, {"id": "conf/kbse/BaoB0M19", "title": "Statistical Log Differencing.", "authors": ["Lingfeng Bao", "Nimrod Busany", "David Lo", "Shahar Maoz"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00084"], "tag": ["Main Track"], "abstract": "Recent works have considered the problem of log differencing: given two or more system's execution logs, output a model of their differences. Log differencing has potential applications in software evolution, testing, and security. In this paper we present statistical log differencing, which accounts for frequencies of behaviors found in the logs. We present two algorithms, s2KDiff for differencing two logs, and snKDiff, for differencing of many logs at once, both presenting their results over a single inferred model. A unique aspect of our algorithms is their use of statistical hypothesis testing: we let the engineer control the sensitivity of the analysis by setting the target distance between probabilities and the statistical significance value, and report only (and all) the statistically significant differences. Our evaluation shows the effectiveness of our work in terms of soundness, completeness, and performance. It also demonstrates its effectiveness compared to previous work via a user-study and its potential applications via a case study using real-world logs."}, {"id": "conf/kbse/LiuZHHZL19", "title": "Logzip: Extracting Hidden Structures via Iterative Clustering for Log Compression.", "authors": ["Jinyang Liu", "Jieming Zhu", "Shilin He", "Pinjia He", "Zibin Zheng", "Michael R. Lyu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00085"], "tag": ["Main Track"], "abstract": "System logs record detailed runtime information of software systems and are used as the main data source for many tasks around software engineering. As modern software systems are evolving into large scale and complex structures, logs have become one type of fast-growing big data in industry. In particular, such logs often need to be stored for a long time in practice (e.g., a year), in order to analyze recurrent problems or track security issues. However, archiving logs consumes a large amount of storage space and computing resources, which in turn incurs high operational cost. Data compression is essential to reduce the cost of log storage. Traditional compression tools (e.g., gzip) work well for general texts, but are not tailed for system logs. In this paper, we propose a novel and effective log compression method, namely logzip. Logzip is capable of extracting hidden structures from raw logs via fast iterative clustering and further generating coherent intermediate representations that allow for more effective compression. We evaluate logzip on five large log datasets of different system types, with a total of 63.6 GB in size. The results show that logzip can save about half of the storage space on average over traditional compression tools. Meanwhile, the design of logzip is highly parallel and only incurs negligible overhead. In addition, we share our industrial experience of applying logzip to Huawei's real products."}, {"id": "conf/kbse/Boronat19", "title": "Code-First Model-Driven Engineering: On the Agile Adoption of MDE Tooling.", "authors": ["Artur Boronat"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00086"], "tag": ["Main Track"], "abstract": "Domain models are the most important asset in widely accepted software development approaches, like Domain-Driven Design (DDD), yet those models are still implicitly represented in programs. Model-Driven Engineering (MDE) regards those models as representable entities that are amenable to automated analysis and processing, facilitating quality assurance while increasing productivity in software development processes. Although this connection is not new, very few approaches facilitate adoption of MDE tooling without compromising existing value, their data. Moreover, switching to MDE tooling usually involves re-engineering core parts of an application, hindering backward compatibility and, thereby, continuous integration, while requiring an up-front investment in training in specialized modeling frameworks. In those approaches that overcome the previous problem, there is no clear indication - from a quantitative point of view - of the extent to which adopting state-of-the-art MDE practices and tooling is feasible or advantageous. In this work, we advocate a code-first approach to modeling through an approach for applying MDE techniques and tools to existing object-oriented software applications that fully preserves the semantics of the original application, which need not be modified. Our approach consists both of a semi-automated method for specifying explicit view models out of existing object-oriented applications and of a conservative extension mechanism that enables the use of such view models at run time, where view model queries are resolved on demand and view model updates are propagated incrementally to the original application. This mechanism enables an iterative, flexible application of MDE tooling to software applications, where metamodels and models do not exist explicitly. An evaluation of this extension mechanism, implemented for Java applications and for view models atop the Eclipse Modeling Framework (EMF), has been conducted with an industry-targeted benchmark for decision support systems, analyzing performance and scalability of the synchronization mechanism. Backward propagation of large updates over very large views is instant."}, {"id": "conf/kbse/BusanyMY19", "title": "Size and Accuracy in Model Inference.", "authors": ["Nimrod Busany", "Shahar Maoz", "Yehonatan Yulazari"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00087"], "tag": ["Main Track"], "abstract": "Many works infer finite-state models from execution logs. Large models are more accurate but also more difficult to present and understand. Small models are easier to present and understand but are less accurate. In this work we investigate the tradeoff between model size and accuracy in the context of the classic k-Tails model inference algorithm. First, we define mk-Tails, a generalization of k-Tails from one to many parameters, which enables fine-grained control over the tradeoff. Second, we extend mk-Tails with a reduction based on past-equivalence, which effectively reduces the size of the model without decreasing its accuracy. We implemented our work and evaluated its performance and effectiveness on real-world logs as well as on models and generated logs from the literature."}, {"id": "conf/kbse/PaulsenSPW19", "title": "Debreach: Mitigating Compression Side Channels via Static Analysis and Transformation.", "authors": ["Brandon Paulsen", "Chungha Sung", "Peter A. H. Peterson", "Chao Wang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00088"], "tag": ["Main Track"], "abstract": "Compression is an emerging source of exploitable side-channel leakage that threatens data security, particularly in web applications where compression is indispensable for performance reasons. Current approaches to mitigating compression side channels have drawbacks in that they either degrade compression ratio drastically or require too much effort from developers to be widely adopted. To bridge the gap, we develop Debreach, a static analysis and program transformation based approach to mitigating compression side channels. Debreach consists of two steps. First, it uses taint analysis to soundly identify flows of sensitive data in the program and uses code instrumentation to annotate data before feeding them to the compressor. Second, it enhances the compressor to exploit the freedom to not compress of standard compression protocols, thus removing the dependency between sensitive data and the size of the compressor's output. Since Debreach automatically instruments applications and does not change the compression protocols, it has the advantage of being non-disruptive and compatible with existing systems. We have evaluated Debreach on a set of web server applications written in PHP. Our experiments show that, while ensuring leakage-freedom, Debreach can achieve significantly higher compression performance than state-of-the-art approaches."}, {"id": "conf/kbse/Nowack19", "title": "Fine-Grain Memory Object Representation in Symbolic Execution.", "authors": ["Martin Nowack"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00089"], "tag": ["Main Track"], "abstract": "Dynamic Symbolic Execution (DSE) has seen rising popularity as it allows to check applications for behaviours such as error patterns automatically. One of its biggest challenges is the state space explosion problem: DSE tries to evaluate all possible execution paths of an application. For every path, it needs to represent the allocated memory and its accesses. Even though different approaches have been proposed to mitigate the state space explosion problem, DSE still needs to represent a multitude of states in parallel to analyse them. If too many states are present, they cannot fit into memory, and DSE needs to terminate them prematurely or store them on disc intermediately. With a more efficient representation of allocated memory, DSE can handle more states simultaneously, improving its performance. In this work, we introduce an enhanced, fine-grain and efficient representation of memory that mimics the allocations of tested applications. We tested Coreutils using three different search strategies with our implementation on top of the symbolic execution engine KLEE. We achieve a significant reduction of the memory consumption of states by up to 99.06% (mean DFS: 2%, BFS: 51%, Cov.: 49%), allowing to represent more states in memory more efficiently. The total execution time is reduced by up to 97.81% (mean DFS: 9%, BFS: 7%, Cov.:4%)-a speedup of 49x in comparison to baseline KLEE."}, {"id": "conf/kbse/MuGCCGXMS19", "title": "RENN: Efficient Reverse Execution with Neural-Network-Assisted Alias Analysis.", "authors": ["Dongliang Mu", "Wenbo Guo", "Alejandro Cuevas", "Yueqi Chen", "Jinxuan Gai", "Xinyu Xing", "Bing Mao", "Chengyu Song"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00090"], "tag": ["Main Track"], "abstract": "Reverse execution and coredump analysis have long been used to diagnose the root cause of software crashes. Each of these techniques, however, face inherent challenges, such as insufficient capability when handling memory aliases. Recent works have used hypothesis testing to address this drawback, albeit with high computational complexity, making them impractical for real world applications. To address this issue, we propose a new deep neural architecture, which could significantly improve memory alias resolution. At the high level, our approach employs a recurrent neural network (RNN) to learn the binary code pattern pertaining to memory accesses. It then infers the memory region accessed by memory references. Since memory references to different regions naturally indicate a non-alias relationship, our neural architecture can greatly reduce the burden of doing hypothesis testing to track down non-alias relation in binary code. Different from previous researches that have utilized deep learning for other binary analysis tasks, the neural network proposed in this work is fundamentally novel. Instead of simply using off-the-shelf neural networks, we designed a new recurrent neural architecture that could capture the data dependency between machine code segments. To demonstrate the utility of our deep neural architecture, we implement it as RENN, a neural network-assisted reverse execution system. We utilize this tool to analyze software crashes corresponding to 40 memory corruption vulnerabilities from the real world. Our experiments show that RENN can significantly improve the efficiency of locating the root cause for the crashes. Compared to a state-of-the-art technique, RENN has 36.25% faster execution time on average, detects an average of 21.35% more non-alias pairs, and successfully identified the root cause of 12.5% more cases."}, {"id": "conf/kbse/VeduradaN19", "title": "Batch Alias Analysis.", "authors": ["Jyothi Vedurada", "V. Krishna Nandivada"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00091"], "tag": ["Main Track"], "abstract": "Many program-analysis based tools require precise points-to/alias information only for some program variables. To meet this requirement efficiently, there have been many works on demand-driven analyses that perform only the work necessary to compute the points-to or alias information on the requested variables (queries). However, these demand-driven analyses can be very expensive when applied on large systems where the number of queries can be significant. Such a blow-up in analysis time is unacceptable in cases where scalability with real-time constraints is crucial; for example, when program analysis tools are plugged into an IDE (Integrated Development Environment). In this paper, we propose schemes to improve the scalability of demand-driven analyses without compromising on precision. Our work is based on novel ideas for eliminating irrelevant and redundant data-flow paths for the given queries. We introduce the idea of batch analysis, which can answer multiple given queries in batch mode. Batch analysis suits the environments with strict time constraints, where the queries come in batch. We present a batch alias analysis framework that can be used to speed up given demand-driven alias analysis. To show the effectiveness of this framework, we use two demand-driven alias analyses (1) the existing best performing demand-driven alias analysis tool for race-detection clients and (2) an optimized version thereof that avoids irrelevant computation. Our evaluations on a simulated data-race client, and on a recent program-understanding tool, show that batch analysis leads to significant performance gains, along with minor gains in precision."}, {"id": "conf/kbse/PalmerinoYDK19", "title": "Improving the Decision-Making Process of Self-Adaptive Systems by Accounting for Tactic Volatility.", "authors": ["Jeffrey Palmerino", "Qi Yu", "Travis Desell", "Daniel E. Krutz"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00092"], "tag": ["Main Track"], "abstract": "When self-adaptive systems encounter changes withintheir surrounding environments, they enacttacticsto performnecessary adaptations. For example, a self-adaptive cloud-basedsystem may have a tactic that initiates additional computingresources when response time thresholds are surpassed, or theremay be a tactic to activate a specific security measure when anintrusion is detected. In real-world environments, these tacticsfrequently experiencetactic volatilitywhich is variable behaviorduring the execution of the tactic.Unfortunately, current self-adaptive approaches do not accountfor tactic volatility in their decision-making processes, and merelyassume that tactics do not experience volatility. This limitationcreates uncertainty in the decision-making process and mayadversely impact the system's ability to effectively and efficientlyadapt. Additionally, many processes do not properly account forvolatility that may effect the system's Service Level Agreement(SLA). This can limit the system's ability to act proactively, especially when utilizing tactics that contain latency.To address the challenge of sufficiently accounting for tacticvolatility, we propose aTactic Volatility Aware(TVA) solution.Using Multiple Regression Analysis (MRA), TVA enables self-adaptive systems to accurately estimate the cost and timerequired to execute tactics. TVA also utilizesAutoregressiveIntegrated Moving Average(ARIMA) for time series forecasting, allowing the system to proactively maintain specifications."}, {"id": "conf/kbse/0001P0A019", "title": "Learning-Guided Network Fuzzing for Testing Cyber-Physical System Defences.", "authors": ["Yuqi Chen", "Christopher M. Poskitt", "Jun Sun", "Sridhar Adepu", "Fan Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00093"], "tag": ["Main Track"], "abstract": "The threat of attack faced by cyber-physical systems (CPSs), especially when they play a critical role in automating public infrastructure, has motivated research into a wide variety of attack defence mechanisms. Assessing their effectiveness is challenging, however, as realistic sets of attacks to test them against are not always available. In this paper, we propose smart fuzzing, an automated, machine learning guided technique for systematically finding 'test suites' of CPS network attacks, without requiring any knowledge of the system's control programs or physical processes. Our approach uses predictive machine learning models and metaheuristic search algorithms to guide the fuzzing of actuators so as to drive the CPS into different unsafe physical states. We demonstrate the efficacy of smart fuzzing by implementing it for two real-world CPS testbeds-a water purification plant and a water distribution system-finding attacks that drive them into 27 different unsafe states involving water flow, pressure, and tank levels, including six that were not covered by an established attack benchmark. Finally, we use our approach to test the effectiveness of an invariant-based defence system for the water treatment plant, finding two attacks that were not detected by its physical invariant checks, highlighting a potential weakness that could be exploited in certain conditions."}, {"id": "conf/kbse/MaiaVCYZN19", "title": "Cautious Adaptation of Defiant Components.", "authors": ["Paulo Henrique M. Maia", "Lucas Vieira", "Matheus Chagas", "Yijun Yu", "Andrea Zisman", "Bashar Nuseibeh"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00094"], "tag": ["Main Track"], "abstract": "Systems-of-systems are formed by the composition of independently created software components. These components are designed to satisfy their individual requirements, rather than the global requirements of the systems-of-systems. We refer to components that cannot be adapted to meet both individual and global requirements as \"defiant\" components. In this paper, we propose a \"cautious\" adaptation approach which supports changing the behaviour of such defiant components under exceptional conditions to satisfy global requirements, while continuing to guarantee the satisfaction of the components' individual requirements. The approach represents both normal and exceptional conditions as scenarios; models the behaviour of exceptional conditions as wrappers implemented using an aspect-oriented technique; and deals with both single and multiple instances of defiant components with different precedence order at runtime. We evaluated an implementation of the approach using drones and boats for an organ delivery application conceived by our industrial partners, in which we assess how the proposed approach help achieve the system-of-systems' global requirements while accommodating increased complexity of hybrid aspects such as multiplicity, precedence ordering, openness and heterogeneity."}, {"id": "conf/kbse/FengCKC0F19", "title": "Active Hotspot: An Issue-Oriented Model to Monitor Software Evolution and Degradation.", "authors": ["Qiong Feng", "Yuanfang Cai", "Rick Kazman", "Di Cui", "Ting Liu", "Hongzhou Fang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00095"], "tag": ["Main Track"], "abstract": "Architecture degradation has a strong negative impact on software quality and can result in significant losses. Severe software degradation does not happen overnight. Software evolves continuously, through numerous issues, fixing bugs and adding new features, and architecture flaws emerge quietly and largely unnoticed until they grow in scope and significance when the system becomes difficult to maintain. Developers are largely unaware of these flaws or the accumulating debt as they are focused on their immediate tasks of address individual issues. As a consequence, the cumulative impacts of their activities, as they affect the architecture, go unnoticed. To detect these problems early and prevent them from accumulating into severe ones we propose to monitor software evolution by tracking the interactions among files revised to address issues. In particular, we propose and show how we can automatically detect active hotspots, to reveal architecture problems. We have studied hundreds of hotspots along the evolution timelines of 21 open source projects and showed that there exist just a few dominating active hotspots per project at any given time. Moreover, these dominating active hotspots persist over long time periods, and thus deserve special attention. Compared with state-of-the-art design and code smell detection tools we report that, using active hotspots, it is possible to detect signs of software degradation both earlier and more precisely."}, {"id": "conf/kbse/Gerostathopoulos19", "title": "Automated Trainability Evaluation for Smart Software Functions.", "authors": ["Ilias Gerostathopoulos", "Stefan Kugele", "Christoph Segler", "Tom\u00e1s Bures", "Alois Knoll"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00096"], "tag": ["Main Track"], "abstract": "More and more software-intensive systems employ machine learning and runtime optimization to improve their functionality by providing advanced features (e. g. personal driving assistants or recommendation engines). Such systems incorporate a number of smart software functions (SSFs) which gradually learn and adapt to the users' preferences. A key property of SSFs is their ability to learn based on data resulting from the interaction with the user (implicit and explicit feedback)-which we call trainability. Newly developed and enhanced features in a SSF must be evaluated based on their effect on the trainability of the system. Despite recent approaches for continuous deployment of machine learning systems, trainability evaluation is not yet part of continuous integration and deployment (CID) pipelines. In this paper, we describe the different facets of trainability for the development of SSFs. We also present our approach for automated trainability evaluation within an automotive CID framework which proposes to use automated quality gates for the continuous evaluation of machine learning models. The results from our indicative evaluation based on real data from eight BMW cars highlight the importance of continuous and rigorous trainability evaluation in the development of SSFs."}, {"id": "conf/kbse/CavalcantiBSA19", "title": "The Impact of Structure on Software Merging: Semistructured Versus Structured Merge.", "authors": ["Guilherme Cavalcanti", "Paulo Borba", "Georg Seibt", "Sven Apel"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00097"], "tag": ["Main Track"], "abstract": "Merge conflicts often occur when developers concurrently change the same code artifacts. While state of practice unstructured merge tools (e.g Git merge) try to automatically resolve merge conflicts based on textual similarity, semistructured and structured merge tools try to go further by exploiting the syntactic structure and semantics of the artifacts involved. Although there is evidence that semistructured merge has significant advantages over unstructured merge, and that structured merge reports significantly fewer conflicts than unstructured merge, it is unknown how semistructured merge compares with structured merge. To help developers decide which kind of tool to use, we compare semistructured and structured merge in an empirical study by reproducing more than 40,000 merge scenarios from more than 500 projects. In particular, we assess how often the two merge strategies report different results, we identify conflicts incorrectly reported by one but not by the other (false positives), and conflicts correctly reported by one but missed by the other (false negatives). Our results show that semistructured and structured merge differ in 24% of the scenarios with conflicts. Semistructured merge reports more false positives, whereas structured merge has more false negatives. Finally, we found that adapting a semistructured merge tool to resolve a particular kind of conflict makes semistructured and structured merge even closer."}, {"id": "conf/kbse/TavaresBCS19", "title": "Semistructured Merge in JavaScript Systems.", "authors": ["Alberto Trindade Tavares", "Paulo Borba", "Guilherme Cavalcanti", "S\u00e9rgio Soares"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00098"], "tag": ["Main Track"], "abstract": "Industry widely uses unstructured merge tools that rely on textual analysis to detect and resolve conflicts between code contributions. Semistructured merge tools go further by partially exploring the syntactic structure of code artifacts, and, as a consequence, obtaining significant merge accuracy gains for Java-like languages. To understand whether semistructured merge and the observed gains generalize to other kinds of languages, we implement two semistructured merge tools for JavaScript, and compare them to an unstructured tool. We find that current semistructured merge algorithms and frameworks are not directly applicable for scripting languages like JavaScript. By adapting the algorithms, and studying 10,345 merge scenarios from 50 JavaScript projects on GitHub, we find evidence that our JavaScript tools report fewer spurious conflicts than unstructured merge, without compromising the correctness of the merging process. The gains, however, are much smaller than the ones observed for Java-like languages, suggesting that semistructured merge advantages might be limited for languages that allow both commutative and non-commutative declarations at the same syntactic level."}, {"id": "conf/kbse/NafiKRRS19", "title": "CLCDSA: Cross Language Code Clone Detection using Syntactical Features and API Documentation.", "authors": ["Kawser Wazed Nafi", "Tonny Shekha Kar", "Banani Roy", "Chanchal K. Roy", "Kevin A. Schneider"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00099"], "tag": ["Main Track"], "abstract": "Software clones are detrimental to software maintenance and evolution and as a result many clone detectors have been proposed. These tools target clone detection in software applications written in a single programming language. However, a software application may be written in different languages for different platforms to improve the application's platform compatibility and adoption by users of different platforms. Cross language clones (CLCs) introduce additional challenges when maintaining multi-platform applications and would likely go undetected using existing tools. In this paper, we propose CLCDSA, a cross language clone detector which can detect CLCs without extensive processing of the source code and without the need to generate an intermediate representation. The proposed CLCDSA model analyzes different syntactic features of source code across different programming languages to detect CLCs. To support large scale clone detection, the CLCDSA model uses an action filter based on cross language API call similarity to discard non-potential clones. The design methodology of CLCDSA is two-fold: (a) it detects CLCs on the fly by comparing the similarity of features, and (b) it uses a deep neural network based feature vector learning model to learn the features and detect CLCs. Early evaluation of the model observed an average precision, recall and F-measure score of 0.55, 0.86, and 0.64 respectively for the first phase and 0.61, 0.93, and 0.71 respectively for the second phase which indicates that CLCDSA outperforms all available models in detecting cross language clones."}, {"id": "conf/kbse/FengYLBWTSYXPXH19", "title": "B2SFinder: Detecting Open-Source Software Reuse in COTS Software.", "authors": ["Muyue Feng", "Zimu Yuan", "Feng Li", "Gu Ban", "Shiyang Wang", "Qian Tang", "He Su", "Chendong Yu", "Jiahuan Xu", "Aihua Piao", "Jingling Xue", "Wei Huo"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00100"], "tag": ["Main Track"], "abstract": "COTS software products are developed extensively on top of OSS projects, resulting in OSS reuse vulnerabilities. To detect such vulnerabilities, finding OSS reuses in COTS software has become imperative. While scalable to tens of thousands of OSS projects, existing binary-to-source matching approaches are severely imprecise in analyzing COTS software products, since they support only a limited number of code features, compute matching scores only approximately in measuring OSS reuses, and neglect the code structures in OSS projects. We introduce a novel binary-to-source matching approach, called B2SFINDER<sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>, to address these limitations. First of all, B2SFINDER can reason about seven kinds of code features that are traceable in both binary and source code. In order to compute matching scores precisely, B2SFINDER employs a weighted feature matching algorithm that combines three matching methods (for dealing with different code features) with two importance-weighting methods (for computing the weight of an instance of a code feature in a given COTS software application based on its specificity and occurrence frequency). Finally, B2SFINDER identifies different types of code reuses based on matching scores and code structures of OSS projects. We have implemented B2SFINDER using an optimized data structure. We have evaluated B2SFINDER using 21991 binaries from 1000 popular COTS software products and 2189 candidate OSS projects. Our experimental results show that B2SFINDER is not only precise but also scalable. Compared with the state ofthe art, B2SFINDER has successfully found up to 2.15\u00d7 as many reuse cases in 53.85 seconds per binary file on average. We also discuss how B2SFINDER can be leveraged in detecting OSS reuse vulnerabilities in practice."}, {"id": "conf/kbse/WangLZX19", "title": "CoRA: Decomposing and Describing Tangled Code Changes for Reviewer.", "authors": ["Min Wang", "Zeqi Lin", "Yanzhen Zou", "Bing Xie"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00101"], "tag": ["Main Track"], "abstract": "Code review is an important mechanism for code quality assurance both in open source software and industrial software. Reviewers usually suffer from numerous, tangled and loosely related code changes that are bundled in a single commit, which makes code review very difficult. In this paper, we propose CoRA (Code Review Assistant), an automatic approach to decompose a commit into different parts and generate concise descriptions for reviewers. More specifically, CoRA can decompose a commit into independent parts (e.g., bug fixing, new feature adding, or refactoring) by code dependency analysis and tree-based similar-code detection, then identify the most important code changes in each part based on the PageRank algorithm and heuristic rules. As a result, CoRA can generate a concise description for each part of the commit. We evaluate our approach in seven open source software projects and 50 code commits. The results indicate that CoRA can improve the accuracy of decomposing code changes by 6.3% over the state-ofart practice. At the same time, CoRA can identify the important part from the fine-grained code changes with a mean average precision (MAP) of 87.7%. We also conduct a human study with eight participants to evaluate the performance and usefulness of CoRA, the user feedback indicates that CoRA can effectively help reviewers."}, {"id": "conf/kbse/DuX000Z19", "title": "A Quantitative Analysis Framework for Recurrent Neural Network.", "authors": ["Xiaoning Du", "Xiaofei Xie", "Yi Li", "Lei Ma", "Yang Liu", "Jianjun Zhao"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00102"], "tag": ["Tool Demonstrations"], "abstract": "Recurrent neural network (RNN) has achieved great success in processing sequential inputs for applications such as automatic speech recognition, natural language processing and machine translation. However, quality and reliability issues of RNNs make them vulnerable to adversarial attacks and hinder their deployment in real-world applications. In this paper, we propose a quantitative analysis framework - DeepStellar - to pave the way for effective quality and security analysis of software systems powered by RNNs. DeepStellar is generic to handle various RNN architectures, including LSTM and GRU, scalable to work on industrial-grade RNN models, and extensible to develop customized analyzers and tools. We demonstrated that, with DeepStellar, users are able to design efficient test generation tools, and develop effective adversarial sample detectors. We tested the developed applications on three real RNN models, including speech recognition and image classification. DeepStellar outperforms existing approaches three hundred times in generating defect-triggering tests and achieves 97% accuracy in detecting adversarial attacks. A video demonstration which shows the main features of DeepStellar is available at: https://sites.google.com/view/deepstellar/tool-demo."}, {"id": "conf/kbse/YuF0Z019", "title": "LIRAT: Layout and Image Recognition Driving Automated Mobile Testing of Cross-Platform.", "authors": ["Shengcheng Yu", "Chunrong Fang", "Yang Feng", "Wenyuan Zhao", "Zhenyu Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00103"], "tag": ["Tool Demonstrations"], "abstract": "The fragmentation issue spreads over multiple mobile platforms such as Android, iOS, mobile web, and WeChat, which hinders test scripts from running across platforms. To reduce the cost of adapting scripts for various platforms, some existing tools apply conventional computer vision techniques to replay the same script on multiple platforms. However, because these solutions can hardly identify dynamic or similar widgets. It becomes difficult for engineers to apply them in practice. In this paper, we present an image-driven tool, namely LIRAT, to record and replay test scripts cross platforms, solving the problem of test script cross-platform replay for the first time. LIRAT records screenshots and layouts of the widgets, and leverages image understanding techniques to locate them in the replay process. Based on accurate widget localization, LIRAT supports replaying test scripts across devices and platforms. We employed LIRAT to replay 25 scripts from 5 application across 8 Android devices and 2 iOS devices. The results show that LIRAT can replay 88% scripts on Android platforms and 60% on iOS platforms. The demo can be found at: https: //github.com/YSC9848/LIRAT."}, {"id": "conf/kbse/LiY0C19", "title": "Humanoid: A Deep Learning-Based Approach to Automated Black-box Android App Testing.", "authors": ["Yuanchun Li", "Ziyue Yang", "Yao Guo", "Xiangqun Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00104"], "tag": ["Tool Demonstrations"], "abstract": "Automated input generators must constantly choose which UI element to interact with and how to interact with it, in order to achieve high coverage with a limited time budget. Currently, most black-box input generators adopt pseudo-random or brute-force searching strategies, which may take very long to find the correct combination of inputs that can drive the app into new and important states. We propose Humanoid, an automated black-box Android app testing tool based on deep learning. The key technique behind Humanoid is a deep neural network model that can learn how human users choose actions based on an app's GUI from human interaction traces. The learned model can then be used to guide test input generation to achieve higher coverage. Experiments on both open-source apps and market apps demonstrate that Humanoid is able to reach higher coverage, and faster as well, than the state-of-the-art test input generators. Humanoid is open-sourced at https://github.com/yzygitzh/Humanoid and a demo video can be found at https://youtu.be/PDRxDrkyORs."}, {"id": "conf/kbse/BeyerL19", "title": "TestCov: Robust Test-Suite Execution and Coverage Measurement.", "authors": ["Dirk Beyer", "Thomas Lemberger"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00105"], "tag": ["Tool Demonstrations"], "abstract": "We present TestCov, a tool for robust test-suite execution and test-coverage measurement on C programs. TestCov executes program tests in isolated containers to ensure system integrity and reliable resource control. The tool provides coverage statistics per test and for the whole test suite. TestCov uses the simple, XML -based exchange format for test-suite specifications that was established as standard by Test-Comp. TestCov has been successfully used in Test-Comp '19 to execute almost 9 million tests on 1720 different programs. The source code of TestCov is released under the open-source license Apache 2.0 and available at https://gitlab.com/sosy-lab/software/test-suite-validator. A full artifact, including a demonstration video, is available at https://doi.org/10.5281/zenodo.3418726."}, {"id": "conf/kbse/ZhouWLLS019", "title": "VisFuzz: Understanding and Intervening Fuzzing with Interactive Visualization.", "authors": ["Chijin Zhou", "Mingzhe Wang", "Jie Liang", "Zhe Liu", "Chengnian Sun", "Yu Jiang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00106"], "tag": ["Tool Demonstrations"], "abstract": "Fuzzing is widely used for vulnerability detection. One of the challenges for an efficient fuzzing is covering code guarded by constraints such as the magic number and nested conditions. Recently, academia has partially addressed the challenge via whitebox methods. However, high-level constraints such as array sorts, virtual function invocations, and tree set queries are yet to be handled. To meet this end, we present VisFuzz, an interactive tool for better understanding and intervening fuzzing process via real-time visualization. It extracts call graph and control flow graph from source code, maps each function and basic block to the line of source code and tracks real-time execution statistics with detail constraint contexts. With VisFuzz, test engineers first locate blocking constraints and then learn its semantic context, which helps to craft targeted inputs or update test drivers. Preliminary evaluations are conducted on four real-world programs in Google fuzzer-test-suite. Given additional 15 minutes to understand and intervene the state of fuzzing, the intervened fuzzing outperform the original pure AFL fuzzing, and the path coverage improvements range from 10.84% to 150.58%, equally fuzzed by for 12 hours."}, {"id": "conf/kbse/AmreenKM19", "title": "Developer Reputation Estimator (DRE).", "authors": ["Sadika Amreen", "Andrey Karnauch", "Audris Mockus"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00107"], "tag": ["Tool Demonstrations"], "abstract": "Evidence shows that developer reputation is extremely important when accepting pull requests or resolving reported issues. It is particularly salient in Free/Libre Open Source Software since the developers are distributed around the world, do not work for the same organization and, in most cases, never meet face to face. The existing solutions to expose developer reputation tend to be forge specific (GitHub), focus on activity instead of impact, do not leverage social or technical networks, and do not correct often misspelled developer identities. We aim to remedy this by amalgamating data from all public Git repositories, measuring the impact of developer work, expose developer's collaborators, and correct notoriously problematic developer identity data. We leverage World of Code (WoC), a collection of an almost complete (and continuously updated) set of Git repositories by first allowing developers to select which of the 34 million(M) Git commit author IDs belong to them and then generating their profiles by treating the selected collection of IDs as that single developer. As a side-effect, these selections serve as a training set for a supervised learning algorithm that merges multiple identity strings belonging to a single individual. As we evaluate the tool and the proposed impact measure, we expect to build on these findings to develop reputation badges that could be associated with pull requests and commits so developers could easier trust and prioritize them."}, {"id": "conf/kbse/DuCWLSC19", "title": "CocoQa: Question Answering for Coding Conventions Over Knowledge Graphs.", "authors": ["Tianjiao Du", "Junming Cao", "Qinyue Wu", "Wei Li", "Beijun Shen", "Yuting Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00108"], "tag": ["Tool Demonstrations"], "abstract": "Coding convention plays an important role in guaranteeing software quality. However, coding conventions are usually informally presented and inconvenient for programmers to use. In this paper, we present CocoQa, a system that answers programmer's questions about coding conventions. CocoQa answers questions by querying a knowledge graph for coding conventions. It employs 1) a subgraph matching algorithm that parses the question into a SPARQL query, and 2) a machine comprehension algorithm that uses an end-to-end neural network to detect answers from searched paragraphs. We have implemented CocoQa, and evaluated it on a coding convention QA dataset. The results show that CocoQa can answer questions about coding conventions precisely. In particular, CocoQa can achieve a precision of 82.92% and a recall of 91.10%. Repository: https://github.com/14dtj/CocoQa/ Video: https://youtu.be/VQaXi1WydAU."}, {"id": "conf/kbse/Escobar-Velasquez19", "title": "MutAPK: Source-Codeless Mutant Generation for Android Apps.", "authors": ["Camilo Escobar-Velasquez", "Michael Osorio-Ria\u00f1o", "Mario Linares-V\u00e1squez"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00109"], "tag": ["Tool Demonstrations"], "abstract": "The amount of Android application is having a tremendous increasing trend, exerting pressure over practitioners and researchers around application quality, frequent releases, and quick fixing of bugs. This pressure leads practitioners to make usage of automated approaches based on using source-code as input. Nevertheless, third-party services are not able to use these approaches due to privacy factors. In this paper we present MutAPK, an open source mutation testing tool that enables the usage of APK as input for this task. MutAPK generates mutants without the need of having access to source code, because the mutations are done in an intermediate representation of the code (i.e., SMALI) that does not require compilation. MutAPK is publicly available at GitHub: https://bit.ly/2KYvgP9 VIDEO: https://bit.ly/2WOjiyy."}, {"id": "conf/kbse/PiskachevDJB19", "title": "SWAN_ASSIST: Semi-Automated Detection of Code-Specific, Security-Relevant Methods.", "authors": ["Goran Piskachev", "Lisa Nguyen Quang Do", "Oshando Johnson", "Eric Bodden"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00110"], "tag": ["Tool Demonstrations"], "abstract": "To detect specific types of bugs and vulnerabilities, static analysis tools must be correctly configured with security-relevant methods (SRM), e.g., sources, sinks, sanitizers and authentication methods-usually a very labour-intensive and error-prone process. This work presents the semi-automated tool SWAN_ASSIST, which aids the configuration with an IntelliJ plugin based on active machine learning. It integrates our novel automated machine-learning approach SWAN, which identifies and classifies Java SRM. SWAN_ASSIST further integrates user feedback through iterative learning. SWAN_ASSIST aids developers by asking them to classify at each point in time exactly those methods whose classification best impact the classification result. Our experiments show that SWAN_ASSIST classifies SRM with a high precision, and requires a relatively low effort from the user. A video demo of SWAN_ASSIST can be found at https://youtu.be/fSyD3V6EQOY. The source code is available at https://github.com/secure-software-engineering/swan."}, {"id": "conf/kbse/SadiqLLAL19", "title": "Sip4J: Statically Inferring Access Permission Contracts for Parallelising Sequential Java Programs.", "authors": ["Ayesha Sadiq", "Li Li", "Yuan-Fang Li", "Ijaz Ahmed", "Sea Ling"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00111"], "tag": ["Tool Demonstrations"], "abstract": "This paper presents Sip4J, a fully automated, scalable and effective tool to automatically generate access permission contracts for a sequential Java program. The access permission contracts, which represent the dependency of code blocks, have been frequently used to enable concurrent execution of sequential programs. Those permission contracts, unfortunately, need to be manually created by programmers, which is known to be time-consuming, laborious and error-prone. To mitigate those manual efforts, Sip4J performs inter-procedural static analysis of Java source code to automatically extract the implicit dependencies in the program and subsequently leverages them to automatically generate access permission contracts, following the Design by Contract principle. The inferred specifications are then used to identify the concurrent (immutable) methods in the program. Experimental results further show that Sip4J is useful and effective towards generating access permission contracts for sequential Java programs. The implementation of Sip4J has been published as an open-sourced project at https://github.com/Sip4J/Sip4J and a demo video of Sip4J can be found at https://youtu.be/RjMTIxlhHTg."}, {"id": "conf/kbse/ArthoPT19", "title": "Visual Analytics for Concurrent Java Executions.", "authors": ["Cyrille Artho", "Monali Pande", "Qiyi Tang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00112"], "tag": ["Tool Demonstrations"], "abstract": "Analyzing executions of concurrent software is very difficult. Even if a trace is available, such traces are very hard to read and interpret. A textual trace contains a lot of data, most of which is not relevant to the issue at hand. Past visualization attempts either do not show concurrent behavior, or result in a view that is overwhelming for the user. We provide a visual analytics tool, VA4JVM, for error traces produced by either the Java Virtual Machine, or by Java Pathfinder. Its key features are a layout that spatially associates events with threads, a zoom function, and the ability to filter event data in various ways. We show in examples how filtering and zooming in can highlight a problem without having to read lengthy textual data."}, {"id": "conf/kbse/ZhangYFSL019", "title": "NeuralVis: Visualizing and Interpreting Deep Learning Models.", "authors": ["Xufan Zhang", "Ziyue Yin", "Yang Feng", "Qingkai Shi", "Jia Liu", "Zhenyu Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00113"], "tag": ["Tool Demonstrations"], "abstract": "Deep Neural Network (DNN) techniques have been prevalent in software engineering. They are employed to facilitate various software engineering tasks and embedded into many software applications. However, because DNNs are built upon a rich data-driven programming paradigm that employs plenty of labeled data to train a set of neurons to construct the internal system logic, analyzing and understanding their behaviors becomes a difficult task for software engineers. In this paper, we present an instance-based visualization tool for DNN, namely NeuralVis, to support software engineers in visualizing and interpreting deep learning models. NeuralVis is designed for: 1). visualizing the structure of DNN models, i.e., neurons, layers, as well as connections; 2). visualizing the data transformation process; 3). integrating existing adversarial attack algorithms for test input generation; 4). comparing intermediate layers' outputs of different inputs. To demonstrate the effectiveness of NeuralVis, we design a task-based user study involving ten participants on two classic DNN models, i.e., LeNet and VGG-12. The result shows NeuralVis can assist engineers in identifying critical features that determine the prediction results. Video: https://youtu.be/solkJri4Z44."}, {"id": "conf/kbse/TankovGB19", "title": "Kotless: A Serverless Framework for Kotlin.", "authors": ["Vladislav Tankov", "Yaroslav Golubev", "Timofey Bryksin"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00114"], "tag": ["Tool Demonstrations"], "abstract": "Recent trends in Web development demonstrate an increased interest in serverless applications, i.e. applications that utilize computational resources provided by cloud services on demand instead of requiring traditional server management. This approach enables better resource management while being scalable, reliable, and cost-effective. However, it comes with a number of organizational and technical difficulties which stem from the interaction between the application and the cloud infrastructure, for example, having to set up a recurring task of reuploading updated files. In this paper, we present Kotless - a Kotlin Serverless Framework. Kotless is a cloud-agnostic toolkit that solves these problems by interweaving the deployed application into the cloud infrastructure and automatically generating the necessary deployment code. This relieves developers from having to spend their time integrating and managing their applications instead of developing them. Kotless has proven its capabilities and has been used to develop several serverless applications already in production. Its source code is available at https://github.com/JetBrains/kotless, a tool demo can be found at https://www.youtube.com/watch?v=IMSakPNl3TY."}, {"id": "conf/kbse/LiuFXLGGY19", "title": "FogWorkflowSim: An Automated Simulation Toolkit for Workflow Performance Evaluation in Fog Computing.", "authors": ["Xiao Liu", "Lingmin Fan", "Jia Xu", "Xuejun Li", "Lina Gong", "John C. Grundy", "Yun Yang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00115"], "tag": ["Tool Demonstrations"], "abstract": "Workflow underlies most process automation software, such as those for product lines, business processes, and scientific computing. However, current Cloud Computing based workflow systems cannot support real-time applications due to network latency, which limits their application in many IoT systems such as smart healthcare and smart traffic. Fog Computing extends the Cloud by providing virtualized computing resources close to the End Devices so that the response time of accessing computing resources can be reduced significantly. However, how to most effectively manage heterogeneous resources and different computing tasks in the Fog is a big challenge. In this paper, we introduce \"FogWorkflowSim\" an efficient and extensible toolkit for automatically evaluating resource and task management strategies in Fog Computing with simulated user-defined workflow applications. Specifically, FogWorkflowSim is able to: 1) automatically set up a simulated Fog Computing environment for workflow applications; 2) automatically execute user submitted workflow applications; 3) automatically evaluate and compare the performance of different computation offloading and task scheduling strategies with three basic performance metrics, including time, energy and cost. FogWorkflowSim can serve as an effective experimental platform for researchers in Fog based workflow systems as well as practitioners interested in adopting Fog Computing and workflow systems for their new software projects. (Demo video: https://youtu.be/AsMovcuSkx8)."}, {"id": "conf/kbse/GhanbariZ19", "title": "PraPR: Practical Program Repair via Bytecode Mutation.", "authors": ["Ali Ghanbari", "Lingming Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00116"], "tag": ["Tool Demonstrations"], "abstract": "Automated program repair (APR) is one of the recent advances in automated software engineering aiming for reducing the burden of debugging by suggesting high-quality patches that either directly fix the bugs, or help the programmers in the course of manual debugging. We believe scalability, applicability, and accurate patch validation are the main design objectives for a practical APR technique. In this paper, we present PraPR, our implementation of a practical APR technique that operates at the level of JVM bytecode. We discuss design decisions made in the development of PraPR, and argue that the technique is a viable baseline toward attaining aforementioned objectives. Our experimental results show that: (1) PraPR can fix more bugs than state-of-the-art APR techniques and can be over 10X faster, (2) state-of-the-art APR techniques suffer from dataset overfitting, while the simplistic template-based PraPR performs more consistently on different datasets, and (3) PraPR can fix bugs for other JVM languages, such as Kotlin. PraPR is publicly available at https://github.com/prapr/prapr."}, {"id": "conf/kbse/MaYLYZ19", "title": "SPrinter: A Static Checker for Finding Smart Pointer Errors in C++ Programs.", "authors": ["Xutong Ma", "Jiwei Yan", "Yaqi Li", "Jun Yan", "Jian Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00117"], "tag": ["Tool Demonstrations"], "abstract": "Smart pointers are widely used to prevent memory errors in modern C++ code. However, improper usage of smart pointers may also lead to common memory errors, which makes the code not as safe as expected. To avoid smart pointer errors as early as possible, we present a coding style checker to detect possible bad smart pointer usages during compile time, and notify programmers about bug-prone behaviors. The evaluation indicates that the currently available state-of-the-art static code checkers can only detect 25 out of 116 manually inserted errors, while our tool can detect all these errors. And we also found 521 bugs among 8 open source projects with only 4 false positives."}, {"id": "conf/kbse/Laguna19", "title": "FPChecker: Detecting Floating-Point Exceptions in GPU Applications.", "authors": ["Ignacio Laguna"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00118"], "tag": ["Tool Demonstrations"], "abstract": "Floating-point arithmetic is widely used in applications from several fields including scientific computing, machine learning, graphics, and finance. Many of these applications are rapidly adopting the use of GPUs to speedup computations. GPUs, however, have limited support to detect floating-point exceptions, which hinders the development of reliable applications in GPU-based systems. We present FPCHECKER, the first tool to automatically detect floating-point exceptions in GPU applications. FPCHECKER uses the clang/LLVM compiler to instrument GPU kernels and to detect exceptions at runtime. Once an exception is detected, it reports to the programmer the code location of the exception as well as other useful information. The programmer can then use this report to avoid the exception, e.g., by modifying the application algorithm or changing the input. We present the design of FPCHECKER, an evaluation of the overhead of the tool, and a real-world case scenario on which the tool is used to identify a hidden exception. The slowdown of FPCHECKER is moderate and the code is publicly available as open source."}, {"id": "conf/kbse/CastroPA19", "title": "Pangolin: An SFL-Based Toolset for Feature Localization.", "authors": ["Bruno Castro", "Alexandre Perez", "Rui Abreu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00119"], "tag": ["Tool Demonstrations"], "abstract": "Pinpointing the location where a given unit of functionality-or feature-was implemented is a demanding and time-consuming task, yet prevalent in most software maintenance or evolution efforts. To that extent, we present PANGOLIN, an Eclipse plugin that helps developers identifying features among the source code. It borrows Spectrum-based Fault Localization techniques from the software diagnosis research field by framing feature localization as a diagnostic problem. PANGOLIN prompts users to label system executions based on feature involvement, and subsequently presents its spectrum-based feature localization analysis to users with the aid of a color-coded, hierarchic, and navigable visualization which was shown to be effective at conveying diagnostic information to users. Our evaluation shows that PANGOLIN accurately pinpoints feature implementations and is resilient to misclassifications by users. The tool can be downloaded at https://tqrg.github.io/pangolin/."}, {"id": "conf/kbse/ReulingKRL19", "title": "SiMPOSE - Configurable N-Way Program Merging Strategies for Superimposition-Based Analysis of Variant-Rich Software.", "authors": ["Dennis Reuling", "Udo Kelter", "Sebastian Ruland", "Malte Lochau"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00120"], "tag": ["Tool Demonstrations"], "abstract": "Modern software often exists in many different, yet similar versions and/or variants, usually derived from a common code base (e.g., via clone-and-own). In the context of product-line engineering, family-based analysis has shown very promising potential for improving efficiency in applying quality-assurance techniques to variant-rich software, as compared to a variant-by-variant approach. Unfortunately, these strategies rely on a product-line representation superimposing all program variants in a syntactically well-formed, semantically sound and variant-preserving manner, which is manually hard to obtain in practice. We demonstrate the SiMPOSE methodology for automatically generating superimpositions of N given program versions and/or variants facilitating family-based analysis of variant-rich software. SiMPOSE is based on a novel N-way model-merging technique operating at the level of control-flow automata (CFA) representations of C programs. CFAs constitute a unified program abstraction utilized by many recent software-analysis tools. We illustrate different merging strategies supported by SiMPOSE, namely variant-by-variant, N-way merging, incremental 2-way merging, and partition-based N/2-way merging, and demonstrate how SiMPOSE can be used to systematically compare their impact on efficiency and effectiveness of family-based unit-test generation. The SiMPOSE tool, the demonstration of its usage as well as related artifacts and documentation can be found at http://pi.informatik.uni-siegen.de/projects/variance/simpose."}, {"id": "conf/kbse/AfzalACCDDKV19", "title": "VeriAbs : Verification by Abstraction and Test Generation.", "authors": ["Mohammad Afzal", "A. Asia", "Avriti Chauhan", "Bharti Chimdyalwar", "Priyanka Darke", "Advaita Datar", "Shrawan Kumar", "R. Venkatesh"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00121"], "tag": ["Tool Demonstrations"], "abstract": "Verification of programs continues to be a challenge and no single known technique succeeds on all programs. In this paper we present VeriAbs, a reachability verifier for C programs that incorporates a portfolio of techniques implemented as four strategies, where each strategy is a set of techniques applied in a specific sequence. It selects a strategy based on the kind of loops in the program. We analysed the effectiveness of the implemented strategies on the 3831 verification tasks from the ReachSafety category of the 8th International Competition on Software Verification (SV-COMP) 2019 and found that although classic techniques - explicit state model checking and bounded model checking, succeed on a majority of the programs, a wide range of further techniques are required to analyse the rest. A screencast of the tool is available at https://youtu.be/Hzh3PPiODwk."}, {"id": "conf/kbse/LiW0ZCM19", "title": "SGUARD: A Feature-Based Clustering Tool for Effective Spreadsheet Defect Detection.", "authors": ["Da Li", "Huiyan Wang", "Chang Xu", "Ruiqing Zhang", "Shing-Chi Cheung", "Xiaoxing Ma"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00122"], "tag": ["Tool Demonstrations"], "abstract": "Spreadsheets are widely used but subject to various defects. In this paper, we present SGuard to effectively detect spreadsheet defects. SGuard learns spreadsheet features to cluster cells with similar computational semantics, and then refines these clusters to recognize anomalous cells as defects. SGuard well balances the trade-off between the precision (87.8%) and recall rate (71.9%) in the defect detection, and achieves an F-measure of 0.79, exceeding existing spreadsheet defect detection techniques. We introduce the SGuard implementation and its usage by a video presentation (https://youtu.be/gNPmMvQVf5Q), and provide its public download repository (https://github.com/sheetguard/sguard)."}, {"id": "conf/kbse/ReicheltKH19", "title": "PeASS: A Tool for Identifying Performance Changes at Code Level.", "authors": ["David Georg Reichelt", "Stefan K\u00fchne", "Wilhelm Hasselbring"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00123"], "tag": ["Tool Demonstrations"], "abstract": "We present PeASS (Performance Analysis of Software System versions), a tool for detecting performance changes at source code level that occur between different code versions. By using PeASS, it is possible to identify performance regressions that happened in the past to fix them. PeASS measures the performance of unit tests in different source code versions. To achieve statistic rigor, measurements are repeated and analyzed using an agnostic t-test. To execute a minimal amount of tests, PeASS uses a regression test selection. We evaluate PeASS on a selection of Apache Commons projects and show that 81% of all unit test covered performance changes can be found by PeASS. A video presentation is available at https://www.youtube.com/watch?v=RORFEGSCh6Y and PeASS can be downloaded from https://github.com/DaGeRe/peass."}, {"id": "conf/kbse/0002TP19", "title": "VeriSmart 2.0: Swarm-Based Bug-Finding for Multi-threaded Programs with Lazy-CSeq.", "authors": ["Bernd Fischer", "Salvatore La Torre", "Gennaro Parlato"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00124"], "tag": ["Tool Demonstrations"], "abstract": "Swarm-based verification methods split a verification problem into a large number of independent simpler tasks and so exploit the availability of large numbers of cores to speed up verification. Lazy-CSeq is a BMC-based bug-finding tool for C programs using POSIX threads that is based on sequentialization. Here we present the tool VeriSmart 2.0, which extends Lazy-CSeq with a swarm-based bug-finding method. The key idea of this approach is to constrain the interleaving such that context switches can only happen within selected tiles (more specifically, contiguous code segments within the individual threads). This under-approximates the program's behaviours, with the number and size of tiles as additional parameters, which allows us to vary the complexity of the tasks. Overall, this significantly improves peak memory consumption and (wall-clock) analysis time."}, {"id": "conf/kbse/MengZYL0Y19", "title": "CONVUL: An Effective Tool for Detecting Concurrency Vulnerabilities.", "authors": ["Ruijie Meng", "Biyun Zhu", "Hao Yun", "Haicheng Li", "Yan Cai", "Zijiang Yang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00125"], "tag": ["Tool Demonstrations"], "abstract": "Concurrency vulnerabilities are extremely harmful and can be frequently exploited to launch severe attacks. Due to the non-determinism of multithreaded executions, it is very difficult to detect them. Recently, data race detectors and techniques based on maximal casual model have been applied to detect concurrency vulnerabilities. However, the former are ineffective and the latter report many false negatives. In this paper, we present CONVUL, an effective tool for concurrency vulnerability detection. CONVUL is based on exchangeable events, and adopts novel algorithms to detect three major kinds of concurrency vulnerabilities. In our experiments, CONVUL detected 9 of 10 known vulnerabilities, while other tools only detected at most 2 out of these 10 vulnerabilities. The 10 vulnerabilities are available at https://github.com/mryancai/ConVul."}, {"id": "conf/kbse/Hu0XY0Z19", "title": "DeepMutation++: A Mutation Testing Framework for Deep Learning Systems.", "authors": ["Qiang Hu", "Lei Ma", "Xiaofei Xie", "Bing Yu", "Yang Liu", "Jianjun Zhao"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00126"], "tag": ["Tool Demonstrations"], "abstract": "Deep neural networks (DNNs) are increasingly expanding their real-world applications across domains, e.g., image processing, speech recognition and natural language processing. However, there is still limited tool support for DNN testing in terms of test data quality and model robustness. In this paper, we introduce a mutation testing-based tool for DNNs, DeepMutation++, which facilitates the DNN quality evaluation, supporting both feed-forward neural networks (FNNs) and stateful recurrent neural networks (RNNs). It not only enables to statically analyze the robustness of a DNN model against the input as a whole, but also allows to identify the vulnerable segments of a sequential input (e.g. audio input) by runtime analysis. It is worth noting that DeepMutation++ specially features the support of RNNs mutation testing. The tool demo video can be found on the project website https://sites.google.com/view/deepmutationpp."}, {"id": "conf/kbse/XieCLM0Z19", "title": "Coverage-Guided Fuzzing for Feedforward Neural Networks.", "authors": ["Xiaofei Xie", "Hongxu Chen", "Yi Li", "Lei Ma", "Yang Liu", "Jianjun Zhao"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00127"], "tag": ["Tool Demonstrations"], "abstract": "Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc."}, {"id": "conf/kbse/HuangFZZWJMP19", "title": "Prema: A Tool for Precise Requirements Editing, Modeling and Analysis.", "authors": ["Yihao Huang", "Jincao Feng", "Hanyue Zheng", "Jiayi Zhu", "Shang Wang", "Siyuan Jiang", "Weikai Miao", "Geguang Pu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00128"], "tag": ["Tool Demonstrations"], "abstract": "We present Prema, a tool for Precise Requirement Editing, Modeling and Analysis. It can be used in various fields for describing precise requirements using formal notations and performing rigorous analysis. By parsing the requirements written in formal modeling language, Prema is able to get a model which aptly depicts the requirements. It also provides different rigorous verification and validation techniques to check whether the requirements meet users' expectation and find potential errors. We show that our tool can provide a unified environment for writing and verifying requirements without using tools that are not well inter-related. For experimental demonstration, we use the requirements of the automatic train protection (ATP) system of CASCO signal co. LTD., the largest railway signal control system manufacturer of China. The code of the tool cannot be released here because the project is commercially confidential. However, a demonstration video of the tool is available at https://youtu.be/BX0yv8pRMWs."}, {"id": "conf/kbse/WangC00S19", "title": "TsmartGP: A Tool for Finding Memory Defects with Pointer Analysis.", "authors": ["Yuexing Wang", "Guang Chen", "Min Zhou", "Ming Gu", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00129"], "tag": ["Tool Demonstrations"], "abstract": "Precise pointer analysis is desired since it is a core technique to find memory defects. There are several dimensions of pointer analysis precision, flow sensitivity, context sensitivity, field sensitivity and path sensitivity. For static analysis tools utilizing pointer analysis, considering all dimensions is difficult because the trade-off between precision and efficiency should be balanced. This paper presents TsmartGP, a static analysis tool for finding memory defects in C programs with a precise and efficient pointer analysis. The pointer analysis algorithm is flow, context, field, and quasi path sensitive. Control flow automatons are the key structures for our analysis to be flow sensitive. Function summaries are applied to get context information and elements of aggregate structures are handled to improve precision. Path conditions are used to filter unreachable paths. For efficiency, a multi-entry mechanism is proposed. Utilizing the pointer analysis algorithm, we implement a checker in TsmartGP to find uninitialized pointer errors in 13 real-world applications. Cppcheck and Clang Static Analyzer are chosen for comparison. The experimental results show that TsmartGP can find more errors while its accuracy is also higher than Cppcheck and Clang Static Analyzer. The demo video is available at https://youtu.be/IQlshemk6OA."}, {"id": "conf/kbse/Li0G0019", "title": "Ares: Inferring Error Specifications through Static Analysis.", "authors": ["Chi Li", "Min Zhou", "Zuxing Gu", "Ming Gu", "Hongyu Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00130"], "tag": ["Tool Demonstrations"], "abstract": "Misuse of APIs happens frequently due to misunderstanding of API semantics and lack of documentation. An important category of API-related defects is the error handling defects, which may result in security and reliability flaws. These defects can be detected with the help of static program analysis, provided that error specifications are known. The error specification of an API function indicates how the function can fail. Writing error specifications manually is time-consuming and tedious. Therefore, automatic inferring the error specification from API usage code is preferred. In this paper, we present Ares, a tool for automatic inferring error specifications for C code through static analysis. We employ multiple heuristics to identify error handling blocks and infer error specifications by analyzing the corresponding condition logic. Ares is evaluated on 19 real world projects, and the results reveal that Ares outperforms the state-of-the-art tool APEx by 37% in precision. Ares can also identify more error specifications than APEx. Moreover, the specifications inferred from Ares help find dozens of API-related bugs in well-known projects such as OpenSSL, among them 10 bugs are confirmed by developers. Video: https://youtu.be/nf1QnFAmu8Q. Repository: https://github.com/lc3412/Ares."}, {"id": "conf/kbse/BagherzadehJKD19", "title": "PMExec: An Execution Engine of Partial UML-RT Models.", "authors": ["Mojtaba Bagherzadeh", "Karim Jahed", "Nafiseh Kahani", "Juergen Dingel"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00131"], "tag": ["Tool Demonstrations"], "abstract": "This paper presents PMExec, a tool that supports the execution of partial UML-RT models. To this end, the tool implements the following steps: static analysis, automatic refinement, and input-driven execution. The static analysis that respects the execution semantics of UML-RT models is used to detect problematic model elements, i.e., elements that cause problems during execution due to the partiality. Then, the models are refined automatically using model transformation techniques, which mostly add decision points where missing information can be supplied. Third, the refined models are executed, and when the execution reaches the decision points, input required to continue the execution is obtained either interactively or from a script that captures how to deal with partial elements. We have evaluated PMExec using several use-cases that show that the static analysis, refinement, and application of user input can be carried out with reasonable performance, and that the overhead of approach is manageable. https://youtu.be/BRKsselcMnc Note: Interested readers can refer to [1] for a thorough discussion and evaluation of this work."}, {"id": "conf/kbse/AhmadiJD19", "title": "mCUTE: A Model-Level Concolic Unit Testing Engine for UML State Machines.", "authors": ["Reza Ahmadi", "Karim Jahed", "Juergen Dingel"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00132"], "tag": ["Tool Demonstrations"], "abstract": "Model Driven Engineering (MDE) techniques raise the level of abstraction at which developers construct software. However, modern cyber-physical systems are becoming more prevalent and complex and hence software models that represent the structure and behavior of such systems still tend to be large and complex. These models may have numerous if not infinite possible behaviors, with complex communications between their components. Appropriate software testing techniques to generate test cases with high coverage rate to put these systems to test at the model-level (without the need to understand the underlying code generator or refer to the generated code) are therefore important. Concolic testing, a hybrid testing technique that benefits from both concrete and symbolic execution, gains a high execution coverage and is used extensively in the industry for program testing but not for software models. In this paper, we present a novel technique and its tool mCUTE1, an open source 2 model-level concolic testing engine. We describe the implementation of our tool in the context of Papyrus-RT, an open source Model Driven Engineering (MDE) tool based on UML-RT, and report the results of validating our tool using a set of benchmark models."}, {"id": "conf/kbse/MossbergMHGGFBD19", "title": "Manticore: A User-Friendly Symbolic Execution Framework for Binaries and Smart Contracts.", "authors": ["Mark Mossberg", "Felipe Manzano", "Eric Hennenfent", "Alex Groce", "Gustavo Grieco", "Josselin Feist", "Trent Brunson", "Artem Dinaburg"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00133"], "tag": ["Tool Demonstrations"], "abstract": "An effective way to maximize code coverage in software tests is through dynamic symbolic execution-a technique that uses constraint solving to systematically explore a program's state space. We introduce an open-source dynamic symbolic execution framework called Manticore for analyzing binaries and Ethereum smart contracts. Manticore's flexible architecture allows it to support both traditional and exotic execution environments, and its API allows users to customize their analysis. Here, we discuss Manticore's architecture and demonstrate the capabilities we have used to find bugs and verify the correctness of code for our commercial clients."}, {"id": "conf/kbse/ChittimalliAPMP19", "title": "BuRRiTo: A Framework to Extract, Specify, Verify and Analyze Business Rules.", "authors": ["Pavan Kumar Chittimalli", "Kritika Anand", "Shrishti Pradhan", "Sayandeep Mitra", "Chandan Prakash", "Rohit Shere", "Ravindra Naik"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00134"], "tag": ["Tool Demonstrations"], "abstract": "An enterprise system operates business by providing various services that are guided by set of certain business rules (BR) and constraints. These BR are usually written using plain Natural Language in operating procedures, terms and conditions, and other documents or in source code of legacy enterprise systems. For implementing the BR in a software system, expressing them as UML use-case specifications, or preparing for Merger & Acquisition (M&A) activity, analysts manually interpret the documents or try to identify constraints from the source code, leading to potential discrepancies and ambiguities. These issues in the software system can be resolved only after testing, which is a very tedious and expensive activity. To minimize such errors and efforts, we propose BuRRiTo framework consisting of automatic extraction of BR by mining documents and source code, ability to clean them of various anomalies like inconsistency, redundancies, conflicts, etc. and able to analyze the functional gaps present and performing semantic querying and searching."}, {"id": "conf/kbse/MehraSKP19", "title": "XRaSE: Towards Virtually Tangible Software using Augmented Reality.", "authors": ["Rohit Mehra", "Vibhu Saujanya Sharma", "Vikrant Kaulgud", "Sanjay Podder"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00135"], "tag": ["Tool Demonstrations"], "abstract": "Software engineering has seen much progress in recent past including introduction of new methodologies, new paradigms for software teams, and from smaller monolithic applications to complex, intricate, and distributed software applications. However, the way we represent, discuss, and collaborate on software applications throughout the software development life cycle is still primarily using the source code, textual representations, or charts on 2D computer screens - the confines of which have long limited how we visualize and comprehend software systems. In this paper, we present XRaSE, a novel prototype implementation that leverages augmented reality to visualize a software application as a virtually tangible entity. This immersive approach is aimed at making activities like application comprehension, architecture analysis, knowledge communication, and analysis of a software's dynamic aspects, more intuitive, richer and collaborative."}, {"id": "conf/kbse/LiWXWZ019", "title": "MuSC: A Tool for Mutation Testing of Ethereum Smart Contract.", "authors": ["Zixin Li", "Haoran Wu", "Jiehui Xu", "Xingya Wang", "Lingming Zhang", "Zhenyu Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00136"], "tag": ["Tool Demonstrations"], "abstract": "The smart contract cannot be modified when it has been deployed on a blockchain. Therefore, it must be given thorough test before its being deployed. Mutation testing is considered as a practical test methodology to evaluate the adequacy of software testing. In this paper, we introduce MuSC, a mutation testing tool for Ethereum Smart Contract (ESC). It can generate numerous mutants at a fast speed and supports the automatic operations such as creating test nets, deploying and executing tests. Specially, MuSC implements a set of novel mutation operators w.r.t ESC programming language, Solidity. Therefore, it can expose the defects of smart contracts to a certain degree. The demonstration video of MuSC is available at https: //youtu.be/3KBKXJPVjbQ, and the source code can be downloaded at https://github.com/belikout/MuSC-Tool-Demo-repo."}, {"id": "conf/kbse/ZhouSZ19", "title": "Lancer: Your Code Tell Me What You Need.", "authors": ["Shufan Zhou", "Beijun Shen", "Hao Zhong"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00137"], "tag": ["Tool Demonstrations"], "abstract": "Programming is typically a difficult and repetitive task. Programmers encounter endless problems during programming, and they often need to write similar code over and over again. To prevent programmers from reinventing wheels thus increase their productivity, we propose a context-aware code-to-code recommendation tool named Lancer. With the support of a Library-Sensitive Language Model (LSLM) and the BERT model, Lancer is able to automatically analyze the intention of the incomplete code and recommend relevant and reusable code samples in real-time. A video demonstration of Lancer can be found at https://youtu.be/tO9nhqZY35g. Lancer is open source and the code is available at https://github.com/sfzhou5678/Lancer."}, {"id": "conf/kbse/TokumotoT19", "title": "PHANTA: Diversified Test Code Quality Measurement for Modern Software Development.", "authors": ["Susumu Tokumoto", "Kuniharu Takayama"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00138"], "tag": ["Industry Showcase"], "abstract": "Test code is becoming more essential to the modern software development process. However, practitioners often pay inadequate attention to key aspects of test code quality, such as bug detectability, maintainability and speed. Existing tools also typically report a single test code quality measure, such as code coverage, rather than a diversified set of metrics. To measure and visualize quality of test code in a comprehensive fashion, we developed an integrated test code analysis tool called Phanta. In this show case, we posit that the enhancement of test code quality is key to modernizing software development, and show how Phanta's techniques measure the quality using mutation analysis, test code clone detection, and so on. Further, we present an industrial case study where Phanta was applied to analyze test code in a real Fujitsu project, and share lessons learned from the case study."}, {"id": "conf/kbse/SungKKJK19", "title": "Test Automation and Its Limitations: A Case Study.", "authors": ["Ahyoung Sung", "Sangjun Kim", "Yangsu Kim", "Younggun Jang", "Jongin Kim"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00139"], "tag": ["Industry Showcase"], "abstract": "Modern embedded systems are increasingly complex and contain multiple software layers from BSP (Board Support Packages) to OS to middleware to AI (Artificial Intelligence) algorithms like perception and voice recognition. Integrations of inter-layer and intra-layer in embedded systems provide dedicated services such as taking a picture or movie-streaming. Accordingly, it gets more complicated to find out the root cause of a system failure. This industrial proposal describes a difficulty of testing embedded systems, and presents a case study in terms of integration testing."}, {"id": "conf/kbse/WenCC19", "title": "PTracer: A Linux Kernel Patch Trace Bot.", "authors": ["Yang Wen", "Jicheng Cao", "Shengyu Cheng"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00140"], "tag": ["Industry Showcase"], "abstract": "We present PTracer, a Linux kernel patch trace bot based on an improved PatchNet. PTracer continuously monitors new patches in the git repository of the mainline Linux kernel, filters out unconcerned ones, classifies the rest as bug-fixing or non bug-fixing patches, and reports bug-fixing patches to the kernel experts of commercial operating systems. We use the patches in February 2019 of the mainline Linux kernel to perform the test. As a result, PTracer recommended 151 patches to CGEL kernel experts out of 5,142, and 102 of which were accepted. PTracer has been successfully applied to a commercial operating system and has the advantages of improving software quality and saving labor cost."}, {"id": "conf/kbse/SingiBPB19", "title": "Trusted Software Supply Chain.", "authors": ["Kapil Singi", "R. P. Jagadeesh Chandra Bose", "Sanjay Podder", "Adam P. Burden"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00141"], "tag": ["Industry Showcase"], "abstract": "Modern software delivery happens in a geographically distributed environment and resembles like a supply chain - consists of various participants, involves various phases, needs adherence to multiple regulations and needs to maintain artifacts' integrity throughout the delivery phases. This shift in software development brings along with it several challenges ranging from communication of information/knowledge, coordination and control of teams, activities adhering to goals and policies and artifacts adhering to quality, visibility, and management. With the dispersion of centralized control over software delivery to autonomous delivery organizations, the variety of processes and tools used turns transparency into opacity as autonomous teams use different software processes, tools, and metrics, leading to issues like ineffective compliance monitoring, friction prone coordination, and lack of provenance, and thereby trust. In this paper, we present a delivery governance framework based on distributed ledger technology that uses a notion of `software telemetry' to record data from disparate delivery partners and enables compliance monitoring and adherence, provenance and traceability, transparency, and thereby trust."}, {"id": "conf/kbse/SharmaMPB19", "title": "A Journey Towards Providing Intelligence and Actionable Insights to Development Teams in Software Delivery.", "authors": ["Vibhu Saujanya Sharma", "Rohit Mehra", "Sanjay Podder", "Adam P. Burden"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00142"], "tag": ["Industry Showcase"], "abstract": "For delivering high-quality artifacts within the budget and on schedule, software delivery teams ideally should have a holistic and in-process view of the current health and future trajectory of the project. However, such insights need to be at the right level of granularity and need to be derived typically from a heterogeneous project environment, in a way that helps development team members with their tasks at hand. Due to client mandates, software delivery project environments employ many disparate tools and teams tend to be distributed, thus making the relevant information retrieval, insight generation, and developer intelligence augmentation process fairly complex. In this paper, we discuss our journey in this area spanning across facets like software project modelling and new development metrics, studying developer priorities, adoption of new metrics, and different approaches of developer intelligence augmentation. Finally, we present our exploration of new immersive technologies for human-centered software engineering."}, {"id": "conf/kbse/Wu0C19", "title": "Better Development of Safety Critical Systems: Chinese High Speed Railway System Development Experience Report.", "authors": ["ZhiWei Wu", "Jing Liu", "Xiang Chen"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00143"], "tag": ["Industry Showcase"], "abstract": "Ensure the correctness of safety critical systems play a key role in the worldwide software engineering. Over the past years we have been helping CASCO Signal Ltd which is the Chinese biggest high speed railway company to develop high speed railway safety critical software. We have also contributed specific methods for developing better safety critical software, including a search-based model-driven software development approach which uses SysML diagram refinement method to construct SysML model and SAT solver to check the model. This talk aims at sharing the challenge of developing high speed railway safety critical system, what we learn from develop a safety critical software with a Chinese high speed railway company, and we use ZC subsystem as a case study to show the systematic model-driven safety critical software development method."}, {"id": "conf/kbse/Zhou19", "title": "Improving Collaboration Efficiency in Fork-Based Development.", "authors": ["Shurui Zhou"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00144"], "tag": ["Doctoral Symposium"], "abstract": "Fork-based development is a lightweight mechanism that allows developers to collaborate with or without explicit coordination. Although it is easy to use and popular, when developers each create their own fork and develop independently, their contributions are usually not easily visible to others. When the number of forks grows, it becomes very difficult to maintain an overview of what happens in individual forks, which would lead to additional problems and inefficient practices: lost contributions, redundant development, fragmented communities, and so on. Facing the problems mentioned above, we developed two complementary strategies: (1) Identifying existing best practices and suggesting evidence-based interventions for projects that are inefficient; (2) designing new interventions that could improve the awareness of a community using fork-based development, and help developers to detect redundant development to reduce unnecessary effort."}, {"id": "conf/kbse/Reich19", "title": "Inference of Properties from Requirements and Automation of Their Formal Verification.", "authors": ["Marina Reich"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00145"], "tag": ["Doctoral Symposium"], "abstract": "Over the past decades, various techniques for the application of formal program analysis of software for embedded systems have been proposed. However, the application of formal methods for software verification is still limited in practise. It is acknowledged that the task of formally stating requirements by specifying the formal properties is a major hindrance. The verification step itself has its shortcoming in its scalability and its limitation to predefined proof tactics in case of automated theorem proving (ATP). These constraints are reduced today by the interaction of the user with the theorem prover (TP) during the execution of the proof. However, this is difficult for non-experts. The objectives of the presented PhD project are the automated inference of declarative property specifications from example data specified by the engineer for a function under development and their automated verification on abstract model level and on code level. We propose the meta-model for Scenario Modeling Language (SML) that allows to specify example data. For the automated property generation we are motivated by Inductive Logic Programming (ILP) techniques for first-order logic in pure mathematics. We propose modifications to its algorithm that allow to process the information that is incorporated in the meta-model of SML. However, this technique is expected to produce too many uninteresting properties. To turn this weakness into strength, our approach proposes to tailor the algorithm towards selection of the right properties that facilitate the automation of the proof. Automated property generation and less user interaction with the prover will leverage formal verification as it will relieve the engineer in the specification as well as in proofing tasks."}, {"id": "conf/kbse/Lukasczyk19", "title": "Generating Tests to Analyse Dynamically-Typed Programs.", "authors": ["Stephan Lukasczyk"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00146"], "tag": ["Doctoral Symposium"], "abstract": "The increasing popularity of dynamically-typed programming languages, such as JavaScript or Python, requires specific support methods for developers to avoid pitfalls arising from the dynamic nature of these languages. Static analyses are frequently used but the dynamic type systems limit their applicability. Dynamic analyses, in contrast, depend on the execution of the code under analysis, and thus depend on the quality of existing tests. This quality of the test suite can be improved by the use of automated test generation but automated test generation for dynamically-typed programming languages itself is hard due to the lack of type information in the programs. The limitations of each of these approaches will be overcome by iteratively combining test generation with static and dynamic analysis techniques for dynamically-typed programs."}, {"id": "conf/kbse/Soto19", "title": "Improving Patch Quality by Enhancing Key Components of Automatic Program Repair.", "authors": ["Mauricio Soto"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00147"], "tag": ["Doctoral Symposium"], "abstract": "The error repair process in software systems is, historically, a resource-consuming task that relies heavily in developer manual effort. Automatic program repair approaches enable the repair of software with minimum human interaction, therefore, mitigating the burden from developers. However, a problem automatically generated patches commonly suffer is generating low-quality patches (which overfit to one program specification, thus not generalizing to an independent oracle evaluation). This work proposes a set of mechanisms to increase the quality of plausible patches including an analysis of test suite behavior and their key characteristics for automatic program repair, analyzing developer behavior to inform the mutation operator selection distribution, and a study of patch diversity as a means to create consolidated higher quality fixes."}, {"id": "conf/kbse/Kolthoff19", "title": "Automatic Generation of Graphical User Interface Prototypes from Unrestricted Natural Language Requirements.", "authors": ["Kristian Kolthoff"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00148"], "tag": ["Doctoral Symposium"], "abstract": "High-fidelity GUI prototyping provides a meaningful manner for illustrating the developers' understanding of the requirements formulated by the customer and can be used for productive discussions and clarification of requirements and expectations. However, high-fidelity prototypes are time-consuming and expensive to develop. Furthermore, the interpretation of requirements expressed in informal natural language is often error-prone due to ambiguities and misunderstandings. In this dissertation project, we will develop a methodology based on Natural Language Processing (NLP) for supporting GUI prototyping by automatically translating Natural Language Requirements (NLR) into a formal Domain-Specific Language (DSL) describing the GUI and its navigational schema. The generated DSL can be further translated into corresponding target platform prototypes and directly provided to the user for inspection. Most related systems stop after generating artifacts, however, we introduce an intelligent and automatic interaction mechanism that allows users to provide natural language feedback on generated prototypes in an iterative fashion, which accordingly will be translated into respective prototype changes."}, {"id": "conf/kbse/Sharma19", "title": "Automatically Repairing Binary Programs Using Adapter Synthesis.", "authors": ["Vaibhav Sharma"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00149"], "tag": ["Doctoral Symposium"], "abstract": "Bugs in commercial software and third-party components are an undesirable and expensive phenomenon. Such software is usually released to users only in binary form. The lack of source code renders users of such software dependent on their software vendors for repairs of bugs. Such dependence is even more harmful if the bugs introduce new vulnerabilities in the software. Automatically repairing security and functionality bugs in binary code increases software robustness without any developer effort. In this research, we propose development of a binary program repair tool that uses existing bug-free fragments of code to repair buggy code."}, {"id": "conf/kbse/Hassan19", "title": "Tackling Build Failures in Continuous Integration.", "authors": ["Foyzul Hassan"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00150"], "tag": ["Doctoral Symposium"], "abstract": "In popular continuous integration(CI) practice, coding is followed by building, integration and system testing, pre-release inspection, and deploying artifacts. This can reduce integration risk and speed up the development process. But large number of CI build failures may interrupt the normal software development process. So, the failures need to be analyzed and fixed quickly. Although various automated program repair techniques have great potential to resolve software failures, the existing techniques mostly focus on repairing source code. So, those techniques cannot directly help resolve software build failures. Apart from that, a special challenge to fix build failures in CI environment is that the failures are often involved with both source code and build scripts. This paper outlines promising preliminary work towards automatic build repair in CI environment that involves both source code and build script. As the first step, we conducted an empirical study on software build failures and build fix patterns. Based on the findings of the empirical study, we developed an approach that can automatically fix build errors involving build scripts. We plan to extend this repair approach considering both source code and build script. Moreover, we plan to quantify our automatic fixes by user study and comparison between fixes generated by our approach and actual fixes."}, {"id": "conf/kbse/Vassallo19", "title": "Enabling Continuous Improvement of a Continuous Integration Process.", "authors": ["Carmine Vassallo"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00151"], "tag": ["Doctoral Symposium"], "abstract": "Continuous Integration (CI) is a widely-adopted software engineering practice. Despite its undisputed benefits, like higher software quality and improved developer productivity, mastering CI is not easy. Among the several barriers when transitioning to CI, developers need to face a new type of software failures (i.e., build failures) that requires them to understand complex build logs. Even when a team has successfully introduced a CI culture, living up to its principles and improving the CI practice are also challenging. In my research, I want to provide developers with the right support for establishing CI and the proper recommendations for continuously improving their CI process."}, {"id": "conf/kbse/Wei19", "title": "Retrieve and Refine: Exemplar-Based Neural Comment Generation.", "authors": ["Bolin Wei"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00152"], "tag": ["ACM Student Research Competition"], "abstract": "Code comment generation is a crucial task in the field of automatic software development. Most previous neural comment generation systems used an encoder-decoder neural network and encoded only information from source code as input. Software reuse is common in software development. However, this feature has not been introduced to existing systems. Inspired by the traditional IR-based approaches, we propose to use the existing comments of similar source code as exemplars to guide the comment generation process. Based on an open source search engine, we first retrieve a similar code and treat its comment as an exemplar. Then we applied a seq2seq neural network to conduct an exemplar-based comment generation. We evaluate our approach on a large-scale Java corpus, and experimental results demonstrate that our model significantly outperforms the state-of-the-art methods."}, {"id": "conf/kbse/Nam19", "title": "API Design Implications of Boilerplate Client Code.", "authors": ["Daye Nam"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00153"], "tag": ["ACM Student Research Competition"], "abstract": "Designing usable APIs is critical to developers' productivity and software quality but is quite difficult. In this paper, I focus on \"boilerplate\" code, sections of code that have to be included in many places with little or no alteration, which many experts in API design have said can be an indicator of API usability problems. I investigate what properties make code count as boilerplate, and present a novel approach to automatically mine boilerplate code from a large set of client code. The technique combines an existing API usage mining algorithm, with novel filters using AST comparison and graph partitioning. With boilerplate candidates identified by the technique, I discuss how this technique could help API designers in reviewing their design decisions and identifying usability issues."}, {"id": "conf/kbse/Kellogg19", "title": "Compile-Time Detection of Machine Image Sniping.", "authors": ["Martin Kellogg"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00154"], "tag": ["ACM Student Research Competition"], "abstract": "Machine image sniping is a difficult-to-detect security vulnerability in cloud computing code. When programmatically initializing a machine, a developer specifies a machine image (operating system and file system). The developer should restrict the search to only those machine images which their organization controls: otherwise, an attacker can insert a similarly-named malicious image into the public database, where it might be selected instead of the image the developer intended. We present a lightweight type and effect system that detects requests to a cloud provider that are vulnerable to an image sniping attack, or proves that no vulnerable request exists in a codebase. We prototyped our type system for Java programs that initialize Amazon Web Services machines, and evaluated it on more than 500 codebases, detecting 14 vulnerable requests with only 3 false positives."}, {"id": "conf/kbse/Xiao19", "title": "An Image-Inspired and CNN-Based Android Malware Detection Approach.", "authors": ["Xusheng Xiao"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00155"], "tag": ["ACM Student Research Competition"], "abstract": "Until 2017, Android smartphones occupied approximately 87% of the smartphone market. The vast market also promotes the development of Android malware. Nowadays, the number of malware targeting Android devices found daily is more than 38,000. With the rapid progress of mobile application programming and anti-reverse-engineering techniques, it is harder to detect all kinds of malware. To address challenges in existing detection techniques, such as data obfuscation and limited code coverage, we propose a detection approach that directly learns features of malware from Dalvik bytecode based on deep learning technique (CNN). The average detection time of our model is0.22 seconds, which is much lower than other existing detection approaches. In the meantime, the overall accuracy of our model achieves over 93%."}, {"id": "conf/kbse/Ghanbari19", "title": "Toward Practical Automatic Program Repair.", "authors": ["Ali Ghanbari"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00156"], "tag": ["ACM Student Research Competition"], "abstract": "Automated program repair (APR) reduces the burden of debugging by directly suggesting likely fixes for the bugs. We believe scalability, applicability, and accurate patch validation are among the main challenges for building practical APR techniques that the researchers in this area are dealing with. In this paper, we describe the steps that we are taking toward addressing these challenges."}, {"id": "conf/kbse/Ramamoorthy19", "title": "User Preference Aware Multimedia Pricing Model using Game Theory and Prospect Theory for Wireless Communications.", "authors": ["Krishna Murthy Kattiyan Ramamoorthy"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00157"], "tag": ["ACM Student Research Competition"], "abstract": "Providing user satisfaction is a major concern for on-demand multimedia service providers and Internet carriers in Wireless Communications. Traditionally, user satisfaction was measured objectively in terms of throughput and latency. Nowadays the user satisfaction is measured using subjective metrices such as Quality of Experience (QoE). Recently, Smart Media Pricing (SMP) was conceptualized to price the QoE rather than the binary data traffic in multimedia services. In this research, we have leveraged the SMP concept to chalk up a QoE-sensitive multimedia pricing framework to allot price, based on the user preference and multimedia quality achieved by the customer. We begin by defining the utility equations for the provider-carrier and the customer. Then we translate the profit maximizing interplay between the parties into a two-stage Stackelberg game. We model the user personal preference using Prelec weighting function which follows the postulates Prospect Theory (PT). An algorithm has been developed to implement the proposed pricing scheme and determine the Nash Equilibrium. Finally, the proposed smart pricing scheme was tested against the traditional pricing method and simulation results indicate a significant boost in the utility achieved by the mobile customers."}, {"id": "conf/kbse/Neupane19", "title": "An Approach for Investigating Emotion Dynamics in Software Development.", "authors": ["Krishna Neupane"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00158"], "tag": ["ACM Student Research Competition"], "abstract": "Emotion awareness is critical to interpersonal communication, including that in software development. The SE community has studied emotion in software development using isolated emotion states but it has not considered the dynamic nature of emotion. To investigate the emotion dynamics, SE community needs an effective approach. In this paper, we propose such an approach which can automatically collect project teams' communication records, identify the emotions and their intensities in them, model the emotion dynamics into time series, and provide efficient data management. We demonstrate that this approach can provide end-to-end support for various emotion awareness research and practices through automated data collection, modeling, storage, analysis, and presentation using the IPython's project data on GitHub."}, {"id": "conf/kbse/Mudduluru19", "title": "Verifying Determinism in Sequential Programs.", "authors": ["Rashmi Mudduluru"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00159"], "tag": ["ACM Student Research Competition"], "abstract": "A nondeterministic program is difficult to test and debug. Nondeterminism occurs even in sequential programs: for example, iterating over the elements of a hash table can result in diverging test results. We have created a type system that can express whether a computation is deterministic, nondeterministic, or ordernondeterministic (like a set). While state-of-the-art nondeterminism detection tools unsoundly rely on observing run-time output, our approach soundly verifies determinism at compile time. Our implementation found previously-unknown nondeterminism errors in a 24,000 line program that had been heavily vetted by its developers."}, {"id": "conf/kbse/Yu19", "title": "Empirical Study of Python Call Graph.", "authors": ["Li Yu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00160"], "tag": ["ACM Student Research Competition"], "abstract": "In recent years, the extensive application of the Python language has made its analysis work more and more valuable. Many static analysis algorithms need to rely on the construction of call graphs. In this paper, we did a comparative empirical analysis of several widely used Python static call graph tools both quantitatively and qualitatively. Experiments show that the existing Python static call graph tools have a large difference in the construction effectiveness, and there is still room for improvement."}, {"id": "conf/kbse/Yu19a", "title": "Crowdsourced Report Generation via Bug Screenshot Understanding.", "authors": ["Shengcheng Yu"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00161"], "tag": ["ACM Student Research Competition"], "abstract": "Quality control is a challenge of crowdsourcing, especially in software testing. As some unprofessional workers involved, low-quality yieldings may hinder crowdsourced testing from satisfying requesters' requirements. Therefore, it is in demand to assist crowdworkers to raise bug report quality. In this paper, we propose a novel auxiliary method, namely CroReG, to generate crowdsourcing bug reports by analyzing bug screenshots uploaded by crowdworkers with image understanding techniques. The preliminary experiment results show that CroReG can effectively generate bug reports containing accurate screenshot captions and providing positive guidance for crowdworkers."}, {"id": "conf/kbse/Jiang19", "title": "Boosting Neural Commit Message Generation with Code Semantic Analysis.", "authors": ["Shuyao Jiang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00162"], "tag": ["ACM Student Research Competition"], "abstract": "It has been long suggested that commit messages can greatly facilitate code comprehension. However, developers may not write good commit messages in practice. Neural machine translation (NMT) has been suggested to automatically generate commit messages. Despite the efforts in improving NMT algorithms, the quality of the generated commit messages is not yet satisfactory. This paper, instead of improving NMT algorithms, suggests that proper preprocessing of code changes into concise inputs is quite critical to train NMT. We approach it with semantic analysis of code changes. We collect a real-world dataset with 50k+ commits of popular Java projects, and verify our idea with comprehensive experiments. The results show that preprocessing inputs with code semantic analysis can improve NMT significantly. This work sheds light to how to apply existing DNNs designed by the machine learning community, e.g., NMT models, to complete software engineering tasks."}, {"id": "conf/kbse/Balasubramaniam19", "title": "Towards Comprehensible Representation of Controllers using Machine Learning.", "authors": ["Gargi Balasubramaniam"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00163"], "tag": ["ACM Student Research Competition"], "abstract": "From the point of view of a software engineer, having safe and optimal controllers for real life systems like cyber physical systems is a crucial requirement before deployment. Given the mathematical model of these systems along with their specifications, model checkers can be used to synthesize controllers for them. The given work proposes novel approaches for making controller analysis easier by using machine learning to represent the controllers synthesized by model checkers in a succinct manner, while also incorporating the domain knowledge of the system. It also proposes the implementation of a visualization tool which will be integrated into existing model checkers. A lucid controller representation along with a tool to visualize it will help the software engineer debug and monitor the system much more efficiently."}, {"id": "conf/kbse/Zhang19", "title": "A Machine Learning Based Approach to Identify SQL Injection Vulnerabilities.", "authors": ["Kevin Zhang"], "DOIs": ["https://doi.org/10.1109/ASE.2019.00164"], "tag": ["ACM Student Research Competition"], "abstract": "This paper presents a machine learning classifier designed to identify SQL injection vulnerabilities in PHP code. Both classical and deep learning based machine learning algorithms were used to train and evaluate classifier models using input validation and sanitization features extracted from source code files. On ten-fold cross validations a model trained using Convolutional Neural Network(CNN) achieved the highest precision (95.4%), while a model based on Multilayer Perceptron(MLP) achieved the highest recall (63.7%) and the highest f-measure (0.746)."}]}, "issta/issta": {"2017": [{"id": "conf/issta/GroceHK17", "title": "One test to rule them all.", "authors": ["Alex Groce", "Josie Holmes", "Kevin Kellar"], "DOIs": ["https://doi.org/10.1145/3092703.3092704"], "tag": ["Improving Testing"], "abstract": "ABSTRACT Test reduction has long been seen as critical for automated testing. However, traditional test reduction simply reduces the length of a test, but does not attempt to reduce semantic complexity. This paper extends previous efforts with algorithms for normalizing and generalizing tests. Rewriting tests into a normal form can reduce semantic complexity and even remove steps from an already delta-debugged test. Moreover, normalization dramatically reduces the number of tests that a reader must examine, partially addressing the ``fuzzer taming'' problem of discovering distinct faults in a set of failing tests. Generalization, in contrast, takes a test and reports what aspects of the test could have been changed while preserving the property that the test fails. Normalization plus generalization aids understanding of tests, including tests for complex and widely used APIs such as the NumPy numeric computation library and the ArcPy GIS scripting package. Normalization frequently reduces the number of tests to be examined by well over an order of magnitude, and often to just one test per fault. Together, ideally, normalization and generalization allow a user to replace reading a large set of tests that vary in unimportant ways with reading one annotated summary test."}, {"id": "conf/issta/SpiekerGMM17", "title": "Reinforcement learning for automatic test case prioritization and selection in continuous integration.", "authors": ["Helge Spieker", "Arnaud Gotlieb", "Dusica Marijan", "Morten Mossige"], "DOIs": ["https://doi.org/10.1145/3092703.3092709"], "tag": ["Improving Testing"], "abstract": "ABSTRACT Testing in Continuous Integration (CI) involves test case prioritization, selection, and execution at each cycle. Selecting the most promising test cases to detect bugs is hard if there are uncertainties on the impact of committed code changes or, if traceability links between code and tests are not available. This paper introduces Retecs, a new method for automatically learning test case selection and prioritization in CI with the goal to minimize the round-trip time between code commits and developer feedback on failed test cases. The Retecs method uses reinforcement learning to select and prioritize test cases according to their duration, previous last execution and failure history. In a constantly changing environment, where new test cases are created and obsolete test cases are deleted, the Retecs method learns to prioritize error-prone test cases higher under guidance of a reward function and by observing previous CI cycles. By applying Retecs on data extracted from three industrial case studies, we show for the first time that reinforcement learning enables fruitful automatic adaptive test case selection and prioritization in CI and regression testing."}, {"id": "conf/issta/MostafaWX17", "title": "PerfRanker: prioritization of performance regression tests for collection-intensive software.", "authors": ["Shaikh Mostafa", "Xiaoyin Wang", "Tao Xie"], "DOIs": ["https://doi.org/10.1145/3092703.3092725"], "tag": ["Improving Testing"], "abstract": "ABSTRACT Regression performance testing is an important but time/resource-consuming phase during software development. Developers need to detect performance regressions as early as possible to reduce their negative impact and fixing cost. However, conducting regression performance testing frequently (e.g., after each commit) is prohibitively expensive. To address this issue, in this paper, we propose PerfRanker, the first approach to prioritizing test cases in performance regression testing for collection-intensive software, a common type of modern software heavily using collections. Our test prioritization is based on performance impact analysis that estimates the performance impact of a given code revision on a given test execution. Evaluation shows that our approach can cover top 3 test cases whose performance is most affected within top 30% to 37% prioritized test cases, in contrast to top 65% to 79% by 3 baseline techniques."}, {"id": "conf/issta/YanevaRD17", "title": "Compiler-assisted test acceleration on GPUs for embedded software.", "authors": ["Vanya Yaneva", "Ajitha Rajan", "Christophe Dubach"], "DOIs": ["https://doi.org/10.1145/3092703.3092720"], "tag": ["Improving Testing"], "abstract": "ABSTRACTEmbedded software is found everywhere from our highly visible mobile devices to the confines of our car in the form of smart sensors. Embedded software companies are under huge pressure to produce safe applications that limit risks, and testing is absolutely critical to alleviate concerns regarding safety and user privacy. This requires using large test suites throughout the development process, increasing time-to-market and ultimately hindering competitivity. Speeding up test execution is, therefore, of paramount importance for embedded software developers. This is traditionally achieved by running, in parallel, multiple tests on large-scale clusters of computers. However, this approach is costly in terms of infrastructure maintenance and energy consumed, and is at times inconvenient as developers have to wait for their tests to be scheduled on a shared resource. We propose to look at exploiting GPUs (Graphics Processing Units) for running embedded software testing. GPUs are readily available in most computers and offer tremendous amounts of parallelism, making them an ideal target for embedded software testing. In this paper, we demonstrate, for the first time, how test executions of embedded C programs can be automatically performed on a GPU, without involving the end user. We take a compiler-assisted approach which automatically compiles the C program into GPU kernels for parallel execution of the input tests. Using this technique, we achieve an average speedup of 16\u00d7 when compared to CPU execution of input tests across nine programs from an industry standard embedded benchmark suite."}, {"id": "conf/issta/LoscherS17", "title": "Targeted property-based testing.", "authors": ["Andreas L\u00f6scher", "Konstantinos Sagonas"], "DOIs": ["https://doi.org/10.1145/3092703.3092711"], "tag": ["Testing"], "abstract": "ABSTRACT We introduce targeted property-based testing, an enhanced form of property-based testing that aims to make the input generation component of a property-based testing tool guided by a search strategy rather than being completely random. Thus, this testing technique combines the advantages of both search-based and property-based testing. We demonstrate the technique with the framework we have built, called Target, and show its effectiveness on three case studies. The first of them demonstrates how Target can employ simulated annealing to generate sensor network topologies that form configurations with high energy consumption. The second case study shows how the generation of routing trees for a wireless network equipped with directional antennas can be guided to fulfill different energy metrics. The third case study employs Target to test the noninterference property of information-flow control abstract machine designs, and compares it with a sophisticated hand-written generator for programs of these abstract machines."}, {"id": "conf/issta/DakaRF17", "title": "Generating unit tests with descriptive names or: would you name your children thing1 and thing2?", "authors": ["Ermira Daka", "Jos\u00e9 Miguel Rojas", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1145/3092703.3092727"], "tag": ["Testing"], "abstract": "ABSTRACT The name of a unit test helps developers to understand the purpose and scenario of the test, and test names support developers when navigating amongst sets of unit tests. When unit tests are generated automatically, however, they tend to be given non-descriptive names such as \u201ctest0\u201d, which provide none of the benefits a descriptive name can give a test. The underlying challenge is that automatically generated tests typically do not represent real scenarios and have no clear purpose other than covering code, which makes naming them di cult. In this paper, we present an automated approach which generates descriptive names for automatically generated unit tests by summarizing API-level coverage goals. The tests are optimized to be short, descriptive of the test, have a clear relation to the covered code under test, and allow developers to uniquely distinguish tests in a test suite. An empirical evaluation with 47 participants shows that developers agree with the synthesized names, and the synthesized names are equally descriptive as manually written names. Study participants were even more accurate and faster at matching code and tests with synthesized names compared to manually derived names."}, {"id": "conf/issta/PerryMZC17", "title": "Accelerating array constraints in symbolic execution.", "authors": ["David Mitchel Perry", "Andrea Mattavelli", "Xiangyu Zhang", "Cristian Cadar"], "DOIs": ["https://doi.org/10.1145/3092703.3092728"], "tag": ["Symbolic Execution"], "abstract": "ABSTRACT Despite significant recent advances, the effectiveness of symbolic execution is limited when used to test complex, real-world software. One of the main scalability challenges is related to constraint solving: large applications and long exploration paths lead to complex constraints, often involving big arrays indexed by symbolic expressions. In this paper, we propose a set of semantics-preserving transformations for array operations that take advantage of contextual information collected during symbolic execution. Our transformations lead to simpler encodings and hence better performance in constraint solving. The results we obtain are encouraging: we show, through an extensive experimental analysis, that our transformations help to significantly improve the performance of symbolic execution in the presence of arrays. We also show that our transformations enable the analysis of new code, which would be otherwise out of reach for symbolic execution."}, {"id": "conf/issta/SunXE17", "title": "Improving the cost-effectiveness of symbolic testing techniques for transport protocol implementations under packet dynamics.", "authors": ["Wei Sun", "Lisong Xu", "Sebastian G. Elbaum"], "DOIs": ["https://doi.org/10.1145/3092703.3092706"], "tag": ["Symbolic Execution"], "abstract": "ABSTRACT The majority of Internet traffic is transferred by transport protocols. The correctness of these transport protocol implementations is hard to validate as their behaviors depend not only on their protocols but also on their network environments that can introduce dynamic packet delay and loss. Random testing, widely used in industry due to its simplicity and low cost, struggles to detect packet delay related faults which occur with low probability. Symbolic execution based testing is promising at detecting such low probability faults, but it requires large testing budgets as it attempts to cover a prohibitively large input space of packet dynamics. To improve its cost-effectiveness, we propose two domain-specific heuristic techniques, called packet retransmission based priority and network state based priority, which are motivated by two common transport protocol properties. In our experiments using the Linux TFTP programs, our techniques improve the cost-effectiveness of symbolic execution based testing for transport protocols, detecting three times as many faults when the budget is in the range of minutes and hours."}, {"id": "conf/issta/BraioneDMP17", "title": "Combining symbolic execution and search-based testing for programs with complex heap inputs.", "authors": ["Pietro Braione", "Giovanni Denaro", "Andrea Mattavelli", "Mauro Pezz\u00e8"], "DOIs": ["https://doi.org/10.1145/3092703.3092715"], "tag": ["Symbolic Execution"], "abstract": "ABSTRACT Despite the recent improvements in automatic test case generation, handling complex data structures as test inputs is still an open problem. Search-based approaches can generate sequences of method calls that instantiate structured inputs to exercise a relevant portion of the code, but fall short in building inputs to execute program elements whose reachability is determined by the structural features of the input structures themselves. Symbolic execution techniques can effectively handle structured inputs, but do not identify the sequences of method calls that instantiate the input structures through legal interfaces. In this paper, we propose a new approach to automatically generate test cases for programs with complex data structures as inputs. We use symbolic execution to generate path conditions that characterise the dependencies between the program paths and the input structures, and convert the path conditions to optimisation problems that we solve with search-based techniques to produce sequences of method calls that instantiate those inputs. Our preliminary results show that the approach is indeed effective in generating test cases for programs with complex data structures as inputs, thus opening a promising research direction."}, {"id": "conf/issta/MaiyaK17", "title": "Efficient computation of happens-before relation for event-driven programs.", "authors": ["Pallavi Maiya", "Aditya Kanade"], "DOIs": ["https://doi.org/10.1145/3092703.3092733"], "tag": ["Concurrency"], "abstract": "ABSTRACTAn emerging style of programming is to use both threads and events to achieve better scalability. The improved scalability comes at the price of increased complexity, as both threads and events can follow non-deterministic schedules. The happens-before (HB) relation captures the space of possible schedules and forms the basis of various concurrency analyses. Improving efficiency of the HB computation can speed up these analyses. In this paper, we identify a major bottleneck in computation of the HB relation for such event-driven programs. Event-driven programs are designed to interact continuously with their environment, and usually receive a large number of events even within a short span of time. This increases the cost of discovering the HB order among the events. We propose a novel data structure, called event graph, that maintains a subset of the HB relation to efficiently infer order between any pair of events. We present an algorithm, called EventTrack, which improves efficiency of vector clock based HB computation for event-driven programs using event graphs. We have implemented EventTrack and evaluated it on traces of eight Android applications. Compared to the state-of-the-art technique, EventTrack gave an average speedup of 4.9X. The speedup ranged from 1.8X to 10.3X across the applications."}, {"id": "conf/issta/WangWYZL17", "title": "Automatic detection and validation of race conditions in interrupt-driven embedded software.", "authors": ["Yu Wang", "Linzhang Wang", "Tingting Yu", "Jianhua Zhao", "Xuandong Li"], "DOIs": ["https://doi.org/10.1145/3092703.3092724"], "tag": ["Concurrency"], "abstract": "ABSTRACT Interrupt-driven programs are widely deployed in safety-critical embedded systems to perform hardware and resource dependent data operation tasks. The frequent use of interrupts in these systems can cause race conditions to occur due to interactions between application tasks and interrupt handlers. Numerous program analysis and testing techniques have been proposed to detect races in multithreaded programs. Little work, however, has addressed race condition problems related to hardware interrupts. In this paper, we present SDRacer, an automated framework that can detect and validate race conditions in interrupt-driven embedded software. It uses a combination of static analysis and symbolic execution to generate input data for exercising the potential races. It then employs virtual platforms to dynamically validate these races by forcing the interrupts to occur at the potential racing points. We evaluate SDRacer on nine real-world embedded programs written in C language. The results show that SDRacer can precisely detect race conditions."}, {"id": "conf/issta/El-HokayemF17", "title": "Monitoring decentralized specifications.", "authors": ["Antoine El-Hokayem", "Yli\u00e8s Falcone"], "DOIs": ["https://doi.org/10.1145/3092703.3092723"], "tag": ["Concurrency"], "abstract": "ABSTRACT We define two complementary approaches to monitor decentralized systems. The first relies on those with a centralized specification, i.e, when the specification is written for the behavior of the entire system. To do so, our approach introduces a data-structure that i) keeps track of the execution of an automaton, ii) has predictable parameters and size, and iii) guarantees strong eventual consistency. The second approach defines decentralized specifications wherein multiple specifications are provided for separate parts of the system. We study decentralized monitorability, and present a general algorithm for monitoring decentralized specifications. We map three existing algorithms to our approaches and provide a framework for analyzing their behavior. Lastly, we introduce our tool, which is a framework for designing such decentralized algorithms, and simulating their behavior."}, {"id": "conf/issta/ChenYLACC17", "title": "Effective online software anomaly detection.", "authors": ["Yizhen Chen", "Ming Ying", "Daren Liu", "Adil Alim", "Feng Chen", "Mei-Hwa Chen"], "DOIs": ["https://doi.org/10.1145/3092703.3092730"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACTWhile automatic online software anomaly detection is crucial for ensuring the quality of production software, current techniques are mostly inefficient and ineffective. For online software, its inputs are usually provided by the users at runtime and the validity of the outputs cannot be automatically verified without a predefined oracle. Furthermore, some online anomalous behavior may be caused by the anomalies in the execution context, rather than by any code defect, which are even more difficult to detect. Existing approaches tackle this problem by identifying certain properties observed from the executions of the software during a training process and using them to monitor online software behavior. However, they may require a large execution overhead for monitoring the properties, which limits the applicability of these approaches for online monitoring. We present a methodology that applies effective algorithms to select a close to optimal set of anomaly-revealing properties, which enables online anomaly detection with minimal execution overhead. Our empirical results show that an average of 76.5% of anomalies were detected by using at most 5.5% of execution overhead."}, {"id": "conf/issta/KochCERK17", "title": "Semi-automated discovery of server-based information oversharing vulnerabilities in Android applications.", "authors": ["William Koch", "Abdelberi Chaabane", "Manuel Egele", "William K. Robertson", "Engin Kirda"], "DOIs": ["https://doi.org/10.1145/3092703.3092708"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACT Modern applications are often split into separate client and server tiers that communicate via message passing over the network. One well-understood threat to privacy for such applications is the leakage of sensitive user information either in transit or at the server. In response, an array of defensive techniques have been developed to identify or block unintended or malicious information leakage. However, prior work has primarily considered privacy leaks originating at the client directed at the server, while leakage in the reverse direction -- from the server to the client -- is comparatively under-studied. The question of whether and to what degree this leakage constitutes a threat remains an open question. We answer this question in the affirmative with Hush, a technique for semi-automatically identifying Server-based InFormation OvershariNg (SIFON) vulnerabilities in multi-tier applications. In particular, the technique detects SIFON vulnerabilities using a heuristic that overshared sensitive information from server-side APIs will not be displayed by the application's user interface. The technique first performs a scalable static program analysis to screen applications for potential vulnerabilities, and then attempts to confirm these candidates as true vulnerabilities with a partially-automated dynamic analysis. Our evaluation over a large corpus of Android applications demonstrates the effectiveness of the technique by discovering several previously-unknown SIFON vulnerabilities in eight applications."}, {"id": "conf/issta/KwonWZZX17", "title": "CPR: cross platform binary code reuse via platform independent trace program.", "authors": ["Yonghwi Kwon", "Weihang Wang", "Yunhui Zheng", "Xiangyu Zhang", "Dongyan Xu"], "DOIs": ["https://doi.org/10.1145/3092703.3092707"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACT The rapid growth of Internet of Things (IoT) has been created a number of new platforms recently. Unfortunately, such variety of IoT devices causes platform fragmentation which makes software development on such devices challenging. In particular, existing programs cannot be simply reused on such devices as they rely on certain underlying hardware and software interfaces which we call platform dependencies. In this paper, we present CPR, a novel technique that synthesizes a platform independent program from a platform dependent program. Specifically, we leverage an existing system called PIEtrace which can generate a platform independent trace program. The generated trace program is platform independent while it can only reproduce a specific execution path. Hence, we develop an algorithm to merge a set of platform independent trace programs and synthesize a general program that can take multiple inputs. The synthesized platform-independent program is representative of the merged trace programs and the results produced by the program is correct if no exceptions occur. Our evaluation results on 15 real-world applications show that CPR is highly effective on reusing existing binaries across platforms."}, {"id": "conf/issta/SelakovicGP17", "title": "An actionable performance profiler for optimizing the order of evaluations.", "authors": ["Marija Selakovic", "Thomas Glaser", "Michael Pradel"], "DOIs": ["https://doi.org/10.1145/3092703.3092716"], "tag": ["Dynamic Analysis"], "abstract": "ABSTRACT The efficiency of programs often can be improved by applying rel- atively simple changes. To find such optimization opportunities, developers either rely on manual performance tuning, which is time-consuming and requires expert knowledge, or on traditional profilers, which show where resources are spent but not how to optimize the program. This paper presents a profiler that provides actionable advice, by not only finding optimization opportunities but by also suggesting code transformations that exploit them. Specifically, we focus on optimization opportunities related to the order of evaluating subexpressions that are part of a decision made by the program. To help developers find such reordering opportuni- ties, we present DecisionProf, a dynamic analysis that automatically identifies the optimal order, for a given input, of checks in logical expressions and in switch statements. The key idea is to assess the computational costs of all possible orders, to find the optimal order, and to suggest a code transformation to the developer only if reordering yields a statistically significant performance improve- ment. Applying DecisionProf to 43 real-world JavaScript projects reveals 52 beneficial reordering opportunities. Optimizing the code as proposed by DecisionProf reduces the execution time of indi- vidual functions between 2.5% and 59%, and leads to statistically significant application-level performance improvements that range between 2.5% and 6.5%."}, {"id": "conf/issta/AthaiyaK17", "title": "Testing and analysis of web applications using page models.", "authors": ["Snigdha Athaiya", "Raghavan Komondoor"], "DOIs": ["https://doi.org/10.1145/3092703.3092734"], "tag": ["The Web"], "abstract": "ABSTRACT Web applications are difficult to analyze using code-based tools because data-flow and control-flow through the application occurs via both server-side code and client-side pages. Client-side pages are typically specified in a scripting language that is different from the main server-side language; moreover, the pages are generated dynamically from the scripts. To address these issues we propose a static-analysis approach that automatically constructs a ``model'' of each page in a given application. A page model is a code fragment in the same language as the server-side code, which faithfully over-approximates the possible elements of the page as well as the control-flows and data-flows due to these elements. The server-side code in conjunction with the page models then becomes a standard (non-web) program, thus amenable to analysis using standard code-based tools. We have implemented our approach in the context of J2EE applications. We demonstrate the versatility and usefulness of our approach by applying three standard analysis tools on the resultant programs from our approach: a concolic-execution based model checker (JPF), a dynamic fault localization tool (Zoltar), and a static slicer (Wala)."}, {"id": "conf/issta/WalshKM17", "title": "Automated layout failure detection for responsive web pages without an explicit oracle.", "authors": ["Thomas A. Walsh", "Gregory M. Kapfhammer", "Phil McMinn"], "DOIs": ["https://doi.org/10.1145/3092703.3092712"], "tag": ["The Web"], "abstract": "ABSTRACT As the number and variety of devices being used to access the World Wide Web grows exponentially, ensuring the correct presentation of a web page, regardless of the device used to browse it, is an important and challenging task. When developers adopt responsive web design (RWD) techniques, web pages modify their appearance to accommodate a device\u2019s display constraints. However, a current lack of automated support means that presentation failures may go undetected in a page\u2019s layout when rendered for different viewport sizes. A central problem is the difficulty in providing an automated \u201coracle\u201d to validate RWD layouts against, meaning that checking for failures is largely a manual process in practice, which results in layout failures in many live responsive web sites. This paper presents an automated failure detection technique that checks the consistency of a responsive page\u2019s layout across a range of viewport widths, obviating the need for an explicit oracle. In an empirical study, this method found failures in 16 of 26 real-world production pages studied, detecting 33 distinct failures in total."}, {"id": "conf/issta/GuarnieriTBDB17", "title": "Test execution checkpointing for web applications.", "authors": ["Marco Guarnieri", "Petar Tsankov", "Tristan Buchs", "Mohammad Torabi Dashti", "David A. Basin"], "DOIs": ["https://doi.org/10.1145/3092703.3092710"], "tag": ["The Web"], "abstract": "ABSTRACT Test isolation is a prerequisite for the correct execution of test suites on web applications. We present Test Execution Checkpointing, a method for efficient test isolation. Our method instruments web applications to support checkpointing and exploits this support to isolate and optimize tests. We have implemented and evaluated this method on five popular PHP web applications. The results show that our method not only provides test isolation essentially for free, it also reduces testing time by 44% on average."}, {"id": "conf/issta/MostafaRW17", "title": "Experience paper: a study on behavioral backward incompatibilities of Java software libraries.", "authors": ["Shaikh Mostafa", "Rodney Rodriguez", "Xiaoyin Wang"], "DOIs": ["https://doi.org/10.1145/3092703.3092721"], "tag": ["Experience Report"], "abstract": "ABSTRACT Nowadays, due to the frequent technological innovation and market changes, software libraries are evolving very quickly. Backward compatibility has always been one of the most important requirements during the evolution of software platforms and libraries. However, backward compatibility is seldom fully achieved in practice, and many relevant software failures are reported. Therefore, it is important to understand the status, major reasons, and impact of backward incompatibilities in real world software. This paper presents an empirical study to understand behavioral changes of APIs during evolution of software libraries. Specifically, we performed a large-scale cross-version regression testing on 68 consecutive version pairs from 15 popular Java software libraries. Furthermore, we collected and studied 126 real-world software bugs reports on backward incompatibilities of software libraries. Our major findings include: (1) 1,094 test failures / errors and 296 behavioral backward incompatibilities are detected from 52 of 68 consecutive version pairs; (2) there is a distribution mismatch between incompatibilities detected by library-side regression testing, and bug-inducing incompatibilities; (3) the majority of behavioral backward incompatibilities are not well documented in API documents or release notes; and (4) 67% of fixed client bugs caused by backward incompatibilities in software libraries are fixed by client developers, through several simple change patterns made to the backward incompatible API invocation."}, {"id": "conf/issta/XinR17", "title": "Identifying test-suite-overfitted patches through test case generation.", "authors": ["Qi Xin", "Steven P. Reiss"], "DOIs": ["https://doi.org/10.1145/3092703.3092718"], "tag": ["Program Repair and Patching"], "abstract": "ABSTRACTA typical automatic program repair technique that uses a test suite as the correct criterion can produce a patched program that is test-suite-overfitted, or overfitting, which passes the test suite but does not actually repair the bug. In this paper, we propose DiffTGen which identifies a patched program to be overfitting by first generating new test inputs that uncover semantic differences between the original faulty program and the patched program, then testing the patched program based on the semantic differences, and finally generating test cases. Such a test case could be added to the original test suite to make it stronger and could prevent the repair technique from generating a similar overfitting patch again. We evaluated DiffTGen on 89 patches generated by four automatic repair techniques for Java with 79 of them being likely to be overfitting and incorrect. DiffTGen identifies in total 39 (49.4%) overfitting patches and yields the corresponding test cases. We further show that an automatic repair technique, if configured with DiffTGen, could avoid yielding overfitting patches and potentially produce correct ones."}, {"id": "conf/issta/KoyuncuBKKMT17", "title": "Impact of tool support in patch construction.", "authors": ["Anil Koyuncu", "Tegawend\u00e9 F. Bissyand\u00e9", "Dongsun Kim", "Jacques Klein", "Martin Monperrus", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3092703.3092713"], "tag": ["Program Repair and Patching"], "abstract": "ABSTRACT In this work, we investigate the practice of patch construction in the Linux kernel development, focusing on the differences between three patching processes: (1) patches crafted entirely manually to fix bugs, (2) those that are derived from warnings of bug detection tools, and (3) those that are automatically generated based on fix patterns. With this study, we provide to the research community concrete insights on the practice of patching as well as how the development community is currently embracing research and commercial patching tools to improve productivity in repair. The result of our study shows that tool-supported patches are increasingly adopted by the developer community while manually-written patches are accepted more quickly. Patch application tools enable developers to remain committed to contributing patches to the code base. Our findings also include that, in actual development processes, patches generally implement several change operations spread over the code, even for patches fixing warnings by bug detection tools. Finally, this study has shown that there is an opportunity to directly leverage the output of bug detection tools to readily generate patches that are appropriate for fixing the problem, and that are consistent with manually-written patches."}, {"id": "conf/issta/MahajanAMH17", "title": "Automated repair of layout cross browser issues using search-based techniques.", "authors": ["Sonal Mahajan", "Abdulmajeed Alameer", "Phil McMinn", "William G. J. Halfond"], "DOIs": ["https://doi.org/10.1145/3092703.3092726"], "tag": ["Program Repair and Patching"], "abstract": "ABSTRACT A consistent cross-browser user experience is crucial for the success of a website. Layout Cross Browser Issues (XBIs) can severely undermine a website\u2019s success by causing web pages to render incorrectly in certain browsers, thereby negatively impacting users\u2019 impression of the quality and services that the web page delivers. Existing Cross Browser Testing (XBT) techniques can only detect XBIs in websites. Repairing them is, hitherto, a manual task that is labor intensive and requires significant expertise. Addressing this concern, our paper proposes a technique for automatically repairing layout XBIs in websites using guided search-based techniques. Our empirical evaluation showed that our approach was able to successfully fix 86% of layout XBIs reported for 15 different web pages studied, thereby improving their cross-browser consistency."}, {"id": "conf/issta/ZhangLZK17", "title": "Boosting spectrum-based fault localization using PageRank.", "authors": ["Mengshi Zhang", "Xia Li", "Lingming Zhang", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1145/3092703.3092731"], "tag": ["Fault Localization and Mutation Testing"], "abstract": "ABSTRACT Manual debugging is notoriously tedious and time consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. A focus of existing SBFL techniques is to consider how to differentiate program source code entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work.  We present PRFL, a lightweight technique that boosts spectrum-based fault localization by differentiating tests using PageRank algorithm. Given the original program spectrum information, PRFL uses PageRank to recompute the spectrum information by considering the contributions of different tests. Then, traditional SBFL techniques can be applied on the recomputed spectrum information to achieve more effective fault localization. Although simple and lightweight, PRFL has been demonstrated to outperform state-of-the-art SBFL techniques significantly (e.g., ranking 42% more real faults within Top-1 compared with the most effective traditional SBFL technique) with low overhead (e.g., around 2 minute average extra overhead on real faults) on 357 real faults from 5 Defects4J projects and 30692 artificial (i.e., mutation) faults from 87 GitHub projects, demonstrating a promising future for considering the contributions of different tests during fault localization."}, {"id": "conf/issta/SohnY17", "title": "FLUCCS: using code and change metrics to improve fault localization.", "authors": ["Jeongju Sohn", "Shin Yoo"], "DOIs": ["https://doi.org/10.1145/3092703.3092717"], "tag": ["Fault Localization and Mutation Testing"], "abstract": "ABSTRACT Fault localization aims to support the debugging activities of human developers by highlighting the program elements that are suspected to be responsible for the observed failure. Spectrum Based Fault Localization (SBFL), an existing localization technique that only relies on the coverage and pass/fail results of executed test cases, has been widely studied but also criticized for the lack of precision and limited effort reduction. To overcome restrictions of techniques based purely on coverage, we extend SBFL with code and change metrics that have been studied in the context of defect prediction, such as size, age and code churn. Using suspiciousness values from existing SBFL formulas and these source code metrics as features, we apply two learn-to-rank techniques, Genetic Programming (GP) and linear rank Support Vector Machines (SVMs). We evaluate our approach with a ten-fold cross validation of method level fault localization, using 210 real world faults from the Defects4J repository. GP with additional source code metrics ranks the faulty method at the top for 106 faults, and within the top five for 173 faults. This is a significant improvement over the state-of-the-art SBFL formulas, the best of which can rank 49 and 127 faults at the top and within the top five, respectively."}, {"id": "conf/issta/JustKA17", "title": "Inferring mutant utility from program context.", "authors": ["Ren\u00e9 Just", "Bob Kurtz", "Paul Ammann"], "DOIs": ["https://doi.org/10.1145/3092703.3092732"], "tag": ["Fault Localization and Mutation Testing"], "abstract": "ABSTRACT Existing mutation techniques produce vast numbers of equivalent, trivial, and redundant mutants. Selective mutation strategies aim to reduce the inherent redundancy of full mutation analysis to obtain most of its benefit for a fraction of the cost. Unfortunately, recent research has shown that there is no fixed selective mutation strategy that is effective across a broad range of programs; the utility (i.e., usefulness) of a mutant produced by a given mutation operator varies greatly across programs.  This paper hypothesizes that mutant utility, in terms of equivalence, triviality, and dominance, can be predicted by incorporating context information from the program in which the mutant is embedded. Specifically, this paper (1) explains the intuition behind this hypothesis with a motivational example, (2) proposes an approach for modeling program context using a program's abstract syntax tree, and (3) proposes and evaluates a series of program-context models for predicting mutant utility. The results for 129 mutation operators show that program context information greatly increases the ability to predict mutant utility. The results further show that it is important to consider program context for individual mutation operators rather than mutation operator groups."}, {"id": "conf/issta/WangXSZH17", "title": "Faster mutation analysis via equivalence modulo states.", "authors": ["Bo Wang", "Yingfei Xiong", "Yangqingwei Shi", "Lu Zhang", "Dan Hao"], "DOIs": ["https://doi.org/10.1145/3092703.3092714"], "tag": ["Fault Localization and Mutation Testing"], "abstract": "ABSTRACT Mutation analysis has many applications, such as asserting the quality of test suites and localizing faults. One important bottleneck of mutation analysis is scalability. The latest work explores the possibility of reducing the redundant execution via split-stream execution. However, split-stream execution is only able to remove redundant execution before the first mutated statement.  In this paper we try to also reduce some of the redundant execution after the execution of the first mutated statement. We observe that, although many mutated statements are not equivalent, the execution result of those mutated statements may still be equivalent to the result of the original statement. In other words, the statements are equivalent modulo the current state. In this paper we propose a fast mutation analysis approach, AccMut. AccMut automatically detects the equivalence modulo states among a statement and its mutations, then groups the statements into equivalence classes modulo states, and uses only one process to represent each class. In this way, we can significantly reduce the number of split processes. Our experiments show that our approach can further accelerate mutation analysis on top of split-stream execution with a speedup of 2.56x on average."}, {"id": "conf/issta/DoALBSM17", "title": "Just-in-time static analysis.", "authors": ["Lisa Nguyen Quang Do", "Karim Ali", "Benjamin Livshits", "Eric Bodden", "Justin Smith", "Emerson R. Murphy-Hill"], "DOIs": ["https://doi.org/10.1145/3092703.3092705"], "tag": ["Static Analysis"], "abstract": "ABSTRACT We present the concept of Just-In-Time (JIT) static analysis that interleaves code development and bug fixing in an integrated development environment. Unlike traditional batch-style analysis tools, a JIT analysis tool presents warnings to code developers over time, providing the most relevant results quickly, and computing less relevant results incrementally later. In this paper, we describe general guidelines for designing JIT analyses. We also present a general recipe for transforming static data-flow analyses to JIT analyses through a concept of layered analysis execution. We illustrate this transformation through CHEETAH, a JIT taint analysis for Android applications. Our empirical evaluation of CHEETAH on real-world applications shows that our approach returns warnings quickly enough to avoid disrupting the normal workflow of developers. This result is confirmed by our user study, in which developers fixed data leaks twice as fast when using CHEETAH compared to an equivalent batch-style analysis."}, {"id": "conf/issta/GyoriLP17", "title": "Refining interprocedural change-impact analysis using equivalence relations.", "authors": ["Alex Gyori", "Shuvendu K. Lahiri", "Nimrod Partush"], "DOIs": ["https://doi.org/10.1145/3092703.3092719"], "tag": ["Static Analysis"], "abstract": "ABSTRACT Change-impact analysis (CIA) is the task of determining the set of program elements impacted by a program change. Precise CIA has great potential to avoid expensive testing and code reviews for (parts of) changes that are refactorings (semantics-preserving). However most statement-level CIA techniques suffer from imprecision as they do not incorporate the semantics of the change.  We formalize change impact in terms of the trace semantics of two program versions. We show how to leverage equivalence relations to make dataflow-based CIA aware of the change semantics, thereby improving precision in the presence of semantics-preserving changes. We propose an anytime algorithm that applies costly equivalence-relation inference incrementally to refine the set of impacted statements. We implemented a prototype and evaluated it on 322 real-world changes from open-source projects and benchmark programs used by prior research. The evaluation results show an average 35% improvement in the number of impacted statements compared to prior dataflow-based techniques."}, {"id": "conf/issta/FanSLX17", "title": "Boosting the precision of virtual call integrity protection with partial pointer analysis for C++.", "authors": ["Xiaokang Fan", "Yulei Sui", "Xiangke Liao", "Jingling Xue"], "DOIs": ["https://doi.org/10.1145/3092703.3092729"], "tag": ["Static Analysis"], "abstract": "ABSTRACT We present, VIP, an approach to boosting the precision of Virtual call Integrity Protection for large-scale real-world C++ programs (e.g., Chrome) by using pointer analysis for the first time. VIP introduces two new techniques: (1) a sound and scalable partial pointer analysis for discovering statically the sets of legitimate targets at virtual callsites from separately compiled C++ modules and (2) a lightweight instrumentation technique for performing (virtual call) integrity checks at runtime. VIP raises the bar against vtable hijacking attacks by providing stronger security guarantees than the CHA-based approach with comparable performance overhead.  VIP is implemented in LLVM-3.8.0 and evaluated using SPEC programs and Chrome. Statically, VIP protects virtual calls more effectively than CHA by significantly reducing the sets of legitimate targets permitted at 20.3% of the virtual callsites per program, on average. Dynamically, VIP incurs an average (maximum) instrumentation overhead of 0.7% (3.3%), making it practically deployable as part of a compiler tool chain."}, {"id": "conf/issta/OreDE17", "title": "Lightweight detection of physical unit inconsistencies without program annotations.", "authors": ["John-Paul Ore", "Carrick Detweiler", "Sebastian G. Elbaum"], "DOIs": ["https://doi.org/10.1145/3092703.3092722"], "tag": ["Static Analysis"], "abstract": "ABSTRACT Systems interacting with the physical world operate on quantities measured with physical units. When unit operations in a program are inconsistent with the physical units' rules, those systems may suffer. Existing approaches to support unit consistency in programs can impose an unacceptable burden on developers. In this paper, we present a lightweight static analysis approach focused on physical unit inconsistency detection that requires no end-user program annotation, modification, or migration. It does so by capitalizing on existing shared libraries that handle standardized physical units, common in the cyber-physical domain, to link class attributes of shared libraries to physical units. Then, leveraging rules from dimensional analysis, the approach propagates and infers units in programs that use these shared libraries, and detects inconsistent unit usage. We implement and evaluate the approach in a tool, analyzing 213 open-source systems containing +900,000 LOC, finding inconsistencies in 11% of them, with an 87% true positive rate for a class of inconsistencies detected with high confidence. An initial survey of robot system developers finds that the unit inconsistencies detected by our tool are 'problematic', and we investigate how and when these inconsistencies occur."}, {"id": "conf/issta/OreDE17a", "title": "Phriky-units: a lightweight, annotation-free physical unit inconsistency detection tool.", "authors": ["John-Paul Ore", "Carrick Detweiler", "Sebastian G. Elbaum"], "DOIs": ["https://doi.org/10.1145/3092703.3098219"], "tag": ["Demonstrations"], "abstract": "ABSTRACT Systems that interact with the physical world use software that represents and manipulates physical quantities. To operate correctly, these systems must obey the rules of how quantities with physical units can be combined, compared, and manipulated. Incorrectly manipulating physical quantities can cause faults that go undetected by the type system, likely manifesting later as incorrect behavior. Existing approaches for inconsistency detection require code annotation, physical unit libraries, or specialized programming languages. We introduce Phriky-Units, a static analysis tool that detects physical unit inconsistencies in robotic software without developer annotations. It does so by capitalizing on existing shared libraries that handle standardized physical units, common in the cyber-physical domain, to link class attributes of shared libraries to physical units. In this work, we describe how Phriky-Units works, provide details of the implementation, and explain how Phriky-Units can be used. Finally we present a summary of an empirical evaluation showing it has an 87% true positive rate for a class of inconsistencies we detect with high-confidence."}, {"id": "conf/issta/HolmesG17", "title": "A suite of tools for making effective use of automatically generated tests.", "authors": ["Josie Holmes", "Alex Groce"], "DOIs": ["https://doi.org/10.1145/3092703.3098220"], "tag": ["Demonstrations"], "abstract": "ABSTRACT Automated test generation tools (we hope) produce failing tests from time to time. In a world of fault-free code this would not be true, but in such a world we would not need automated test generation tools. Failing tests are generally speaking the most valuable products of the testing process, and users need tools that extract their full value. This paper describes the tools provided by the TSTL testing language for making use of tests (which are not limited to failing tests). In addition to the usual tools for simple delta-debugging and executing tests as regressions, TSTL provides tools for 1) minimizing tests by criteria other than failure, such as code coverage, 2) normalizing tests to achieve further reduction and canonicalization than provided by delta-debugging, 3) generalizing tests to describe the neighborhood of similar tests that fail in the same fashion, and 4) avoiding slippage, where delta-debugging causes a failing test to change underlying fault. These tools can be accessed both by easy-to-use command-line tools and via a powerful API that supports more complex custom test manipulations."}, {"id": "conf/issta/WalshKM17a", "title": "ReDeCheck: an automatic layout failure checking tool for responsively designed web pages.", "authors": ["Thomas A. Walsh", "Gregory M. Kapfhammer", "Phil McMinn"], "DOIs": ["https://doi.org/10.1145/3092703.3098221"], "tag": ["Demonstrations"], "abstract": "ABSTRACT Since people frequently access websites with a wide variety of devices (e.g., mobile phones, laptops, and desktops), developers need frameworks and tools for creating layouts that are useful at many viewport widths. While responsive web design (RWD) principles and frameworks facilitate the development of such sites, there is a lack of tools supporting the detection of failures in their layout. Since the quality assurance process for responsively designed websites is often manual, time-consuming, and error-prone, this paper presents ReDeCheck, an automated layout checking tool that alerts developers to both potential unintended regressions in responsive layout and common types of layout failure. In addition to summarizing ReDeCheck\u2019s benefits, this paper explores two different usage scenarios for this tool that is publicly available on GitHub."}, {"id": "conf/issta/GambiKLZ17", "title": "CUT: automatic unit testing in the cloud.", "authors": ["Alessio Gambi", "Sebastian Kappler", "Johannes Lampel", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1145/3092703.3098222"], "tag": ["Demonstrations"], "abstract": "ABSTRACT Unit tests can be significantly sped up by running them in parallel over distributed execution environments, such as the cloud. However, manually setting up such environments and configuring the testing frameworks to effectively use them is cumbersome and requires specialized expertise that developers might lack.  We present Cloud Unit Testing (CUT), a tool for automatically executing unit tests in distributed execution environments. Given a set of unit tests, CUT allocates appropriate computational resources, i.e., virtual machines or containers, and schedules the execution of tests over them. Developers do not need to change existing unit test code, and can easily control relevant aspects of test execution, including resource allocation and test scheduling. Additionally, during the execution CUT monitors and publishes events about the running tests which enables stream analytics.  CUT and videos showcasing its main features are freely available at: https://www.st.cs.uni-saarland.de/testing/cut/"}, {"id": "conf/issta/MahajanAMH17a", "title": "XFix: an automated tool for the repair of layout cross browser issues.", "authors": ["Sonal Mahajan", "Abdulmajeed Alameer", "Phil McMinn", "William G. J. Halfond"], "DOIs": ["https://doi.org/10.1145/3092703.3098223"], "tag": ["Demonstrations"], "abstract": "ABSTRACT Differences in the rendering of a website across different browsers can cause inconsistencies in its appearance and usability, resulting in Layout Cross Browser Issues (XBIs). Such XBIs can negatively impact the functionality of a website as well as users\u2019 impressions of its trustworthiness and reliability. Existing techniques can only detect XBIs, and therefore require developers to manually perform the labor intensive task of repair. In this demo paper we introduce our tool, XFix, that automatically repairs layout XBIs in web applications. To the best of our knowledge, XFix is the first automated technique for generating XBI repairs."}, {"id": "conf/issta/El-HokayemF17a", "title": "THEMIS: a tool for decentralized monitoring algorithms.", "authors": ["Antoine El-Hokayem", "Yli\u00e8s Falcone"], "DOIs": ["https://doi.org/10.1145/3092703.3098224"], "tag": ["Demonstrations"], "abstract": "ABSTRACT THEMIS is a tool to facilitate the design, development, and analysis of decentralized monitoring algorithms; developed using Java and AspectJ. It consists of a library and command-line tools. THEMIS provides an API, data structures and measures for decentralized monitoring. These building blocks can be reused or extended to modify existing algorithms, design new more intricate algorithms, and elaborate new approaches to assess existing algorithms. We illustrate the usage of THEMIS by comparing two variants of a monitoring algorithm."}, {"id": "conf/issta/LeCLGV17", "title": "JFIX: semantics-based repair of Java programs via symbolic PathFinder.", "authors": ["Xuan-Bach D. Le", "Duc-Hiep Chu", "David Lo", "Claire Le Goues", "Willem Visser"], "DOIs": ["https://doi.org/10.1145/3092703.3098225"], "tag": ["Demonstrations"], "abstract": "ABSTRACT Recently there has been a proliferation of automated program repair (APR) techniques, targeting various programming languages. Such techniques can be generally classified into two families: syntactic- and semantics-based. Semantics-based APR, on which we focus, typically uses symbolic execution to infer semantic constraints and then program synthesis to construct repairs conforming to them. While syntactic-based APR techniques have been shown success- ful on bugs in real-world programs written in both C and Java, semantics-based APR techniques mostly target C programs. This leaves empirical comparisons of the APR families not fully explored, and developers without a Java-based semantics APR technique. We present JFix, a semantics-based APR framework that targets Java, and an associated Eclipse plugin. JFix is implemented atop Symbolic PathFinder, a well-known symbolic execution engine for Java programs. It extends one particular APR technique (Angelix), and is designed to be sufficiently generic to support a variety of such techniques. We demonstrate that semantics-based APR can indeed efficiently and effectively repair a variety of classes of bugs in large real-world Java programs. This supports our claim that the framework can both support developers seeking semantics-based repair of bugs in Java programs, as well as enable larger scale empirical studies comparing syntactic- and semantics-based APR targeting Java. The demonstration of our tool is available via the project website at: https://xuanbachle.github.io/semanticsrepair/"}, {"id": "conf/issta/SpencerBMB17", "title": "ArtForm: a tool for exploring the codebase of form-based websites.", "authors": ["Ben Spencer", "Michael Benedikt", "Anders M\u00f8ller", "Franck van Breugel"], "DOIs": ["https://doi.org/10.1145/3092703.3098226"], "tag": ["Demonstrations"], "abstract": "ABSTRACT We describe ArtForm, a tool for exploring the codebase of dynamic data-driven websites where users enter data via forms. ArtForm extends an instrumented browser, so it can directly implement user interactions, adding in symbolic and concolic execution of JavaScript. The tool supports a range of exploration modes with varying degrees of user intervention. It includes a number of adaptations of concolic execution to the setting of form-based web programs."}, {"id": "conf/issta/YanevaRD17a", "title": "ParTeCL: parallel testing using OpenCL.", "authors": ["Vanya Yaneva", "Ajitha Rajan", "Christophe Dubach"], "DOIs": ["https://doi.org/10.1145/3092703.3098227"], "tag": ["Demonstrations"], "abstract": "ABSTRACTWith the growing complexity of software, the number of test cases needed for effective validation is extremely large. Executing these large test suites is expensive and time consuming, putting an enormous pressure on the software development cycle. In previous work, we proposed using Graphics Processing Units (GPUs) to accelerate test execution by running test cases in parallel on the GPU threads. However, the complexity of GPU programming poses challenges to the usability and effectiveness of the proposed approach. In this paper we present ParTeCL - a compiler-assisted framework to automatically generate GPU code from sequential programs and execute their tests in parallel on the GPU. We show feasibilitiy and performance achieved when executing test suites for 9 programs from an industry standard benchmark suite on the GPU. ParTeCL achieves an average speedup of 16\u00d7 when compared to a single CPU for these benchmarks."}, {"id": "conf/issta/ChavesBCKF17", "title": "Verifying digital systems with MATLAB.", "authors": ["Lennon C. Chaves", "Iury Bessa", "Lucas C. Cordeiro", "Daniel Kroening", "Eddie Batista de Lima Filho"], "DOIs": ["https://doi.org/10.1145/3092703.3098228"], "tag": ["Demonstrations"], "abstract": "ABSTRACT A MATLAB toolbox is presented, with the goal of checking occurrences of design errors typically found in fixed-point digital systems, considering finite word-length effects. In particular, the present toolbox works as a front-end to a recently introduced verification tool, known as Digital-System Verifier (DSVerifier), and checks overflow, limit cycle, quantization, stability, and minimum phase errors in digital systems represented by transfer-function and state-space equations. It provides a command-line version with simplified access to specific functionality and a graphical-user interface, which was developed as a MATLAB application. The resulting toolbox enables application of verification to real-world systems by control engineers."}, {"id": "conf/issta/HalleK17", "title": "SealTest: a simple library for test sequence generation.", "authors": ["Sylvain Hall\u00e9", "Rapha\u00ebl Khoury"], "DOIs": ["https://doi.org/10.1145/3092703.3098229"], "tag": ["Demonstrations"], "abstract": "ABSTRACT SealTest is a Java library for generating test sequences based on a formal specification. It allows a user to easily define a wide range of coverage metrics using multiple specification languages. Its simple and generic architecture makes it a useful testing tool for dynamic software systems, as well as an appropriate research testbed for implementing and experimentally comparing test sequence generation algorithms."}, {"id": "conf/issta/CasalnuovoSRR17", "title": "GitcProc: a tool for processing and classifying GitHub commits.", "authors": ["Casey Casalnuovo", "Yagnik Suchak", "Baishakhi Ray", "Cindy Rubio-Gonz\u00e1lez"], "DOIs": ["https://doi.org/10.1145/3092703.3098230"], "tag": ["Demonstrations"], "abstract": "ABSTRACT Sites such as GitHub have created a vast collection of software artifacts that researchers interested in understanding and improving software systems can use. Current tools for processing such GitHub data tend to target project metadata and avoid source code processing, or process source code in a manner that requires significant effort for each language supported. This paper presents GitcProc, a lightweight tool based on regular expressions and source code blocks, which downloads projects and extracts their project history, including fine-grained source code information and development time bug fixes. GitcProc can track changes to both single-line and block source code structures and associate these changes to the surrounding function context with minimal set up required from users. We demonstrate GitcProc's ability to capture changes in multiple languages by evaluating it on C, C++, Java, and Python projects, and show it finds bug fixes and the context of source code changes effectively with few false positives."}, {"id": "conf/issta/NurmuradovB17", "title": "Caret-HM: recording and replaying Android user sessions with heat map generation using UI state clustering.", "authors": ["Dmitry Nurmuradov", "Ren\u00e9e C. Bryce"], "DOIs": ["https://doi.org/10.1145/3092703.3098231"], "tag": ["Demonstrations"], "abstract": "ABSTRACT The Caret-HM framework allows Android developers to record and replay user sessions and convert them into heatmaps. One advantage of our framework over existing solutions is that it allows developers to control the environment while simplifying the recording process by giving users access to their applications via a web browser. The heatmap generation using Android user sessions and clustering UI states is a unique feature of our framework. Heat maps allow developers to identify the usage of application features for testing and guiding business decisions. We provide a qualitative comparison to the existing solutions. The video with demonstration is available at https://www.youtube.com/watch?v=eMSNAKM1Bj4"}, {"id": "conf/issta/Halle17", "title": "LabPal: repeatable computer experiments made easy.", "authors": ["Sylvain Hall\u00e9"], "DOIs": ["https://doi.org/10.1145/3092703.3098232"], "tag": ["Demonstrations"], "abstract": "ABSTRACT LabPal is a Java library designed to easily create and run experiments on a computer. It provides a user-friendly web console, support for automated plotting, a pause-resume facility, a mechanism for handling data traceability and linking, and the possibility of saving all experiment input and output data in an open format. These functionalities greatly reduce the amount of boilerplate scripting needed to run experiments on a computer, and simplify their re-execution by independent parties."}, {"id": "conf/issta/Bendik17", "title": "Consistency checking in requirements analysis.", "authors": ["Jaroslav Bend\u00edk"], "DOIs": ["https://doi.org/10.1145/3092703.3098239"], "tag": ["Analysis"], "abstract": "ABSTRACT In the last decade it became a common practise to formalise software requirements using a mathematical language of temporal logics, e.g., LTL. The formalisation removes ambiguity and improves understanding. Formal description also enables various model-based techniques, like formal verification. Moreover, we get the opportunity to check the requirements earlier, even before any system model is built. This so called requirements sanity checking aims to assure that a given set of requirements is consistent, i.e., that a product satisfying all the requirements can be developed. If inconsistencies are found, it is desirable to present them to the user in a minimal fashion, exposing the core problems among the requirements. Such cores are called minimal inconsistent subsets (MISes). In this work, we present a framework for online MISes enumeration in the domain of temporal logics."}, {"id": "conf/issta/Athaiya17", "title": "Testing and analysis of web applications using page models.", "authors": ["Snigdha Athaiya"], "DOIs": ["https://doi.org/10.1145/3092703.3098240"], "tag": ["Analysis"], "abstract": "ABSTRACT Web applications are difficult to analyze using code-based tools because data-flow and control-flow through the application occurs via both server-side code and client-side pages. Client-side pages are typically specified in a scripting language that is different from the main server-side language; moreover, the pages are generated dynamically from the scripts. To address these issues we propose a static-analysis approach that automatically constructs a \"model\" of each page in a given application. A page model is a code fragment in the same language as the server-side code, which faithfully over-approximates the possible elements of the page as well as the control-flows and data-flows due to these elements. The server-side code in conjunction with the page models then becomes a standard (non-web) program, thus amenable to analysis using standard code-based tools."}, {"id": "conf/issta/Brennan17", "title": "Path cost analysis for side channel detection.", "authors": ["Tegan Brennan"], "DOIs": ["https://doi.org/10.1145/3092703.3098242"], "tag": ["Analysis"], "abstract": "ABSTRACT Side-channels have been increasingly demonstrated as a practical threat to the confidentiality of private user information. Being able to statically detect these kinds of vulnerabilites is a key challenge in current computer security research. We introduce a new technique, path-cost analysis (PCA), for the detection of side-channels. Given a cost model for a type of side-channel, path-cost analysis assigns a symbolic cost expression to every node and every back edge of a method's control flow graph that gives an over-approximation for all possible observable values at that node or after traversing that cycle. Queries to a satisfiability solver on the maximum distance between specific pairs of nodes allow us to detect the presence of imbalanced paths through the control flow graph. When combined with taint analysis, we are able to answer the following question: does there exist a pair of paths in the method's control flow graph, differing only on branch conditions influenced by the secret, that differs in observable value by more than some given threshold? In fact, we are able to answer the specifically state what sets of secret-sensitive conditional statements introduce a side-channel detectable given some noise parameter. We extend this approach to an interprocedural analysis, resulting in a over-approximation of the number of true side-channels in the program according to the given cost model. Greater precision can be obtained by combining our method with predicate abstraction or symbolic execution to eliminate a subset of the infeasible paths through the control flow graph. We propose evaluating our method on a set of sizeable Java server-client applications."}, {"id": "conf/issta/Hotzkow17", "title": "Automatically inferring and enforcing user expectations.", "authors": ["Jenny Hotzkow"], "DOIs": ["https://doi.org/10.1145/3092703.3098236"], "tag": ["Modeling and Learning"], "abstract": "ABSTRACT Can we automatically learn how users expect an application to behave? Yes, if we consider an application from the users perspective. Whenever presented with an unfamiliar app, the user not only regards the context presented by this particular application, but rather considers previous experiences from other applications. This research presents an approach to reflect this procedure by automatically learning user expectations from the semantic contexts over multiple applications. Once the user expectations are established, this knowledge can be used as an oracle, to test if an application follows the user's expectations or entails surprising behavior by error or deliberately."}, {"id": "conf/issta/Katz17", "title": "Understanding intended behavior using models of low-level signals.", "authors": ["Deborah S. Katz"], "DOIs": ["https://doi.org/10.1145/3092703.3098237"], "tag": ["Modeling and Learning"], "abstract": "ABSTRACT As software systems increase in complexity and operate with less human supervision, it becomes more difficult to use traditional techniques to detect when software is not behaving as intended. Furthermore, many systems operating today are nondeterministic and operate in unpredictable environments, making it difficult to even define what constitutes correct behavior. I propose a family of novel techniques to model the behavior of executing programs using low-level signals collected during executions. The models provide a basis for predicting whether an execution of the program or program unit under test represents intended behavior. I have demonstrated success with these techniques for detecting faulty and unexpected behavior on small programs. I propose to extend the work to smaller units of large, complex programs."}, {"id": "conf/issta/Santolucito17", "title": "Version space learning for verification on temporal differentials.", "authors": ["Mark Santolucito"], "DOIs": ["https://doi.org/10.1145/3092703.3098238"], "tag": ["Modeling and Learning"], "abstract": "ABSTRACT Configuration files provide users with the ability to quickly alter the behavior of their software system. Ensuring that a configuration file does not induce errors in the software is a complex verification issue. The types of errors can be easy to measure, such as an initialization failure of system boot, or more insidious such as performance degrading over time under heavy network loads. In order to warn a user of potential configuration errors ahead of time, we propose using version space learning specifications for configuration languages. We frame an existing tool, ConfigC, in terms of version space learning. We extend that algorithm to leverage the temporal structuring available in training sets scraped from versioning control systems. We plan to evaluate our system on a case study using TravisCI configuration files collected from Github."}, {"id": "conf/issta/Borges17", "title": "Data flow oriented UI testing: exploiting data flows and UI elements to test Android applications.", "authors": ["Nataniel P. Borges Jr."], "DOIs": ["https://doi.org/10.1145/3092703.3098234"], "tag": ["Testing"], "abstract": "ABSTRACT Testing user interfaces (UIs) is a challenging task. Ideally, every sequence of UI elements should be tested to guarantee that the application works correctly. This is, however, unfeasible due to the number of UI elements in an application. A better approach is to limit the evaluation to UI elements that affect a specific functionality. In this paper I present a novel technique to identify the relation between UI elements using the statically extracted data flows. I also present a method to refine these relations using dynamic analysis, in order to ensure that relations extracted from unreachable data flows are removed. Using these relations it is possible to more efficiently test a functionality. Finally, I present an approach to evaluate how these UI-aware data flows can be used as an heuristic to measure test coverage."}, {"id": "conf/issta/Mathis17", "title": "Dynamic tainting for automatic test case generation.", "authors": ["Bj\u00f6rn Mathis"], "DOIs": ["https://doi.org/10.1145/3092703.3098233"], "tag": ["Testing"], "abstract": "ABSTRACT Dynamic tainting is an important part of modern software engineering research. State-of-the-art tools for debugging, bug detection and program analysis make use of this technique. Nonetheless, the research area based on dynamic tainting still has open questions, among others the automatic generation of program inputs.  My proposed work concentrates on the use of dynamic tainting for test case generation. The goal is the generation of complex and valid test inputs from scratch. Therefore, I use byte level taint information enhanced with additional static and dynamic program analysis. This information is used in an evolutionary algorithm to create new offsprings and mutations. Concretely, instead of crossing and mutating the whole input randomly, taint information can be used to define which parts of the input have to be mutated. Furthermore, the taint information may also be used to define evolutionary operators.  Eventually, the evolutionary algorithm is able to generate valid inputs for a program. Such inputs can be used together with the taint information for further program analysis, e.g. the generation of input grammars."}, {"id": "conf/issta/Oliveira17", "title": "Mapping hardness of automated software testing.", "authors": ["Carlos Oliveira"], "DOIs": ["https://doi.org/10.1145/3092703.3098241"], "tag": ["Testing"], "abstract": "ABSTRACT Automated Test Case Generation (ATCG) is an important topic in Software Testing, with a wide range of techniques and tools being used in academia and industry. While their usefulness is widely recognized, due to the labor-intensive nature of the task, the effectiveness of the different techniques in automatically generating test cases for different software systems is not thoroughly understood. Despite many studies introducing various ATCG techniques, much remains to be learned, however, about what makes a particular technique work well (or not) for a specific software system. Therefore, we propose a new methodology to evaluate and select the most effective ATCG technique using structure-based complexity measures. Empirical tests are going to be performed using two different techniques: Search-based Software Testing (SBST) and Random Testing (RT)."}, {"id": "conf/issta/Jahangirova17", "title": "Oracle problem in software testing.", "authors": ["Gunel Jahangirova"], "DOIs": ["https://doi.org/10.1145/3092703.3098235"], "tag": ["Testing"], "abstract": "ABSTRACT The oracle problem remains one of the key challenges in software testing, for which little automated support has been developed so far. In my thesis work we introduce a technique for assessing and improving test oracles by reducing the incidence of both false positives and false negatives. Our technique combines test case generation to reveal false positives and mutation testing to reveal false negatives. The experimental results on five real-world subjects show that the fault detection rate of the oracles after improvement increases, on average, by 48.6% (86% over the implicit oracle). Three actual, exposed faults in the studied systems were subsequently confirmed and fixed by the developers. However, our technique contains a human in the loop, which was represented only by the author during the initial experiments. Our next goal is to conduct further experiments where the human in the loop will be represented by real developers. Our second future goal is to address the oracle placement problem. When testing software, developers can place oracles externally or internally to a method. Given a faulty execution state, i.e., one that differs from the expected one, an oracle might be unable to expose the fault if it is placed at a program point with no access to the incorrect program state or where the program state is no longer corrupted. In such a case, the oracle is subject to failed error propagation. Internal oracles are in principle less subject to failed error propagation than external oracles. However, they are also more difficult to define manually. Hence, a key research question is whether a more intrusive oracle placement is justified by its higher fault detection capability."}], "2018": [{"id": "conf/issta/KhurshidPV18", "title": "Test input generation with Java PathFinder: then and now (invited talk abstract).", "authors": ["Sarfraz Khurshid", "Corina S. Pasareanu", "Willem Visser"], "DOIs": ["https://doi.org/10.1145/3213846.3234687"], "tag": ["ISSTA 2018 Retrospective Impact Paper Award"], "abstract": "ABSTRACTThe paper Test Input Generation With Java PathFinder was published in the International Symposium on Software Testing and Analysis (ISSTA) 2004 Proceedings, and has now been selected to receive the ISSTA 2018 Retrospective Impact Paper Award. The paper described black-box and white-box techniques for the automated testing of software systems. These techniques were based on model checking and symbolic execution and incorporated in the Java PathFinder analysis tool. The main contribution of the paper was to describe how to perform efficient test input generation for code manipulating complex data that takes into account complex method preconditions and evaluate the techniques for generating high coverage tests.  We review the original paper and we discuss the research that preceded it and the research that has happened between then (2004) and now (2018) in the context of the Java PathFinder tool, its symbolic execution component that is now called Symbolic PathFinder, and closely related approaches that target testing of software that manipulates complex data structures. We close with directions for future work."}, {"id": "conf/issta/KelloggDME18", "title": "Lightweight verification of array indexing.", "authors": ["Martin Kellogg", "Vlastimil Dort", "Suzanne Millstein", "Michael D. Ernst"], "DOIs": ["https://doi.org/10.1145/3213846.3213849"], "tag": ["Secure and Sound"], "abstract": "ABSTRACTIn languages like C, out-of-bounds array accesses lead to security vulnerabilities and crashes. Even in managed languages like Java, which check array bounds at run time, out-of-bounds accesses cause exceptions that terminate the program.  We present a lightweight type system that certifies, at compile time, that array accesses in the program are in-bounds. The type system consists of several cooperating hierarchies of dependent types, specialized to the domain of array bounds-checking. Programmers write type annotations at procedure boundaries, allowing modular verification at a cost that scales linearly with program size.  We implemented our type system for Java in a tool called the Index Checker. We evaluated the Index Checker on over 100,000 lines of open-source code and discovered array access errors even in well-tested, industrial projects such as Google Guava."}, {"id": "conf/issta/WuGS018", "title": "Eliminating timing side-channel leaks using program repair.", "authors": ["Meng Wu", "Shengjian Guo", "Patrick Schaumont", "Chao Wang"], "DOIs": ["https://doi.org/10.1145/3213846.3213851"], "tag": ["Secure and Sound"], "abstract": "ABSTRACTWe propose a method, based on program analysis and transformation, for eliminating timing side channels in software code that implements security-critical applications. Our method takes as input the original program together with a list of secret variables (e.g., cryptographic keys, security tokens, or passwords) and returns the transformed program as output. The transformed program is guaranteed to be functionally equivalent to the original program and free of both instruction- and cache-timing side channels. Specifically, we ensure that the number of CPU cycles taken to execute any path is independent of the secret data, and the cache behavior of memory accesses, in terms of hits and misses, is independent of the secret data. We have implemented our method in LLVM and validated its effectiveness on a large set of applications, which are cryptographic libraries with 19,708 lines of C/C++ code in total. Our experiments show the method is both scalable for real applications and effective in eliminating timing side channels."}, {"id": "conf/issta/BrennanSBP18", "title": "Symbolic path cost analysis for side-channel detection.", "authors": ["Tegan Brennan", "Seemanta Saha", "Tevfik Bultan", "Corina S. Pasareanu"], "DOIs": ["https://doi.org/10.1145/3213846.3213867"], "tag": ["Secure and Sound"], "abstract": "ABSTRACTSide-channels in software are an increasingly significant threat to the confidentiality of private user information, and the static detection of such vulnerabilities is a key challenge in secure software development. In this paper, we introduce a new technique for scalable detection of side- channels in software. Given a program and a cost model for a side-channel (such as time or memory usage), we decompose the control flow graph of the program into nested branch and loop components, and compositionally assign a symbolic cost expression to each component. Symbolic cost expressions provide an over-approximation of all possible observable cost values that components can generate. Queries to a satisfiability solver on the difference between possible cost values of a component allow us to detect the presence of imbalanced paths (with respect to observable cost) through the control flow graph. When combined with taint analysis that identifies conditional statements that depend on secret information, our technique answers the following question: Does there exist a pair of paths in the program's control flow graph, differing only on branch conditions influenced by the secret, that differ in observable side-channel value by more than some given threshold? Additional optimization queries allow us to identify the minimal number of loop iterations necessary for the above to hold or the maximal cost difference between paths in the graph. We perform symbolic execution based feasibility analyses to eliminate control flow paths that are infeasible. We implemented our techniques in a prototype, and we demonstrate its favourable performance against state-of-the-art tools as well as its effectiveness and scalability on a set of sizable, realistic Java server-client and peer-to-peer applications."}, {"id": "conf/issta/MadsenL18", "title": "Safe and sound program analysis with Flix.", "authors": ["Magnus Madsen", "Ondrej Lhot\u00e1k"], "DOIs": ["https://doi.org/10.1145/3213846.3213847"], "tag": ["Secure and Sound"], "abstract": "ABSTRACTProgram development tools such as bug finders, build automation tools, compilers, debuggers, integrated development environments, and refactoring tools increasingly rely on static analysis techniques to reason about program behavior. Implementing such static analysis tools is a complex and difficult task with concerns about safety and soundness. Safety guarantees that the fixed point computation -- inherent in most static analyses -- converges and ultimately terminates with a deterministic result. Soundness guarantees that the computed result over-approximates the concrete behavior of the program under analysis. But how do we know if we can trust the result of the static analysis itself? Who will guard the guards?  In this paper, we propose the use of automatic program verification techniques based on symbolic execution and SMT solvers to verify the correctness of the abstract domains used in static analysis tools. We implement a verification toolchain for Flix, a functional and logic programming language tailored for the implementation of static analyses. We apply this toolchain to several abstract domains. The experimental results show that we are able to prove 99.5% and 96.3% of the required safety and soundness properties, respectively."}, {"id": "conf/issta/ShinNSBZ18", "title": "Test case prioritization for acceptance testing of cyber physical systems: a multi-objective search-based approach.", "authors": ["Seung Yeob Shin", "Shiva Nejati", "Mehrdad Sabetzadeh", "Lionel C. Briand", "Frank Zimmer"], "DOIs": ["https://doi.org/10.1145/3213846.3213852"], "tag": ["Testing and Fault Localization"], "abstract": "ABSTRACTAcceptance testing validates that a system meets its requirements and determines whether it can be sufficiently trusted and put into operation. For cyber physical systems (CPS), acceptance testing is a hardware-in-the-loop process conducted in a (near-)operational environment. Acceptance testing of a CPS often necessitates that the test cases be prioritized, as there are usually too many scenarios to consider given time constraints. CPS acceptance testing is further complicated by the uncertainty in the environment and the impact of testing on hardware. We propose an automated test case prioritization approach for CPS acceptance testing, accounting for time budget constraints, uncertainty, and hardware damage risks. Our approach is based on multi-objective search, combined with a test case minimization algorithm that eliminates redundant operations from an ordered sequence of test cases. We evaluate our approach on a representative case study from the satellite domain. The results indicate that, compared to test cases that are prioritized manually by satellite engineers, our automated approach more than doubles the number of test cases that fit into a given time frame, while reducing to less than one third the number of operations that entail the risk of damage to key hardware components."}, {"id": "conf/issta/LeeKBJT18", "title": "Bench4BL: reproducibility study on the performance of IR-based bug localization.", "authors": ["Jaekwon Lee", "Dongsun Kim", "Tegawend\u00e9 F. Bissyand\u00e9", "Woosung Jung", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3213846.3213856"], "tag": ["Testing and Fault Localization"], "abstract": "ABSTRACTIn recent years, the use of Information Retrieval (IR) techniques to automate the localization of buggy files, given a bug report, has shown promising results. The abundance of approaches in the literature, however, contrasts with the reality of IR-based bug localization (IRBL) adoption by developers (or even by the research community to complement other research approaches). Presumably, this situation is due to the lack of comprehensive evaluations for state-of-the-art approaches which offer insights into the actual performance of the techniques.  We report on a comprehensive reproduction study of six state-of-the-art IRBL techniques. This study applies not only subjects used in existing studies (old subjects) but also 46 new subjects (61,431 Java files and 9,459 bug reports) to the IRBL techniques. In addition, the study compares two different version matching (between bug reports and source code files) strategies to highlight some observations related to performance deterioration. We also vary test file inclusion to investigate the effectiveness of IRBL techniques on test files, or its noise impact on performance. Finally, we assess potential performance gain if duplicate bug reports are leveraged."}, {"id": "conf/issta/StrandbergOWSA18", "title": "Automated test mapping and coverage for network topologies.", "authors": ["Per Erik Strandberg", "Thomas J. Ostrand", "Elaine J. Weyuker", "Daniel Sundmark", "Wasif Afzal"], "DOIs": ["https://doi.org/10.1145/3213846.3213859"], "tag": ["Testing and Fault Localization"], "abstract": "ABSTRACTCommunication devices such as routers and switches play a critical role in the reliable functioning of embedded system networks. Dozens of such devices may be part of an embedded system network, and they need to be tested in conjunction with various computational elements on actual hardware, in many different configurations that are representative of actual operating networks. An individual physical network topology can be used as the basis for a test system that can execute many test cases, by identifying the part of the physical network topology that corresponds to the configuration required by each individual test case. Given a set of available test systems and a large number of test cases, the problem is to determine for each test case, which of the test systems are suitable for executing the test case, and to provide the mapping that associates the test case elements (the logical network topology) with the appropriate elements of the test system (the physical network topology).  We studied a real industrial environment where this problem was originally handled by a simple software procedure that was very slow in many cases, and also failed to provide thorough coverage of each network's elements. In this paper, we represent both the test systems and the test cases as graphs, and develop a new prototype algorithm that a) determines whether or not a test case can be mapped to a subgraph of the test system, b) rapidly finds mappings that do exist, and c) exercises diverse sets of network nodes when multiple mappings exist for the test case. The prototype has been implemented and applied to over 10,000 combinations of test cases and test systems, and reduced the computation time by a factor of more than 80 from the original procedure. In addition, relative to a meaningful measure of network topology coverage, the mappings achieved an increased level of thoroughness in exercising the elements of each test system."}, {"id": "conf/issta/ShiGMZM18", "title": "Evaluating test-suite reduction in real software evolution.", "authors": ["August Shi", "Alex Gyori", "Suleman Mahmood", "Peiyuan Zhao", "Darko Marinov"], "DOIs": ["https://doi.org/10.1145/3213846.3213875"], "tag": ["Testing and Fault Localization"], "abstract": "ABSTRACTTest-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.  We perform the first extensive study of TSR using real test failures in (failed) builds that occurred for real code changes. We analyze 1478 failed builds from 32 GitHub projects that run their tests on Travis. Each failed build can have multiple faults, so we propose a family of mappings from test failures to faults. We use these mappings to compute Failed-Build Detection Loss (FBDL), the percentage of failed builds where the reduced test suite misses to detect all the faults detected by the original test suite. We find that FBDL can be up to 52.2%, which is higher than suggested by traditional TSR metrics. Moreover, traditional TSR metrics are not good predictors of FBDL, making it difficult for developers to decide whether to use reduced test suites."}, {"id": "conf/issta/CumminsPML18", "title": "Compiler fuzzing through deep learning.", "authors": ["Chris Cummins", "Pavlos Petoumenos", "Alastair Murray", "Hugh Leather"], "DOIs": ["https://doi.org/10.1145/3213846.3213848"], "tag": ["Machine Learning"], "abstract": "ABSTRACTRandom program generation \u2014 fuzzing \u2014 is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03\u00d7 less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing."}, {"id": "conf/issta/LeL18", "title": "Deep specification mining.", "authors": ["Tien-Duy B. Le", "David Lo"], "DOIs": ["https://doi.org/10.1145/3213846.3213876"], "tag": ["Machine Learning"], "abstract": "ABSTRACTFormal specifcations are essential but usually unavailable in software systems. Furthermore, writing these specifcations is costly and requires skills from developers. Recently, many automated techniques have been proposed to mine specifcations in various formats including fnite-state automaton (FSA). However, more works in specifcation mining are needed to further improve the accuracy of the inferred specifcations. In this work, we propose Deep Specifcation Miner (DSM), a new approach that performs deep learning for mining FSA-based specifcations. Our proposed approach uses test case generation to generate a richer set of execution traces for training a Recurrent Neural Network Based Language Model (RNNLM). From these execution traces, we construct a Prefx Tree Acceptor (PTA) and use the learned RNNLM to extract many features. These features are subsequently utilized by clustering algorithms to merge similar automata states in the PTA for constructing a number of FSAs. Then, our approach performs a model selection heuristic to estimate F-measure of FSAs and returns the one with the highest estimated Fmeasure. We execute DSM to mine specifcations of 11 target library classes. Our empirical analysis shows that DSM achieves an average F-measure of 71.97%, outperforming the best performing baseline by 28.22%. We also demonstrate the value of DSM in sandboxing Android apps."}, {"id": "conf/issta/DwarakanathASRB18", "title": "Identifying implementation bugs in machine learning based image classifiers using metamorphic testing.", "authors": ["Anurag Dwarakanath", "Manish Ahuja", "Samarth Sikand", "Raghotham M. Rao", "R. P. Jagadeesh Chandra Bose", "Neville Dubash", "Sanjay Podder"], "DOIs": ["https://doi.org/10.1145/3213846.3213858"], "tag": ["Machine Learning"], "abstract": "ABSTRACTWe have recently witnessed tremendous success of Machine Learning (ML) in practical applications. Computer vision, speech recognition and language translation have all seen a near human level performance. We expect, in the near future, most business applications will have some form of ML. However, testing such applications is extremely challenging and would be very expensive if we follow today's methodologies. In this work, we present an articulation of the challenges in testing ML based applications. We then present our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based application. Empirical validation showed that our approach was able to catch 71% of the implementation bugs in the ML applications."}, {"id": "conf/issta/ZhangCCXZ18", "title": "An empirical study on TensorFlow program bugs.", "authors": ["Yuhao Zhang", "Yifan Chen", "Shing-Chi Cheung", "Yingfei Xiong", "Lu Zhang"], "DOIs": ["https://doi.org/10.1145/3213846.3213866"], "tag": ["Machine Learning"], "abstract": "ABSTRACTDeep learning applications become increasingly popular in important domains such as self-driving systems and facial identity systems. Defective deep learning applications may lead to catastrophic consequences. Although recent research efforts were made on testing and debugging deep learning applications, the characteristics of deep learning defects have never been studied. To fill this gap, we studied deep learning applications built on top of TensorFlow and collected program bugs related to TensorFlow from StackOverflow QA pages and Github projects. We extracted information from QA pages, commit messages, pull request messages, and issue discussions to examine the root causes and symptoms of these bugs. We also studied the strategies deployed by TensorFlow users for bug detection and localization. These findings help researchers and TensorFlow users to gain a better understanding of coding defects in TensorFlow programs and point out a new direction for future research."}, {"id": "conf/issta/FazziniPdO18", "title": "Automatically translating bug reports into test cases for mobile apps.", "authors": ["Mattia Fazzini", "Martin Prammer", "Marcelo d'Amorim", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1145/3213846.3213869"], "tag": ["Mobile"], "abstract": "ABSTRACTWhen users experience a software failure, they have the option of submitting a bug report and provide information about the failure and how it happened. If the bug report contains enough information, developers can then try to recreate the issue and investigate it, so as to eliminate its causes. Unfortunately, the number of bug reports filed by users is typically large, and the tasks of analyzing bug reports and reproducing the issues described therein can be extremely time consuming. To help make this process more efficient, in this paper we propose Yakusu, a technique that uses a combination of program analysis and natural language processing techniques to generate executable test cases from bug reports. We implemented Yakusu for Android apps and performed an empirical evaluation on a set of over 60 real bug reports for different real-world apps. Overall, our technique was successful in 59.7% of the cases; that is, for a majority of the bug reports, developers would not have to study the report to reproduce the issue described and could simply use the test cases automatically generated by Yakusu. Furthermore, in many of the remaining cases, Yakusu was unsuccessful due to limitations that can be addressed in future work."}, {"id": "conf/issta/0029BWK18", "title": "CiD: automating the detection of API-related compatibility issues in Android apps.", "authors": ["Li Li", "Tegawend\u00e9 F. Bissyand\u00e9", "Haoyu Wang", "Jacques Klein"], "DOIs": ["https://doi.org/10.1145/3213846.3213857"], "tag": ["Mobile"], "abstract": "ABSTRACTThe Android Application Programming Interface provides the necessary building blocks for app developers to harness the functionalities of the Android devices, including for interacting with services and accessing hardware. This API thus evolves rapidly to meet new requirements for security, performance and advanced features, creating a race for developers to update apps. Unfortunately, given the extent of the API and the lack of automated alerts on important changes, Android apps are suffered from API-related compatibility issues. These issues can manifest themselves as runtime crashes creating a poor user experience. We propose in this paper an automated approach named CiD for systematically modelling the lifecycle of the Android APIs and analysing app bytecode to flag usages that can lead to potential compatibility issues. We demonstrate the usefulness of CiD by helping developers repair their apps, and we validate that our tool outperforms the state-of-the-art on benchmark apps that take into account several challenges for automatic detection."}, {"id": "conf/issta/BehrangO18", "title": "Test migration for efficient large-scale assessment of mobile app coding assignments.", "authors": ["Farnaz Behrang", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1145/3213846.3213854"], "tag": ["Mobile"], "abstract": "ABSTRACTIn recent years, there has been a growing interest in making education widely accessible using Internet technologies. Whether it is Massive Open Online Courses (MOOCs) or simply college courses offered to a large student population using an online platform, both education-focused companies and universities, often in collaboration with one another, have been investing massively in online education. The fact that hundreds, and more often thousands, of students take these online courses raises scalability challenges in assessing student assignments. In this paper, in particular, we present a technique (GUITestMigrator) that addresses the challenge of assessing mobile app coding assignments. Given a set of apps that implement the same specification, but can have completely different user interfaces, instructors normally have to manually run and check each app to make sure it behaves correctly and according to the specification. GUITestMigrator, conversely, allows for developing tests for one of these apps and automatically migrating these tests to the other apps, thus dramatically reducing the burden on the instructor. We implemented GUITestMigrator for Android apps and evaluated it on three sets of apps developed over three different semesters by students of an online graduate-level software engineering course. Our initial results show that our approach is promising and motivates further research in this direction. The paper also discusses possible applications of this approach for test evolution and test migration for real-world apps."}, {"id": "conf/issta/QiuWR18", "title": "Analyzing the analyzers: FlowDroid/IccTA, AmanDroid, and DroidSafe.", "authors": ["Lina Qiu", "Yingying Wang", "Julia Rubin"], "DOIs": ["https://doi.org/10.1145/3213846.3213873"], "tag": ["Mobile"], "abstract": "ABSTRACTNumerous static analysis techniques have recently been proposed for identifying information flows in mobile applications. These techniques are compared to each other, usually on a set of syntactic benchmarks. Yet, configurations used for such comparisons are rarely described. Our experience shows that tools are often compared under different setup, rendering the comparisons irreproducible and largely inaccurate. In this paper, we provide a large, controlled, and independent comparison of the three most prominent static analysis tools: FlowDroid combined with IccTA, Amandroid, and DroidSafe. We evaluate all tools using common configuration setup and the same set of benchmark applications. We compare the results of our analysis to the results reported in previous studies, identify main reasons for inaccuracy in existing tools, and provide suggestions for future research."}, {"id": "conf/issta/MuskeTS18", "title": "Repositioning of static analysis alarms.", "authors": ["Tukaram Muske", "Rohith Talluri", "Alexander Serebrenik"], "DOIs": ["https://doi.org/10.1145/3213846.3213850"], "tag": ["Static Analysis"], "abstract": "ABSTRACTThe large number of alarms reported by static analysis tools is often recognized as one of the major obstacles to industrial adoption of such tools.  We present repositioning of alarms, a novel automatic postprocessing technique intended to reduce the number of reported alarms without affecting the errors uncovered by them. The reduction in the number of alarms is achieved by moving groups of related alarms along the control flow to a program point where they can be replaced by a single alarm. In the repositioning technique, as the locations of repositioned alarms are different than locations of the errors uncovered by them, we also maintain traceability links between a repositioned alarm and its corresponding original alarm(s). The presented technique is tool-agnostic and orthogonal to many other techniques available for postprocessing alarms.  To evaluate the technique, we applied it as a postprocessing step to alarms generated for 4 verification properties on 16 open source and 4 industry applications. The results indicate that the alarms repositioning technique reduces the alarms count by up to 20% over the state-of-the-art alarms grouping techniques with a median reduction of 7.25%."}, {"id": "conf/issta/GrechFFS18", "title": "Shooting from the heap: ultra-scalable static analysis with heap snapshots.", "authors": ["Neville Grech", "George Fourtounis", "Adrian Francalanza", "Yannis Smaragdakis"], "DOIs": ["https://doi.org/10.1145/3213846.3213860"], "tag": ["Static Analysis"], "abstract": "ABSTRACTTraditional whole-program static analysis (e.g., a points-to analysis that models the heap) encounters scalability problems for realistic applications. We propose a ``featherweight'' analysis that combines a dynamic snapshot of the heap with otherwise full static analysis of program behavior.  The analysis is extremely scalable, offering speedups of well over 3x, with complexity empirically evaluated to grow linearly relative to the number of reachable methods. The analysis is also an excellent tradeoff of precision and recall (relative to different dynamic executions): while it can never fully capture all program behaviors (i.e., it cannot match the near-perfect recall of a full static analysis) it often approaches it closely while achieving much higher (3.5x) precision."}, {"id": "conf/issta/FourtounisKS18", "title": "Static analysis of Java dynamic proxies.", "authors": ["George Fourtounis", "George Kastrinis", "Yannis Smaragdakis"], "DOIs": ["https://doi.org/10.1145/3213846.3213864"], "tag": ["Static Analysis"], "abstract": "ABSTRACTThe dynamic proxy API is one of Java\u2019s most widely-used dynamic features, permitting principled run-time code generation and link- ing. Dynamic proxies can implement any set of interfaces and for- ward method calls to a special object that handles them reflectively. The flexibility of dynamic proxies, however, comes at the cost of having a dynamically generated layer of bytecode that cannot be penetrated by current static analyses. In this paper, we observe that the dynamic proxy API is stylized enough to permit static analysis. We show how the semantics of dynamic proxies can be modeled in a straightforward manner as logical rules in the Doop static analysis framework. This concise set of rules enables Doop\u2019s standard analyses to process code behind dynamic proxies. We evaluate our approach by analyzing XCorpus, a corpus of real-world Java programs: we fully handle 95% of its reported proxy creation sites. Our handling results in the analysis of significant portions of previously unreachable or incompletely- modeled code."}, {"id": "conf/issta/Blaser18", "title": "Practical detection of concurrency issues at coding time.", "authors": ["Luc Bl\u00e4ser"], "DOIs": ["https://doi.org/10.1145/3213846.3213853"], "tag": ["Static Analysis"], "abstract": "ABSTRACTWe have developed a practical static checker that is designed to interactively mark data races and deadlocks in program source code at development time. As this use case requires a checker to be both fast and precise, we engaged a simple technique of randomized bounded concrete concurrent interpretation that is experimentally effective for this purpose. Implemented as a tool for C# in Visual Studio, the checker covers the broad spectrum of concurrent language concepts, including task and data parallelism, asynchronous programming, UI dispatching, the various synchronization primitives, monitor, atomic and volatile accesses, and finalizers. Its application to popular open-source C# projects revealed several real issues with only a few false positives."}, {"id": "conf/issta/KrikavaV18", "title": "Tests from traces: automated unit test extraction for R.", "authors": ["Filip Krikava", "Jan Vitek"], "DOIs": ["https://doi.org/10.1145/3213846.3213863"], "tag": ["Test and Oracle Generation"], "abstract": "ABSTRACTUnit tests are labor-intensive to write and maintain. This paper looks into how well unit tests for a target software package can be extracted from the execution traces of client code. Our objective is to reduce the effort involved in creating test suites while minimizing the number and size of individual tests, and maximizing coverage. To evaluate the viability of our approach, we select a challenging target for automated test extraction, namely R, a programming language that is popular for data science applications. The challenges presented by R are its extreme dynamism, coerciveness, and lack of types. This combination decrease the efficacy of traditional test extraction techniques. We present Genthat, a tool developed over the last couple of years to non-invasively record execution traces of R programs and extract unit tests from those traces. We have carried out an evaluation on 1,545 packages comprising 1.7M lines of R code. The tests extracted by Genthat improved code coverage from the original rather low value of 267,496 lines to 700,918 lines. The running time of the generated tests is 1.9 times faster than the code they came from"}, {"id": "conf/issta/BlasiGKGEPC18", "title": "Translating code comments to procedure specifications.", "authors": ["Arianna Blasi", "Alberto Goffi", "Konstantin Kuznetsov", "Alessandra Gorla", "Michael D. Ernst", "Mauro Pezz\u00e8", "Sergio Delgado Castellanos"], "DOIs": ["https://doi.org/10.1145/3213846.3213872"], "tag": ["Test and Oracle Generation"], "abstract": "ABSTRACTProcedure specifications are useful in many software development tasks. As one example, in automatic test case generation they can guide testing, act as test oracles able to reveal bugs, and identify illegal inputs. Whereas formal specifications are seldom available in practice, it is standard practice for developers to document their code with semi-structured comments. These comments express the procedure specification with a mix of predefined tags and natural language. This paper presents Jdoctor, an approach that combines pattern, lexical, and semantic matching to translate Javadoc comments into executable procedure specifications written as Java expressions. In an empirical evaluation, Jdoctor achieved precision of 92% and recall of 83% in translating Javadoc into procedure specifications. We also supplied the Jdoctor-derived specifications to an automated test case generation tool, Randoop. The specifications enabled Randoop to generate test cases of higher quality."}, {"id": "conf/issta/LemieuxPSS18", "title": "PerfFuzz: automatically generating pathological inputs.", "authors": ["Caroline Lemieux", "Rohan Padhye", "Koushik Sen", "Dawn Song"], "DOIs": ["https://doi.org/10.1145/3213846.3213874"], "tag": ["Test and Oracle Generation"], "abstract": "ABSTRACTPerformance problems in software can arise unexpectedly when programs are provided with inputs that exhibit worst-case behavior. A large body of work has focused on diagnosing such problems via statistical profiling techniques. But how does one find these inputs in the first place? We present PerfFuzz, a method to automatically generate inputs that exercise pathological behavior across program locations, without any domain knowledge. PerfFuzz generates inputs via feedback-directed mutational fuzzing. Unlike previous approaches that attempt to maximize only a scalar characteristic such as the total execution path length, PerfFuzz uses multi-dimensional feedback and independently maximizes execution counts for all program locations. This enables PerfFuzz to (1) find a variety of inputs that exercise distinct hot spots in a program and (2) generate inputs with higher total execution path length than previous approaches by escaping local maxima. PerfFuzz is also effective at generating inputs that demonstrate algorithmic complexity vulnerabilities. We implement PerfFuzz on top of AFL, a popular coverage-guided fuzzing tool, and evaluate PerfFuzz on four real-world C programs typically used in the fuzzing literature. We find that PerfFuzz outperforms prior work by generating inputs that exercise the most-hit program branch 5x to 69x times more, and result in 1.9x to 24.7x longer total execution paths."}, {"id": "conf/issta/AlmasiHFMB18", "title": "Search-based detection of deviation failures in the migration of legacy spreadsheet applications.", "authors": ["Mohammad Moein Almasi", "Hadi Hemmati", "Gordon Fraser", "Phil McMinn", "Janis Benefelds"], "DOIs": ["https://doi.org/10.1145/3213846.3213861"], "tag": ["Porting and Repair"], "abstract": "ABSTRACTMany legacy financial applications exist as a collection of formulas implemented in spreadsheets. Migration of these spreadsheets to a full-fledged system, written in a language such as Java, is an error- prone process. While small differences in the outputs of numerical calculations from the two systems are inevitable and tolerable, large discrepancies can have serious financial implications. Such discrepancies are likely due to faults in the migrated implementation, and are referred to as deviation failures. In this paper, we present a search-based technique that seeks to reveal deviation failures automatically. We evaluate different variants of this approach on two financial applications involving 40 formulas. These applications were produced by SEB Life & Pension Holding AB, who migrated their Microsoft Excel spreadsheets to a Java application. While traditional random and branch coverage-based test generation techniques were only able to detect approximately 25% and 32% of known faults in the migrated code respectively, our search-based approach detected up to 70% of faults with the same test generation budget. Without restriction of the search budget, up to 90% of known deviation failures were detected. In addition, three previously unknown faults were detected by this method that were confirmed by SEB experts."}, {"id": "conf/issta/KhazemBH18", "title": "Making data-driven porting decisions with Tuscan.", "authors": ["Kareem Khazem", "Earl T. Barr", "Petr Hosek"], "DOIs": ["https://doi.org/10.1145/3213846.3213855"], "tag": ["Porting and Repair"], "abstract": "ABSTRACTSoftware typically outlives the platform that it was originally written for. To smooth the transition to new tools and platforms, programs should depend on the underlying platform as little as possible. In practice, however, software build processes are highly sensitive to their build platform, notably the implementation of the compiler and standard library. This makes it difficult to port existing, mature software to emerging platforms---web based runtimes like WebAssembly, resource-constrained environments for Internet-of-Things devices, or innovative new operating systems like Fuchsia.  We present Tuscan, a framework for conducting automatic, deterministic, reproducible tests on build systems. Tuscan is the first framework to solve the problem of reproducibly testing builds cross-platform at massive scale. We also wrote a build wrapper, Red, which hijacks builds to tolerate common failures that arise from platform dependence, allowing the test harness to discover errors later in the build. Authors of innovative platforms can use Tuscan and Red to test the extent of unportability in the software ecosystem, and to quantify the effort necessary to port legacy software.  We evaluated Tuscan by building an operating system distribution, consisting of 2,699 Red-wrapped programs, on four platforms, yielding a `catalog' of the most common portability errors. This catalog informs data-driven porting decisions and motivates changes to programs, build systems, and language standards; systematically quantifies problems that platform writers have hitherto discovered only on an ad-hoc basis; and forms the basis for a common substrate of portability fixes that developers can apply to their software."}, {"id": "conf/issta/JustPDE18", "title": "Comparing developer-provided to user-provided tests for fault localization and automated program repair.", "authors": ["Ren\u00e9 Just", "Chris Parnin", "Ian Drosos", "Michael D. Ernst"], "DOIs": ["https://doi.org/10.1145/3213846.3213870"], "tag": ["Porting and Repair"], "abstract": "ABSTRACTTo realistically evaluate a software testing or debugging technique, it must be run on defects and tests that are characteristic of those a developer would encounter in practice. For example, to determine the utility of a fault localization or automated program repair technique, it could be run on real defects from a bug tracking system, using real tests that are committed to the version control repository along with the fixes. Although such a methodology uses real tests, it may not use tests that are characteristic of the information a developer or tool would have in practice. The tests that a developer commits after fixing a defect may encode more information than was available to the developer when initially diagnosing the defect.  This paper compares, both quantitatively and qualitatively, the developer-provided tests committed along with fixes (as found in the version control repository) versus the user-provided tests extracted from bug reports (as found in the issue tracker). It provides evidence that developer-provided tests are more targeted toward the defect and encode more information than user-provided tests. For fault localization, developer-provided tests overestimate a technique\u2019s ability to rank a defective statement in the list of the top-n most suspicious statements. For automated program repair, developer-provided tests overestimate a technique\u2019s ability to (efficiently) generate correct patches\u2014user-provided tests lead to fewer correct patches and increased repair time. This paper also provides suggestions for improving the design and evaluation of fault localization and automated program repair techniques."}, {"id": "conf/issta/JiangXZGC18", "title": "Shaping program repair space with existing patches and similar code.", "authors": ["Jiajun Jiang", "Yingfei Xiong", "Hongyu Zhang", "Qing Gao", "Xiangqun Chen"], "DOIs": ["https://doi.org/10.1145/3213846.3213871"], "tag": ["Porting and Repair"], "abstract": "ABSTRACTAutomated program repair (APR) has great potential to reduce bug-fixing effort and many approaches have been proposed in recent years. APRs are often treated as a search problem where the search space consists of all the possible patches and the goal is to identify the correct patch in the space. Many techniques take a data-driven approach and analyze data sources such as existing patches and similar source code to help identify the correct patch. However, while existing patches and similar code provide complementary information, existing techniques analyze only a single source and cannot be easily extended to analyze both.  In this paper, we propose a novel automatic program repair approach that utilizes both existing patches and similar code. Our approach mines an abstract search space from existing patches and obtains a concrete search space by differencing with similar code snippets. Then we search within the intersection of the two search spaces. We have implemented our approach as a tool called SimFix, and evaluated it on the Defects4J benchmark. Our tool successfully fixed 34 bugs. To our best knowledge, this is the largest number of bugs fixed by a single technology on the Defects4J benchmark. Furthermore, as far as we know, 13 bugs fixed by our approach have never been fixed by the current approaches."}, {"id": "conf/issta/LyuLH18", "title": "Remove RATs from your code: automated optimization of resource inefficient database writes for mobile applications.", "authors": ["Yingjun Lyu", "Ding Li", "William G. J. Halfond"], "DOIs": ["https://doi.org/10.1145/3213846.3213865"], "tag": ["Optimization and Performance"], "abstract": "ABSTRACTDevelopers strive to build feature-filled apps that are responsive and consume as few resources as possible. Most of these apps make use of local databases to store and access data locally. Prior work has found that local database services have become one of the major drivers of a mobile device's resource consumption. In this paper we propose an approach to reduce the energy consumption and improve runtime performance of database operations in Android apps by optimizing inefficient database writes. Our approach automatically detects database writes that happen within loops and that will trigger inefficient autocommit behaviors. Our approach then uses additional analyses to identify those that are optimizable and rewrites the code so that it is more efficient. We evaluated our approach on a set of marketplace Android apps and found it could reduce the energy and runtime of events containing the inefficient database writes by 25% to 90% and needed, on average, thirty-six seconds to analyze and transform each app."}, {"id": "conf/issta/NollerKP18", "title": "Badger: complexity analysis with fuzzing and symbolic execution.", "authors": ["Yannic Noller", "Rody Kersten", "Corina S. Pasareanu"], "DOIs": ["https://doi.org/10.1145/3213846.3213868"], "tag": ["Optimization and Performance"], "abstract": "ABSTRACTHybrid testing approaches that involve fuzz testing and symbolic execution have shown promising results in achieving high code coverage, uncovering subtle errors and vulnerabilities in a variety of software applications. In this paper we describe Badger - a new hybrid approach for complexity analysis, with the goal of discovering vulnerabilities which occur when the worst-case time or space complexity of an application is significantly higher than the average case.  Badger uses fuzz testing to generate a diverse set of inputs that aim to increase not only coverage but also a resource-related cost associated with each path. Since fuzzing may fail to execute deep program paths due to its limited knowledge about the conditions that influence these paths, we complement the analysis with a symbolic execution, which is also customized to search for paths that increase the resource-related cost. Symbolic execution is particularly good at generating inputs that satisfy various program conditions but by itself suffers from path explosion. Therefore, Badger uses fuzzing and symbolic execution in tandem, to leverage their benefits and overcome their weaknesses.  We implemented our approach for the analysis of Java programs, based on Kelinci and Symbolic PathFinder. We evaluated Badger on Java applications, showing that our approach is significantly faster in generating worst-case executions compared to fuzzing or symbolic execution on their own."}, {"id": "conf/issta/GuoR18", "title": "Exploiting community structure for floating-point precision tuning.", "authors": ["Hui Guo", "Cindy Rubio-Gonz\u00e1lez"], "DOIs": ["https://doi.org/10.1145/3213846.3213862"], "tag": ["Optimization and Performance"], "abstract": "ABSTRACTFloating-point types are notorious for their intricate representation. The effective use of mixed precision, i.e., using various precisions in different computations, is critical to achieve a good balance between accuracy and performance. Unfortunately, reasoning about mixed precision is difficult even for numerical experts. Techniques have been proposed to systematically search over floating-point variables and/or program instructions to find a faster, mixed-precision version of a given program. These techniques, however, are characterized by their black box nature, and face scalability limitations due to the large search space. In this paper, we exploit the community structure of floating-point variables to devise a scalable hierarchical search for precision tuning. Specifically, we perform dependence analysis and edge profiling to create a weighted dependence graph that presents a network of floating-point variables. We then formulate hierarchy construction on the network as a community detection problem, and present a hierarchical search algorithm that iteratively lowers precision with regard to communities. We implement our algorithm in the tool HiFPTuner, and show that it exhibits higher search efficiency over the state of the art for 75.9% of the experiments taking 59.6% less search time on average. Moreover, HiFPTuner finds more profitable configurations for 51.7% of the experiments, with one known to be as good as the global optimum found through exhaustive search."}, {"id": "conf/issta/PalmskogCG18", "title": "piCoq: parallel regression proving for large-scale verification projects.", "authors": ["Karl Palmskog", "Ahmet \u00c7elik", "Milos Gligoric"], "DOIs": ["https://doi.org/10.1145/3213846.3213877"], "tag": ["Optimization and Performance"], "abstract": "ABSTRACTLarge-scale verification projects using proof assistants typically contain many proofs that must be checked at each new project revision. While proof checking can sometimes be parallelized at the coarse-grained file level to save time, recent changes in some proof assistant in the LCF family, such as Coq, enable fine-grained parallelism at the level of proofs. However, these parallel techniques are not currently integrated with regression proof selection, a technique that checks only the subset of proofs affected by a change. We present techniques that blend the power of parallel proof checking and selection to speed up regression proving in verification projects, suitable for use both on users' own machines and in workflows involving continuous integration services. We implemented the techniques in a tool, piCoq, which supports Coq projects. piCoq can track dependencies between files, definitions, and lemmas and perform parallel checking of only those files or proofs affected by changes between two project revisions. We applied piCoq to perform regression proving over many revisions of several large open source projects and measured the proof checking time. While gains from using proof-level parallelism and file selection can be considerable, our results indicate that proof-level parallelism and proof selection is consistently much faster than both sequential checking from scratch and sequential checking with proof selection. In particular, 4-way parallelization is up to 28.6 times faster than the former, and up to 2.8 times faster than the latter."}, {"id": "conf/issta/FiedorMSVV18", "title": "Advances in the ANaConDA framework for dynamic analysis and testing of concurrent C/C++ programs.", "authors": ["Jan Fiedor", "Monika Muzikovsk\u00e1", "Ales Smrcka", "Ondrej Vas\u00edcek", "Tom\u00e1s Vojnar"], "DOIs": ["https://doi.org/10.1145/3213846.3229505"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTThe paper presents advances in the ANaConDA framework for dynamic analysis and testing of concurrent C/C++ programs. ANaConDA comes with several built-in analysers, covering detection of data races, deadlocks, or contract violations, and allows for an easy creation of new analysers. To increase the variety of tested interleavings, ANaConDA offers various noise injection techniques. The framework performs the analysis on a binary level, thus not requiring the source code of the program to be available. Apart from many academic experiments, ANaConDA has also been successfully used to discover various errors in industrial code."}, {"id": "conf/issta/YanPLYZ18", "title": "LAND: a user-friendly and customizable test generation tool for Android apps.", "authors": ["Jiwei Yan", "Linjie Pan", "Yaqi Li", "Jun Yan", "Jian Zhang"], "DOIs": ["https://doi.org/10.1145/3213846.3229500"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTModel-based GUI exploration techniques are widely used to generate test cases for event-driven programs (such as Android apps). These techniques traverse the elements of screens during the user interaction and simultaneously construct the GUI model. Although there are a number of automatic model-based exploration tools, most of them pay more attention to the exploration procedure than the model reusing. This paper presents LAND, an effective and user-friendly test generation tool based on GUI exploration of Android apps, which constructs an elaborate window transition model ``LATTE'' that considers more Android specific characteristics and provides a customizable test generation interface by reusing the model. Experiments on 20 real-world Android apps are conducted to construct their models as well as test cases. The experimental results indicate that LAND can achieve higher code coverage and trigger exceptions in shorter sequence. It is also demonstrated that LATTE can be well reused under different requirements of test suite generation. A demo video of our tool can be found at the website https://www.youtube.com/watch?v=iqtr12eiJ_0."}, {"id": "conf/issta/GaoYJLYS018", "title": "Managing concurrent testing of data race with ComRaDe.", "authors": ["Jian Gao", "Xin Yang", "Yu Jiang", "Han Liu", "Weiliang Ying", "Wanting Sun", "Ming Gu"], "DOIs": ["https://doi.org/10.1145/3213846.3229502"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTAs a result of the increasing number of concurrent programs, the researchers put forward a number of tools with different implementation strategies to detect data race. However, confirming data races from the collection of true and false positives reported by race detectors is extremely the time-consuming process during the evaluation period.  In this paper, we presented ComRaDe, a management platform for concurrent testing of data race with three main functions: manage and filter data races, run evaluation programs to select race detectors, generate detection report automatically. We integrated and compared three different race detectors on ComRaDe in terms of race detection capability. The results demonstrated the potential of ComRaDe on effectively identifying the advantages and limitations of different race detectors, and in further helping researchers to select and improve the capability of detectors for its convenience."}, {"id": "conf/issta/JahangirovaCHT18", "title": "OASIs: oracle assessment and improvement tool.", "authors": ["Gunel Jahangirova", "David Clark", "Mark Harman", "Paolo Tonella"], "DOIs": ["https://doi.org/10.1145/3213846.3229503"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTThe oracle problem remains one of the key challenges in software testing, for which little automated support has been developed so far. We introduce OASIs, a search-based tool for Java that assists testers in oracle assessment and improvement. It does so by combining test case generation to reveal false positives and mutation testing to reveal false negatives. In this work, we describe how OASIs works, provide details of its implementation, and explain how it can be used in an iterative oracle improvement process with a human in the loop. Finally, we present a summary of previous empirical evaluation showing that the fault detection rate of the oracles after improvement using OASIs increases, on average, by 48.6%."}, {"id": "conf/issta/Rodriguez-Baquero18", "title": "Mutode: generic JavaScript and Node.js mutation testing tool.", "authors": ["Diego Rodr\u00edguez-Baquero", "Mario Linares V\u00e1squez"], "DOIs": ["https://doi.org/10.1145/3213846.3229504"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTMutation testing is a technique in which faults (mutants) are injected into a program or application to assess its test suite effectiveness. It works by inserting mutants and running the application\u2019s test suite to identify if the mutants are detected (killed) or not (survived) by the tests. Although computationally expensive, it has proven to be an effective method to assess application test suites. Several mutation testing frameworks and tools have been built for the various programing languages, however, very few tools have been built for the JavaScript language, more specifically, there is a lack of mutation testing tools for the Node.js runtime and npm based applications. The npm Registry is a public collection of modules of open-source code for Node.js, front-end web applications, mobile applications, robots, routers, and countless other needs of the JavaScript community. The over 700,000 packages hosted in npm are downloaded more than 5 billion times per week. More and more software is published in npm every day, representing a huge opportunity to share code and solutions, but also to share bugs and faulty software. In this paper, we briefly describe prior work for mutation operators in JavaScript and Node.js, and propose Mutode, an open source tool which leverages the npm package ecosystem to perform mutation testing for JavaScript and Node.js applications. We empirically evaluated Mutode effectiveness by running it on 12 of the top 20 npm modules that have automated test suites."}, {"id": "conf/issta/NguyenND18", "title": "MalViz: an interactive visualization tool for tracing malware.", "authors": ["Vinh The Nguyen", "Akbar Siami Namin", "Tommy Dang"], "DOIs": ["https://doi.org/10.1145/3213846.3229501"], "tag": ["Tool Demonstrations"], "abstract": "ABSTRACTThis demonstration paper introduces MalViz, a visual analytic tool for analyzing malware behavioral patterns through process monitoring events. The goals of this tool are: 1) to investigate the relationship and dependencies among processes interacted with a running malware over a certain period of time, 2) to support professional security experts in detecting and recognizing unusual signature-based patterns exhibited by a running malware, and 3) to help users identify infected system and users' libraries that the malware has reached and possibly tampered. A case study is conducted in a virtual machine environment with a sample of four malware programs. The result of the case study shows that the visualization tool offers a great support for experts in software and system analysis and digital forensics to profile and observe malicious behavior and further identify the traces of affected software artifacts."}], "2019": [{"id": "conf/issta/AlshahwanCH0MMM19", "title": "Some challenges for software testing research (invited talk paper).", "authors": ["Nadia Alshahwan", "Andrea Ciancone", "Mark Harman", "Yue Jia", "Ke Mao", "Alexandru Marginean", "Alexander Mols", "Hila Peleg", "Federica Sarro", "Ilya Zorin"], "DOIs": ["https://doi.org/10.1145/3293882.3338991"], "tag": ["Keynote"], "abstract": "ABSTRACTThis paper outlines 4 open challenges for Software Testing in general and Search Based Software Testing in particular, arising from our experience with the Sapienz System Deployment at Facebook. The challenges may also apply more generally, thereby representing opportunities for the research community to further benefit from the growing interest in automated test design in industry."}, {"id": "conf/issta/YahavFDRG19", "title": "From typestate verification to interpretable deep models (invited talk abstract).", "authors": ["Eran Yahav", "Stephen J. Fink", "Nurit Dor", "G. Ramalingam", "Emmanuel Geay"], "DOIs": ["https://doi.org/10.1145/3293882.3338992"], "tag": ["ISSTA 2019 Retrospective Impact Paper Award"], "abstract": "ABSTRACTThe paper ``Effective Typestate Verification in the Presence of Aliasing'' was published in the International Symposium on Software Testing and Analysis (ISSTA) 2006 Proceedings, and has now been selected to receive the ISSTA 2019 Retrospective Impact Paper Award. The paper described a scalable framework for verification of typestate properties in real-world Java programs. The paper introduced several techniques that have been used widely in the static analysis of real-world programs. Specifically, it introduced an abstract domain combining access-paths, aliasing information, and typestate that turned out to be simple, powerful, and useful. We review the original paper and show the evolution of the ideas over the years. We show how some of these ideas have evolved into work on machine learning for code completion, and discuss recent general results in machine learning for programming."}, {"id": "conf/issta/KiezunGHEG19", "title": "Theory and practice of string solvers (invited talk abstract).", "authors": ["Adam Kiezun", "Philip J. Guo", "Pieter Hooimeijer", "Michael D. Ernst", "Vijay Ganesh"], "DOIs": ["https://doi.org/10.1145/3293882.3338993"], "tag": ["ISSTA 2019 Impact Paper Award"], "abstract": "ABSTRACTThe paper titled \"Hampi: A Solver for String Constraints\" was published in the proceedings of the International Symposium on Software Testing and Analysis (ISSTA) 2009, and has been selected to receive the ISSTA 2019 Impact Paper Award. The paper describes HAMPI, one of the first practical solver aimed at solving the satisfiability problem for a theory of string (word) equations, operations over strings, predicates over regular expressions and context-free grammars. HAMPI has been used widely to solve many software engineering and security problems, and has inspired considerable research on string solving algorithms and their applications.  In this talk, we review the state of research on the theory and practice of string solving algorithms, specifically highlighting key historical developments that have led to their widespread use. On the practical front, we discuss different kinds of algorithmic paradigms, such as word- and automata-based, that have been developed to solve string and regular expression constraints. We then focus on the many hardness results that theorists have proved for fragments of theories over strings. Finally, we conclude with open theoretical problems, practical algorithmic challenges, and future applications of string solvers."}, {"id": "conf/issta/GaoMR19", "title": "Crash-avoiding program repair.", "authors": ["Xiang Gao", "Sergey Mechtaev", "Abhik Roychoudhury"], "DOIs": ["https://doi.org/10.1145/3293882.3330558"], "tag": ["Program Repair"], "abstract": "ABSTRACTExisting program repair systems modify a buggy program so that the modified program passes given tests. The repaired program may not satisfy even the most basic notion of correctness, namely crash-freedom. In other words, repair tools might generate patches which over-fit the test data driving the repair, and the automatically repaired programs may even introduce crashes or vulnerabilities. We propose an integrated approach for detecting and discarding crashing patches. Our approach fuses test and patch generation into a single process, in which patches are generated with the objective of passing existing tests, and new tests are generated with the objective of filtering out over-fitted patches by distinguishing candidate patches in terms of behavior. We use crash-freedom as the oracle to discard patch candidates which crash on the new tests. In its core, our approach defines a grey-box fuzzing strategy that gives higher priority to new tests that separate patches behaving equivalently on existing tests. This test generation strategy identifies semantic differences between patch candidates, and reduces over-fitting in program repair. We evaluated our approach on real-world vulnerabilities and open-source subjects from the Google OSS-Fuzz infrastructure. We found that our tool Fix2Fit (implementing patch space directed test generation), produces crash-avoiding patches. While we do not give formal guarantees about crash-freedom, cross-validation with fuzzing tools and their sanitizers provides greater confidence about the crash-freedom of our suggested patches."}, {"id": "conf/issta/GhanbariBZ19", "title": "Practical program repair via bytecode mutation.", "authors": ["Ali Ghanbari", "Samuel Benton", "Lingming Zhang"], "DOIs": ["https://doi.org/10.1145/3293882.3330559"], "tag": ["Program Repair"], "abstract": "ABSTRACTAutomated Program Repair (APR) is one of the most recent advances in automated debugging, and can directly fix buggy programs with minimal human intervention. Although various advanced APR techniques (including search-based or semantic-based ones) have been proposed, they mainly work at the source-code level and it is not clear how bytecode-level APR performs in practice. Also, empirical studies of the existing techniques on bugs beyond what has been reported in the original papers are rather limited. In this paper, we implement the first practical bytecode-level APR technique, PraPR, and present the first extensive study on fixing real-world bugs (e.g., Defects4J bugs) using JVM bytecode mutation. The experimental results show that surprisingly even PraPR with only the basic traditional mutators can produce genuine fixes for 17 bugs; with simple additional commonly used APR mutators, PraPR is able to produce genuine fixes for 43 bugs, significantly outperforming state-of-the-art APR, while being over 10X faster. Furthermore, we performed an extensive study of PraPR and other recent APR tools on a large number of additional real-world bugs, and demonstrated the overfitting problem of recent advanced APR tools for the first time. Lastly, PraPR has also successfully fixed bugs for other JVM languages (e.g., for the popular Kotlin language), indicating PraPR can greatly complement existing source-code-level APR."}, {"id": "conf/issta/LiuK0B19", "title": "TBar: revisiting template-based automated program repair.", "authors": ["Kui Liu", "Anil Koyuncu", "Dongsun Kim", "Tegawend\u00e9 F. Bissyand\u00e9"], "DOIs": ["https://doi.org/10.1145/3293882.3330577"], "tag": ["Program Repair"], "abstract": "ABSTRACTWe revisit the performance of template-based APR to build comprehensive knowledge about the effectiveness of fix patterns, and to highlight the importance of complementary steps such as fault localization or donor code retrieval. To that end, we first investigate the literature to collect, summarize and label recurrently-used fix patterns. Based on the investigation, we build TBar, a straightforward APR tool that systematically attempts to apply these fix patterns to program bugs. We thoroughly evaluate TBar on the Defects4J benchmark. In particular, we assess the actual qualitative and quantitative diversity of fix patterns, as well as their effectiveness in yielding plausible or correct patches. Eventually, we find that, assuming a perfect fault localization, TBar correctly/plausibly fixes 74/101 bugs. Replicating a standard and practical pipeline of APR assessment, we demonstrate that TBar correctly fixes 43 bugs from Defects4J, an unprecedented performance in the literature (including all approaches, i.e., template-based, stochastic mutation-based or synthesis-based APR)."}, {"id": "conf/issta/Lou0ZH019", "title": "History-driven build failure fixing: how far are we?", "authors": ["Yiling Lou", "Junjie Chen", "Lingming Zhang", "Dan Hao", "Lu Zhang"], "DOIs": ["https://doi.org/10.1145/3293882.3330578"], "tag": ["Program Repair"], "abstract": "ABSTRACTBuild systems are essential for modern software development and maintenance since they are widely used to transform source code artifacts into executable software. Previous work shows that build systems break frequently during software evolution. Therefore, automated build-fixing techniques are in huge demand. In this paper we target a mainstream build system, Gradle, which has become the most widely used build system for Java projects in the open-source community (e.g., GitHub). HireBuild, state-of-the-art build-fixing tool for Gradle, has been recently proposed to fix Gradle build failures via mining the history of prior fixes. Although HireBuild has been shown to be effective for fixing real-world Gradle build failures, it was evaluated on only a limited set of build failures, and largely depends on the quality/availability of historical fix information. To investigate the efficacy and limitations of the history-driven build fix, we first construct a new and large build failure dataset from Top-1000 GitHub projects. Then, we evaluate HireBuild on the extended dataset both quantitatively and qualitatively. Inspired by the findings of the study, we propose a simplistic new technique that generates potential patches via searching from the present project under test and external resources rather than the historical fix information. According to our experimental results, the simplistic approach based on present information successfully fixes 2X more reproducible build failures than the state-of-art HireBuild based on historical fix information. Furthermore, our results also reveal various findings/guidelines for future advanced build failure fixing."}, {"id": "conf/issta/ZhangBK19", "title": "LibID: reliable identification of obfuscated third-party Android libraries.", "authors": ["Jiexin Zhang", "Alastair R. Beresford", "Stephan A. Kollmann"], "DOIs": ["https://doi.org/10.1145/3293882.3330563"], "tag": ["Mobile App Testing"], "abstract": "ABSTRACTThird-party libraries are vital components of Android apps, yet they can also introduce serious security threats and impede the accuracy and reliability of app analysis tasks, such as app clone detection. Several library detection approaches have been proposed to address these problems. However, we show these techniques are not robust against popular code obfuscators, such as ProGuard, which is now used in nearly half of all apps. We then present LibID, a library detection tool that is more resilient to code shrinking and package modification than state-of-the-art tools. We show that the library identification problem can be formulated using binary integer programming models. LibID is able to identify specific versions of third-party libraries in candidate apps through static analysis of app binaries coupled with a database of third-party libraries. We propose a novel approach to generate synthetic apps to tune the detection thresholds. Then, we use F-Droid apps as the ground truth to evaluate LibID under different obfuscation settings, which shows that LibID is more robust to code obfuscators than state-of-the-art tools. Finally, we demonstrate the utility of LibID by detecting the use of a vulnerable version of the OkHttp library in nearly 10% of 3,958 most popular apps on the Google Play Store."}, {"id": "conf/issta/SharmaN19", "title": "QADroid: regression event selection for Android applications.", "authors": ["Aman Sharma", "Rupesh Nasre"], "DOIs": ["https://doi.org/10.1145/3293882.3330550"], "tag": ["Mobile App Testing"], "abstract": "ABSTRACTPopular Android applications undergo frequent releases. Ensuring functional testing of the new features, as well as regression testing of the previous functionality, are time-consuming and error-prone. Thus, there is a need for a tool that eases the testing efforts as well as saves the overall time of the product release cycle. In this work, we present QADroid, the first activity- and event-aware regression selection tool for Android apps. Salient features of QADroid are: (i) a richer change-set analyzer that covers code as well as non-code components for regression, (ii) it presents a pictorial representation of the app\u2019s functioning, and (iii) it displays the regression points in the app as a mapping between activities to user-elements to events. Features (ii) and (iii) help the testers in understanding the technical findings better. We evaluated QADroid on 1105 releases of 50 open source Android projects. The results show that QADroid reduced the activity selection by 58% and event selection by 74% compared to the traditional way of exhaustive testing of all activities and events, thereby significantly reducing the manual testing efforts."}, {"id": "conf/issta/KongLGBK19", "title": "Mining Android crash fixes in the absence of issue- and change-tracking systems.", "authors": ["Pingfan Kong", "Li Li", "Jun Gao", "Tegawend\u00e9 F. Bissyand\u00e9", "Jacques Klein"], "DOIs": ["https://doi.org/10.1145/3293882.3330572"], "tag": ["Mobile App Testing"], "abstract": "ABSTRACTAndroid apps are prone to crash. This often arises from the misuse of Android framework APIs, making it harder to debug since official Android documentation does not discuss thoroughly potential exceptions.Recently, the program repair community has also started to investigate the possibility to fix crashes automatically. Current results, however, apply to limited example cases. In both scenarios of repair, the main issue is the need for more example data to drive the fix processes due to the high cost in time and effort needed to collect and identify fix examples. We propose in this work a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We developed a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches."}, {"id": "conf/issta/GuoLLYL19", "title": "Sara: self-replay augmented record and replay for Android in industrial cases.", "authors": ["Jiaqi Guo", "Shuyue Li", "Jian-Guang Lou", "Zijiang Yang", "Ting Liu"], "DOIs": ["https://doi.org/10.1145/3293882.3330557"], "tag": ["Mobile App Testing"], "abstract": "ABSTRACTRecord-and-replay tools are indispensable for quality assurance of mobile applications. Due to its importance, an increasing number of tools are being developed to record and replay user interactions for Android. However, by conducting an empirical study of various existing tools in industrial settings, researchers have revealed a gap between the characteristics requested from industry and the performance of publicly available record-and-replay tools. The study concludes that no existing tools under evaluation are sufficient for industrial applications. In this paper, we present a record-and-replay tool called SARA towards bridging the gap and targeting a wide adoption. Specifically, a dynamic instrumentation technique is used to accommodate rich sources of inputs in the application layer satisfying various constraints requested from industry. A self-replay mechanism is proposed to record more information of user inputs for accurate replaying without degrading user experience. In addition, an adaptive replay method is designed to enable replaying events on different devices with diverse screen sizes and OS versions. Through an evaluation on 53 highly popular industrial Android applications and 265 common usage scenarios, we demonstrate the effectiveness of SARA in recording and replaying rich sources of inputs on the same or different devices."}, {"id": "conf/issta/LamGNST19", "title": "Root causing flaky tests in a large-scale industrial setting.", "authors": ["Wing Lam", "Patrice Godefroid", "Suman Nath", "Anirudh Santhiar", "Suresh Thummalapenta"], "DOIs": ["https://doi.org/10.1145/3293882.3330570"], "tag": ["Regression Testing"], "abstract": "ABSTRACTIn today\u2019s agile world, developers often rely on continuous integration pipelines to help build and validate their changes by executing tests in an efficient manner. One of the significant factors that hinder developers\u2019 productivity is flaky tests\u2014tests that may pass and fail with the same version of code. Since flaky test failures are not deterministically reproducible, developers often have to spend hours only to discover that the occasional failures have nothing to do with their changes. However, ignoring failures of flaky tests can be dangerous, since those failures may represent real faults in the production code. Furthermore, identifying the root cause of flakiness is tedious and cumbersome, since they are often a consequence of unexpected and non-deterministic behavior due to various factors, such as concurrency and external dependencies.  As developers in a large-scale industrial setting, we first describe our experience with flaky tests by conducting a study on them. Our results show that although the number of distinct flaky tests may be low, the percentage of failing builds due to flaky tests can be substantial. To reduce the burden of flaky tests on developers, we describe our end-to-end framework that helps identify flaky tests and understand their root causes. Our framework instruments flaky tests and all relevant code to log various runtime properties, and then uses a preliminary tool, called RootFinder, to find differences in the logs of passing and failing runs. Using our framework, we collect and publicize a dataset of real-world, anonymized execution logs of flaky tests. By sharing the findings from our study, our framework and tool, and a dataset of logs, we hope to encourage more research on this important problem."}, {"id": "conf/issta/Shi0M19", "title": "Mitigating the effects of flaky tests on mutation testing.", "authors": ["August Shi", "Jonathan Bell", "Darko Marinov"], "DOIs": ["https://doi.org/10.1145/3293882.3330568"], "tag": ["Regression Testing"], "abstract": "ABSTRACTMutation testing is widely used in research as a metric for evaluating the quality of test suites. Mutation testing runs the test suite on generated mutants (variants of the code under test), where a test suite kills a mutant if any of the tests fail when run on the mutant. Mutation testing implicitly assumes that tests exhibit deterministic behavior, in terms of their coverage and the outcome of a test (not) killing a certain mutant. Such an assumption does not hold in the presence of flaky tests, whose outcomes can non-deterministically differ even when run on the same code under test. Without reliable test outcomes, mutation testing can result in unreliable results, e.g., in our experiments, mutation scores vary by four percentage points on average between repeated executions, and 9% of mutant-test pairs have an unknown status. Many modern software projects suffer from flaky tests. We propose techniques that manage flakiness throughout the mutation testing process, largely based on strategically re-running tests. We implement our techniques by modifying the open-source mutation testing tool, PIT. Our evaluation on 30 projects shows that our techniques reduce the number of \"unknown\" (flaky) mutants by 79.4%."}, {"id": "conf/issta/SchwahnC0S19", "title": "Assessing the state and improving the art of parallel testing for C.", "authors": ["Oliver Schwahn", "Nicolas Coppik", "Stefan Winter", "Neeraj Suri"], "DOIs": ["https://doi.org/10.1145/3293882.3330573"], "tag": ["Regression Testing"], "abstract": "ABSTRACTThe execution latency of a test suite strongly depends on the degree of concurrency with which test cases are executed. However, if test cases are not designed for concurrent execution, they may interfere, causing result deviations compared to sequential execution. To prevent this, each test case can be provided with an isolated execution environment, but the resulting overheads diminish the merit of parallel testing. Our large-scale analysis of the Debian Buster package repository shows that existing test suites in C projects make limited use of parallelization. We present an approach to (a) analyze the potential of C test suites for safe concurrent execution, i.e., result invariance compared to sequential execution, and (b) execute tests concurrently with different parallelization strategies using processes or threads if it is found to be safe. Applying our approach to 9 C projects, we find that most of them cannot safely execute tests in parallel due to unsafe test code or unsafe usage of shared variables or files within the program code. Parallel test execution shows a significant acceleration over sequential execution for most projects. We find that multi-threading rarely outperforms multi-processing. Finally, we observe that the lack of a common test framework for C leaves make as the standard driver for running tests, which introduces unnecessary performance overheads for test execution."}, {"id": "conf/issta/GolaghaLPI19", "title": "Failure clustering without coverage.", "authors": ["Mojdeh Golagha", "Constantin Lehnhoff", "Alexander Pretschner", "Hermann Ilmberger"], "DOIs": ["https://doi.org/10.1145/3293882.3330561"], "tag": ["Regression Testing"], "abstract": "ABSTRACTDeveloping and integrating software in the automotive industry is a complex task and requires extensive testing. An important cost factor in testing and debugging is the time required to analyze failing tests. In the context of regression testing, usually, large numbers of tests fail due to a few underlying faults. Clustering failing tests with respect to their underlying faults can, therefore, help in reducing the required analysis time. In this paper, we propose a clustering technique to group failing hardware-in-the-loop tests based on non-code-based features, retrieved from three different sources. To effectively reduce the analysis effort, the clustering tool selects a representative test for each cluster. Instead of analyzing all failing tests, testers only inspect the representative tests to find the underlying faults. We evaluated the effectiveness and efficiency of our solution in a major automotive company using 86 regression test runs, 8743 failing tests, and 1531 faults. The results show that utilizing our clustering tool, testers can reduce the analysis time more than 60% and find more than 80% of the faults only by inspecting the representative tests."}, {"id": "conf/issta/XieMJXCLZLYS19", "title": "DeepHunter: a coverage-guided fuzz testing framework for deep neural networks.", "authors": ["Xiaofei Xie", "Lei Ma", "Felix Juefei-Xu", "Minhui Xue", "Hongxu Chen", "Yang Liu", "Jianjun Zhao", "Bo Li", "Jianxiong Yin", "Simon See"], "DOIs": ["https://doi.org/10.1145/3293882.3330579"], "tag": ["Testing and Machine Learning"], "abstract": "ABSTRACTThe past decade has seen the great potential of applying deep neural network (DNN) based software to safety-critical scenarios, such as autonomous driving. Similar to traditional software, DNNs could exhibit incorrect behaviors, caused by hidden defects, leading to severe accidents and losses. In this paper, we propose DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs. To this end, we first propose a metamorphic mutation strategy to generate new semantically preserved tests, and leverage multiple extensible coverage criteria as feedback to guide the test generation. We further propose a seed selection strategy that combines both diversity-based and recency-based seed selection. We implement and incorporate 5 existing testing criteria and 4 seed selection strategies in DeepHunter. Large-scale experiments demonstrate that (1) our metamorphic mutation strategy is useful to generate new valid tests with the same semantics as the original seed, by up to a 98% validity ratio; (2) the diversity-based seed selection generally weighs more than recency-based seed selection in boosting the coverage and in detecting defects; (3) DeepHunter outperforms the state of the arts by coverage as well as the quantity and diversity of defects identified; (4) guided by corner-region based criteria, DeepHunter is useful to capture defects during the DNN quantization for platform migration."}, {"id": "conf/issta/CordyMPT19", "title": "Search-based test and improvement of machine-learning-based anomaly detection systems.", "authors": ["Maxime Cordy", "Steve Muller", "Mike Papadakis", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3293882.3330580"], "tag": ["Testing and Machine Learning"], "abstract": "ABSTRACTMachine-learning-based anomaly detection systems can be vulnerable to new kinds of deceptions, known as training attacks, which exploit the live learning mechanism of these systems by progressively injecting small portions of abnormal data. The injected data seamlessly swift the learned states to a point where harmful data can pass unnoticed. We focus on the systematic testing of these attacks in the context of intrusion detection systems (IDS). We propose a search-based approach to test IDS by making training attacks. Going a step further, we also propose searching for countermeasures, learning from the successful attacks and thereby increasing the resilience of the tested IDS. We evaluate our approach on a denial-of-service attack detection scenario and a dataset recording the network traffic of a real-world system. Our experiments show that our search-based attack scheme generates successful attacks bypassing the current state-of-the-art defences. We also show that our approach is capable of generating attack patterns for all configuration states of the studied IDS and that it is capable of providing appropriate countermeasures. By co-evolving our attack and defence mechanisms we succeeded at improving the defence of the IDS under test by making it resilient to 49 out of 50 independently generated attacks."}, {"id": "conf/issta/LiLZZ19", "title": "DeepFL: integrating multiple fault diagnosis dimensions for deep fault localization.", "authors": ["Xia Li", "Wei Li", "Yuqun Zhang", "Lingming Zhang"], "DOIs": ["https://doi.org/10.1145/3293882.3330574"], "tag": ["Testing and Machine Learning"], "abstract": "ABSTRACTLearning-based fault localization has been intensively studied recently. Prior studies have shown that traditional Learning-to-Rank techniques can help precisely diagnose fault locations using various dimensions of fault-diagnosis features, such as suspiciousness values computed by various off-the-shelf fault localization techniques. However, with the increasing dimensions of features considered by advanced fault localization techniques, it can be quite challenging for the traditional Learning-to-Rank algorithms to automatically identify effective existing/latent features. In this work, we propose DeepFL, a deep learning approach to automatically learn the most effective existing/latent features for precise fault localization. Although the approach is general, in this work, we collect various suspiciousness-value-based, fault-proneness-based and textual-similarity-based features from the fault localization, defect prediction and information retrieval areas, respectively. DeepFL has been studied on 395 real bugs from the widely used Defects4J benchmark. The experimental results show DeepFL can significantly outperform state-of-the-art TraPT/FLUCCS (e.g., localizing 50+ more faults within Top-1). We also investigate the impacts of deep model configurations (e.g., loss functions and epoch settings) and features. Furthermore, DeepFL is also surprisingly effective for cross-project prediction."}, {"id": "conf/issta/PiskachevDB19", "title": "Codebase-adaptive detection of security-relevant methods.", "authors": ["Goran Piskachev", "Lisa Nguyen Quang Do", "Eric Bodden"], "DOIs": ["https://doi.org/10.1145/3293882.3330556"], "tag": ["Testing and Machine Learning"], "abstract": "ABSTRACTMore and more companies use static analysis to perform regular code reviews to detect security vulnerabilities in their code, configuring them to detect various types of bugs and vulnerabilities such as the SANS top 25 or the OWASP top 10. For such analyses to be as precise as possible, they must be adapted to the code base they scan. The particular challenge we address in this paper is to provide analyses with the correct security-relevant methods (Srm): sources, sinks, etc. We present SWAN, a fully-automated machine-learning approach to detect sources, sinks, validators, and authentication methods for Java programs. SWAN further classifies the Srm into specific vulnerability classes of the SANS top 25. To further adapt the lists detected by SWAN to the code base and to improve its precision, we also introduce SWANAssist, an extension to SWAN that allows analysis users to refine the classifications. On twelve popular Java frameworks, SWAN achieves an average precision of 0.826, which is better or comparable to existing approaches. Our experiments show that SWANAssist requires a relatively low effort from the developer to significantly improve its precision."}, {"id": "conf/issta/KechagiaDPGD19", "title": "Effective and efficient API misuse detection via exception propagation and search-based testing.", "authors": ["Maria Kechagia", "Xavier Devroey", "Annibale Panichella", "Georgios Gousios", "Arie van Deursen"], "DOIs": ["https://doi.org/10.1145/3293882.3330552"], "tag": ["APIs and Symbolic Execution"], "abstract": "ABSTRACTApplication Programming Interfaces (APIs) typically come with (implicit) usage constraints. The violations of these constraints (API misuses) can lead to software crashes. Even though there are several tools that can detect API misuses, most of them suffer from a very high rate of false positives. We introduce Catcher, a novel API misuse detection approach that combines static exception propagation analysis with automatic search-based test case generation to effectively and efficiently pinpoint crash-prone API misuses in client applications. We validate Catcher against 21 Java applications, targeting misuses of the Java platform's API. Our results indicate that Catcher is able to generate test cases that uncover 243 (unique) API misuses that result in crashes. Our empirical evaluation shows that Catcher can detect a large number of misuses (77 cases) that would remain undetected by the traditional coverage-based test case generator EvoSuite. Additionally, on average, Catcher is eight times faster than EvoSuite in generating test cases for the identified misuses. Finally, we find that the majority of the exceptions triggered by Catcher are unexpected to developers, i.e., not only unhandled in the source code but also not listed in the documentation of the client applications."}, {"id": "conf/issta/FazziniXO19", "title": "Automated API-usage update for Android apps.", "authors": ["Mattia Fazzini", "Qi Xin", "Alessandro Orso"], "DOIs": ["https://doi.org/10.1145/3293882.3330571"], "tag": ["APIs and Symbolic Execution"], "abstract": "ABSTRACTMobile apps rely heavily on the application programming interface (API) provided by their underlying operating system (OS). Because OS and API can change frequently, developers must quickly update their apps to ensure that the apps behave as intended with new API and OS versions. To help developers with this tedious, error prone, and time consuming task, we developed a technique that can automatically perform app updates for API changes based on examples of how other developers evolved their apps for the same changes. Given a target app to be updated and information about the changes in the API, our technique performs four main steps. First, it analyzes the target app to identify code affected by API changes. Second, it searches existing code bases for examples of updates to the new version of the API. Third, it analyzes, ranks, and transforms into generic patches the update examples found in the previous step. Finally, it applies the generated patches to the target app in order of ranking, while performing differential testing to validate the update. We implemented our technique and performed an empirical evaluation on 15 real-world apps with promising results. Overall, our technique was able to update 85% of the API changes considered and automatically validate 68% of the updates performed."}, {"id": "conf/issta/CaiZ0F19", "title": "A large-scale study of application incompatibilities in Android.", "authors": ["Haipeng Cai", "Ziyi Zhang", "Li Li", "Xiaoqin Fu"], "DOIs": ["https://doi.org/10.1145/3293882.3330564"], "tag": ["APIs and Symbolic Execution"], "abstract": "ABSTRACTThe rapid expansion of the Android ecosystem is accompanied by continuing diversification of platforms and devices, resulting in increasing incompatibility issues which damage user experiences and impede app development productivity. In this paper, we conducted a large-scale, longitudinal study of compatibility issues in 62,894 benign apps developed in the past eight years, to understand the symptoms and causes of these issues. We further investigated the incompatibilities that are actually exercised at runtime through the system logs and execution traces of 15,045 apps. Our study revealed that, among others, (1) compatibility issues were prevalent and persistent at both installation and run time, with greater prevalence of run-time incompatibilities, (2) there were no certain Android versions that consistently saw more or less app incompatibilities than others, (3) installation-time incompatibilities were strongly correlated with the minSdkVersion specified in apps, while run-time incompatibilities were most significantly correlated with the underlying platform\u2019s API level, and (4) installation-time incompatibilities were mostly due to apps\u2019 use of architecture-incompatible native libraries, while run-time incompatibilities were mostly due to API changes during SDK evolution. We offered further insights into app incompatibilities, as well as recommendations on dealing with the issues for bother developers and end users of Android apps."}, {"id": "conf/issta/PandeyKR19", "title": "Deferred concretization in symbolic execution via fuzzing.", "authors": ["Awanish Pandey", "Phani Raj Goutham Kotcharlakota", "Subhajit Roy"], "DOIs": ["https://doi.org/10.1145/3293882.3330554"], "tag": ["APIs and Symbolic Execution"], "abstract": "ABSTRACTConcretization is an effective weapon in the armory of symbolic execution engines. However, concretization can lead to loss in coverage, path divergence, and generation of test-cases on which the intended bugs are not reproduced. In this paper, we propose an algorithm, Deferred Concretization, that uses a new category for values within symbolic execution (referred to as the symcrete values) to pend concretization till they are actually needed. Our tool, COLOSSUS, built around these ideas, was able to gain an average coverage improvement of 66.94% and reduce divergence by more than 55% relative to the state-of-the-art symbolic execution engine, KLEE. Moreover, we found that KLEE loses about 38.60% of the states in the symbolic execution tree that COLOSSUS is able to recover, showing that COLOSSUS is capable of covering a much larger coverage space."}, {"id": "conf/issta/KlingerCW19", "title": "Differentially testing soundness and precision of program analyzers.", "authors": ["Christian Klinger", "Maria Christakis", "Valentin W\u00fcstholz"], "DOIs": ["https://doi.org/10.1145/3293882.3330553"], "tag": ["Static Analysis and Debugging"], "abstract": "ABSTRACTIn the last decades, numerous program analyzers have been developed both in academia and industry. Despite their abundance however, there is currently no systematic way of comparing the effectiveness of different analyzers on arbitrary code. In this paper, we present the first automated technique for differentially testing soundness and precision of program analyzers. We used our technique to compare six mature, state-of-the art analyzers on tens of thousands of automatically generated benchmarks. Our technique detected soundness and precision issues in most analyzers, and we evaluated the implications of these issues to both designers and users of program analyzers."}, {"id": "conf/issta/ReifKEHM19", "title": "Judge: identifying, understanding, and evaluating sources of unsoundness in call graphs.", "authors": ["Michael Reif", "Florian K\u00fcbler", "Michael Eichberg", "Dominik Helm", "Mira Mezini"], "DOIs": ["https://doi.org/10.1145/3293882.3330555"], "tag": ["Static Analysis and Debugging"], "abstract": "ABSTRACTCall graphs are widely used; in particular for advanced control- and data-flow analyses. Even though many call graph algorithms with different precision and scalability properties have been proposed, a comprehensive understanding of sources of unsoundness, their relevance, and the capabilities of existing call graph algorithms in this respect is missing. To address this problem, we propose Judge, a toolchain that helps with understanding sources of unsoundness and improving the soundness of call graphs. In several experiments, we use Judge and an extensive test suite related to sources of unsoundness to (a) compute capability profiles for call graph implementations of Soot, WALA, DOOP, and OPAL, (b) to determine the prevalence of language features and APIs that affect soundness in modern Java Bytecode, (c) to compare the call graphs of Soot, WALA, DOOP, and OPAL \u2013 highlighting important differences in their implementations, and (d) to evaluate the necessary effort to achieve project-specific reasonable sound call graphs. We show that soundness-relevant features/APIs are frequently used and that support for them differs vastly, up to the point where comparing call graphs computed by the same base algorithms (e.g., RTA) but different frameworks is bogus. We also show that Judge can support users in establishing the soundness of call graphs with reasonable effort."}, {"id": "conf/issta/LeeR19", "title": "Adlib: analyzer for mobile ad platform libraries.", "authors": ["Sungho Lee", "Sukyoung Ryu"], "DOIs": ["https://doi.org/10.1145/3293882.3330562"], "tag": ["Static Analysis and Debugging"], "abstract": "ABSTRACTMobile advertising has become a popular advertising approach by taking advantage of various information from mobile devices and rich interaction with users. Mobile advertising platforms show advertisements of nearby restaurants to users using the geographic locations of their mobile devices, and also allow users to make reservations easily using their phone numbers. However, at the same time, they may open the doors for advertisements to steal device information or to perform malicious behaviors. When application developers integrate mobile advertising platform SDKs (AdSDKs) to their applications, they are informed of only the permissions required by the AdSDKs, and they may not be aware of the rich functionalities of the SDKs that are available to advertisements.  In this paper, we first report that various AdSDKs provide powerful functionalities to advertisements, which are seriously vulnerable to security threats. We present representative malicious behaviors by advertisements using APIs provided by AdSDKs. To mitigate the security vulnerability, we develop a static analyzer, Adlib, which analyzes Android Java libraries that use hybrid features to enable communication with JavaScript code and detects possible flows from the APIs that are accessible from third-party advertisements to device-specific features like geographic locations. Our evaluation shows that Adlib found genuine security vulnerabilities from real-world AdSDKs."}, {"id": "conf/issta/TolksdorfLP19", "title": "Interactive metamorphic testing of debuggers.", "authors": ["Sandro Tolksdorf", "Daniel Lehmann", "Michael Pradel"], "DOIs": ["https://doi.org/10.1145/3293882.3330567"], "tag": ["Static Analysis and Debugging"], "abstract": "ABSTRACTWhen improving their code, developers often turn to interactive debuggers. The correctness of these tools is crucial, because bugs in the debugger itself may mislead a developer, e.g., to believe that executed code is never reached or that a variable has another value than in the actual execution. Yet, debuggers are difficult to test because their input consists of both source code and a sequence of debugging actions, such as setting breakpoints or stepping through code. This paper presents the first metamorphic testing approach for debuggers. The key idea is to transform both the debugged code and the debugging actions in such a way that the behavior of the original and the transformed inputs should differ only in specific ways. For example, adding a breakpoint should not change the control flow of the debugged program. To support the interactive nature of debuggers, we introduce interactive metamorphic testing. It differs from traditional metamorphic testing by determining the input transformation and the expected behavioral change it causes while the program under test is running. Our evaluation applies the approach to the widely used debugger in the Chromium browser, where it finds eight previously unknown bugs with a true positive rate of 51%. All bugs have been confirmed by the developers, and one bug has even been marked as release-blocking."}, {"id": "conf/issta/QinZW19", "title": "TestMig: migrating GUI test cases from iOS to Android.", "authors": ["Xue Qin", "Hao Zhong", "Xiaoyin Wang"], "DOIs": ["https://doi.org/10.1145/3293882.3330575"], "tag": ["Testing GUIs and Cars"], "abstract": "ABSTRACTNowadays, Apple iOS and Android are two most popular platforms for mobile applications. To attract more users, many software companies and organizations are migrating their applications from one platform to the other, and besides source files, they also need to migrate their GUI tests. The migration of GUI tests is tedious and difficult to be automated, since two platforms have subtle differences and there are often few or even no migrated GUI tests for learning. To address the problem, in this paper, we propose a novel approach, TestMig, that migrates GUI tests from iOS to Android, without any migrated code samples. Specifically, TestMig first executes the GUI tests of the iOS version, and records their GUI event sequences. Guided by the iOS GUI events, TestMig explores the Android version of the application to generate the corresponding Android event sequences. We conducted an evaluation on five well known mobile applications: 2048, SimpleNote, Wire, Wikipedia, and WordPress. The results show that, on average, TestMig correctly converts 80.2% of recorded iOS UI events to Android UI events and have them successfully executed, and our migrated Android test cases achieve similar statement coverage compared with the original iOS test cases (59.7% vs 60.4%)."}, {"id": "conf/issta/DegottBZ19", "title": "Learning user interface element interactions.", "authors": ["Christian Degott", "Nataniel P. Borges Jr.", "Andreas Zeller"], "DOIs": ["https://doi.org/10.1145/3293882.3330569"], "tag": ["Testing GUIs and Cars"], "abstract": "ABSTRACTWhen generating tests for graphical user interfaces, one central problem is to identify how individual UI elements can be interacted with\u2014clicking, long- or right-clicking, swiping, dragging, typing, or more. We present an approach based on reinforcement learning that automatically learns which interactions can be used for which elements, and uses this information to guide test generation. We model the problem as an instance of the multi-armed bandit problem (MAB problem) from probability theory, and show how its traditional solutions work on test generation, with and without relying on previous knowledge. The resulting guidance yields higher coverage. In our evaluation, our approach shows improvements in statement coverage between 18% (when not using any previous knowledge) and 20% (when reusing previously generated models)."}, {"id": "conf/issta/WhiteFB19", "title": "Improving random GUI testing with image-based widget detection.", "authors": ["Thomas D. White", "Gordon Fraser", "Guy J. Brown"], "DOIs": ["https://doi.org/10.1145/3293882.3330551"], "tag": ["Testing GUIs and Cars"], "abstract": "ABSTRACTGraphical User Interfaces (GUIs) are amongst the most common user interfaces, enabling interactions with applications through mouse movements and key presses. Tools for automated testing of programs through their GUI exist, however they usually rely on operating system or framework specific knowledge to interact with an application. Due to frequent operating system updates, which can remove required information, and a large variety of different GUI frameworks using unique underlying data structures, such tools rapidly become obsolete, Consequently, for an automated GUI test generation tool, supporting many frameworks and operating systems is impractical. We propose a technique for improving GUI testing by automatically identifying GUI widgets in screen shots using machine learning techniques. As training data, we generate randomized GUIs to automatically extract widget information. The resulting model provides guidance to GUI testing tools in environments not currently supported by deriving GUI widget information from screen shots only. In our experiments, we found that identifying GUI widgets in screen shots and using this information to guide random testing achieved a significantly higher branch coverage in 18 of 20 applications, with an average increase of 42.5% when compared to conventional random testing."}, {"id": "conf/issta/GambiMF19", "title": "Automatically testing self-driving cars with search-based procedural content generation.", "authors": ["Alessio Gambi", "Marc M\u00fcller", "Gordon Fraser"], "DOIs": ["https://doi.org/10.1145/3293882.3330566"], "tag": ["Testing GUIs and Cars"], "abstract": "ABSTRACTSelf-driving cars rely on software which needs to be thoroughly tested. Testing self-driving car software in real traffic is not only expensive but also dangerous, and has already caused fatalities. Virtual tests, in which self-driving car software is tested in computer simulations, offer a more efficient and safer alternative compared to naturalistic field operational tests. However, creating suitable test scenarios is laborious and difficult. In this paper we combine procedural content generation, a technique commonly employed in modern video games, and search-based testing, a testing technique proven to be effective in many domains, in order to automatically create challenging virtual scenarios for testing self-driving car soft- ware. Our AsFault prototype implements this approach to generate virtual roads for testing lane keeping, one of the defining features of autonomous driving. Evaluation on two different self-driving car software systems demonstrates that AsFault can generate effective virtual road networks that succeed in revealing software failures, which manifest as cars departing their lane. Compared to random testing AsFault was not only more efficient, but also caused up to twice as many lane departures."}, {"id": "conf/issta/PadhyeLSPT19", "title": "Semantic fuzzing with zest.", "authors": ["Rohan Padhye", "Caroline Lemieux", "Koushik Sen", "Mike Papadakis", "Yves Le Traon"], "DOIs": ["https://doi.org/10.1145/3293882.3330576"], "tag": ["Potpourri"], "abstract": "ABSTRACTPrograms expecting structured inputs often consist of both a syntactic analysis stage, which parses raw input, and a semantic analysis stage, which conducts checks on the parsed input and executes the core logic of the program. Generator-based testing tools in the lineage of QuickCheck are a promising way to generate random syntactically valid test inputs for these programs. We present Zest, a technique which automatically guides QuickCheck-like random input generators to better explore the semantic analysis stage of test programs. Zest converts random-input generators into deterministic parametric input generators. We present the key insight that mutations in the untyped parameter domain map to structural mutations in the input domain. Zest leverages program feedback in the form of code coverage and input validity to perform feedback-directed parameter search. We evaluate Zest against AFL and QuickCheck on five Java programs: Maven, Ant, BCEL, Closure, and Rhino. Zest covers 1.03x-2.81x as many branches within the benchmarks' semantic analysis stages as baseline techniques. Further, we find 10 new bugs in the semantic analysis stages of these benchmarks. Zest is the most effective technique in finding these bugs reliably and quickly, requiring at most 10 minutes on average to find each bug."}, {"id": "conf/issta/ChenYKQX19", "title": "Detecting memory errors at runtime with source-level instrumentation.", "authors": ["Zhe Chen", "Junqi Yan", "Shuanglong Kan", "Ju Qian", "Jingling Xue"], "DOIs": ["https://doi.org/10.1145/3293882.3330581"], "tag": ["Potpourri"], "abstract": "ABSTRACTThe unsafe language features of C, such as low-level control of memory, often lead to memory errors, which can result in silent data corruption, security vulnerabilities, and program crashes. Dynamic analysis tools, which have been widely used for detecting memory errors at runtime, usually perform instrumentation at the IR-level or binary-level. However, their underlying non-source-level instrumentation techniques have three inherent limitations: optimization sensitivity, platform dependence and DO-178C non-compliance. Due to optimization sensitivity, these tools are used to trade either performance for effectiveness by compiling the program at -O0 or effectiveness for performance by compiling the program at a higher optimization level, say, -O3.  In this paper, we overcome these three limitations by proposing a new source-level instrumentation technique and implementing it in a new dynamic analysis tool, called MOVEC, in a pointer-based instrumentation framework. Validation against a set of 86 microbenchmarks (with ground truth) and a set of 10 MiBench benchmarks shows that MOVEC outperforms state-of-the-art tools, SoftBoundCETS, Google's AddressSanitizer and Valgrind, in terms of both effectiveness and performance considered together."}, {"id": "conf/issta/AlbertBGIS19", "title": "Optimal context-sensitive dynamic partial order reduction with observers.", "authors": ["Elvira Albert", "Maria Garcia de la Banda", "Miguel G\u00f3mez-Zamalloa", "Miguel Isabel", "Peter J. Stuckey"], "DOIs": ["https://doi.org/10.1145/3293882.3330565"], "tag": ["Potpourri"], "abstract": "ABSTRACTDynamic Partial Order Reduction (DPOR) algorithms are used in stateless model checking to avoid the exploration of equivalent execution sequences. DPOR relies on the notion of independence between execution steps to detect equivalence. Recent progress in the area has introduced more accurate ways to detect independence: Context-Sensitive DPOR considers two steps p and t independent in the current state if the states obtained by executing p \u00b7 t and t \u00b7 p are the same; Optimal DPOR with Observers makes their dependency conditional to the existence of future events that observe their operations. We introduce a new algorithm, Optimal Context-Sensitive DPOR with Observers, that combines these two notions of conditional independence, and goes beyond them by exploiting their synergies. Experimental evaluation shows that our gains increase exponentially with the size of the considered inputs."}, {"id": "conf/issta/KolluriNSHS19", "title": "Exploiting the laws of order in smart contracts.", "authors": ["Aashish Kolluri", "Ivica Nikolic", "Ilya Sergey", "Aquinas Hobor", "Prateek Saxena"], "DOIs": ["https://doi.org/10.1145/3293882.3330560"], "tag": ["Potpourri"], "abstract": "ABSTRACTWe investigate a family of bugs in blockchain-based smart contracts, which we dub event-ordering (or EO) bugs. These bugs are intimately related to the dynamic ordering of contract events, i.e. calls of its functions, and enable potential exploits of millions of USD worth of crypto-coins. Previous techniques to detect EO bugs have been restricted to those bugs that involve just one or two event orderings. Our work provides a new formulation of the general class of EO bugs arising in long permutations of such events by using techniques from concurrent program analysis. The technical challenge in detecting EO bugs in blockchain contracts is the inherent combinatorial blowup in path and state space analysis, even for simple contracts. We propose the first use of partial-order reduction techniques, using automatically extracted happens-before relations along with several dynamic symbolic execution optimizations. We build EthRacer, an automatic analysis tool that runs directly on Ethereum bytecode and requires no hints from users. It flags 8% of over 10, 000 contracts analyzed, providing compact event traces (witnesses) that human analysts can examine in only a few minutes per contract. More than half of the flagged contracts are likely to have unintended behaviour."}, {"id": "conf/issta/WangGJXZY0S19", "title": "Go-clone: graph-embedding based clone detector for Golang.", "authors": ["Cong Wang", "Jian Gao", "Yu Jiang", "Zhenchang Xing", "Huafeng Zhang", "Weiliang Yin", "Ming Gu", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3293882.3338996"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTGolang (short for Go programming language) is a fast and compiled language, which has been increasingly used in industry due to its excellent performance on concurrent programming. Golang redefines concurrent programming grammar, making it a challenge for traditional clone detection tools and techniques. However, there exist few tools for detecting duplicates or copy-paste related bugs in Golang. Therefore, an effective and efficient code clone detector on Golang is especially needed.  In this paper, we present Go-Clone, a learning-based clone detector for Golang. Go-Clone contains two modules -- the training module and the user interaction module. In the training module, firstly we parse Golang source code into llvm IR (Intermediate Representation). Secondly, we calculate LSFG (labeled semantic flow graph) for each program function automatically. Go-Clone trains a deep neural network model to encode LSFGs for similarity classification. In the user interaction module, users can choose one or more Golang projects. Go-Clone identifies and presents a list of function pairs, which are most likely clone code for user inspection. To evaluate Go-Clone's performance, we collect 6,110 commit versions from 48 Github projects to construct a Golang clone detection data set. Go-Clone can reach the value of AUC (Area Under Curve) and ACC (Accuracy) for 89.61% and 83.80% in clone detection. By testing several groups of unfamiliar data, we also demonstrates the generility of Go-Clone. The address of the abstract demo video: https://youtu.be/o5DogtYGbeo"}, {"id": "conf/issta/ChenWZS19", "title": "VFQL: combinational static analysis as query language.", "authors": ["Guang Chen", "Yuexing Wang", "Min Zhou", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1145/3293882.3338997"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTValue flow are widely used in static analysis to detect bugs. Existing techniques usually employ a pointer analysis and generate source sink summaries defined by problem domain, then a solver is invoked to determine whether the path is feasible. However, most of the tools does not provide an easy way for users to find user defined bugs within the same architecture of finding pre-defined bugs. This paper presents VFQL, an expressive query language on value flow graph and the framework to execute the query to find user defined defects. Moreover, VFQL provides a nice GUI to demonstrate the value flow graph and a modeling language to define system libraries or user libraries without code, which further enhances its usability. The experimental results on open benchmarks show that VFQL achieve a competitive performance against other state of art tools. The result of case study conducted on open source program shows that the flexible query and modeling language provide a great support in finding user specified defects."}, {"id": "conf/issta/LiZGCWWG19", "title": "VBSAC: a value-based static analyzer for C.", "authors": ["Chi Li", "Min Zhou", "Zuxing Gu", "Guang Chen", "Yuexing Wang", "Jiecheng Wu", "Ming Gu"], "DOIs": ["https://doi.org/10.1145/3293882.3338998"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTStatic analysis has long prevailed as a promising approach to detect program bugs at an early development process to increase software quality. However, such tools face great challenges to balance the false-positive rate and the false-negative rate in practical use. In this paper, we present VBSAC, a value-based static analyzer for C aiming to improve the precision and recall. In our tool, we employ a pluggable value-based analysis strategy. A memory skeleton recorder is designed to maintain the memory objects as a baseline. While traversing the control flow graph, diverse value-based plug-ins analyze the specific abstract domains and share program information to strengthen the computation. Simultaneously, checkers consume the corresponding analysis results to detect bugs. We also provide a user-friendly web interface to help users audit the bug detection results. Evaluation on two widely-used benchmarks shows that we perform better to state-of-the-art bug detection tools by finding 221-339 more bugs and improving F-Score 9.88%-40.32%."}, {"id": "conf/issta/AlbertCGRR19", "title": "SAFEVM: a safety verifier for Ethereum smart contracts.", "authors": ["Elvira Albert", "Jes\u00fas Correas", "Pablo Gordillo", "Guillermo Rom\u00e1n-D\u00edez", "Albert Rubio"], "DOIs": ["https://doi.org/10.1145/3293882.3338999"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTEthereum smart contracts are public, immutable and distributed and, as such, they are prone to vulnerabilities sourcing from programming mistakes of developers. This paper presents SAFEVM, a verification tool for Ethereum smart contracts that makes use of state-of-the-art verification engines for C programs. SAFEVM takes as input an Ethereum smart contract (provided either in Solidity source code, or in compiled EVM bytecode), optionally with assert and require verification annotations, and produces in the output a report with the verification results. Besides general safety annotations, SAFEVM handles the verification of array accesses: it automatically generates SV-COMP verification assertions such that C verification engines can prove safety of array accesses. Our experimental evaluation has been undertaken on all contracts pulled from etherscan.io (more than 24,000) by using as back-end verifiers CPAchecker, SeaHorn and VeryMax."}, {"id": "conf/issta/LiFW019", "title": "CoCoTest: collaborative crowdsourced testing for Android applications.", "authors": ["Haoyu Li", "Chunrong Fang", "Zhibin Wei", "Zhenyu Chen"], "DOIs": ["https://doi.org/10.1145/3293882.3339000"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTTesting Android applications is becoming more and more challenging due to the notorious fragmentation issues and the complexity of usage scenarios in different environments. Crowdsourced testing has grown as a trend, especially in mobile application testing. However, due to the lack of professionalism and communication, the crowd workers tend to submit low-quality and duplicate bug reports, leading to a waste of test resources on inspecting and aggregating such reports. To solve these problems, we developed a platform, CoCoTest, embracing the idea of collective intelligence. With the help of CoCoTest Android SDK, workers can efficiently capture a screenshot, write a short description and create a bug report. A series of bug reports are aggregated online and then recommended to the other workers in real time. The crowdsourced workers can (1) help review, verify and enrich each others' bug reports; (2) escape duplicate bug reports; (3) be guided to conduct more professional testing with the help of collective intelligence. CoCoTest can improve the quality of the final report and reduce test costs. The demo video can be found at https://youtu.be/PuVuPbNP4tY."}, {"id": "conf/issta/PanCYMYZ19", "title": "Androlic: an extensible flow, context, object, field, and path-sensitive static analysis framework for Android.", "authors": ["Linjie Pan", "Baoquan Cui", "Jiwei Yan", "Xutong Ma", "Jun Yan", "Jian Zhang"], "DOIs": ["https://doi.org/10.1145/3293882.3339001"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTStatic analysis is widely used to detect potential defects in apps. Existing analysis tools focus on specific problems and vary in supported sensitivity, which make them difficult to reuse and extend for new analysis tasks. This paper presents Androlic, a precise static analysis framework for Android which is flow, context, object, field and path-sensitive. Through configuration items and APIs provided by Androlic, developers can easily extend it to perform custom analysis tasks. Evaluation on an example program and 20 real-world apps show that Androlic can analyze apps with high precision and efficiency."}, {"id": "conf/issta/PadhyeLS19", "title": "JQF: coverage-guided property-based testing in Java.", "authors": ["Rohan Padhye", "Caroline Lemieux", "Koushik Sen"], "DOIs": ["https://doi.org/10.1145/3293882.3339002"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTWe present JQF, a platform for performing coverage-guided fuzz testing in Java. JQF is designed both for practitioners, who wish to find bugs in Java programs, as well as for researchers, who wish to implement new fuzzing algorithms.  Practitioners write QuickCheck-style test methods that take inputs as formal parameters. JQF instruments the test program's bytecode and continuously executes tests using inputs that are generated in a coverage-guided fuzzing loop. JQF's input-generation mechanism is extensible. Researchers can implement custom fuzzing algorithms by extending JQF's Guidance interface. A Guidance instance responds to code coverage events generated during the execution of a test case, such as function calls and conditional jumps, and provides the next input. We describe several guidances that currently ship with JQF, such as: semantic fuzzing with Zest, binary fuzzing with AFL, and complexity fuzzing with PerfFuzz.  JQF is a mature tool that is open-source and publicly available. At the time of writing, JQF has been successful in discovering 42 previously unknown bugs in widely used open-source software such as OpenJDK, Apache Commons, and the Google Closure Compiler."}, {"id": "conf/issta/RwemalikaKPTL19", "title": "Ukwikora: continuous inspection for keyword-driven testing.", "authors": ["Renaud Rwemalika", "Marinos Kintis", "Mike Papadakis", "Yves Le Traon", "Pierre Lorrach"], "DOIs": ["https://doi.org/10.1145/3293882.3339003"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTAutomation of acceptance test suites becomes necessary in the context of agile software development practices, which require rapid feedback on the quality of code changes. To this end, companies try to automate their acceptance tests as much as possible. Unfortunately, the growth of the automated test suites, by several automation testers, gives rise to potential test smells, i.e., poorly designed test code, being introduced in the test code base, which in turn may increase the cost of maintaining the code and creating new one. In this paper, we investigate this problem in the context of our industrial partner, BGL BNP Paribas, and introduce Ukwikora, an automated tool that statically analyzes acceptance test suites, enabling the continuous inspection of the test code base. Ukwikora targets code written in the Robot Framework syntax, a popular framework for writing Keyword-Driven tests. Ukwikora has been successfully deployed at BGL BNP Paribas, detecting issues otherwise unknown to the automation testers, such as the presence of duplicated test code, dead test code and dependency issues among the tests. The success of our case study reinforces the need for additional research and tooling for acceptance test suites."}, {"id": "conf/issta/LiHFJZC19", "title": "CTRAS: a tool for aggregating and summarizing crowdsourced test reports.", "authors": ["Yuying Li", "Rui Hao", "Yang Feng", "James A. Jones", "Xiaofang Zhang", "Zhenyu Chen"], "DOIs": ["https://doi.org/10.1145/3293882.3339004"], "tag": ["Tool Demonstration"], "abstract": "ABSTRACTIn this paper, we present CTRAS, a tool for automatically aggregating and summarizing duplicate crowdsourced test reports on the fly. CTRAS can automatically detect duplicates based on both textual information and the screenshots, and further aggregates and summarizes the duplicate test reports. CTRAS provides end users with a comprehensive and comprehensible understanding of all duplicates by identifying the main topics across the group of aggregated test reports and highlighting supplementary topics that are mentioned in subgroups of test reports. Also, it provides the classic tool of issue tracking systems, such as the project-report dashboard and keyword searching, and automates their classic functionalities, such as bug triaging and best fixer recommendation, to assist end users in managing and diagnosing test reports. Video: https://youtu.be/PNP10gKIPFs"}, {"id": "conf/issta/Laaber19", "title": "Continuous software performance assessment: detecting performance problems of software libraries on every build.", "authors": ["Christoph Laaber"], "DOIs": ["https://doi.org/10.1145/3293882.3338982"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTDegradation of software performance can become costly for companies and developers, yet it is hardly assessed continuously. A strategy that would allow continuous performance assessment of software libraries is software microbenchmarking, which faces problems such as excessive execution times and unreliable results that hinder wide-spread adoption in continuous integration. In my research, I want to develop techniques that allow including software microbenchmarks into continuous integration by utilizing cloud infrastructure and execution time reduction techniques. These will allow assessing performance on every build and therefore catching performance problems before they are released into the wild."}, {"id": "conf/issta/Mera19", "title": "Mining constraints for grammar fuzzing.", "authors": ["Micha\u00ebl Mera"], "DOIs": ["https://doi.org/10.1145/3293882.3338983"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTGrammar-based fuzzing has been shown to significantly improve bug detection in programs with highly structured inputs. However, since grammars are largely handwritten, it is rarely used as a standalone technique in large-spectrum fuzzers as it requires human expertise. To fill this gap, promising techniques begin to emerge to automate the extraction of context-free grammars directly from the program under test. Unfortunately, the resulting grammars are usually not expressive enough and generate too many wrong inputs to provide results capable of competing with other fuzzing techniques. In this paper we propose a technique to automate the creation of attribute grammars from context-free grammars, thus significantly lowering the barrier of entry for efficient and effective large-scale grammar-based fuzzing."}, {"id": "conf/issta/Grano19", "title": "A new dimension of test quality: assessing and generating higher quality unit test cases.", "authors": ["Giovanni Grano"], "DOIs": ["https://doi.org/10.1145/3293882.3338984"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTUnit tests form the first defensive line against the introduction of bugs in software systems. Therefore, their quality is of a paramount importance to produce robust and reliable software. To assess test quality, many organizations relies on metrics like code and mutation coverage. However, they are not always optimal to fulfill such a purpose. In my research, I want to make mutation testing scalable by devising a lightweight approach to estimate test effectiveness. Moreover, I plan to introduce a new metric measuring test focus\u2014as a proxy for the effort needed by developers to understand and maintain a test\u2014 that both complements code coverage to assess test quality and can be used to drive automated test case generation of higher quality tests."}, {"id": "conf/issta/KudjoC19", "title": "A cost-effective strategy for software vulnerability prediction based on bellwether analysis.", "authors": ["Patrick Kwaku Kudjo", "Jinfu Chen"], "DOIs": ["https://doi.org/10.1145/3293882.3338985"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTVulnerability Prediction Models (VPMs) aims to identify vulnerable and non-vulnerable components in large software systems. Consequently, VPMs presents three major drawbacks (i) finding an effective method to identify a representative set of features from which to construct an effective model. (ii) the way the features are utilized in the machine learning setup (iii) making an implicit assumption that parameter optimization would not change the outcome of VPMs. To address these limitations, we investigate the significant effect of the Bellwether analysis on VPMs. Specifically, we first develop a Bellwether algorithm to identify and select an exemplary subset of data to be considered as the Bellwether to yield improved prediction accuracy against the growing portfolio benchmark. Next, we build a machine learning approach with different parameter settings to show the improvement of performance of VPMs. The prediction results of the suggested models were assessed in terms of precision, recall, F-measure, and other statistical measures. The preliminary result shows the Bellwether approach outperforms the benchmark technique across the applications studied with F-measure values ranging from 51.1%-98.5%."}, {"id": "conf/issta/Tang19", "title": "Identifying error code misuses in complex system.", "authors": ["Wensheng Tang"], "DOIs": ["https://doi.org/10.1145/3293882.3338986"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTMany complex software systems use error codes to differentiate error states. Therefore, it is crucial to ensure those error codes are used correctly. Misuses of error codes can lead to hardly sensible but fatal system failures. These errors are especially difficult to debug, since the failure points are usually far away from the root causes. Existing static analysis approaches to detecting error handling bugs mainly focus on how an error code is propagated or used in a program. However, they do not consider whether an error code is correctly chosen for propagation or usage within different program contexts, and thus miss to detect many error code misuse bugs. In this work, we conduct an empirical study on error code misuses in a mature commercial system. We collect error code issues from the commit history and conclude three main causes of them. To further resolve this problem, we propose a static approach that can automatically detect error code misuses. Our approach takes error code definition and error domain assignment as the input, and uses a novel static analysis method to detect the occurrence of the three categories of error code misuses in the source code."}, {"id": "conf/issta/Isabel19", "title": "Conditional dynamic partial order reduction and optimality results.", "authors": ["Miguel Isabel"], "DOIs": ["https://doi.org/10.1145/3293882.3338987"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTTesting concurrent systems requires exploring all possible non-deterministic interleavings that the concurrent execution may have, as any of the interleavings may reveal an erroneous behaviour of the system. This introduces a combinatorial explosion on the number of states that must be considered, which leads often to a computationally intractable problem. In the present PhD thesis, this challenge will be addressed through the development of new Partial Order Reduction techniques (POR). The cornerstone of POR theory is the notion of independence, that is used to decided whether each pair of concurrent events p and t are in a race and thus both executions p\u00b7 t and t \u00b7 p must be explored. A fundamental goal of this thesis is to introduce notions of conditional independence \u2013which ensures the commutativity of the considered events p and t under certain conditions that can be evaluated in the explored state\u2013 with a DPOR algorithm in order to alleviate the combinatorial explosion problem. The new techniques that we propose in the thesis have been implemented within the SYCO tool. We have carried out accompanying experimental evaluations to prove the effectiveness and applicability of the proposed techniques. Finally, we have successfully verified a range of properties for several case studies of Software-Defined Networks to illustrate the potential of the approach, scaling to larger networks than related techniques."}, {"id": "conf/issta/Fu19", "title": "Towards scalable defense of information flow security for distributed systems.", "authors": ["Xiaoqin Fu"], "DOIs": ["https://doi.org/10.1145/3293882.3338988"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTIt is particularly challenging to defend common distributed systems against security vulnerabilities because of the complexity and their large sizes. However, traditional solutions, that attack the information flow security problem, often fail for large, complex real-world distributed systems due to scalability problems. The problem would be even exacerbated for the online defense of continuously-running systems. My proposed research consists of three connected themes. First, I have developed metrics to help users understand and analyze the security characteristics of distributed systems at runtime in relation to their coupling measures. Then, I have also developed a highly scalable, cost-effective dynamic information flow analysis approach for distributed systems. It can detect implicit dependencies and find real security vulnerabilities in industrial distributed systems with practical portability and scalability. In order to thoroughly solve the scalability problem in general scenarios, I am developing a self-adaptive dynamic dependency analysis framework to monitor security issues during continuous running. In this proposal, I outline the three projects in a related manner as to how they consistently target the central objective of my thesis research."}, {"id": "conf/issta/Peng19", "title": "On the correctness of GPU programs.", "authors": ["Chao Peng"], "DOIs": ["https://doi.org/10.1145/3293882.3338989"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTTesting is an important and challenging part of software development and its effectiveness depends on the quality of test cases. However, there exists no means of measuring quality of tests developed for GPU programs and as a result, no test case generation techniques for GPU programs aiming at high test effectiveness. Existing criteria for sequential and multithreaded CPU programs cannot be directly applied to GPU programs as GPU follows a completely different memory and execution model.  We surveyed existing work on GPU program verification and bug fixes of open source GPU programs. Based on our findings, we define barrier, branch and loop coverage criteria and propose a set of mutation operators to measure fault finding capabilities of test cases. CLTestCheck, a framework for measuring quality of tests developed for GPU programs by code coverage analysis, fault seeding and work-group schedule amplification has been developed and evaluated using industry standard benchmarks. Experiments show that the framework is able to automatically measure test effectiveness and reveal unusual behaviours. Our planned work includes data flow coverage adopted for GPU programs to probe the underlying cause of unusual kernel behaviours and a more comprehensive work-group scheduler. We also plan to design and develop an automatic test case generator aiming at generating high quality test suites for GPU programs."}, {"id": "conf/issta/Lee19", "title": "JNI program analysis with automatically extracted C semantic summary.", "authors": ["Sungho Lee"], "DOIs": ["https://doi.org/10.1145/3293882.3338990"], "tag": ["Doctoral Symposium"], "abstract": "ABSTRACTFrom Oracle JVM to Android Runtime, most Java runtime environments officially support Java Native Interface (JNI) for interaction between Java and C. Using JNI, developers can improve Java program performance or reuse existing libraries implemented in C. At the same time, differences between the languages can lead to various kinds of unexpected bugs when developers do not understand the differences or comprehensive interoperation semantics completely. Furthermore, existing program analysis techniques do not cover the interoperation, which can reduce the quality of JNI programs.  We propose a JNI program analysis technique that analyzes Java and C code of JNI programs using analyzers targeting each language respectively. The C analyzer generates a semantic summary for each C function callable from Java and the Java analyzer constructs call graphs using the semantic summaries and Java code. In addition to the call graph construction, we extend the analysis technique to detect four bug types that can occur in the interoperation between the languages. We believe that our approach would be able to detect genuine bugs as well as improve the quality of JNI programs."}]}, "issre/issre": {"2017": [{"id": "conf/issre/IannilloNCN17", "title": "Chizpurfle: A Gray-Box Android Fuzzer for Vendor Service Customizations.", "authors": ["Antonio Ken Iannillo", "Roberto Natella", "Domenico Cotroneo", "Cristina Nita-Rotaru"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.16", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.16"], "tag": ["Best Paper Session"], "abstract": "Android has become the most popular mobile OS, as it enables device manufacturers to introduce customizations to compete with value-added services. However, customizations make the OS less dependable and secure, since they can introduce software flaws. Such flaws can be found by using fuzzing, a popular testing technique among security researchers.This paper presents Chizpurfle, a novel \"gray-box\" fuzzing tool for vendor-specific Android services. Testing these services is challenging for existing tools, since vendors do not provide source code and the services cannot be run on a device emulator. Chizpurfle has been designed to run on an unmodified Android OS on an actual device. The tool automatically discovers, fuzzes, and profiles proprietary services. This work evaluates the applicability and performance of Chizpurfle on the Samsung Galaxy S6 Edge, and discusses software bugs found in privileged vendor services."}, {"id": "conf/issre/LiuLTX17", "title": "Reflection Analysis for Java: Uncovering More Reflective Targets Precisely.", "authors": ["Jie Liu", "Yue Li", "Tian Tan", "Jingling Xue"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.36", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.36"], "tag": ["Best Paper Session"], "abstract": "Reflection, which is widely used in practice and abused by many security exploits, poses a significant obstacle to program analysis. Reflective calls can be analyzed statically or dynamically. Static analysis is more sound but also more imprecise (by introducing many false reflective targets and thus affecting its scalability). Dynamic analysis can be precise but often miss many true reflective targets due to low code coverage.We introduce MIRROR, the first automatic reflection analysis for Java that increases significantly the code coverage of dynamic analysis while keeping false reflective targets low. In its static analysis, a novel reflection-oriented slicing technique is applied to identify a small number of small path-based slices for a reflective call so that different reflective targets are likely exercised along these different paths. This preserves the soundness of pure static reflection analysis as much as possible, improves its scalability, and reduces substantially its false positive rate. In its dynamic analysis, these slices are executed with automatically generated test cases to report the reflective targets accessed. This significantly improves the code coverage of pure dynamic analysis. We evaluate MIRROR against a state-of-the-art dynamic reflection analysis tool, TAMIFLEX, by using 10 large real-world Java applications. MIRROR detects 12.5% - 933.3% more reflective targets efficiently (in 362.8 seconds on average) without producing any false positives. These new targets enable 5 - 174949 callgraph edges to be reachable in the application code."}, {"id": "conf/issre/XuSKX17", "title": "GEMS: An Extract Method Refactoring Recommender.", "authors": ["Sihan Xu", "Aishwarya Sivaraman", "Siau-Cheng Khoo", "Jing Xu"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.35", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.35"], "tag": ["Best Paper Session"], "abstract": "Extract Method is a widely used refactoring operation to improve method comprehension and maintenance. Much research has been done to extract codefragments within the method body to form a new method. Criteria used for identifying extractable code is usually centered around degrees of cohesiveness, coupling and length of the method. However, automatic method extraction techniques have not been highly successful, since it can be hard to concretizethe criteria. In this work, we present a novel system that learns these criteria for Extract Method refactorings from open source repositories. We extractstructural and functional features, which encode the concepts of complexity, cohesion and coupling in our learning model, and train it to extract suitablecode fragments from a given source of a method. Our tool, GEMS, recommends a ranked list of code fragments with high accuracy and greatspeed. We evaluated our approach on several open source repositories and compared it against three state-of-the-art approaches-SEMI, JExtract andJDeodorant. The results on these open-source data show the superiority of our machine-learning-based approach in terms of effectiveness. We develop GEMS asan Eclipse plugin, with the intention to support software reliability through method extraction."}, {"id": "conf/issre/OkamuraD17", "title": "A Generalized Bivariate Modeling Framework of Fault Detection and Correction Processes.", "authors": ["Hiroyuki Okamura", "Tadashi Dohi"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.22", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.22"], "tag": ["Modelling"], "abstract": "This paper presents a generalized modeling framework of fault detection and correction processes with bivariate distributions. The presented framework includes almost all existing software reliability growth models, namely the models in which both fault detection and correction processes are described by non-homogeneous Poisson processes. In our framework, the time dependency of fault correction time corresponds to the correlation between fault detection and correction times. Moreover, we propose a new fault detection and correction process model with hyper-Erlang distributions, and develop the model parameter estimation algorithm via EM (expectation-maximization) algorithm. In numerical examples, we demonstrate the data fitting ability of hyper-Erlang model with actual fault detection and correction data of open source projects."}, {"id": "conf/issre/MasettiCG17", "title": "A Stochastic Modeling Approach for an Efficient Dependability Evaluation of Large Systems with Non-anonymous Interconnected Components.", "authors": ["Giulio Masetti", "Silvano Chiaradonna", "Felicita Di Giandomenico"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.17", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.17"], "tag": ["Modelling"], "abstract": "This paper addresses the generation of stochastic models for dependability and performability analysis of complex systems, through automatic replication of template models. The proposed solution is tailored to systems composed by large populations of similar non-anonymous components, interconnected with each other according to a variety of topologies. A new efficient replication technique is presented and its implementation is discussed. The goal is to improve the performance of simulation solvers with respect to standard approaches, when employed in the modeling of the addressed class of systems, in particular for loosely interconnected system components (as typically encountered in the electrical or transportation sectors). Effectiveness of the new technique is demonstrated by comparison with a state of the art alternative solution on a representative case study."}, {"id": "conf/issre/QiuZTY17", "title": "Understanding the Impacts of Influencing Factors on Time to a DataRace Software Failure.", "authors": ["Kun Qiu", "Zheng Zheng", "Kishor S. Trivedi", "Bei-Bei Yin"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.26", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.26"], "tag": ["Modelling"], "abstract": "Datarace is a common problem on shared-memory parallel computers, including multicores. Due to its dependence on the thread scheduling scheme of its execution environment, the time to a datarace failure is usually very long. How to accelerate the occurrence of a datarace failure and further estimate the mean time to failure (MTTF) is an important topic to be studied. In this paper, the influencing factors for failures triggered by datarace bugs are explored and their influences on the time to datarace failure including the relationship with the MTTF are empirically studied. Experiments are conducted on real datarace suffering programs to verify the factors and their influences. Empirical results show that the influencing factors do have influences on the time to datarace failure of the subjects. They can be used to accelerate the occurrence of datarace failures and accurately estimate the MTTF."}, {"id": "conf/issre/GazzolaMPP17", "title": "An Exploratory Study of Field Failures.", "authors": ["Luca Gazzola", "Leonardo Mariani", "Fabrizio Pastore", "Mauro Pezz\u00e8"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.10", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.10"], "tag": ["Faults and Failures Analysis (1)"], "abstract": "Field failures, that is, failures caused by faults that escape the testing phase leading to failures in the field, are unavoidable. Improving verification and validation activities before deployment can identify and timely remove many but not all faults, and users may still experience a number of annoying problems while using their software systems.This paper investigates the nature of field failures, to understand to what extent further improving in-house verification and validation activities can reduce the number of failures in the field, and frames the need of new approaches that operate in the field.We report the results of the analysis of the bug reports of five applications belonging to three different ecosystems, propose a taxonomy of field failures, and discuss the reasons why failures belonging to the identified classes cannot be detected at design time but shall be addressed at runtime. We observe that many faults (70%) are intrinsically hard to detect at design-time."}, {"id": "conf/issre/YuLYJLY17", "title": "Learning from Imbalanced Data for Predicting the Number of Software Defects.", "authors": ["Xiao Yu", "Jin Liu", "Zijiang Yang", "Xiangyang Jia", "Qi Ling", "Sizhe Ye"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.18", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.18"], "tag": ["Faults and Failures Analysis (1)"], "abstract": "Predicting the number of defects in software modules can be more helpful in the case of limited testing resources. The highly imbalanced distribution of the target variable values (i.e., the number of defects) degrades the performance of models for predicting the number of defects. As the first effort of an in-depth study, this paper explores the potential of using resampling techniques and ensemble learning techniques to learn from imbalanced defect data for predicting the number of defects. We study the use of two extended resampling strategies (i.e., SMOTE and RUS) for regression problem and an ensemble learning technique (i.e., the AdaBoost.R2 algorithm) to handle imbalanced defect data for predicting the number of defects. We refer to the extension of SMOTE and RUS for predicting the Number of Defects as SmoteND and RusND, respectively. Experimental results on 6 datasets with two performance measures show that these approaches are effective in handling imbalanced defect data. To further improve the performance of these approaches, we propose two novel hybrid resampling/boosting algorithms, called SmoteNDBoost and RusNDBoost, which introduce SmoteND and RusND into the AdaBoost.R2 algorithm, respectively. Experimental results show that SmoteNDBoost and RusNDBoost both outperform their individual components (i.e., SmoteND, RusND and AdaBoost.R2)."}, {"id": "conf/issre/CotroneoNR17", "title": "A Fault Correlation Approach to Detect Performance Anomalies in Virtual Network Function Chains.", "authors": ["Domenico Cotroneo", "Roberto Natella", "Stefano Rosiello"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.12", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.12"], "tag": ["Faults and Failures Analysis (1)"], "abstract": "Network Function Virtualization is an emerging paradigm to allow the creation, at software level, of complex network services by composing simpler ones. However, this paradigm shift exposes network services to faults and bottlenecks in the complex software virtualization infrastructure they rely on. Thus, NFV services require effective anomaly detection systems to detect the occurrence of network problems. The paper proposes a novel approach to ease the adoption of anomaly detection in production NFV services, by avoiding the need to train a model or to calibrate a threshold. The approach infers the service health status by collecting metrics from multiple elements in the NFV service chain, and by analyzing their (lack of) correlation over the time. We validate this approach on an NFV-oriented Interactive Multimedia System, to detect problems affecting the quality of service, such as the overload, component crashes, avalanche restarts and physical resource contention."}, {"id": "conf/issre/XiaoZYTDC17", "title": "Experience Report: Fault Triggers in Linux Operating System: from Evolution Perspective.", "authors": ["Guanping Xiao", "Zheng Zheng", "Beibei Yin", "Kishor S. Trivedi", "Xiaoting Du", "Kai-Yuan Cai"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.21", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.21"], "tag": ["Faults and Failures Analysis (2)"], "abstract": "Linux operating system is a complex system that is prone to suffer failures during usage, and increases difficulties of fixing bugs. Different testing strategies and fault mitigation methods can be developed and applied based on different types of bugs, which leads to the necessity to have a deep understanding of the nature of bugs in Linux. In this paper, an empirical study is carried out on 5741 bug reports of Linux kernel from an evolution perspective. A bug classification is conducted based on fault triggering conditions, followed by the analysis of the evolution of bug type proportions over versions and time, together with their comparisons across versions, products and regression bugs. Moreover, the relationship between bug type proportions and clustering coefficient, as well as the relation between bug types and time to fix are presented. This paper reveals 13 interesting findings based on the empirical results and further provides guidance for developers and users based on these findings."}, {"id": "conf/issre/Leeke17", "title": "Simultaneous Fault Models for the Generation of Efficient Error Detection Mechanisms.", "authors": ["Matthew Leeke"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.29", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.29"], "tag": ["Faults and Failures Analysis (2)"], "abstract": "The application of machine learning to software fault injection data has been shown to be an effective approach for the generation of efficient error detection mechanisms (EDMs). However, such approaches to the design of EDMs have invariably adopted a fault model with a single-fault assumption, limiting the practical relevance of the detectors and their evaluation. Software containing more than a single fault is commonplace, with prominent safety standards recognising that critical failures are often the result of unlikely or unforeseen combinations of faults. This paper addresses this shortcoming, demonstrating that it is possible to generate similarly efficient EDMs under more realistic fault models. In particular, it is shown that (i) efficient EDMs can be designed using fault data collected under models accounting for the occurrence of simultaneous faults, (ii) exhaustive fault injection under a simultaneous bit flip model can yield improvements to EDM efficiency, and (iii) exhaustive fault injection under a simultaneous bit flip model can made non-exhaustive, reducing the resource costs of experimentation to practicable levels, without sacrificing resultant EDM efficiency."}, {"id": "conf/issre/HuangLXWL17", "title": "Which Packages Would be Affected by This Bug Report?", "authors": ["Qiao Huang", "David Lo", "Xin Xia", "Qingye Wang", "Shanping Li"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.24", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.24"], "tag": ["Faults and Failures Analysis (2)"], "abstract": "A large project (e.g., Ubuntu) usually contains a large number of software packages. Sometimes the same bug report in such project would affect multiple packages, and developers of different packages need to collaborate with one another to fix the bug. Unfortunately, the total number of packages involved in a project like Ubuntu is relatively large, which makes it time-consuming to manually identify packages that are affected by a bug report. In this paper, we propose an approach named PkgRec that consists of 2 components: a name matching component and an ensemble learning component. In the name matching component, we assign a confidence score for a package if it is mentioned by a bug report. In the ensemble learning component, we divide the training dataset into n subsets and build a sub-classifier on each subset. Then we automatically determine an appropriate weight for each sub-classifier and combine them to predict the confidence score of a package being affected by a new bug report. Finally, PkgRec combines the name matching component and the ensemble learning component to assign a final confidence score to each potential package. A list of top-k packages with the highest confidence scores would then be recommended. We evaluate PkgRec on 3 datasets including Ubuntu, OpenStack, and GNOME with a total number of 42,094 bug reports. We show that PkgRec could achieve recall@5 and recall@10 scores of 0.511-0.737, and 0.614-0.785, respectively. We also compare PkgRec with other state-of-art approaches, namely LDA-KL and MLkNN. The experiment results show that PkgRec on average improves recall@5 and recall@10 scores of LDA-KL by 47% and 31%, and MLkNN by 52% and 37%, respectively."}, {"id": "conf/issre/CerveiraBM17", "title": "Experience Report: On the Impact of Software Faults in the Privileged Virtual Machine.", "authors": ["Frederico Cerveira", "Raul Barbosa", "Henrique Madeira"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.39", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.39"], "tag": ["WAP and PER"], "abstract": "Cloud computing is revolutionizing how organizations treat computing resources. The privileged virtual machine is a key component in systems that use virtualization, but poses a dependability risk for several reasons. The activation of residual software faults that exist in every software project is a real threat and can impact the correct operation of the entire virtualized system. To study this question, we begin by performing a detailed analysis of the privileged virtual machine and its components, followed by software fault injection campaigns that target two of those important components - toolstack and a device driver. The obstacles faced during this experimental phase and how they were overcome is herein described with practitioners in mind. The results show that software faults in those components can have either no impact or lead to drastic failures, showing that the privileged virtual machine is a single point of failure that must be protected (for 4-9% of the faults). Most of the failures are detectable by monitoring basic functionalities, but some faults caused inconsistent states that manifest later on. No silent data failures (SDF) have been observed, but the number of faults injected so far only allows to conclude that SDF are not very frequent."}, {"id": "conf/issre/LuoW17", "title": "WAP: SAT-Based Computation of Minimal Cut Sets.", "authors": ["Weilin Luo", "Ou Wei"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.13", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.13"], "tag": ["WAP and PER"], "abstract": "Fault tree analysis (FTA) is a prominent reliability analysis method widely used in safety-critical industries. Computing minimal cut sets (MCSs), i.e., finding all the smallest combination of basic events that result in the top level event, plays a fundamental role in FTA. Classical methods have been proposed based on manipulation of boolean expressions of fault trees and Binary Decision Diagrams. However, given the inherent intractability of computing MCSs, developing new methods over different paradigms remains to be an interesting research direction. In this paper, motivated by recent progress on modern SAT solver, we present a new method for computing MCSs based on SAT solving. Specifically, given a fault tree, we iteratively search for a cut set based on the DPLL framework. By exploiting local failure propagation paths in the fault tree, we provide efficient algorithms for extracting an MCS from the cut set. The information of a new MCS is learned as a blocking clause for SAT solving, which helps to prune search space and ensures completeness of the results. We compare our method with a popular commercial FTA tool on practical fault trees. Preliminary results show that our method exhibits better performance on time and memory usage."}, {"id": "conf/issre/Goseva-Popstojanova17", "title": "Experience Report: Security Vulnerability Profiles of Mission Critical Software: Empirical Analysis of Security Related Bug Reports.", "authors": ["Katerina Goseva-Popstojanova", "Jacob Tyo"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.42", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.42"], "tag": ["WAP and PER"], "abstract": "While some prior research work exists on characteristics of software faults (i.e., bugs) and failures, very little work has been published on analysis of software applications vulnerabilities. This paper aims to contribute towards filling that gap by presenting an empirical investigation of application vulnerabilities. The results are based on data extracted from issue tracking systems of two NASA missions. These data were organized in three datasets: Ground mission IV&V issues, Flight mission IV&V issues, and Flight mission Developers issues. In each dataset, we identified the security related software bugs and classified them in specific vulnerability classes. Then, we created the vulnerability profiles, i.e., determined where and when the security vulnerabilities were introduced and what were the dominant vulnerabilities classes. Our main findings include: (1) In IV&V issues datasets the majority of vulnerabilities were code related and were introduced in the Implementation phase. (2) For all datasets, close to 90% of the vulnerabilities were located in two to four subsystems. (3) Out of 21 primary vulnerability classes, five dominated: Exception Management, Memory Access, Other, Risky Values, and Unused Entities. Together, they contributed from around 80% to 90% of vulnerabilities in each dataset."}, {"id": "conf/issre/MurakamiTU17", "title": "WAP: Does Reviewer Age Affect Code Review Performance?", "authors": ["Yukasa Murakami", "Masateru Tsunoda", "Hidetake Uwano"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.37", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.37"], "tag": ["WAP and PER"], "abstract": "We focus on developer code review performance, and analyze whether the age of a subject affects the efficiency and preciseness of their code. Generally, older coders have more experience. Therefore, the age is considered to positively affect code review. However, in our past study, code understanding speed was relatively slow for older subjects, and memory is needed to understand programs. Similarly, during code review, a subject's age may affect efficiency (e.g., the number of indications per unit time). In the experiment, subjects reviewed source code, referring to mini specification documents. When the code did not follow the document, the subjects indicated the error. We classified subjects into senior and junior groups. In the analysis, we stratified the results based on age, and used correlation coefficients and multiple linear regression to clarify the relationship between age and review performance. We found that age does not affect the efficiency and correctness of code review. Also, the software development experience of subjects is not significantly correlated to performance."}, {"id": "conf/issre/BragaDALV17", "title": "Practical Evaluation of Static Analysis Tools for Cryptography: Benchmarking Method and Case Study.", "authors": ["Alexandre Melo Braga", "Ricardo Dahab", "Nuno Antunes", "Nuno Laranjeiro", "Marco Vieira"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.27", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.27"], "tag": ["Dynamic and Static Analysis"], "abstract": "The incorrect use of cryptography is a common source of critical software vulnerabilities. As developers lack knowledge in applied cryptography and support from experts is scarce, this situation is frequently addressed by adopting static code analysis tools to automatically detect cryptography misuse during coding and reviews, even if the effectiveness of such tools is far from being well understood. This paper proposes a method for benchmarking static code analysis tools for the detection of cryptography misuse, and evaluates the method in a case study, with the goal of selecting the most adequate tools for specific development contexts. Our method classifies cryptography misuse in nine categories recognized by developers (weak cryptography, poor key management, bad randomness, etc.) and provides the workload, metrics and procedure needed for a fair assessment and comparison of tools. We found that all evaluated tools together detected only 35% of cryptography misuses in our tests. Furthermore, none of the evaluated tools detected insecure elliptic curves, weak parameters in key agreement, and most insecure configurations for RSA and ECDSA. This suggests cryptography misuse is underestimated by tool builders. Despite that, we show that it is possible to benefit from an adequate tool selection during the development of cryptographic software."}, {"id": "conf/issre/JakseFMP17", "title": "Interactive Runtime Verification - When Interactive Debugging Meets Runtime Verification.", "authors": ["Rapha\u00ebl Jakse", "Yli\u00e8s Falcone", "Jean-Fran\u00e7ois M\u00e9haut", "Kevin Pouget"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.19", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.19"], "tag": ["Dynamic and Static Analysis"], "abstract": "Runtime Verification consists in studying a system at runtime, looking for input and output events to discover, check or enforce behavioral properties. Interactive debugging consists in studying a system at runtime in order to discover and understand its bugs and fix them, inspecting interactively its internal state.Interactive Runtime Verification (i-RV) combines runtime verification and interactive debugging. We define an efficient and convenient way to check behavioral properties automatically on a program using a debugger. We aim at helping bug discovery and understanding by guiding classical interactive debugging techniques using runtime verification."}, {"id": "conf/issre/BogdiukiewiczBH17", "title": "Formal Development of Policing Functions for Intelligent Systems.", "authors": ["Chris Bogdiukiewicz", "Michael J. Butler", "Thai Son Hoang", "Martin Paxton", "James Snook", "Xanthippe Waldron", "Toby Wilkinson"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.40", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.40"], "tag": ["Dynamic and Static Analysis"], "abstract": "We present an approach for ensuring safety properties of autonomous systems. Our contribution is a system architecture where a policing function validating system safety properties at runtime is separated from the system's intelligent planning function. The policing function is developed formally by a correct-by-construction method. The separation of concerns enables the possibility of replacing and adapting the intelligent planning function without changing the validation approach. We validate our approach on the example of a multi-UAV system managing route generation. Our prototype runtime validator has been integrated and evaluated with an industrial UAV synthetic environment."}, {"id": "conf/issre/GorbenkoRTB17", "title": "Experience Report: Study of Vulnerabilities of Enterprise Operating Systems.", "authors": ["Anatoliy Gorbenko", "Alexander B. Romanovsky", "Olga Tarasyuk", "Oleksandr Biloborodov"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.20", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.20"], "tag": ["Security Modeling and Empirical Studies"], "abstract": "This experience report analyses security problems of modern computer systems caused by vulnerabilities in their operating systems. An aggregated vulnerability database has been developed by joining vulnerability records from two publicly available vulnerability databases: the Common Vulnerabilities and Exposures system (CVE) and the National Vulnerabilities database (NVD). The aggregated data allow us to investigate the stages of the vulnerability life cycle, vulnerability disclosure and the elimination statistics for different operating systems. The specific technical areas the paper covers are the quantitative assessment of vulnerabilities discovered and fixed in operating systems, the estimation of time that vendors spend on patch issuing, and the analysis of the vulnerability criticality and identification of vulnerabilities common for different operating systems."}, {"id": "conf/issre/MedeirosI0V17", "title": "Software Metrics as Indicators of Security Vulnerabilities.", "authors": ["Nadia Patricia Da Silva Medeiros", "Naghmeh Ivaki", "Pedro Costa", "Marco Vieira"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.11", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.11"], "tag": ["Security Modeling and Empirical Studies"], "abstract": "Detecting software security vulnerabilities and distinguishing vulnerable from non-vulnerable code is anything but simple. Most of the time, vulnerabilities remain undisclosed until they are exposed, for instance, by an attack during the software operational phase. Software metrics are widely-used indicators of software quality, but the question is whether they can be used to distinguish vulnerable software units from the non-vulnerable ones during development. In this paper, we perform an exploratory study on software metrics, their interdependency, and their relation with security vulnerabilities. We aim at understanding: i) the correlation between software architectural characteristics, represented in the form of software metrics, and the number of vulnerabilities; and ii) which are the most informative and discriminative metrics that allow identifying vulnerable units of code. To achieve these goals, we use, respectively, correlation coefficients and heuristic search techniques. Our analysis is carried out on a dataset that includes software metrics and reported security vulnerabilities, exposed by security attacks, for all functions, classes, and files of five widely used projects. Results show: i) a strong correlation between several project-level metrics and the number of vulnerabilities, ii) the possibility of using a group of metrics, at both file and function levels, to distinguish vulnerable and non-vulnerable code with a high level of accuracy."}, {"id": "conf/issre/Popov17", "title": "Models of Reliability of Fault-Tolerant Software Under Cyber-Attacks.", "authors": ["Peter Popov"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.23", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.23"], "tag": ["Security Modeling and Empirical Studies"], "abstract": "This paper offers a new approach to modelling the effect of cyber-attacks on reliability of software used in industrial control applications. The model is based on the view that successful cyber-attacks introduce failure regions, which are not present in non-compromised software. The model is then extended to cover a fault tolerant architecture, such as the 1-out-of-2 software, popular for building industrial protection systems. The model is used to study the effectiveness of software maintenance policies such as patching and \"cleansing\" (\"proactive recovery\") under different adversary models ranging from independent attacks to sophisticated synchronized attacks on the channels. We demonstrate that the effect of attacks on reliability of diverse software significantly depends on the adversary model. Under synchronized attacks system reliability may be more than an order of magnitude worse than under independent attacks on the channels. These findings, although not surprising, highlight the importance of using an adequate adversary model in the assessment of how effective various cyber-security controls are."}, {"id": "conf/issre/MoorselFR17", "title": "Experience Report: How to Design Web-Based Competitions for Legal Proceedings: Lessons from a Court Case.", "authors": ["Aad P. A. van Moorsel", "Matthew Forshaw", "Francisco Rocha"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.41", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.41"], "tag": ["Security Assessment and Quality Assurance"], "abstract": "In this practical experience report we discuss a court case in which one of the authors was expert witness. This UK civil case considered possible fraud in an online product promotion competition, with participants being denied prizes because they were considered to have cheated. The discussion in this paper aims to provide a practice-led perspective on the link between technology and legal issues in the design of online games and web applications. The paper presents the court's questions and the witness responses, and also provides a synopsis of analysis of data in the web server log file presented to court. Based on the insights gained, we present guidelines for the design of online competitions and for client-server web applications implementing it. As we will see, the case turned out to be about design of socio-technical systems, not about advanced technologies. It illustrates the need to identify practically relevant threat models and pragmatic security solutions that balance business, legal and usability concerns."}, {"id": "conf/issre/FuZLKW17", "title": "Perman: Fine-Grained Permission Management for Android Applications.", "authors": ["Jiaojiao Fu", "Yangfan Zhou", "Huan Liu", "Yu Kang", "Xin Wang"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.38", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.38"], "tag": ["Security Assessment and Quality Assurance"], "abstract": "Third-party libraries (3PLs) are widely introduced into Android apps and they typically request permissions for their own functionalities. Current Android systems manage permissions in process (app) granularity. Hence, the host app and the 3PLs share the same permission set. 3PL-apps may therefore introduce security risks. Separating the permission sets of the 3PLs and those of the host app are critical to alleviate such security risks. In this paper, we provide Perman, a tool that allows users to manage permissions of different modules (i.e., a 3PL or the host app) of an app at runtime. Perman relies on dynamic code instrumentation to intercept permission requests, and accordingly provide a policy-based permission control. Unlike existing tools that generally require to redesign 3PL-apps, it can thus be applied to the existing apps in market. We evaluate Perman on real-world apps. The experiment results verify its effectiveness in fine-grained permission management."}, {"id": "conf/issre/JohnsenLHP17", "title": "AQAT: The Architecture Quality Assurance Tool for Critical Embedded Systems.", "authors": ["Andreas Johnsen", "Kristina Lundqvist", "Kaj H\u00e4nninen", "Paul Pettersson"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.32", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.32"], "tag": ["Security Assessment and Quality Assurance"], "abstract": "Architectural engineering of embedded systems comprehensively affects both the development processes and the abilities of the systems. Verification of architectural engineering is consequently essential in the development of safety- and missioncritical embedded system to avoid costly and hazardous faults. In this paper, we present the Architecture Quality Assurance Tool (AQAT), an application program developed to provide a holistic, formal, and automatic verification process for architectural engineering of critical embedded systems. AQAT includes architectural model checking, model-based testing, and selective regression verification features to effectively and efficiently detect design faults, implementation faults, and faults created by maintenance modifications. Furthermore, the tool includes a feature that analyzes architectural dependencies, which in addition to providing essential information for impact analyzes of architectural design changes may be used for hazard analysis, such as the identification of potential error propagations, common cause failures, and single point failures. Overviews of both the graphical user interface and the back-end processes of AQAT are presented with a sensor-to-actuator system example."}, {"id": "conf/issre/JohnsenLHPT17", "title": "Experience Report: Evaluating Fault Detection Effectiveness and Resource Efficiency of the Architecture Quality Assurance Framework and Tool.", "authors": ["Andreas Johnsen", "Kristina Lundqvist", "Kaj H\u00e4nninen", "Paul Pettersson", "Martin Torelm"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.31", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.31"], "tag": ["PER:\nReliability"], "abstract": "The Architecture Quality Assurance Framework (AQAF) is a theory developed to provide a holistic and formal verification process for architectural engineering of critical embedded systems. AQAF encompasses integrated architectural model checking, model-based testing, and selective regression verification techniques to achieve this goal. The Architecture Quality Assurance Tool (AQAT) implements the theory of AQAF and enables automated application of the framework. In this paper, we present an evaluation of AQAT and the underlying AQAF theory by means of an industrial case study, where resource efficiency and fault detection effectiveness are the targeted properties of evaluation. The method of fault injection is utilized to guarantee coverage of fault types and to generate a data sample size adequate for statistical analysis. We discovered important areas of improvement in this study, which required further development of the framework before satisfactory results could be achieved. The final results present a 100% fault detection rate at the design level, a 98.5% fault detection rate at the implementation level, and an average increased efficiency of 6.4% with the aid of the selective regression verification technique."}, {"id": "conf/issre/GoldsteinRS17", "title": "Experience Report: Log-Based Behavioral Differencing.", "authors": ["Maayan Goldstein", "Danny Raz", "Itai Segall"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.14", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.14"], "tag": ["PER:\nReliability"], "abstract": "Monitoring systems and ensuring the required service level is an important operation task. However, doing this based on external visible data, such as systems logs, is very difficult since it is very hard to extract from the logged data the exact state and the root cause to the actions taken by the system. Yet, identifying behavioral changes of complex systems can be used for early identification of problems and allow proactive correction measurements. Since it is practically impossible to perform this task manually, there is a critical need for a methodology that can analyze logs, automatically create a behavioral model, and compare the behavior to the expected behavior.In this paper we propose a novel approach for comparison between serviceexecutions as exhibited in their log files. The behavior is captured by FiniteState Automaton models (FSAs), enhanced with performance related data, bothmined from the logs. Our tool then computes the difference between the current model and behavioral models created when the service was known to operate well. A visual framework that graphically presents and emphasizes the changes in the behavior is then used to trace their root cause. We evaluate our approach over real telecommunication logs."}, {"id": "conf/issre/AyubRS17", "title": "Experience Report: Verifying MPI Java Programs Using Software Model Checking.", "authors": ["Muhammad Sohaib Ayub", "Waqas ur Rehman", "Junaid Haroon Siddiqui"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.15", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.15"], "tag": ["PER:\nReliability"], "abstract": "Parallel and distributed computing have enabled development of much more scalable software. However, developing concurrent software requires the programmer to be aware of nondeterminism, data races, and deadlocks. MPI (message passing interface) is a popular standard for writing message-oriented distributed applications. Some messages in MPI systems can be processed by one of the many machines and in many possible orders. This non-determinism can affect the result of an MPI application. The alternate results may or may not be correct. To verify MPI applications, we need to check all these possible orderings and use an application specific oracle to decide if these orderings give correct output. MPJ Express is an open source Java implementation of the MPI standard. Model checking of MPI Java programs is a challenging task due to their parallel nature. We developed a Java based model of MPJ Express, where processes are modeled as threads, and which can run unmodified MPI Java programs on a single system. This model enabled us to adapt the Java PathFinder explicit state software model checker (JPF) using a custom listener to verify our model running real MPI Java programs. The evaluation of our approach shows that model checking reveals incorrect system behavior that results in very intricate message orderings."}, {"id": "conf/issre/KhalilL17", "title": "On FSM-Based Testing: An Empirical Study: Complete Round-Trip Versus Transition Trees.", "authors": ["Hoda Khalil", "Yvan Labiche"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.34", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.34"], "tag": ["Testing"], "abstract": "Finite state machines being intuitively understandable and suitable for modeling in many domains, they are adopted by many software designers. Therefore, testing systems that are modeled with state machines has received genuine attention. Among the studied testing strategies are complete round-trip paths and transition trees that cover round-trip paths in a piece wise manner. We present an empirical study that aims at comparing the effectiveness of the complete round-trip paths test suites to the transition trees test suites in one hand, and comparing the effectiveness of the different techniques used to generate transition trees (breadth first traversal, depth first traversal, and random traversal) on the other hand. We also compare the effectiveness of all the testing trees generated using each single traversal criterion. This is done through conducting an empirical evaluation using four case studies from different domains. Effectiveness is evaluated with mutants. Experimental results are presented and analyzed."}, {"id": "conf/issre/XieWYL17", "title": "COCOON: Crowdsourced Testing Quality Maximization Under Context Coverage Constraint.", "authors": ["Miao Xie", "Qing Wang", "Guowei Yang", "Mingshu Li"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.25", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.25"], "tag": ["Testing"], "abstract": "Mobile app testing is challenging since each test needs to be executed in a variety of operating contexts including heterogeneous devices, various wireless networks and different locations. Crowdsourcing enables a mobile app test to be distributed as a crowdsourced task to leverage crowd workers to accomplish the test. However, high test quality and expected test context coverage are difficult to achieve in crowdsourced testing. Upon distributing a test task, mobile app providers neither know who to participate nor predict whether all the expected test contexts can be covered in the task. To address this problem, we put forward a novel research problem called Crowdsourced Testing Quality Maximization Under Context Coverage Constraint (Cocoon). Given a mobile app test task, our objective is to recommend a set of workers, from available crowd workers, such that the expected test context coverage and a high test quality can be achieved. We prove that the Cocoon problem is NP-Complete and then introduce two greedy approaches. Based on a real dataset from the largest Chinese crowdsourced testing platform, our evaluation shows the effectiveness and efficiency of the two approaches, which can be potentially used as online services in practice."}, {"id": "conf/issre/ByunSRMH17", "title": "Toward Rigorous Object-Code Coverage Criteria.", "authors": ["Taejoon Byun", "Vaibhav Sharma", "Sanjai Rayadurgam", "Stephen McCamant", "Mats Per Erik Heimdahl"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.33", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.33"], "tag": ["Testing"], "abstract": "Object-branch coverage (OBC) is often used as a measure of the thoroughness of tests suites, augmenting or substituting source-code based structural criteria such as branch coverage and modified condition/decision coverage (MC/DC). In addition, with the increasing use of third-party components for which source-code access may be unavailable, robust object-code coverage criteria are essential to assess how well the components are exercised during testing. While OBC has the advantage of being programming language independent and is amenable to non-intrusive coverage measurement techniques, variations in compilers and the optimizations they perform can substantially change the structure of the generated code and the instructions used to represent branches. To address the need for a robust object coverage criterion, this paper proposes a rigorous definition of OBC such that it captures well the semantics of source code branches for a given instruction set architecture. We report an empirical assessment of these criteria for the Intel x86 instruction set on several examples from embedded control systems software. Preliminary results indicate that object-code coverage can be made robust to compilation variations and is comparable in its bug-finding efficacy to source level MC/DC."}, {"id": "conf/issre/AppeltPB17", "title": "Automatically Repairing Web Application Firewalls Based on Successful SQL Injection Attacks.", "authors": ["Dennis Appelt", "Annibale Panichella", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.28", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.28"], "tag": ["Machine Learning for Reliability and Security"], "abstract": "Testing and fixing Web Application Firewalls (WAFs) are two relevant and complementary challenges for security analysts. Automated testing helps to cost-effectively detect vulnerabilities in a WAF by generating effective test cases, i.e., attacks. Once vulnerabilities have been identified, the WAF needs to be fixed by augmenting its rule set to filter attacks without blocking legitimate requests. However, existing research suggests that rule sets are very difficult to understand and too complex to be manually fixed. In this paper, we formalise the problem of fixing vulnerable WAFs as a combinatorial optimisation problem. To solve it, we propose an automated approach that combines machine learning with multi-objective genetic algorithms. Given a set of legitimate requests and bypassing SQL injection attacks, our approach automatically infers regular expressions that, when added to the WAF's rule set, prevent many attacks while letting legitimate requests go through. Our empirical evaluation based on both open-source and proprietary WAFs shows that the generated filter rules are effective at blocking previously identified and successful SQL injection attacks (recall between 54.6% and 98.3%), while triggering in most cases no or few false positives (false positive rate between 0% and 2%)."}, {"id": "conf/issre/BerteroRST17", "title": "Experience Report: Log Mining Using Natural Language Processing and Application to Anomaly Detection.", "authors": ["Christophe Bertero", "Matthieu Roy", "Carla Sauvanaud", "Gilles Tr\u00e9dan"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.43", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.43"], "tag": ["Machine Learning for Reliability and Security"], "abstract": "Event logging is a key source of information on a system state. Reading logs provides insights on its activity, assess its correct state and allows to diagnose problems. However, reading does not scale: with the number of machines increasingly rising, and the complexification of systems, the task of auditing systems' health based on logfiles is becoming overwhelming for system administrators. This observation led to many proposals automating the processing of logs. However, most of these proposal still require some human intervention, for instance by tagging logs, parsing the source files generating the logs, etc. In this work, we target minimal human intervention for logfile processing and propose a new approach that considers logs as regular text (as opposed to related works that seek to exploit at best the little structure imposed by log formatting). This approach allows to leverage modern techniques from natural language processing. More specifically, we first apply a word embedding technique based on Google's word2vec algorithm: logfiles' words are mapped to a high dimensional metric space, that we then exploit as a feature space using standard classifiers. The resulting pipeline is very generic, computationally efficient, and requires very little intervention. We validate our approach by seeking stress patterns on an experimental platform. Results show a strong predictive performance (\u2248 90% accuracy) using three out-of-the-box classifiers."}, {"id": "conf/issre/LoyolaM17", "title": "Learning Feature Representations from Change Dependency Graphs for Defect Prediction.", "authors": ["Pablo Loyola", "Yutaka Matsuo"], "DOIs": ["https://doi.org/10.1109/ISSRE.2017.30", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2017.30"], "tag": ["Machine Learning for Reliability and Security"], "abstract": "Given the heterogeneity of the data that can be extracted from the software development process, defect prediction techniques have focused on associating different sources of data with the introduction of faulty code, usually relying on handcrafted features. While these efforts have generated considerable progress over the years, little attention has been given to the fact that the performance of any predictive model depends heavily on the representation of the data used, and that different representations can lead to different results. We consider this a relevant problem, as it could be affecting directly the efforts towards generating safer software systems. Therefore, we propose to study the impact of the representation of the data in defect prediction models. To this end, we focus on the use of developer activity data, from which we structure dependency graphs. Then, instead of manually generating features, such as network metrics, we propose two models inspired by recent advances in representation learning which are able to automatically generate feature representations from graph data. These new representations are compared against manually crafted features for defect prediction in real world software projects. Our results show that automatically learned features are competitive, reaching increments in prediction performance up to 13%."}], "2018": [{"id": "conf/issre/DurieuxHM18", "title": "Fully Automated HTML and Javascript Rewriting for Constructing a Self-Healing Web Proxy.", "authors": ["Thomas Durieux", "Youssef Hamadi", "Martin Monperrus"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00012", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00012"], "tag": ["Best Paper Award Nominees"], "abstract": "Over the last few years, the complexity of web applications has increased to provide more dynamic web applications to users. The drawback of this complexity is the growing number of errors in the front-end applications. In this paper, we present BikiniProxy, a novel technique to provide self-healing for the web. BikiniProxy is designed as an HTTP proxy that uses five self-healing strategies to rewrite the buggy HTML and Javascript code. We evaluate BikiniProxy with a new benchmark of 555 reproducible Javascript errors of which 31.76% can be automatically self-healed."}, {"id": "conf/issre/MaZPHD18", "title": "Robust and Rapid Adaption for Concept Drift in Software System Anomaly Detection.", "authors": ["Minghua Ma", "Shenglin Zhang", "Dan Pei", "Xin Huang", "Hongwei Dai"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00013", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00013"], "tag": ["Best Paper Award Nominees"], "abstract": "Anomaly detection is critical for web-based software systems. Anecdotal evidence suggests that in these systems, the accuracy of a static anomaly detection method that was previously ensured is bound to degrade over time. It is due to the significant change of data distribution, namely concept drift, which is caused by software change or personal preferences evolving. Even though dozens of anomaly detectors have been proposed over the years in the context of software system, they have not tackled the problem of concept drift. In this paper, we present a framework, StepWise, which can detect concept drift without tuning detection threshold or per-KPI (Key Performance Indicator) model parameters in a large scale KPI streams, take external factors into account to distinguish the concept drift which under operators' expectations, and help any kind of anomaly detection algorithm to handle it rapidly. For the prototype deployed in Sogou, our empirical evaluation shows StepWise improve the average F-score by 206% for many widely-used anomaly detectors over a baseline without any concept drift detection."}, {"id": "conf/issre/PietrantuonoRG18", "title": "Run-Time Reliability Estimation of Microservice Architectures.", "authors": ["Roberto Pietrantuono", "Stefano Russo", "Antonio Guerriero"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00014", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00014"], "tag": ["Best Paper Award Nominees"], "abstract": "Microservices are gaining popularity as an architectural paradigm for service-oriented applications, especially suited for highly dynamic contexts requiring loosely-coupled independent services, frequent software releases, decentralized governance and data management. Because of the high flexibility and evolvability characterizing microservice architectures (MSAs), it is difficult to estimate their reliability at design time, as it changes continuously due to the services' upgrades and/or to the way applications are used by customers. This paper presents a testing method for on-demand reliability estimation of microservice applications in their operational phase. The method allows to faithfully assess, upon request, the reliability of a MSA-based application under a scarce testing budget, at any time when it is in operation, and exploit field data about microservice usage and failing/successful demands. A new in-vivo testing algorithm is developed based on an adaptive web sampling strategy, named Microservice Adaptive Reliability Testing (MART). The method is evaluated by simulation, as well as by experimentation on an example application based on the Netflix Open Source Software MSA stack, with encouraging results in terms of estimation accuracy and, especially, efficiency."}, {"id": "conf/issre/CamilliBGS18", "title": "Online Model-Based Testing under Uncertainty.", "authors": ["Matteo Camilli", "Carlo Bellettini", "Angelo Gargantini", "Patrizia Scandurra"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00015", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00015"], "tag": ["Reliability, Security and Safety Analysis"], "abstract": "Modern software systems are required to operate in a highly uncertain and changing environment. They have to control the satisfaction of their requirements at run-time, and possibly adapt and cope with situations that have not been completely addressed at design-time. Software engineering methods and techniques are, more than ever, forced to deal with change and uncertainty (lack of knowledge) explicitly. For tackling the challenge posed by uncertainty in delivering more reliable systems, this paper proposes a novel online Model-based Testing technique that complements classic test case generation based on pseudo-random sampling strategies with an uncertainty-aware sampling strategy. To deal with system uncertainty during testing, the proposed strategy builds on an Inverse Uncertainty Quantification approach that is related to the discrepancy between the measured data at run-time (while the system executes) and a Markov Decision Process model describing the behavior of the system under test. To this purpose, a conformance game approach is adopted in which tests feed a Bayesian inference calibrator that continuously learns from test data to tune the system model and the system itself. A comparative evaluation between the proposed uncertainty-aware sampling policy and classical pseudo-random sampling policies is also presented using the Tele Assistance System running example, showing the differences in achieved accuracy and efficiency."}, {"id": "conf/issre/DingMJ18", "title": "Reliability Evaluation of Functionally Equivalent Simulink Implementations of a PID Controller under Silent Data Corruption.", "authors": ["Kai Ding", "Andrey Morozov", "Klaus Janschek"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00016", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00016"], "tag": ["Reliability, Security and Safety Analysis"], "abstract": "Model-based design of embedded control systems becomes more and more popular. Control engineers prefer to use MATLAB Simulink and suitable automatic code generators for the development and deployment of the software. Simulink provides a vast variety of functionally equivalent design solutions. For instance, a proportional-integral-derivative (PID) controller can be implemented in Simulink using i) separate blocks for the P, I, D terms, ii) a dedicated Discrete PID Controller block, iii) a Discrete Transfer Function block, or iv) a Discrete State-Space block. However, these functionally equivalent implementations of the PID controller show completely different reliability properties. This article introduces a new analytical method for the overall system reliability evaluation under data errors occurred in RAM and CPU. The method is based on a stochastic dual-graph error propagation model that captures control and data flow structures of the assembly code and allows the computation of system level reliability metrics in critical system outputs for specified faults probabilities. The analytical method enables an early system reliability evaluation. Also, application of this analytical method to possible implementations of the particular control algorithm helps to select the most reliable one."}, {"id": "conf/issre/MaiPGB18", "title": "A Natural Language Programming Approach for Requirements-Based Security Testing.", "authors": ["Phu X. Mai", "Fabrizio Pastore", "Arda Goknil", "Lionel C. Briand"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00017", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00017"], "tag": ["Reliability, Security and Safety Analysis"], "abstract": "To facilitate communication among stakeholders, software security requirements are typically written in natural language and capture both positive requirements (i.e., what the system is supposed to do to ensure security) and negative requirements (i.e., undesirable behavior undermining security). In this paper, we tackle the problem of automatically generating executable security test cases from security requirements in natural language (NL). More precisely, since existing approaches for the generation of test cases from NL requirements verify only positive requirements, we focus on the problem of generating test cases from negative requirements. We propose, apply and assess Misuse Case Programming (MCP), an approach that automatically generates security test cases from misuse case specifications (i.e., use case specifications capturing the behavior of malicious users). MCP relies on natural language processing techniques to extract the concepts (e.g., inputs and activities) appearing in requirements specifications and generates executable test cases by matching the extracted concepts to the members of a provided test driver API. MCP has been evaluated in an industrial case study, which provides initial evidence of the feasibility and benefits of the approach."}, {"id": "conf/issre/Lutz18", "title": "Safe-AR: Reducing Risk While Augmenting Reality.", "authors": ["Robyn R. Lutz"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00018", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00018"], "tag": ["Reliability, Security and Safety Analysis"], "abstract": "Augmented reality (AR) systems excel at offering users real-time, situation-aware information to support users' decision making. With AR, rich visualizations of relevant data can be displayed to users without blocking their view of the real world. For example, an AR-enabled automotive windshield can display a red outline around a pedestrian to alert a driver starting a turn into that cross street. Other critical uses of AR applications that are or will soon be deployed include surgery, emergency response, vehicle maintenance, and pilot training. Many of these applications can enhance operational safety. However, developing risk analysis methods to handle failure modes in the melded virtual and physical realities remains an open problem. This paper proposes a risk analysis method with which to study computer-generated AR visualizations of system and environment states. The analysis framework incorporates three levels at which AR interfaces with the user: perception, comprehension, and decision-making. This method enables broader risk analysis of the entire cyber-physical-human system that an AR application may indirectly control. Preliminary results show that this method yields improved coverage of user-involved failure modes over current approaches. While the focus here is on safety, the method also appears applicable to AR security risks."}, {"id": "conf/issre/AquinoDS18", "title": "Worst-Case Execution Time Testing via Evolutionary Symbolic Execution.", "authors": ["Andrea Aquino", "Giovanni Denaro", "Pasquale Salza"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00019", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00019"], "tag": ["Test Case Generation"], "abstract": "Worst-case execution time testing amounts to constructing a test case triggering the worst-case execution time of a program, and has many important applications to identify, debug and fix performance bottlenecks and security holes of programs. We propose a novel technique for worst-case execution time testing combining symbolic execution and evolutionary algorithms, which we call \"Evolutionary Symbolic Execution\", that (i) considers the set of the feasible program paths as the search space, (ii) embraces the execution cost of the program paths as the fitness function to pursue the worst path, (iii) exploits symbolic execution with random path selection to collect an initial set of feasible program paths, (iv) incrementally evolves by steering symbolic execution to traverse new program paths that comply with execution conditions combined and refined from the currently collected program paths, and (v) periodically applies local optimizations to the worst currently identified program path to speed up the identification of the worst path. We report on a set of initial experiments indicating that our technique succeeds in generating good worst-case execution time test cases for programs with which existing approaches cannot cope."}, {"id": "conf/issre/ElyasovPH18", "title": "Search-Based Test Data Generation for JavaScript Functions that Interact with the DOM.", "authors": ["Alexander Elyasov", "I. S. W. B. Prasetya", "Jurriaan Hage"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00020", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00020"], "tag": ["Test Case Generation"], "abstract": "The popularity of JavaScript (JS) is enormous. Together with HTML and CSS, it is the core technology for modern web development. Because of the dynamic nature and complex interplay with HTML, JS applications are often error-prone and vulnerable. Despite active research efforts to devise intricate static and dynamic analyses to facilitate JS testing, the problem of test data generation for JS code interacting with the DOM has not yet been addressed. In this paper, we present a Javascript Evolutionary testing framework with DOM as an Input, called JEDI. In order to reach a target branch, it applies genetic search for relevant input parameters of the JS function in combination with the global DOM state. We conducted an empirical evaluation to study the effectiveness and efficiency of our testing framework. It shows that the genetic with restart algorithm, proposed in this paper, is able to achieve complete branch coverage for all experimental subjects, taking on average 19 seconds per branch."}, {"id": "conf/issre/MaZSXLJXLLZW18", "title": "DeepMutation: Mutation Testing of Deep Learning Systems.", "authors": ["Lei Ma", "Fuyuan Zhang", "Jiyuan Sun", "Minhui Xue", "Bo Li", "Felix Juefei-Xu", "Chao Xie", "Li Li", "Yang Liu", "Jianjun Zhao", "Yadong Wang"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00021", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00021"], "tag": ["Test Case Generation"], "abstract": "Deep learning (DL) defines a new data-driven programming paradigm where the internal system logic is largely shaped by the training data. The standard way of evaluating DL models is to examine their performance on a test dataset. The quality of the test dataset is of great importance to gain confidence of the trained models. Using an inadequate test dataset, DL models that have achieved high test accuracy may still lack generality and robustness. In traditional software testing, mutation testing is a well-established technique for quality evaluation of test suites, which analyzes to what extent a test suite detects the injected faults. However, due to the fundamental difference between traditional software and deep learning-based software, traditional mutation testing techniques cannot be directly applied to DL systems. In this paper, we propose a mutation testing framework specialized for DL systems to measure the quality of test data. To do this, by sharing the same spirit of mutation testing in traditional software, we first define a set of source-level mutation operators to inject faults to the source of DL (i.e., training data and training programs). Then we design a set of model-level mutation operators that directly inject faults into DL models without a training process. Eventually, the quality of test data could be evaluated from the analysis on to what extent the injected faults could be detected. The usefulness of the proposed mutation testing techniques is demonstrated on two public datasets, namely MNIST and CIFAR-10, with three DL models."}, {"id": "conf/issre/GyoriLHM18", "title": "Evaluating Regression Test Selection Opportunities in a Very Large Open-Source Ecosystem.", "authors": ["Alex Gyori", "Owolabi Legunsen", "Farah Hariri", "Darko Marinov"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00022", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00022"], "tag": ["Regression Testing"], "abstract": "Regression testing in very large software ecosystems is notoriously costly, requiring computational resources that even large corporations struggle to cope with. Very large ecosystems contain thousands of rapidly evolving, interconnected projects where client projects transitively depend on library projects. Regression test selection (RTS) reduces regression testing costs by rerunning only tests whose pass/fail behavior may flip after code changes. For single projects, researchers showed that class-level RTS is more effective than lower method-or statement-level RTS. Meanwhile, several very large ecosystems in industry, e.g., at Facebook, Google, and Microsoft, perform project-level RTS, rerunning tests in a changed library and in all its transitive clients. However, there was no previous study of the comparative benefits of class-level and project-level RTS in such ecosystems. We evaluate RTS opportunities in the MAVEN Central open-source ecosystem. There, some popular libraries have up to 924589 clients; in turn, clients can depend on up to 11190 libraries. We sampled 408 popular projects and found that 202 (almost half) cannot update to latest library versions without breaking compilation or tests. If developers want to detect these breakages earlier, they need to run very many tests. We compared four variants of class-level RTS with project-level RTS in MAVEN Central. The results showed that class-level RTS may be an order of magnitude less costly than project-level RTS in very large ecosystems. Specifically, various class-level RTS variants select, on average, 7.8%-17.4% of tests selected by project-level RTS."}, {"id": "conf/issre/AssiMT18", "title": "Substate Profiling for Effective Test Suite Reduction.", "authors": ["Rawad Abou Assi", "Wes Masri", "Chadi Trad"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00023", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00023"], "tag": ["Regression Testing"], "abstract": "Test suite reduction (TSR) aims at removing redundant test cases from regression test suites. A typical TSR approach ensures that structural profile elements covered by the original test suite are also covered by the reduced test suite. It is plausible that structural profiles might be unable to segregate failing runs from passing runs, which diminishes the effectiveness of TSR in regard to defect detection. This motivated us to explore state profiles, which are based on the collective values of program variables. This paper presents Substate Profiling, a new form of state profiling that enhances existing profile-based analysis techniques such as TSR and coverage-based fault localization. Compared to current approaches for capturing program states, Substate Profiling is more practical and finer grained. We evaluated our approach using thirteen multi-fault subject programs comprising 53 defects. Our study involved greedy TSR using Substate profiles and four structural profiles, namely, basic-block, branch, def-use pair, and the combination of the three. For the majority of the subjects, Substate Profiling detected considerably more defects with a comparable level of reduction. Also, Substate profiles were found to be complementary to structural profiles in many cases, thus, combining both types is beneficial."}, {"id": "conf/issre/YuW18", "title": "A Study of Regression Test Selection in Continuous Integration Environments.", "authors": ["Tingting Yu", "Ting Wang"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00024", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00024"], "tag": ["Regression Testing"], "abstract": "Continuous integration (CI) systems perform the automated build, test execution, and delivery of the software. CI can provide fast feedback on software changes, minimizing the time and effort required in each iteration. In the meantime, it is important to ensure that enough testing is performed prior to code submission to avoid breaking builds. Recent approaches have been proposed to improve the cost-effectiveness of regression testing through techniques such as regression test selection (RTS). These approaches target at CI environments because traditional RTS techniques often use code instrumentation or very fine-grained dependency analysis, which may not be able to handle rapid changes. In this paper, we study in-depth the usage of RTS in CI environments for different open-source projects. We analyze 918 open-source projects using CI in GitHub to understand 1) under what conditions RTS is needed, and 2) how to balance the trade-offs between granularity levels to perform cost-effective RTS. The findings of this study can aid practitioners and researchers to develop more advanced RTS techniques for being adapted to CI environments."}, {"id": "conf/issre/AziziD18", "title": "ReTEST: A Cost Effective Test Case Selection Technique for Modern Software Development.", "authors": ["Maral Azizi", "Hyunsook Do"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00025", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00025"], "tag": ["Regression Testing"], "abstract": "Regression test selection offers cost savings by selecting a subset of existing tests when testers validate the modified version of the application. The majority of test selection approaches utilize static or dynamic analyses to decide which test cases should be selected, and these analyses are often very time consuming. In this paper, we propose a novel language-independent Regression TEst SelecTion (ReTEST) technique that facilitates a lightweight analysis by using information retrieval. ReTEST uses fault history, test case diversity, and program change history information to select test cases that should be rerun. Our empirical evaluation with four open source programs shows that our approach can be effective and efficient by selecting a far smaller subset of tests compared to the existing techniques."}, {"id": "conf/issre/OliveiraCFV18", "title": "FTMES: A Failed-Test-Oriented Mutant Execution Strategy for Mutation-Based Fault Localization.", "authors": ["Andr\u00e9 Assis L\u00f4bo de Oliveira", "Celso Gon\u00e7alves Camilo-Junior", "Eduardo Noronha de Andrade Freitas", "Auri Marcelo Rizzo Vincenzi"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00026", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00026"], "tag": ["Fault Localization and Debugging"], "abstract": "Fault localization has been one of the most manual and costly software debugging activities. The spectrum-based fault localization is the most studied and evaluated fault localization approach. Mutation-based fault localization is a promising approach but with a high computational cost. We propose a novel mutation execution strategy named Failed-Test Oriented Mutant Execution Strategy (FTMES) for improving the efficacy of fault localization techniques and also reduce the computational cost of these techniques. Our proposed approach and eight other baselines were evaluated against 221 real faults. The results show that FTMES outperformed others with respect to efficiency (computational cost) while maintaining similar accuracy."}, {"id": "conf/issre/HolmesG18", "title": "Causal Distance-Metric-Based Assistance for Debugging after Compiler Fuzzing.", "authors": ["Josie Holmes", "Alex Groce"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00027", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00027"], "tag": ["Fault Localization and Debugging"], "abstract": "Measuring the distance between two program executions is a fundamental problem in dynamic analysis of software, and useful in many test generation and debugging algorithms. This paper proposes a metric for measuring distance between executions, and specializes it to an important application: determining similarity of failing test cases for the purpose of automated fault identification and localization in debugging based on automatically generated compiler tests. The metric is based on a causal concept of distance where executions are similar to the degree that changes in the program itself, introduced by mutation, cause similar changes in the correctness of the executions. Specifically, if two failing test cases (for the original compiler) become successful due to the same mutant, they are more likely to be due to the same fault. We evaluate our metric using more than 50 faults and 2,800 test cases for two widely-used real-world compilers, and demonstrate improvements over state-of-the-art methods for fault identification and localization."}, {"id": "conf/issre/IslamM18", "title": "Bugaroo: Exposing Memory Model Bugs in Many-Core Systems.", "authors": ["Mohammad Majharul Islam", "Abdullah Muzahid"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00028", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00028"], "tag": ["Fault Localization and Debugging"], "abstract": "Modern many-core architectures such as GPUs aggressively reorder and buffer memory accesses. Updates to shared and global data are not guaranteed to be visible to concurrent threads immediately. Such updates can be made visible to other threads by using some fence instructions. Therefore, missing the required fences can introduce subtle bugs, called Memory Model Bugs. We propose Bugaroo to expose memory model bugs in any arbitrary GPU program. It works by statically instrumenting the code to buffer some shared and global data for as long as possible without violating the semantics of any fence or synchronization instruction. Any program failure that results from such buffering indicates the presence of subtle memory model bugs in the program. Bugaroo later provides detailed debugging information regarding the failure. Bugaroo is the first proposal to expose memory model bugs of GPU programs by simulating memory buffers. We present a detailed design and implementation of Bugaroo. We evaluated it using seven programs. Our approach uncovers new findings about missing and redundant fences in two of the programs. This makes Bugaroo an effective and useful tool for GPU programmers."}, {"id": "conf/issre/JiangBWLW18", "title": "RedDroid: Android Application Redundancy Customization Based on Static Analysis.", "authors": ["Yufei Jiang", "Qinkun Bao", "Shuai Wang", "Xiao Liu", "Dinghao Wu"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00029", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00029"], "tag": ["Mobile Systems"], "abstract": "Smartphone users are installing more and bigger apps. At the meanwhile, each app carries considerable amount of unused stuff, called software bloat, in its apk file. As a result, the resources of a smartphone, such as hard disk and network bandwidth, has become even more insufficient than ever before. Therefore, it is critical to investigate existing apps on the market and apps in development to identify the sources of software bloat and develop techniques and tools to remove the bloat. In this paper, we present a comprehensive study of software bloat in Android applications, and categorize them into two types, compile-time redundancy and install-time redundancy. In addition, we further propose a static analysis based approach to identifying and removing software bloat from Android applications. We implemented our approach in a prototype called RedDroid, and we evaluated RedDroid on thousands of Android applications collected from Google Play. Our experimental results not only validate the effectiveness of our approach, but also report the bloatware issue in real-world Android applications for the first time."}, {"id": "conf/issre/OliveiraMCR18", "title": "DroidEH: An Exception Handling Mechanism for Android Applications.", "authors": ["Juliana Oliveira", "Hivana Macedo", "N\u00e9lio Cacho", "Alexander B. Romanovsky"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00030", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00030"], "tag": ["Mobile Systems"], "abstract": "App crashing is the most common cause of complaints about Android mobile phone apps according to recent studies. Since most Android applications are written in Java, exception handling is the primary mechanism they employ to report and handle errors, similar to standard Java applications. Unfortunately, the exception handling mechanism for the Android platform has two liabilities: (1) the \"Terminate ALL\" approach and (2) a lack of a holistic view on exceptional behavior. As a consequence, exceptions easily get \"out of control\" and, as system development progresses, exceptional control flows become less well-understood, with potentially negative effects on program reliability. This paper presents an innovative exception handling mechanism for the Android platform, named DroidEH, that provides abstractions to support systematic engineering of holistic fault tolerance by applying cross-cutting reasoning about systems and their components."}, {"id": "conf/issre/0029BK18", "title": "MoonlightBox: Mining Android API Histories for Uncovering Release-Time Inconsistencies.", "authors": ["Li Li", "Tegawend\u00e9 F. Bissyand\u00e9", "Jacques Klein"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00031", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00031"], "tag": ["Mobile Systems"], "abstract": "In most of the approaches aiming at investigating Android apps, the release time of apps is not appropriately taken into account. Through three empirical studies, we demonstrate that the app release time is key for guaranteeing performance. Indeed, not considering time may result in serious threats to the validity of proposed approaches. Unfortunately, even approaches considering time could present some threats to validity when release times are erroneous. Symptoms of such erroneous release times appear in the form of inconsistencies with the APIs leveraged by the app. We present a tool called MoonlightBox for uncovering time inconsistencies by inferring the lower bound assembly time of a given app based on the used API lifetime information: any assembly time below this lower bound is considered as manipulated. We further perform several experiments and confirm that 1) over 7% of Android apps are subject to time inconsistency, 2) malicious apps are more likely to be targeted by time inconsistency, compared to benign apps, 3) time inconsistencies are favoured by some specific app lineages. We eventually revisit the three motivating empirical studies, leveraging MoonlightBox to compute a more realistic timeline of apps. The experimental results confirm that time indeed matters. The accuracy of release time is even crucial to achieve precise results."}, {"id": "conf/issre/SaidBBDGLQ18", "title": "Detection of Mirai by Syntactic and Behavioral Analysis.", "authors": ["Najah Ben Said", "Fabrizio Biondi", "Vesselin Bontchev", "Olivier Decourbe", "Thomas Given-Wilson", "Axel Legay", "Jean Quilbeuf"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00032", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00032"], "tag": ["Systems Security and Privacy"], "abstract": "The largest botnet distributed denial of service attacks in history have been executed by devices controlled by the Mirai botnet trojan. To prevent Mirai from spreading, this paper presents and evaluates techniques to classify binary samples as Mirai based on their syntactic and behavioral properties. Syntactic malware detection is shown to have a good detection rate and no false positives, but to be very easy to circumvent. Behavioral malware detection is resistant to simple obfuscation and has better detection rate than syntactic detection, while keeping false positives to zero. This paper demonstrates these results, and concludes by showing how to combine syntactic and behavioral analysis techniques for the detection of Mirai."}, {"id": "conf/issre/ZhaoGZWWG18", "title": "You Are Where You App: An Assessment on Location Privacy of Social Applications.", "authors": ["Fanghua Zhao", "Linan Gao", "Yang Zhang", "Zeyu Wang", "Bo Wang", "Shanqing Guo"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00033", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00033"], "tag": ["Systems Security and Privacy"], "abstract": "The development of positioning technologies has digitalized people's mobility traces for the first time in history. GPS sensors resided in people's mobile devices allow smart apps to access location data. This large amount of mobility data can help to build appealing applications. Meanwhile, location privacy has become a major concern. In this paper, we design a general system to assess whether an app is vulnerable to location inference attacks. We utilize a series of automatic testing mechanisms including UI match and API analysis to extract the location information an app provides. According to different characteristics of these apps, we classify them into two categories corresponding to two kinds of attacks, namely attack with distance limitation (AWDL) and attack without distance limitation (AWODL). After evaluating 800 apps, of which 109 passed automated testing, we found that 24.7% of the passing apps are vulnerable to AWDL and 11.0% to AWODL. Moreover, some apps even allow us to modify the parameters in http requests which largely increases the scope of the attacks. Our system demonstrates the severity of location privacy leakage to mobile devices and can serve as an auditing tool for future smart apps."}, {"id": "conf/issre/ZhaoHTXX18", "title": "Iris Template Protection Based on Randomized Response Technique and Aggregated Block Information.", "authors": ["Dongdong Zhao", "Xiaoyi Hu", "Jing Tian", "Shengwu Xiong", "Jianwen Xiang"], "DOIs": ["https://doi.org/10.1109/ISSRE.2018.00034", "http://doi.ieeecomputersociety.org/10.1109/ISSRE.2018.00034"], "tag": ["Systems Security and Privacy"], "abstract": "Nowadays, biometric recognition has been widely used in real-world applications, but it has also brought potential privacy threats to users. Iris template protection enables an effective iris recognition while protecting personal privacy. In this paper, we propose a method for iris template protection based on randomized response technique and aggregated block information. Specifically, the iris data are first permuted according to an application-specific parameter; next, the permuted data are flipped using the randomized response technique; finally, the result is divided into blocks, and the aggregated information (i.e., the sum of all bits) in each block is calculated and stored instead of original iris data for privacy protection. We demonstrate that the proposed method supports the shifting and masking strategies for enhancing recognition performance. Moreover, the proposed method satisfies the three privacy requirements prescribed in ISO/IEC 24745: irreversibility, revocability and unlinkability. Experimental results show that the proposed method could effectively maintain the recognition performance (w.r.t. the original iris recognition system without privacy protection) on the iris database CASIA-IrisV3-Interval."}], "2019": [{"id": "conf/issre/MooreCFW19", "title": "Charting a Course Through Uncertain Environments: SEA Uses Past Problems to Avoid Future Failures.", "authors": ["Preston Moore", "Justin Cappos", "Phyllis G. Frankl", "Thomas Wies"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00011"], "tag": ["Research Session 1:\nBest Paper Nominees"], "abstract": "A common problem for developers is applications exhibiting new bugs after deployment. Many of these bugs can be traced to unexpected network, operating system, and file system differences that cause program executions that were successful in a development environment to fail once deployed. Preventing these bugs is difficult because it is impractical to test an application in every environment. Enter Simulating Environmental Anomalies (SEA), a technique that utilizes evidence of one application's failure in a given environment to generate tests that can be applied to other applications, to see whether they suffer from analogous faults. In SEA, models of unusual properties extracted from interactions between an application, A, and its environment guide simulations of another application, B, running in the anomalous environment. This reveals faults B may experience in this environment without the expense of deployment. By accumulating these anomalies, applications can be tested against an increasing set of problematic conditions. We implemented a tool called CrashSimulator, which uses SEA, and evaluated it against Linux applications selected from coreutils and the Debian popularity contest. Our tests found a total of 63 bugs in 31 applications with effects including hangs, crashes, data loss, and remote denial of service conditions."}, {"id": "conf/issre/ZhaoRFSS19", "title": "Assessing the Safety and Reliability of Autonomous Vehicles from Road Testing.", "authors": ["Xingyu Zhao", "Valentin Robu", "David Flynn", "Kizito Salako", "Lorenzo Strigini"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00012"], "tag": ["Research Session 1:\nBest Paper Nominees"], "abstract": "There is an urgent societal need to assess whether autonomous vehicles (AVs) are safe enough. From published quantitative safety and reliability assessments of AVs, we know that, given the goal of predicting very low rates of accidents, road testing alone requires infeasible numbers of miles to be driven. However, previous analyses do not consider any knowledge prior to road testing \u2013 knowledge which could bring substantial advantages if the AV design allows strong expectations of safety before road testing. We present the advantages of a new variant of Conservative Bayesian Inference (CBI), which uses prior knowledge while avoiding optimistic biases. We then study the trend of disengagements (take-overs by human drivers) by applying Software Reliability Growth Models (SRGMs) to data from Waymo's public road testing over 51 months, in view of the practice of software updates during this testing. Our approach is to not trust any specific SRGM, but to assess forecast accuracy and then improve forecasts. We show that, coupled with accuracy assessment and recalibration techniques, SRGMs could be a valuable test planning aid."}, {"id": "conf/issre/SalayAC19", "title": "A Safety Analysis Method for Perceptual Components in Automated Driving.", "authors": ["Rick Salay", "Matt Angus", "Krzysztof Czarnecki"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00013"], "tag": ["Research Session 1:\nBest Paper Nominees"], "abstract": "The use of machine learning (ML) is increasing in many sectors of safety-critical software development and in particular, for the perceptual components of automated driving (AD) functionality. Although some traditional safety engineering techniques such as FTA and FMEA are applicable to ML components, the unique characteristics of ML create challenges. In this paper, we propose a novel safety analysis method called Classification Failure Mode Effects Analysis (CFMEA) which is specialized to assess classification-based perception in AD. Specifically, it defines a systematic way to assess the risk due to classification failure under adversarial attacks or varying degrees of classification uncertainty across the perception-control linkage. We first present the theoretical and methodological foundations for CFMEA, and then demonstrate it by applying it to an AD case study using semantic segmentation perception trained with the Cityscapes driving dataset. Finally, we discuss how CFMEA results could be used to improve an ML-model."}, {"id": "conf/issre/LiuCNZZSZP19", "title": "FluxRank: A Widely-Deployable Framework to Automatically Localizing Root Cause Machines for Software Service Failure Mitigation.", "authors": ["Ping Liu", "Yu Chen", "Xiaohui Nie", "Jing Zhu", "Shenglin Zhang", "Kaixin Sui", "Ming Zhang", "Dan Pei"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00014"], "tag": ["Research Session 2:\nFailure/Fault Root Cause Analysis 1"], "abstract": "The failures of software service directly affect user experiences and service revenue. Thus operators monitor both service-level KPIs (e.g., response time) and machine-level KPIs (e.g., CPU usage) on each machine underlying the service. When a service fails, the operators must localize the root cause machines, and mitigate the failure as quickly as possible. Existing approaches have limited application due to the difficulty to obtain the required additional measurement data. As a result, failure localization is largely manual and very time-consuming. This paper presents FluxRank, a widely-deployable framework that can automatically and accurately localize the root cause machines, so that some actions can be triggered to mitigate the service failure. Our evaluation using historical cases from five real services (with tens of thousands of machines) of a top search company shows that the root cause machines are ranked top 1 (top 3) for 55 (66) cases out of 70 cases. Comparing to existing approaches, FluxRank cuts the localization time by more than 80% on average. FluxRank has been deployed online at one Internet service and six banking services for three months, and correctly localized the root cause machines as the top 1 for 55 cases out of 59 cases."}, {"id": "conf/issre/LiPLZSSWLJW19", "title": "Generic and Robust Localization of Multi-dimensional Root Causes.", "authors": ["Zeyan Li", "Dan Pei", "Chengyang Luo", "Yiwei Zhao", "Yongqian Sun", "Kaixin Sui", "Xiping Wang", "Dapeng Liu", "Xing Jin", "Qi Wang"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00015"], "tag": ["Research Session 2:\nFailure/Fault Root Cause Analysis 1"], "abstract": "Operators of online software services periodically collect various measures with many attributes. When a measure becomes abnormal, indicating service problems such as reliability degrade, operators would like to rapidly and accurately localize the root cause attribute combinations within a huge multi-dimensional search space. Unfortunately, previous approaches are not generic or robust in that they all suffer from impractical root cause assumptions, handling only directly collected measures but not derived ones, handling only anomalies with signicant magnitudes but not those insignicant but important ones, requiring manual parameter ne-tuning, or being too slow. This paper proposes a generic and robust multi-dimensional root cause localization approach, Squeeze, that overcomes all above limitations, the first in the literature. Through our novel bottom-up then top-down searching strategy and the techniques based on our proposed generalized ripple effect and generalized potential score, Squeeze is able to reach a good trade off between search speed and accuracy in a generic and robust manner. Case studies in several banks and an Internet company show that Squeeze can localize root causes much more rapidly and accurately than the traditional manual analysis. Furthermore, our extensive experiments on semi-synthetic datasets show that the F1-score of Squeeze outperforms previous approaches by 0.4 on average, while its localization time is only about 10 seconds"}, {"id": "conf/issre/LiangHZ0X019", "title": "How to Explain a Patch: An Empirical Study of Patch Explanations in Open Source Projects.", "authors": ["Jingjing Liang", "Yaozong Hou", "Shurui Zhou", "Junjie Chen", "Yingfei Xiong", "Gang Huang"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00016"], "tag": ["Research Session 2:\nFailure/Fault Root Cause Analysis 1"], "abstract": "Abstract-Bugs are inevitable in software development and maintenance processes. Recently a lot of research efforts have been devoted to automatic program repair, aiming to reduce the efforts of debugging. However, since it is difficult to ensure that the generated patches meet all quality requirements such as correctness, developers still need to review the patch. In addition, current techniques produce only patches without explanation, making it difficult for the developers to understand the patch. Therefore, we believe a more desirable approach should generate not only the patch but also an explanation of the patch. To generate a patch explanation, it is important to first understand how patches were explained. In this paper, we explored how developers explain their patches by manually analyzing 300 merged bug-fixing pull requests from six projects on GitHub. Our contribution is twofold. First, we build a patch explanation model, which summarizes the elements in a patch explanation, and corresponding expressive forms. Second, we conducted a quantitative analysis to understand the distributions of elements, and the correlation between elements and their expressive forms."}, {"id": "conf/issre/Chen0S19", "title": "Inferring Performance Bug Patterns from Developer Commits.", "authors": ["Yiqun Chen", "Stefan Winter", "Neeraj Suri"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00017"], "tag": ["Research Session 3:\nFailure/Fault Root Cause Analysis 2"], "abstract": "Performance bugs, i.e., program source code that is unnecessarily inefficient, have received significant attention by the research community in recent years. A number of empirical studies have investigated how these bugs differ from \"ordinary\" bugs that cause functional deviations and several approaches to aid their detection, localization, and removal have been proposed. Many of these approaches focus on certain subclasses of performance bugs, e.g., those resulting from redundant computations or unnecessary synchronization, and the evaluation of their effectiveness is usually limited to a small number of known instances of these bugs. To provide researchers working on performance bug detection and localization techniques with a larger corpus of performance bugs to evaluate against, we conduct a study of more than 700 performance bug fixing commits across 13 popular open source projects written in C and C++ and investigate the relative frequency of bug types as well as their complexity. Our results show that many of these fixes follow a small set of bug patterns, that they are contributed by experienced developers, and that the number of lines needed to fix performance bugs is highly project dependent."}, {"id": "conf/issre/LiangCW0YSJS19", "title": "Engineering a Better Fuzzer with Synergically Integrated Optimizations.", "authors": ["Jie Liang", "Yuanliang Chen", "Mingzhe Wang", "Yu Jiang", "Zijiang Yang", "Chengnian Sun", "Xun Jiao", "Jiaguang Sun"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00018"], "tag": ["Research Session 3:\nFailure/Fault Root Cause Analysis 2"], "abstract": "State-of-the-art fuzzers implement various optimizations to enhance their performance. As the optimizations reside in different stages such as input seed selection and mutation, it is tempting to combine the optimizations in different stages. However, our initial attempts demonstrate that naive combination actually worsens the performance, which explains that most optimizations are still isolated by stages and metrics. In this paper, we present InteFuzz, the first framework that synergically integrates multiple fuzzing optimizations. We analyze the root cause for performance degradation in naive combination, and discover optimizations conflict in coverage criteria and optimization granularity. To resolve the conflicts, we propose a novel priority-based scheduling mechanism. The dynamic integration considers both branch-based and block-based coverage feedbacks that are used by most fuzzing optimizations. In our evaluation, we extract four optimizations from popular fuzzers such as AFLFast and FairFuzz and compare InteFuzz against naive combinations. The evaluation results show that InteFuzz outperforms the naive combination by 29% and 26% in path-and branch-coverage. Additionally, InteFuzz triggers 222 more unique crashes, and discovers 33 zero-day vulnerabilities in real-world projects with 12 registered as CVEs."}, {"id": "conf/issre/Couceiro0BMBDDC19", "title": "Spotting Problematic Code Lines using Nonintrusive Programmers' Biofeedback.", "authors": ["Ricardo Couceiro", "Paulo Carvalho", "Miguel Castelo Branco", "Henrique Madeira", "Raul Barbosa", "Jo\u00e3o Dur\u00e3es", "Gon\u00e7alo Duarte", "Jo\u00e3o Castelhano", "Catarina Duarte", "C\u00e9sar Alexandre Teixeira", "Nuno Laranjeiro", "Julio Medeiros"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00019"], "tag": ["Research Session 3:\nFailure/Fault Root Cause Analysis 2"], "abstract": "Recent studies have shown that programmers' cognitive load during typical code development activities can be assessed using wearable and low intrusive devices that capture peripheral physiological responses driven by the autonomic nervous system. In particular, measures such as heart rate variability (HRV) and pupillography can be acquired by nonintrusive devices and provide accurate indication of programmers' cognitive load and attention level in code related tasks, which are known elements of human error that potentially lead to software faults. This paper presents an experimental study designed to evaluate the possibility of using HRV and pupillography together with eye tracking to identify and annotate specific code lines (or even finer grain lexical tokens) of the program under development (or under inspection) with information on the cognitive load of the programmer while dealing with such lines of code. The experimental data is discussed in the paper to assess different alternatives for using code annotations representing programmers' cognitive load while producing or reading code. In particular, we propose the use of biofeedback code highlighting techniques to provide online programmer's warnings for potentially problematic code lines that may need a second look at (to remove possible bugs), and biofeedback-driven software testing to optimize testing effort, focusing the tests on code areas with higher bug probability"}, {"id": "conf/issre/0001GMLK19", "title": "An Empirical Study of Common Challenges in Developing Deep Learning Applications.", "authors": ["Tianyi Zhang", "Cuiyun Gao", "Lei Ma", "Michael Lyu", "Miryung Kim"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00020"], "tag": ["Research Session 3:\nFailure/Fault Root Cause Analysis 2"], "abstract": "Recent advances in deep learning promote the innovation of many intelligent systems and applications such as autonomous driving and image recognition. Despite enormous efforts and investments in this field, a fundamental question remains under-investigated\u2014what challenges do developers commonly face when building deep learning applications? To seek an answer, this paper presents a large-scale empirical study of deep learning questions in a popular Q&A website, Stack Overflow. We manually inspect a sample of 715 questions and identify seven kinds of frequently asked questions. We further build a classification model to quantify the distribution of different kinds of deep learning questions in the entire set of 39,628 deep learning questions. We find that program crashes, model migration, and implementation questions are the top three most frequently asked questions. After carefully examining accepted answers of these questions, we summarize five main root causes that may deserve attention from the research community, including API misuse, incorrect hyperparameter selection, GPU computation, static graph computation, and limited debugging and profiling support. Our results highlight the need for new techniques such as cross-framework differential testing to improve software development productivity and software reliability in deep learning."}, {"id": "conf/issre/ZhangM19", "title": "TripleAgent: Monitoring, Perturbation and Failure-Obliviousness for Automated Resilience Improvement in Java Applications.", "authors": ["Long Zhang", "Martin Monperrus"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00021"], "tag": ["Research Session 4:\nFault Injection"], "abstract": "In this paper, we present a novel resilience improvement system for Java applications. The unique feature of this system is to combine automated monitoring, automated perturbation injection, and automated resilience improvement. The latter is achieved thanks to the failure-oblivious computing, a concept introduced in 2004 by Rinard and colleagues. We design and implement the system as agents for the Java virtual machine. We evaluate the system on two real-world applications: a file transfer client and an email server. Our results show that it is possible to automatically improve the resilience of Java applications with respect to uncaught or mishandled exceptions."}, {"id": "conf/issre/JiangBL019", "title": "Fuzzing Error Handling Code in Device Drivers Based on Software Fault Injection.", "authors": ["Zu-Ming Jiang", "Jia-Ju Bai", "Julia Lawall", "Shi-Min Hu"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00022"], "tag": ["Research Session 4:\nFault Injection"], "abstract": "Device drivers remain a main source of runtime failures in operating systems. To detect bugs in device drivers, fuzzing has been commonly used in practice. However, a main limitation of existing fuzzing approaches is that they cannot effectively test error handling code. Indeed, these fuzzing approaches require effective inputs to cover target code, but much error handling code in drivers is triggered by occasional errors (such as insufficient memory and hardware malfunctions) that are not related to inputs. In this paper, based on software fault injection, we propose a new fuzzing approach named FIZZER, to test error handling code in device drivers. At compile time, FIZZER uses static analysis to recommend possible error sites that can trigger error handling code. During driver execution, by analyzing runtime information, it automatically fuzzes error-site sequences for fault injection to improve code coverage. We evaluate FIZZER on 18 device drivers in Linux 4.19, and in total find 22 real bugs. The code coverage is increased by over 15% compared to normal execution without fuzzing."}, {"id": "conf/issre/CotroneoSLNB19", "title": "Enhancing Failure Propagation Analysis in Cloud Computing Systems.", "authors": ["Domenico Cotroneo", "Luigi De Simone", "Pietro Liguori", "Roberto Natella", "Nematollah Bidokhti"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00023"], "tag": ["Research Session 4:\nFault Injection"], "abstract": "In order to plan for failure recovery, the designers of cloud systems need to understand how their system can potentially fail. Unfortunately, analyzing the failure behavior of such systems can be very difficult and time-consuming, due to the large volume of events, non-determinism, and reuse of third-party components. To address these issues, we propose a novel approach that joins fault injection with anomaly detection to identify the symptoms of failures. We evaluated the proposed approach in the context of the OpenStack cloud computing platform. We show that our model can significantly improve the accuracy of failure analysis in terms of false positives and negatives, with a low computational cost."}, {"id": "conf/issre/PalazziLFP19", "title": "A Tale of Two Injectors: End-to-End Comparison of IR-Level and Assembly-Level Fault Injection.", "authors": ["Lucas Palazzi", "Guanpeng Li", "Bo Fang", "Karthik Pattabiraman"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00024"], "tag": ["Research Session 4:\nFault Injection"], "abstract": "Fault injection (FI) is a commonly used experimental technique to evaluate the resilience of software techniques for tolerating hardware faults. Software-implemented FI can be performed at different levels of abstraction in the system stack; FI performed at the compiler's intermediate representation (IR) level has the advantage that it is closer to the program being evaluated and is hence easier to derive insights from for the design of software fault-tolerance mechanisms. Unfortunately, it is not clear how accurate IR-level FI is vis-a-vis FI performed at the assembly code level, and prior work has presented contradictory findings. In this paper, we perform an analysis of said prior work, find an inconsistency in the FI methodology used in one study, and show that it results in a flawed comparison between IR-level and assembly-level FI. We further confirm this finding by performing a comprehensive evaluation of the accuracy of IR-level FI across a range of benchmark programs and compiler optimization levels. Our results show that IR-level FI is as accurate as assembly-level FI for silent data corruptions (SDCs) across different benchmarks and optimization levels."}, {"id": "conf/issre/Wan0QQT19", "title": "Supervised Representation Learning Approach for Cross-Project Aging-Related Bug Prediction.", "authors": ["Xiaohui Wan", "Zheng Zheng", "Fangyun Qin", "Yu Qiao", "Kishor S. Trivedi"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00025"], "tag": ["Research Session 5:\nFault Prediction"], "abstract": "Software aging, which is caused by Aging-Related Bugs (ARBs), tends to occur in long-running systems and may lead to performance degradation and increasing failure rate during software execution. ARB prediction can help developers discover and remove ARBs, thus alleviating the impact of software aging. However, ARB-prone files occupy a small percentage of all the analyzed files. It is usually difficult to gather sufficient ARB data within a project. To overcome the limited availability of training data, several researchers have recently developed cross-project models for ARB prediction. A key point for cross-project models is to learn a good representation for instances in different projects. Nevertheless, most of the previous approaches neither consider the reconstruction property of new representation nor encode source samples' label information in learning representation. To address these shortcomings, we propose a Supervised Representation Learning Approach (SRLA), which is based on double encoding-layer autoencoder, to perform cross-project ARB prediction. Moreover, we present a transfer cross-validation framework to select the hyper-parameters of cross-project models. Experiments on three large open-source projects demonstrate the effectiveness and superiority of our approach compared with the state-of-the-art approach TLAP."}, {"id": "conf/issre/CamposVC19", "title": "Propheticus: Machine Learning Framework for the Development of Predictive Models for Reliable and Secure Software.", "authors": ["Jo\u00e3o R. Campos", "Marco Vieira", "Ernesto Costa"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00026"], "tag": ["Research Session 5:\nFault Prediction"], "abstract": "The growing complexity of software calls for innovative solutions that support the deployment of reliable and secure software. Machine Learning (ML) has shown its applicability to various complex problems and is frequently used in the dependability domain, both for supporting systems design and verification activities. However, using ML is complex and highly dependent on the problem in hand, increasing the probability of mistakes that compromise the results. In this paper, we introduce Propheticus, a ML framework that can be used to create predictive models for reliable and secure software systems. Propheticus attempts to abstract the complexity of ML whilst being easy to use and accommodating the needs of the users. To demonstrate its use, we present two case studies (vulnerability prediction and online failure prediction) that show how it can considerably ease and expedite a thorough ML workflow."}, {"id": "conf/issre/XuZZTLLKC19", "title": "Identifying Crashing Fault Residence Based on Cross Project Model.", "authors": ["Zhou Xu", "Tao Zhang", "Yifeng Zhang", "Yutian Tang", "Jin Liu", "Xiapu Luo", "Jacky Keung", "Xiaohui Cui"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00027"], "tag": ["Research Session 5:\nFault Prediction"], "abstract": "Analyzing the crash reports recorded upon software crashes is a critical activity for software quality assurance. Predicting whether or not the fault causing the crash (crashing fault for short) resides in the stack traces of crash reports can speed-up the program debugging process and determine the priority of the debugging efforts. Previous work mostly collected label information from bug-fixing logs, and extracted crash features from stack traces and source code to train classification models for the Identification of Crashing Fault Residence (ICFR) of newly-submitted crashes. However, labeled data are not always fully available in real applications. Hence the classifier training is not always feasible. In this work, we make the first attempt to develop a cross project ICFR model to address the data scarcity problem. This is achieved by transferring the knowledge from external projects to the current project via utilizing a state-of-the-art Balanced Distribution Adaptation (BDA) based transfer learning method. BDA not only combines both marginal distribution and conditional distribution across projects but also assigns adaptive weights to the two distributions for better adjusting specific cross project pair. The experiments on 7 software projects show that BDA is superior to 9 baseline methods in terms of 6 indicators overall."}, {"id": "conf/issre/CarnevaliSV19", "title": "Learning Marked Markov Modulated Poisson Processes for Online Predictive Analysis of Attack Scenarios.", "authors": ["Laura Carnevali", "Francesco Santoni", "Enrico Vicario"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00028"], "tag": ["Research Session 5:\nFault Prediction"], "abstract": "Runtime predictive analysis of quantitative models can support software reliability in various application scenarios. The spread of logging technologies promotes approaches where such models are learned from observed events. We consider a system visiting transient states of a hidden process until reaching a final state and producing observations with stochastic arrival times and types conditioned by visited states, and we abstract it as a marked Markov modulated Poisson Process (MMMPP) with left-to right structure. We present an Expectation-Maximization (EM) algorithm that learns the MMMPP parameters from observation sequences acquired in repeated execution of the transient behavior, and we use the model at runtime to infer the current state of the process from actual observed events and to dynamically evaluate the remaining time to the final state. The approach is illustrated using synthetic datasets generated from a stochastic attack tree of the literature enriched with an observation model associating each state with an expected statistics of observation types and arrival times. Accuracy of prediction is evaluated under different variability of hidden states sojourn durations and of the observations arrival process, and compared against previous literature that mainly exploits either the timing or the types of observed events."}, {"id": "conf/issre/WangG0Z19", "title": "Test Case Generation Based on Client-Server of Web Applications by Memetic Algorithm.", "authors": ["Weiwei Wang", "Xiaohong Guo", "Zheng Li", "Ruilian Zhao"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00029"], "tag": ["Research Session 6:\nTesting 1"], "abstract": "Currently, more than 90% web applications are potentially vulnerable to attacks from both the client side and server side. Test case generation plays a crucial role in testing web applications, where most existing studies focus on test case generation either from client-side or from server-side to detect vulnerabilities, regardless of the interactions between client and server. Consequently, it is difficult for those test cases to discover certain faults which involve both client and server. In this paper, the server-side sensitive paths are considered as vulnerable code paths due to insufficient or erroneous filtering mechanisms. An evolutionary testing approach based on the memetic algorithm is proposed to connect the server-side and client-side, in which test cases are generated from the client-side behavior model, while guided by the coverage of sensitive paths from server-side. The experiments are conducted on four open source web applications, and the results demonstrate that our approach can generate test cases from the client-side behavior model that can cover the server-side sensitive paths, on which the vulnerabilities can be detected more effectively."}, {"id": "conf/issre/SantiagoPAMKC19", "title": "Machine Learning and Constraint Solving for Automated Form Testing.", "authors": ["Dionny Santiago", "Justin Phillips", "Patrick Alt", "Brian Muras", "Tariq M. King", "Peter J. Clarke"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00030"], "tag": ["Research Session 6:\nTesting 1"], "abstract": "In recent years there has been a focus on the automatic generation of test cases using white box testing techniques, however the same cannot be said for the generation of test cases at the system-level from natural language system requirements. Some of the white-box techniques include: the use of constraint solvers for the automatic generation of test inputs at the white box level; the use of control flow graphs generated from code; and the use of path generation and symbolic execution to generate test inputs and test for path feasibility. Techniques such as boundary value analysis (BVA) may also be used for generating stronger test suites. However, for black box testing we rely on specifications or implicit requirements and spend considerable time and effort designing and executing test cases. This paper presents an approach that leverages natural language processing and machine learning techniques to capture black box system behavior in the form of constraints. Constraint solvers are then used to generate test cases using BVA and equivalence class partitioning. We also conduct a proof of concept that applies this approach to a simplified task management application and an enterprise job recruiting application."}, {"id": "conf/issre/ShiZM19", "title": "Understanding and Improving Regression Test Selection in Continuous Integration.", "authors": ["August Shi", "Peiyuan Zhao", "Darko Marinov"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00031"], "tag": ["Research Session 6:\nTesting 1"], "abstract": "Developers rely on regression testing in their continuous integration (CI) environment to find changes that introduce regression faults. While regression testing is widely practiced, it can be costly. Regression test selection (RTS) reduces the cost of regression testing by not running the tests that are unaffected by the changes. Industry has adopted module-level RTS for their CI environment, while researchers have proposed class-level RTS. In this paper, we compare module-and class-level RTS techniques in a cloud-based CI environment, Travis. We also develop and evaluate a hybrid RTS technique that combines aspects of the module-and class-level RTS techniques. We evaluate all the techniques on real Travis builds. We find that the RTS techniques do save testing time compared to running all tests (RetestAll), but the percentage of time for a full build using RTS (76.0%) is not as low as found in previous work, due to the extra overhead in a cloud-based CI environment. Moreover, we inspect test failures from RetestAll builds, and although we find that RTS techniques can miss to select failed tests, these test failures are almost all flaky test failures. As such, RTS techniques provide additional value in helping developers avoid wasting time debugging failures not related to the recent code changes. Overall, our results show that RTS can be beneficial for the developers in the CI environment, and RTS not only saves time but also avoids misleading developers by flaky test failures."}, {"id": "conf/issre/WangXZLW19", "title": "Textout: Detecting Text-Layout Bugs in Mobile Apps via Visualization-Oriented Learning.", "authors": ["Yaohui Wang", "Hui Xu", "Yangfan Zhou", "Michael R. Lyu", "Xin Wang"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00032"], "tag": ["Research Session 6:\nTesting 1"], "abstract": "Layout bugs commonly exist in mobile apps. Due to the fragmentation issues of smartphones, a layout bug may occur only on particular versions of smartphones. It is quite challenging to detect such bugs for state-of-the-art commercial automated testing platforms, although they can test an app with thousands of different smartphones in parallel. The main reason is that typical layout bugs neither crash an app nor generate any error messages. In this paper, we present our work for detecting text-layout bugs, which account for a large portion of layout bugs. We model text-layout bug detection as a classification problem. This then allows us to address it with sophisticated image processing and machine learning techniques. To this end, we propose an approach which we call Textout. Textout takes screenshots as its input and adopts a specifically-tailored text detection method and a convolutional neural network (CNN) classifier to perform automatic text-layout bug detection. We collect 33,102 text-region images as our training dataset and verify the effectiveness of our tool with 1,481 text-region images collected from real-world apps. Textout achieves an AUC (area under the curve) of 0.956 on the test dataset and shows an acceptable overhead. The dataset is open-source released for follow-up research."}, {"id": "conf/issre/GannousA19", "title": "Integrating Safety Certification Into Model-Based Testing of Safety-Critical Systems.", "authors": ["Aiman Gannous", "Anneliese Andrews"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00033"], "tag": ["Research Session 7:\nTesting 2"], "abstract": "Testing plays an important role in assuring the safety of safety-critical systems (SCS). Testing SCSs should include tasks to test how the system operates in the presence of failures. With the increase of autonomous, sensing-based functionality in safety-critical systems, efficient and cost-effective testing that maximizes safety evidences has become increasingly challenging. A previously proposed framework for testing safety-critical systems called Model-Combinatorial based testing (MCbt) has the potential for addressing these challenges. MCbt is a framework that proposes an integration of model-based testing, fault analysis, and combinatorial testing to produce the maximum number of evidences for an efficient safety certification process but was never actually used to derive a specific testing approach. In this paper, we present a concrete application of MCbt with an application to a case study. The validation showed that MCbt is more efficient and produces more safety evidences compared to state-of-the-art testing approaches."}, {"id": "conf/issre/CayreNAAKM19", "title": "Mirage: Towards a Metasploit-Like Framework for IoT.", "authors": ["Romain Cayre", "Vincent Nicomette", "Guillaume Auriol", "Eric Alata", "Mohamed Ka\u00e2niche", "G\u00e9raldine Vache Marconato"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00034"], "tag": ["Research Session 7:\nTesting 2"], "abstract": "Internet of Things (IoT) devices are nowadays widely used in individual homes and factories. Securing these new systems becomes a priority. However, conducting security audits of these connected objects based on experimental evaluation is a challenging task: it requires the use of heterogeneous hardware components leading to a set of specialised software tools, generally incompatible with each other and often complex to use. In this paper, we present a security audit and penetration testing framework called Mirage. This framework, written in Python, is dedicated to the analysis of wireless communications commonly used by IoT devices, and provides a generic, modular, unified and low level audit environment that is easy to adapt to new protocols. The paper describes the software architecture of Mirage, its goals and main features, and presents a concrete example of security audit performed with this framework."}, {"id": "conf/issre/DobslawFMHNT19", "title": "Estimating Return on Investment for GUI Test Automation Frameworks.", "authors": ["Felix Dobslaw", "Robert Feldt", "David Micha\u00eblsson", "Patrick Haar", "Francisco Gomes de Oliveira Neto", "Richard Torkar"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00035"], "tag": ["Research Session 7:\nTesting 2"], "abstract": "Automated graphical user interface (GUI) tests can reduce manual testing activities and increase test frequency. This motivates the conversion of manual test cases into automated GUI tests. However, it is not clear whether such automation is cost-effective given that GUI automation scripts add to the code base and demand maintenance as a system evolves. In this paper, we introduce a method for estimating maintenance cost and Return on Investment (ROI) for Automated GUI Testing (AGT). The method utilizes the existing source code change history and has the potential to be used for the evaluation of other testing or quality assurance automation technologies. We evaluate the method for a real-world, industrial software system and compare two fundamentally different AGT frameworks, namely Selenium and EyeAutomate, to estimate and compare their ROI. We also report on their defect-finding capabilities and usability. The quantitative data is complemented by interviews with employees at the company the study has been conducted at. The method was successfully applied, and estimated maintenance cost and ROI for both frameworks are reported. Overall, the study supports earlier results showing that implementation time is the leading cost for introducing AGT. The findings further suggest that, while EyeAutomate tests are significantly faster to implement, Selenium tests require more of a programming background but less maintenance."}, {"id": "conf/issre/ChauvelMG19", "title": "Amplifying Integration Tests with CAMP.", "authors": ["Franck Chauvel", "Brice Morin", "Enrique Garcia-Ceja"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00036"], "tag": ["Research Session 7:\nTesting 2"], "abstract": "Modern software systems interact with multiple 3rd party dependencies such as the OS file system, libraries, databases or remote services. To verify these interactions, developers write so-called \"integration tests\" that exercise the software within a specific environment. These tests are not only difficult to write as their environment is complicated, but they are also brittle because changes outside the code (i.e., in the environment) might make them fail unexpectedly. Integration tests are thus underused whereas they could help find many more issues. We hence propose CAMP, a tool that amplifies an existing integration test by exploring variations of the given environment. The tests that CAMP generates alter the services orchestration, the software stacks, the individual components' configuration or any combination thereof. We used CAMP to amplify tests from the Sphinx and Atom open-source projects, and in both cases, we were able to spot undocumented issues related to incompatible environments."}, {"id": "conf/issre/ZouSYX19", "title": "TCD: Statically Detecting Type Confusion Errors in C++ Programs.", "authors": ["Changwei Zou", "Yulei Sui", "Hua Yan", "Jingling Xue"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00037"], "tag": ["Research Session 8:\nProgram Analysis"], "abstract": "For performance reasons, C++, albeit unsafe, is often the programming language of choice for developing software infrastructures. A serious type of security vulnerability in C++ programs is type confusion, which may lead to program crashes and control flow hijack attacks. While existing mitigation solutions almost exclusively rely on dynamic analysis techniques, which suffer from low code coverage and high overhead, static analysis has rarely been investigated. This paper presents TCD, a static type confusion detector built on top of a precise demand-driven field-, context-and flow-sensitive pointer analysis. Unlike existing pointer analyses, TCD is type-aware as it not only preserves the type information in the pointed-to objects but also handles complex language features of C++ such as multiple inheritance and placement new, making it therefore possible to reason about type casting in C++ programs. We have implemented TCD in LLVM and evaluated it using seven C++ applications (totaling 526,385 lines of C++ code) from Qt, a widely-adopted C++ toolkit for creating GUIs and cross-platform software. TCD has found five type confusion bugs, including one reported previously in prior work and four new ones, in under 7.3 hours, with a low false positive rate of 28.2%."}, {"id": "conf/issre/MorozovDSJ19", "title": "OpenErrorPro: A New Tool for Stochastic Model-Based Reliability and Resilience Analysis.", "authors": ["Andrey Morozov", "Kai Ding", "Mikael Steurer", "Klaus Janschek"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00038"], "tag": ["Research Session 8:\nProgram Analysis"], "abstract": "Increasing complexity and heterogeneity of modern safety-critical systems require advanced tools for quantitative reliability analysis. Most of the available analytical software exploits classical methods such as event trees, static and dynamic fault trees, reliability block diagrams, simple Bayesian networks, and Markov chains. First, these methods fail to adequately model complex interaction of software, hardware, physical components, dynamic feedback loops, propagation of data errors, nontrivial failure scenarios, sophisticated fault tolerance, and resilience mechanisms. Second, these methods are limited to the evaluation of the fixed set of traditional reliability metrics such as the probability of generic system failure, failure rate, MTTF, MTBF, and MTTR. More flexible models, such as the Dual-graph Error Propagation Model (DEPM) can overcome these limitations but have no available tools. This paper introduces the first open-source DEPM-based analytical software tool OpenErrorPro. The DEPM is a formal stochastic model that captures control and data flow structures and reliability-related properties of executable system components. The numerical analysis in OpenErrorPro is based on the automatic generation of Markov chain models and the utilization of modern Probabilistic Model Checking (PMC) techniques. The PMC enables the analysis of highly-customizable resilience metrics, e.g. \"the probability of system recovery after a specified system failure during the defined time interval\", in addition to the traditional reliability metrics. DEPMs can be automatically generated from Simulink/Stateflow, UML/SysML, and AADL models, as well as source code of software components using LLVM. This allows not only the automated model-based evaluation but also the analysis of systems developed using the combination of several modeling paradigms. The key purpose of the tool is to close the gap between the conventional system design models and advanced analytical methods in order to give system reliability engineers easy and automated access to the full potential of PMC techniques. Finally, OpenErrorPro enables the application of several effective optimizations against the state space explosion of underlying Markov models already in the DEPM level where the system semantics such as control and data flow structures are accessible."}, {"id": "conf/issre/GopinathZWKPK19", "title": "Symbolic Execution for Importance Analysis and Adversarial Generation in Neural Networks.", "authors": ["Divya Gopinath", "Mengshi Zhang", "Kaiyuan Wang", "Ismet Burak Kadron", "Corina S. Pasareanu", "Sarfraz Khurshid"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00039"], "tag": ["Research Session 8:\nProgram Analysis"], "abstract": "Deep Neural Networks (DNN) are increasingly used in a variety of applications, many of them with serious safety and security concerns. This paper describes DeepCheck, a new approach for validating DNNs based on core ideas from program analysis, specifically from symbolic execution. DeepCheck implements novel techniques for lightweight symbolic analysis of DNNs and applies them to address two challenging problems in DNN analysis: 1) identification of important input features and 2) leveraging those features to create adversarial inputs. Experimental results with an MNIST image classification network and a sentiment network for textual data show that DeepCheck promises to be a valuable tool for DNN analysis."}, {"id": "conf/issre/ImtiazMW19", "title": "How Do Developers Act on Static Analysis Alerts? An Empirical Study of Coverity Usage.", "authors": ["Nasif Imtiaz", "Brendan Murphy", "Laurie Williams"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00040"], "tag": ["Research Session 8:\nProgram Analysis"], "abstract": "Static analysis tools (SATs) often fall short of developer satisfaction despite their many benefits. An understanding of how developers in the real-world act on the alerts detected by SATs can help improve the utility of these tools and determine future research directions. The goal of this paper is to aid researchers and tool makers in improving the utility of static analysis tools through an empirical study of developer action on the alerts detected by Coverity, a state-of-the-art static analysis tool. In this paper, we analyze five open source projects as case studies (Linux, Firefox, Samba, Kodi, and Ovirt-engine) that have been actively using Coverity over a period of at least five years. We investigate the alert occurrences and developer triage of the alerts from the Coverity database; identify the alerts that were fixed through code changes (i.e. actionable) by mining the commit history of the projects; analyze the time an alert remain in the code base (i.e. lifespan) and the complexity of code changes (i.e. fix complexity) in fixing the alert. We find that 27.4% to 49.5% (median: 36.7%) of the alerts are actionable across projects, a rate higher than previously reported. We also find that the fixes of Coverity alerts are generally low in complexity (2 to 7 lines of code changes in the affected file, median: 4). However, developers still take from 36 to 245 days (median: 96) to fix these alerts. Finally, our data suggest that severity and fix complexity may correlate with an alert's lifespan in some of the projects."}, {"id": "conf/issre/CotroneoSINRB19", "title": "Analyzing the Context of Bug-Fixing Changes in the OpenStack Cloud Computing Platform.", "authors": ["Domenico Cotroneo", "Luigi De Simone", "Antonio Ken Iannillo", "Roberto Natella", "Stefano Rosiello", "Nematollah Bidokhti"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00041"], "tag": ["Research Session 9:\nContext, Evolution, and Reliability"], "abstract": "Many research areas in software engineering, such as mutation testing, automatic repair, fault localization, and fault injection, rely on empirical knowledge about recurring bug-fixing code changes. Previous studies in this field focus on what has been changed due to bug-fixes, such as in terms of code edit actions. However, such studies did not consider where the bug-fix change was made (i.e., the context of the change), but knowing about the context can potentially narrow the search space for many software engineering techniques (e.g., by focusing mutation only on specific parts of the software). Furthermore, most previous work on bug-fixing changes focused on C and Java projects, but there is little empirical evidence about Python software. Therefore, in this paper we perform a thorough empirical analysis of bug-fixing changes in three OpenStack projects, focusing on both the what and the where of the changes. We observed that all the recurring change patterns are not oblivious with respect to the surrounding code, but tend to occur in specific code contexts."}, {"id": "conf/issre/OizumiSOCGCO19", "title": "On the Density and Diversity of Degradation Symptoms in Refactored Classes: A Multi-case Study.", "authors": ["Willian Nalepa Oizumi", "Leonardo da Silva Sousa", "Anderson Oliveira", "Luiz Carvalho", "Alessandro Garcia", "Thelma Elita Colanzi", "Roberto Felicio Oliveira"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00042"], "tag": ["Research Session 9:\nContext, Evolution, and Reliability"], "abstract": "Root canal refactoring is a software development activity that is intended to improve dependability-related attributes such as modifiability and reusability. Despite being an activity that contributes to these attributes, deciding when applying root canal refactoring is far from trivial. In fact, finding which elements should be refactored is not a cut-and-dried task. One of the main reasons is the lack of consensus on which characteristics indicate the presence of structural degradation. Thus, we evaluated whether the density and diversity of multiple automatically detected symptoms can be used as consistent indicators of the need for root canal refactoring. To achieve our goal, we conducted a multi-case exploratory study involving 6 open source systems and 2 systems from our industry partners. For each system, we identified the classes that were changed through one or more root canal refactorings. After that, we compared refactored and non-refactored classes with respect to the density and diversity of degradation symptoms. We also investigated if the most recurrent combinations of symptoms in refactored classes can be used as strong indicators of structural degradation. Our results show that refactored classes usually present higher density and diversity of symptoms than non-refactored classes. However, root canal refactorings that are performed by developers in practice may not be enough for reducing degradation, since the vast majority had little to no impact on the density and diversity of symptoms. Finally, we observed that symptom combinations in refactored classes are similar to the combinations in non-refactored classes. Based on our findings, we elicited an initial set of requirements for automatically recommending root canal refactorings."}, {"id": "conf/issre/MobilioRMM19", "title": "FILO: FIx-LOcus Recommendation for Problems Caused by Android Framework Upgrade.", "authors": ["Marco Mobilio", "Oliviero Riganelli", "Daniela Micucci", "Leonardo Mariani"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00043"], "tag": ["Research Session 9:\nContext, Evolution, and Reliability"], "abstract": "Dealing with the evolution of operating systems is challenging for developers of mobile apps, who have to deal with frequent upgrades that often include backward incompatible changes of the underlying API framework. As a consequence of framework upgrades, apps may show misbehaviours and unexpected crashes once executed within an evolved environment. Identifying the portion of the app that must be modified to correctly execute on a newly released operating system can be challenging. Although incompatibilities are visibile at the level of the interactions between the app and its execution environment, the actual methods to be changed are often located in classes that do not directly interact with any external element. To facilitate debugging activities for problems introduced by backward incompatible upgrades of the operating system, this paper presents FILO, a technique that can recommend the method that must be changed to implement the fix from the analysis of a single failing execution. FILO can also select key symptomatic anomalous events that can help the developer understanding the reason of the failure and facilitate the implementation of the fix. Our evaluation with multiple known compatibility problems introduced by Android upgrades shows that FILO can effectively and efficiently identify the faulty methods in the apps."}, {"id": "conf/issre/XieKWZL19", "title": "HiRec: API Recommendation using Hierarchical Context.", "authors": ["Rensong Xie", "Xianglong Kong", "Lulu Wang", "Ying Zhou", "Bixin Li"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00044"], "tag": ["Research Session 9:\nContext, Evolution, and Reliability"], "abstract": "Context-aware API recommendation techniques aim to generate a ranked list of candidate APIs on an editing position during development. The basic context used in traditional API recommendation mainly focuses on the APIs from third-party libraries, limit or even ignore the usage of project-specific code. The limited usage of project-specific code may result in the lack of context information, and degrade the effectiveness of API recommendation. To address this problem, we introduce a novel type of context, i.e., hierarchical context, which can leverage the hidden information of project-specific code by analyzing the call graph. In hierarchical context, a project-specific API is presented as a sequence of low-leveled APIs from third-party libraries. We propose an approach, i.e., HiRec, which builds on the basis of hierarchical context. HiRec is evaluated on 108 projects and the results show that HiRec can obtain much more accurate results than all the other selected approaches in terms of top-5 and top-10 accuracy due to the strong ability of context representation. And HiRec performs closely to the outstanding tools in terms of top-1 accuracy. The average time of recommending execution is less than 1 seconds in most cases, which is acceptable for interaction in an IDE. Unlike current approaches, the effectiveness of HiRec is not impacted much by editing positions. And we can obtain more accurate results from HiRec with larger sizes of training data and hierarchical context."}, {"id": "conf/issre/ChowdhuryWPL19", "title": "Criteria to Systematically Evaluate (Safety) Assurance Cases.", "authors": ["Thomas Chowdhury", "Alan Wassyng", "Richard F. Paige", "Mark Lawford"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00045"], "tag": ["Research Session 10:\nProcess Quality"], "abstract": "An assurance case (AC) captures explicit reasoning associated with assuring critical properties, such as safety. A vital attribute of an AC is that it facilitates the identification of fallacies in the validity of any claim. There is considerable published research related to confidence in ACs, which primarily relate to a measure of soundness of reasoning. Evaluation of an AC is more general than measuring confidence and considers multiple aspects of the quality of an AC. Evaluation criteria thus play a significant role in making the evaluation process more systematic. This paper contributes to the identification of effective evaluation criteria for ACs, the rationale for their use, and initial tests of the criteria on existing ACs. We classify these criteria as to whether they apply to the structure of the AC, or to the content of the AC. This paper focuses on safety as the critical property to be assured, but only a very small number of the criteria are specific to safety, and can serve as placeholders for evaluation criteria specific to other critical properties. All of the other evaluation criteria are generic. This separation is useful when evaluating ACs developed using different notations, and when evaluating ACs against safety standards. We explore the rationale for these criteria as well as the way they are used by the developers of the AC and also when they are used by a third-party evaluator."}, {"id": "conf/issre/Valentim0A19", "title": "The Impact of Data Preparation on the Fairness of Software Systems.", "authors": ["In\u00eas Valentim", "Nuno Louren\u00e7o", "Nuno Antunes"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00046"], "tag": ["Research Session 10:\nProcess Quality"], "abstract": "Machine learning models are widely adopted in scenarios that directly affect people. The development of software systems based on these models raises societal and legal concerns, as their decisions may lead to the unfair treatment of individuals based on attributes like race or gender. Data preparation is key in any machine learning pipeline, but its effect on fairness is yet to be studied in detail. In this paper, we evaluate how the fairness and effectiveness of the learned models are affected by the removal of the sensitive attribute, the encoding of the categorical attributes, and instance selection methods (including cross-validators and random undersampling). We used the Adult Income and the German Credit Data datasets, which are widely studied and known to have fairness concerns. We applied each data preparation technique individually to analyse the difference in predictive performance and fairness, using statistical parity difference, disparate impact, and the normalised prejudice index. The results show that fairness is affected by transformations made to the training data, particularly in imbalanced datasets. Removing the sensitive attribute is insufficient to eliminate all the unfairness in the predictions, as expected, but it is key to achieve fairer models. Additionally, the standard random undersampling with respect to the true labels is sometimes more prejudicial than performing no random undersampling."}, {"id": "conf/issre/PradhanN19", "title": "Back to Basics - Redefining Quality Measurement for Hybrid Software Development Organizations.", "authors": ["Satya Pradhan", "Venky Nanniyur"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00047"], "tag": ["Research Session 10:\nProcess Quality"], "abstract": "As the software industry transitions from a license-based model to a subscription-based Software-as-aService (SaaS) model, many software development groups are using a hybrid development model that incorporates Agile and Waterfall methodologies in different parts of the organization. The traditional metrics used for measuring software quality in Waterfall or Agile paradigms do not apply to this new hybrid methodology. In addition, to respond to higher quality demands from customers and to gain a competitive advantage in the market, many companies are starting to prioritize quality as a strategic differentiator. As a result, quality metrics are included in the decision-making activities all the way up to the executive level, including Board of Director reviews. This paper presents key challenges associated with measuring software quality in organizations using the hybrid development model. We developed a framework called PIER (Prevention-InspectionEvaluation-Removal) to provide a comprehensive metric definition for hybrid organizations. The framework includes quality measurements, quality enforcement, and quality decision points at different organizational levels and project milestones during the software development life cycle (SDLC). The metrics framework defined in this paper is being used for all Cisco Systems products used in customer premises. Preliminary field metrics data for one of the product groups show quality improvement after implementation of the proposed measurement system."}, {"id": "conf/issre/Goseva-Popstojanova19", "title": "Benefits and Challenges of Model-Based Software Engineering: Lessons Learned Based on Qualitative and Quantitative Findings.", "authors": ["Katerina Goseva-Popstojanova", "Thomas Kyanko", "Noble Nkwocha"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00048"], "tag": ["Research Session 10:\nProcess Quality"], "abstract": "Even though Model-based Software Engineering (MBSwE) techniques and Autogenerated Code (AGC) have been increasingly used to produce complex software systems, there is only anecdotal knowledge about the state-of-the practice. Furthermore, there is a lack of empirical studies that explore the potential quality improvements due to the use of these techniques. This paper presents in-depth qualitative findings about development and Software Assurance (SWA) practices and detailed quantitative analysis of software bug reports of a NASA mission that used MBSwE and AGC. The mission's flight software is a combination of handwritten code and AGC developed by two different approaches: one based on state chart models (AGC-M) and another on specification dictionaries (AGC-D). The empirical analysis of fault proneness is based on 380 closed bug reports created by software developers. Our main findings include: (1) MBSwE and AGC provide some benefits, but also impose challenges. (2) SWA done only at a model level is not sufficient. AGC code should also be tested and the models and AGC should always be kept in-sync. AGC must not be changed manually. (3) Fixes made to address an individual bug report were spread both across multiple modules and across multiple files. On average, for each bug report 1.4 modules, that is, 3.4 files were fixed. (4) Most bug reports led to changes in more than one type of file. The majority of changes to auto-generated source code files were made in conjunction to changes in either file with state chart models or XML files derived from dictionaries. (5) For newly developed files, AGC-M and handwritten code were of similar quality, while AGC-D files were the least fault prone."}, {"id": "conf/issre/MerrerT19", "title": "TamperNN: Efficient Tampering Detection of Deployed Neural Nets.", "authors": ["Erwan Le Merrer", "Gilles Tr\u00e9dan"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00049"], "tag": ["Research Session 1:\nSecurity"], "abstract": "Neural networks are powering the deployment of embedded devices and Internet of Things. Applications range from personal assistants to critical ones such as self-driving cars. It has been shown recently that models obtained from neural nets can be trojaned; an attacker can then trigger an arbitrary model behavior facing crafted inputs. This has a critical impact on the security and reliability of those deployed devices. We introduce novel algorithms to detect the tampering with deployed models, classifiers in particular. In the remote interaction setup we consider, the proposed strategy is to identify markers of the model input space that are likely to change class if the model is attacked, allowing a user to detect a possible tampering. This setup makes our proposal compatible with a wide range of scenarios, such as embedded models, or models exposed through prediction APIs. We experiment those tampering detection algorithms on the canonical MNIST dataset, over three different types of neural nets, and facing five different attacks (trojaning, quantization, fine-tuning, compression and watermarking). We then validate over five large models (VGG16, VGG19, ResNet, MobileNet, DenseNet) with a state of the art dataset (VGGFace2), and report results demonstrating the possibility of an efficient detection of model tampering."}, {"id": "conf/issre/LemesNV19", "title": "Trustworthiness Assessment of Web Applications: Approach and Experimental Study using Input Validation Coding Practices.", "authors": ["Cristiano In\u00e1cio Lemes", "Vincent Naessens", "Marco Vieira"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00050"], "tag": ["Research Session 1:\nSecurity"], "abstract": "The popularity of web applications and their world-wide use to support business critical operations raised the interest of hackers on exploiting security vulnerabilities to perform malicious operations. Fostering trust calls for assessment techniques that provide indicators about the quality of a web application from a security perspective. This paper studies the problem of using coding practices to characterize the trustworthiness of web applications from a security perspective. The hypothesis is that applying feasible security practices results in applications having a reduced number of unknown vulnerabilities, and can therefore be considered more trustworthy. The proposed approach is instantiated for the concrete case of input validation practices, and includes a Quality Model to compute trustworthiness scores that can be used to compare different applications or different code elements in the same application. Experimental results show that the higher scores are obtained for more secure code, suggesting that it can be used in practice to characterize trustworthiness, also providing guidance to compare and/or improve the security of web applications."}, {"id": "conf/issre/ZoppiCB19", "title": "Evaluation of Anomaly Detection Algorithms Made Easy with RELOAD.", "authors": ["Tommaso Zoppi", "Andrea Ceccarelli", "Andrea Bondavalli"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00051"], "tag": ["Research Session 1:\nSecurity"], "abstract": "Anomaly detection aims at identifying patterns in data that do not conform to the expected behavior. Despite anomaly detection has been arising as one of the most powerful techniques to suspect attacks or failures, dedicated support for the experimental evaluation is actually scarce. In fact, existing frameworks are mostly intended for the broad purposes of data mining and machine learning. Intuitive tools tailored for evaluating anomaly detection algorithms for failure and attack detection with an intuitive support to sliding windows are currently missing. This paper presents RELOAD, a flexible and intuitive tool for the Rapid EvaLuation Of Anomaly Detection algorithms. RELOAD is able to automatically i) fetch data from an existing data set, ii) identify the most informative features of the data set, iii) run anomaly detection algorithms, including those based on sliding windows, iv) apply multiple strategies to features and decide on anomalies, and v) provide conclusive results following an extensive set of metrics, along with plots of algorithms scores. Finally, RELOAD includes a simple GUI to set up the experiments and examine results. After describing the structure of the tool and detailing inputs and outputs of RELOAD, we exercise RELOAD to analyze an intrusion detection dataset available on a public platform, showing its setup, metric scores and plots."}, {"id": "conf/issre/ZhangBPSG19", "title": "MPro: Combining Static and Symbolic Analysis for Scalable Testing of Smart Contract.", "authors": ["William Zhang", "Sebastian Banescu", "Leonardo Pasos", "Steven T. Stewart", "Vijay Ganesh"], "DOIs": ["https://doi.org/10.1109/ISSRE.2019.00052"], "tag": ["Research Session 1:\nSecurity"], "abstract": "Smart contracts are executable programs that enable the building of a programmable trust mechanism between multiple entities without the need of a trusted third-party. At the time of this writing, there were over 10 million smart contracts deployed on the Ethereum networks and this number continues to grow at a rapid pace. Smart contracts are often written in a Turing-complete programming language called Solidity, which is not easy to audit for subtle errors. Further, since smart contracts are immutable, errors have led to attacks resulting in losses of cryptocurrency worth 100s of millions of USD and reputational damage. Unfortunately, manual security analyses do not scale with size and number of smart contracts. Automated and scalable mechanisms are essential if smart contracts are to gain mainstream acceptance. Researchers have developed several security scanners in the past couple of years. However, many of these analyzer either do not scale well, or if they do, produce many false positives. This issue is exacerbated when bugs are triggered only after a series of interactions with the functions of the contract-under-test. A depth-n vulnerability, refers to a vulnerability that requires invoking a specific sequence of n functions to trigger. Depth-n vulnerabilities are time-consuming to detect by existing automated analyzers, because of the combinatorial explosion of sequences of functions that could be executed on smart contracts. In this paper, we present a technique to analyze depth-n vulnerabilities in an efficient and scalable way by combining symbolic execution and data dependency analysis. A significant advantage of combining symbolic with static analysis is that it scales much better than symbolic alone and does not have the problem of false positive that static analysis tools typically have. We have implemented our technique in a tool called MPro, a scalable and automated smart contract analyzer based on the existing symbolic analysis tool Mythril-Classic and the static analysis tool Slither. We analyzed 100 randomly chosen smart contracts on MPro and our evaluation shows that MPro is about n-times faster than Mythril-Classic for detecting depth-n vulnerabilities, while preserving all the detection capabilities of Mythril-Classic."}]}}